HloModule cluster_4__XlaCompiledKernel_true__XlaNumConstantArgs_723__XlaNumResourceArgs_0_.4154

%add_float_.1249 (x.1250: f32[], y.1251: f32[]) -> f32[] {
  %x.1250 = f32[] parameter(0)
  %y.1251 = f32[] parameter(1)
  ROOT %add.1252 = f32[] add(f32[] %x.1250, f32[] %y.1251)
}

%ge_F32.1525 (lhs.1526: f32[], rhs.1527: f32[]) -> pred[] {
  %lhs.1526 = f32[] parameter(0)
  %rhs.1527 = f32[] parameter(1)
  ROOT %compare.1528 = pred[] compare(f32[] %lhs.1526, f32[] %rhs.1527), direction=GE
}

%add_F32.1529 (lhs.1530: f32[], rhs.1531: f32[]) -> f32[] {
  %lhs.1530 = f32[] parameter(0)
  %rhs.1531 = f32[] parameter(1)
  ROOT %add.1532 = f32[] add(f32[] %lhs.1530, f32[] %rhs.1531)
}

%fused_computation (param_0.2: f32[1,128,1,1], param_1.2: f32[1,128,1,1], param_2.3: f32[128], param_3.3: f32[128]) -> f32[128] {
  %param_3.3 = f32[128]{0} parameter(3)
  %bitcast.227 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.3), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.3 = f32[128]{0} parameter(2)
  %negate.48 = f32[128]{0} negate(f32[128]{0} %param_2.3), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.226 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.48), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.2 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.8 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.226, f32[1,128,1,1]{3,2,1,0} %param_1.2), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.7 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.227, f32[1,128,1,1]{3,2,1,0} %multiply.8), metadata={op_type="AddN" op_name="tower0/gradients/AddN_205"}
  %param_0.2 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.7 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.7, f32[1,128,1,1]{3,2,1,0} %param_0.2), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.225 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.7), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3011 (x.3012: f32[], y.3013: f32[]) -> f32[] {
  %x.3012 = f32[] parameter(0)
  %y.3013 = f32[] parameter(1)
  ROOT %add.3014 = f32[] add(f32[] %x.3012, f32[] %y.3013)
}

%tower0_gradients_tower0_group1_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3021 (x.3022: f32[], y.3023: f32[]) -> f32[] {
  %x.3022 = f32[] parameter(0)
  %y.3023 = f32[] parameter(1)
  ROOT %add.3024 = f32[] add(f32[] %x.3022, f32[] %y.3023)
}

%fused_computation.1 (param_0.648: f32[1,128,200,304], param_1.875: f32[1,128,201,305], param_2.761: f32[1,128,200,304]) -> (f32[128], f32[128], f32[1,128,200,304]) {
  %param_2.761 = f32[1,128,200,304]{3,2,1,0} parameter(2)
  %constant_239 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.82.clone.1 = f32[1,128,200,304]{3,2,1,0} broadcast(f32[] %constant_239), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv1/Relu_grad/ReluGrad"}
  %compare.0.clone.1 = pred[1,128,200,304]{3,2,1,0} compare(f32[1,128,200,304]{3,2,1,0} %param_2.761, f32[1,128,200,304]{3,2,1,0} %broadcast.82.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv1/Relu_grad/ReluGrad"}
  %param_1.875 = f32[1,128,201,305]{3,2,1,0} parameter(1)
  %slice.0.clone.1 = f32[1,128,200,304]{3,2,1,0} slice(f32[1,128,201,305]{3,2,1,0} %param_1.875), slice={[0:1], [0:128], [1:201], [1:305]}, metadata={op_type="Slice" op_name="tower0/gradients/tower0/group1/block0/Pad_grad/Slice_1"}
  %select.0.clone.1 = f32[1,128,200,304]{3,2,1,0} select(pred[1,128,200,304]{3,2,1,0} %compare.0.clone.1, f32[1,128,200,304]{3,2,1,0} %slice.0.clone.1, f32[1,128,200,304]{3,2,1,0} %broadcast.82.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv1/Relu_grad/ReluGrad"}
  %param_0.648 = f32[1,128,200,304]{3,2,1,0} parameter(0)
  %multiply.9 = f32[1,128,200,304]{3,2,1,0} multiply(f32[1,128,200,304]{3,2,1,0} %select.0.clone.1, f32[1,128,200,304]{3,2,1,0} %param_0.648), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.13 = f32[128]{0} reduce(f32[1,128,200,304]{3,2,1,0} %multiply.9, f32[] %constant_239), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3011, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.70 = f32[128]{0} reduce(f32[1,128,200,304]{3,2,1,0} %select.0.clone.1, f32[] %constant_239), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3021, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.237 = (f32[128]{0}, f32[128]{0}, f32[1,128,200,304]{3,2,1,0}) tuple(f32[128]{0} %reduce.13, f32[128]{0} %reduce.70, f32[1,128,200,304]{3,2,1,0} %select.0.clone.1)
}

%fused_computation.2 (param_0.7: f32[1,1,256,128], param_1.9: f32[1,1,256,128]) -> f32[1,1,256,128] {
  %param_0.7 = f32[1,1,256,128]{1,0,2,3} parameter(0)
  %param_1.9 = f32[1,1,256,128]{3,2,1,0} parameter(1)
  %copy.365 = f32[1,1,256,128]{1,0,2,3} copy(f32[1,1,256,128]{3,2,1,0} %param_1.9), metadata={op_name="XLA_Args"}
  %constant_240 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.67 = f32[1,1,256,128]{1,0,2,3} broadcast(f32[] %constant_240), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_grad/Mul"}
  %multiply.10 = f32[1,1,256,128]{1,0,2,3} multiply(f32[1,1,256,128]{1,0,2,3} %copy.365, f32[1,1,256,128]{1,0,2,3} %broadcast.67), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_grad/Mul_1"}
  %add.8 = f32[1,1,256,128]{1,0,2,3} add(f32[1,1,256,128]{1,0,2,3} %param_0.7, f32[1,1,256,128]{1,0,2,3} %multiply.10), metadata={op_type="AddN" op_name="tower0/gradients/AddN_204"}
  ROOT %copy.364 = f32[1,1,256,128]{3,2,1,0} copy(f32[1,1,256,128]{1,0,2,3} %add.8), metadata={op_name="XLA_Retvals"}
}

%fused_computation.3 (param_0.10: f32[1,128,1,1], param_1.12: f32[1,128,1,1], param_2.11: f32[128], param_3.7: f32[128]) -> f32[128] {
  %param_3.7 = f32[128]{0} parameter(3)
  %bitcast.230 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.7), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.11 = f32[128]{0} parameter(2)
  %negate.49 = f32[128]{0} negate(f32[128]{0} %param_2.11), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.229 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.49), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.12 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.12 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.229, f32[1,128,1,1]{3,2,1,0} %param_1.12), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.9 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.230, f32[1,128,1,1]{3,2,1,0} %multiply.12), metadata={op_type="AddN" op_name="tower0/gradients/AddN_203"}
  %param_0.10 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.11 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.9, f32[1,128,1,1]{3,2,1,0} %param_0.10), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.228 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.11), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3038 (x.3039: f32[], y.3040: f32[]) -> f32[] {
  %x.3039 = f32[] parameter(0)
  %y.3040 = f32[] parameter(1)
  ROOT %add.3041 = f32[] add(f32[] %x.3039, f32[] %y.3040)
}

%tower0_gradients_tower0_group1_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3048 (x.3049: f32[], y.3050: f32[]) -> f32[] {
  %x.3049 = f32[] parameter(0)
  %y.3050 = f32[] parameter(1)
  ROOT %add.3051 = f32[] add(f32[] %x.3049, f32[] %y.3050)
}

%fused_computation.4 (param_0.647: f32[1,128,100,152], param_1.876: f32[1,128,100,152], param_2.762: f32[1,128,100,152]) -> (f32[128], f32[128], f32[1,128,100,152]) {
  %param_2.762 = f32[1,128,100,152]{3,2,1,0} parameter(2)
  %constant_241 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.195.clone.1 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_241), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.1.clone.1 = pred[1,128,100,152]{3,2,1,0} compare(f32[1,128,100,152]{3,2,1,0} %param_2.762, f32[1,128,100,152]{3,2,1,0} %broadcast.195.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv2/Relu_grad/ReluGrad"}
  %param_1.876 = f32[1,128,100,152]{3,2,1,0} parameter(1)
  %select.1.clone.1 = f32[1,128,100,152]{3,2,1,0} select(pred[1,128,100,152]{3,2,1,0} %compare.1.clone.1, f32[1,128,100,152]{3,2,1,0} %param_1.876, f32[1,128,100,152]{3,2,1,0} %broadcast.195.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv2/Relu_grad/ReluGrad"}
  %param_0.647 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %multiply.13 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %select.1.clone.1, f32[1,128,100,152]{3,2,1,0} %param_0.647), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.14 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %multiply.13, f32[] %constant_241), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3038, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.82 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %select.1.clone.1, f32[] %constant_241), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3048, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.236 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) tuple(f32[128]{0} %reduce.14, f32[128]{0} %reduce.82, f32[1,128,100,152]{3,2,1,0} %select.1.clone.1)
}

%fused_computation.5 (param_0.15: f32[3,3,128,128], param_1.691: f32[3,3,128,128]) -> f32[3,3,128,128] {
  %param_0.15 = f32[3,3,128,128]{1,0,2,3} parameter(0)
  %param_1.691 = f32[3,3,128,128]{3,2,1,0} parameter(1)
  %copy.453 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %param_1.691), metadata={op_name="XLA_Args"}
  %constant_312 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.162 = f32[3,3,128,128]{1,0,2,3} broadcast(f32[] %constant_312), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul"}
  %multiply.14 = f32[3,3,128,128]{1,0,2,3} multiply(f32[3,3,128,128]{1,0,2,3} %copy.453, f32[3,3,128,128]{1,0,2,3} %broadcast.162), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_1_grad/Mul_1"}
  %add.10 = f32[3,3,128,128]{1,0,2,3} add(f32[3,3,128,128]{1,0,2,3} %param_0.15, f32[3,3,128,128]{1,0,2,3} %multiply.14), metadata={op_type="AddN" op_name="tower0/gradients/AddN_202"}
  ROOT %copy.366 = f32[3,3,128,128]{3,2,1,0} copy(f32[3,3,128,128]{1,0,2,3} %add.10), metadata={op_name="XLA_Retvals"}
}

%fused_computation.8 (param_0.24: f32[1,512,1,1], param_1.797: f32[1,512,1,1], param_2.580: f32[512], param_3.302: f32[512], param_4.90: f32[1,512,1,1], param_5.79: f32[1,512,1,1], param_6.74: f32[512]) -> (f32[512], f32[512]) {
  %param_2.580 = f32[512]{0} parameter(2)
  %bitcast.234 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_2.580), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_3.302 = f32[512]{0} parameter(3)
  %negate.108 = f32[512]{0} negate(f32[512]{0} %param_3.302), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/sub_grad/Neg"}
  %bitcast.419 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.108), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/sub_grad/Neg"}
  %param_1.797 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.19 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.419, f32[1,512,1,1]{3,2,1,0} %param_1.797), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.12 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.234, f32[1,512,1,1]{3,2,1,0} %multiply.19), metadata={op_type="AddN" op_name="tower0/gradients/AddN_201"}
  %param_0.24 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.18 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.12, f32[1,512,1,1]{3,2,1,0} %param_0.24), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.233 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.18), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/Reshape_grad/Reshape"}
  %param_6.74 = f32[512]{0} parameter(6)
  %bitcast.232.clone.1 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_6.74), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_5.79 = f32[1,512,1,1]{3,2,1,0} parameter(5)
  %multiply.16.clone.1 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.419, f32[1,512,1,1]{3,2,1,0} %param_5.79), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.11.clone.1 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.232.clone.1, f32[1,512,1,1]{3,2,1,0} %multiply.16.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_199"}
  %param_4.90 = f32[1,512,1,1]{3,2,1,0} parameter(4)
  %multiply.15.clone.1 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.11.clone.1, f32[1,512,1,1]{3,2,1,0} %param_4.90), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.231.clone.1 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.15.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/Reshape_grad/Reshape"}
  ROOT %tuple.186 = (f32[512]{0}, f32[512]{0}) tuple(f32[512]{0} %bitcast.233, f32[512]{0} %bitcast.231.clone.1)
}

%tower0_gradients_tower0_group1_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3118 (x.3119: f32[], y.3120: f32[]) -> f32[] {
  %x.3119 = f32[] parameter(0)
  %y.3120 = f32[] parameter(1)
  ROOT %add.3121 = f32[] add(f32[] %x.3119, f32[] %y.3120)
}

%tower0_gradients_tower0_group1_block0_convshortcut_bn_batchnorm_add_1_grad_Sum_1-reduction.3128 (x.3129: f32[], y.3130: f32[]) -> f32[] {
  %x.3129 = f32[] parameter(0)
  %y.3130 = f32[] parameter(1)
  ROOT %add.3131 = f32[] add(f32[] %x.3129, f32[] %y.3130)
}

%tower0_gradients_tower0_group1_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3065 (x.3066: f32[], y.3067: f32[]) -> f32[] {
  %x.3066 = f32[] parameter(0)
  %y.3067 = f32[] parameter(1)
  ROOT %add.3068 = f32[] add(f32[] %x.3066, f32[] %y.3067)
}

%fused_computation.10 (param_0.646: f32[1,512,100,152], param_1.877: f32[1,512,100,152], param_2.763: f32[1,512,100,152], param_3.490: f32[1,512,100,152], param_4.241: f32[1,512,100,152]) -> (f32[512], f32[512], f32[512], f32[1,512,100,152]) {
  %param_4.241 = f32[1,512,100,152]{3,2,1,0} parameter(4)
  %constant_243 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.196.clone.1 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[] %constant_243), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %compare.2.clone.1 = pred[1,512,100,152]{3,2,1,0} compare(f32[1,512,100,152]{3,2,1,0} %param_4.241, f32[1,512,100,152]{3,2,1,0} %broadcast.196.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/output_grad/ReluGrad"}
  %param_2.763 = f32[1,512,100,152]{3,2,1,0} parameter(2)
  %param_3.490 = f32[1,512,100,152]{3,2,1,0} parameter(3)
  %add.126.clone.1 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %param_2.763, f32[1,512,100,152]{3,2,1,0} %param_3.490), metadata={op_type="AddN" op_name="tower0/gradients/AddN_195"}
  %select.2.clone.1 = f32[1,512,100,152]{3,2,1,0} select(pred[1,512,100,152]{3,2,1,0} %compare.2.clone.1, f32[1,512,100,152]{3,2,1,0} %add.126.clone.1, f32[1,512,100,152]{3,2,1,0} %broadcast.196.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/output_grad/ReluGrad"}
  %param_0.646 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %multiply.20 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %select.2.clone.1, f32[1,512,100,152]{3,2,1,0} %param_0.646), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.16 = f32[512]{0} reduce(f32[1,512,100,152]{3,2,1,0} %multiply.20, f32[] %constant_243), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3118, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.69 = f32[512]{0} reduce(f32[1,512,100,152]{3,2,1,0} %select.2.clone.1, f32[] %constant_243), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_convshortcut_bn_batchnorm_add_1_grad_Sum_1-reduction.3128, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/add_1_grad/Sum_1"}
  %param_1.877 = f32[1,512,100,152]{3,2,1,0} parameter(1)
  %multiply.17.clone.1 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %select.2.clone.1, f32[1,512,100,152]{3,2,1,0} %param_1.877), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.15.clone.1 = f32[512]{0} reduce(f32[1,512,100,152]{3,2,1,0} %multiply.17.clone.1, f32[] %constant_243), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3065, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  ROOT %tuple.235 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) tuple(f32[512]{0} %reduce.16, f32[512]{0} %reduce.69, f32[512]{0} %reduce.15.clone.1, f32[1,512,100,152]{3,2,1,0} %select.2.clone.1)
}

%fused_computation.11 (param_0.31: f32[1,1,128,512], param_1.692: f32[1,1,128,512]) -> f32[1,1,128,512] {
  %param_0.31 = f32[1,1,128,512]{1,0,2,3} parameter(0)
  %param_1.692 = f32[1,1,128,512]{3,2,1,0} parameter(1)
  %copy.368 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %param_1.692), metadata={op_name="XLA_Args"}
  %constant_313 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.163 = f32[1,1,128,512]{1,0,2,3} broadcast(f32[] %constant_313), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul"}
  %multiply.21 = f32[1,1,128,512]{1,0,2,3} multiply(f32[1,1,128,512]{1,0,2,3} %copy.368, f32[1,1,128,512]{1,0,2,3} %broadcast.163), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul_1"}
  %add.13 = f32[1,1,128,512]{1,0,2,3} add(f32[1,1,128,512]{1,0,2,3} %param_0.31, f32[1,1,128,512]{1,0,2,3} %multiply.21), metadata={op_type="AddN" op_name="tower0/gradients/AddN_198"}
  ROOT %copy.367 = f32[1,1,128,512]{3,2,1,0} copy(f32[1,1,128,512]{1,0,2,3} %add.13), metadata={op_name="XLA_Retvals"}
}

%fused_computation.12 (param_0.33: f32[1,1,256,512], param_1.36: f32[1,1,256,512]) -> f32[1,1,256,512] {
  %param_0.33 = f32[1,1,256,512]{1,0,2,3} parameter(0)
  %param_1.36 = f32[1,1,256,512]{3,2,1,0} parameter(1)
  %copy.370 = f32[1,1,256,512]{1,0,2,3} copy(f32[1,1,256,512]{3,2,1,0} %param_1.36), metadata={op_name="XLA_Args"}
  %constant_244 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.68 = f32[1,1,256,512]{1,0,2,3} broadcast(f32[] %constant_244), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_3_grad/Mul"}
  %multiply.22 = f32[1,1,256,512]{1,0,2,3} multiply(f32[1,1,256,512]{1,0,2,3} %copy.370, f32[1,1,256,512]{1,0,2,3} %broadcast.68), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_3_grad/Mul_1"}
  %add.14 = f32[1,1,256,512]{1,0,2,3} add(f32[1,1,256,512]{1,0,2,3} %param_0.33, f32[1,1,256,512]{1,0,2,3} %multiply.22), metadata={op_type="AddN" op_name="tower0/gradients/AddN_200"}
  ROOT %copy.369 = f32[1,1,256,512]{3,2,1,0} copy(f32[1,1,256,512]{1,0,2,3} %add.14), metadata={op_name="XLA_Retvals"}
}

%fused_computation.13 (param_0.36: f32[1,128,1,1], param_1.39: f32[1,128,1,1], param_2.30: f32[128], param_3.15: f32[128]) -> f32[128] {
  %param_3.15 = f32[128]{0} parameter(3)
  %bitcast.238 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.15), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.30 = f32[128]{0} parameter(2)
  %negate.51 = f32[128]{0} negate(f32[128]{0} %param_2.30), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.237 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.51), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.39 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.24 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.237, f32[1,128,1,1]{3,2,1,0} %param_1.39), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.15 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.238, f32[1,128,1,1]{3,2,1,0} %multiply.24), metadata={op_type="AddN" op_name="tower0/gradients/AddN_197"}
  %param_0.36 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.23 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.15, f32[1,128,1,1]{3,2,1,0} %param_0.36), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.236 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.23), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3145 (x.3146: f32[], y.3147: f32[]) -> f32[] {
  %x.3146 = f32[] parameter(0)
  %y.3147 = f32[] parameter(1)
  ROOT %add.3148 = f32[] add(f32[] %x.3146, f32[] %y.3147)
}

%tower0_gradients_tower0_group1_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3155 (x.3156: f32[], y.3157: f32[]) -> f32[] {
  %x.3156 = f32[] parameter(0)
  %y.3157 = f32[] parameter(1)
  ROOT %add.3158 = f32[] add(f32[] %x.3156, f32[] %y.3157)
}

%fused_computation.14 (param_0.645: f32[1,128,100,152], param_1.878: f32[1,128,100,152], param_2.764: f32[1,128,100,152]) -> (f32[128], f32[128], f32[1,128,100,152]) {
  %param_2.764 = f32[1,128,100,152]{3,2,1,0} parameter(2)
  %constant_245 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.197.clone.1 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_245), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.3.clone.1 = pred[1,128,100,152]{3,2,1,0} compare(f32[1,128,100,152]{3,2,1,0} %param_2.764, f32[1,128,100,152]{3,2,1,0} %broadcast.197.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/conv1/Relu_grad/ReluGrad"}
  %param_1.878 = f32[1,128,100,152]{3,2,1,0} parameter(1)
  %select.3.clone.1 = f32[1,128,100,152]{3,2,1,0} select(pred[1,128,100,152]{3,2,1,0} %compare.3.clone.1, f32[1,128,100,152]{3,2,1,0} %param_1.878, f32[1,128,100,152]{3,2,1,0} %broadcast.197.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/conv1/Relu_grad/ReluGrad"}
  %param_0.645 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %multiply.25 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %select.3.clone.1, f32[1,128,100,152]{3,2,1,0} %param_0.645), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.17 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %multiply.25, f32[] %constant_245), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3145, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.83 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %select.3.clone.1, f32[] %constant_245), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3155, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.234 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) tuple(f32[128]{0} %reduce.17, f32[128]{0} %reduce.83, f32[1,128,100,152]{3,2,1,0} %select.3.clone.1)
}

%fused_computation.15 (param_0.41: f32[1,1,512,128], param_1.693: f32[1,1,512,128]) -> f32[1,1,512,128] {
  %param_0.41 = f32[1,1,512,128]{1,0,2,3} parameter(0)
  %param_1.693 = f32[1,1,512,128]{3,2,1,0} parameter(1)
  %copy.372 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %param_1.693), metadata={op_name="XLA_Args"}
  %constant_314 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.164 = f32[1,1,512,128]{1,0,2,3} broadcast(f32[] %constant_314), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %multiply.26 = f32[1,1,512,128]{1,0,2,3} multiply(f32[1,1,512,128]{1,0,2,3} %copy.372, f32[1,1,512,128]{1,0,2,3} %broadcast.164), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_4_grad/Mul_1"}
  %add.16 = f32[1,1,512,128]{1,0,2,3} add(f32[1,1,512,128]{1,0,2,3} %param_0.41, f32[1,1,512,128]{1,0,2,3} %multiply.26), metadata={op_type="AddN" op_name="tower0/gradients/AddN_196"}
  ROOT %copy.371 = f32[1,1,512,128]{3,2,1,0} copy(f32[1,1,512,128]{1,0,2,3} %add.16), metadata={op_name="XLA_Retvals"}
}

%fused_computation.16 (param_0.44: f32[1,128,1,1], param_1.48: f32[1,128,1,1], param_2.37: f32[128], param_3.19: f32[128]) -> f32[128] {
  %param_3.19 = f32[128]{0} parameter(3)
  %bitcast.241 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.19), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.37 = f32[128]{0} parameter(2)
  %negate.52 = f32[128]{0} negate(f32[128]{0} %param_2.37), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.240 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.52), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.48 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.28 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.240, f32[1,128,1,1]{3,2,1,0} %param_1.48), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.17 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.241, f32[1,128,1,1]{3,2,1,0} %multiply.28), metadata={op_type="AddN" op_name="tower0/gradients/AddN_194"}
  %param_0.44 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.27 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.17, f32[1,128,1,1]{3,2,1,0} %param_0.44), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.239 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.27), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3172 (x.3173: f32[], y.3174: f32[]) -> f32[] {
  %x.3173 = f32[] parameter(0)
  %y.3174 = f32[] parameter(1)
  ROOT %add.3175 = f32[] add(f32[] %x.3173, f32[] %y.3174)
}

%tower0_gradients_tower0_group1_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3182 (x.3183: f32[], y.3184: f32[]) -> f32[] {
  %x.3183 = f32[] parameter(0)
  %y.3184 = f32[] parameter(1)
  ROOT %add.3185 = f32[] add(f32[] %x.3183, f32[] %y.3184)
}

%fused_computation.17 (param_0.644: f32[1,128,100,152], param_1.879: f32[1,128,100,152], param_2.765: f32[1,128,100,152]) -> (f32[128], f32[128], f32[1,128,100,152]) {
  %param_2.765 = f32[1,128,100,152]{3,2,1,0} parameter(2)
  %constant_246 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.198.clone.1 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_246), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.4.clone.1 = pred[1,128,100,152]{3,2,1,0} compare(f32[1,128,100,152]{3,2,1,0} %param_2.765, f32[1,128,100,152]{3,2,1,0} %broadcast.198.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/conv2/Relu_grad/ReluGrad"}
  %param_1.879 = f32[1,128,100,152]{3,2,1,0} parameter(1)
  %select.4.clone.1 = f32[1,128,100,152]{3,2,1,0} select(pred[1,128,100,152]{3,2,1,0} %compare.4.clone.1, f32[1,128,100,152]{3,2,1,0} %param_1.879, f32[1,128,100,152]{3,2,1,0} %broadcast.198.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/conv2/Relu_grad/ReluGrad"}
  %param_0.644 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %multiply.29 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %select.4.clone.1, f32[1,128,100,152]{3,2,1,0} %param_0.644), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.18 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %multiply.29, f32[] %constant_246), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3172, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.84 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %select.4.clone.1, f32[] %constant_246), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3182, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.233 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) tuple(f32[128]{0} %reduce.18, f32[128]{0} %reduce.84, f32[1,128,100,152]{3,2,1,0} %select.4.clone.1)
}

%fused_computation.18 (param_0.49: f32[3,3,128,128], param_1.694: f32[3,3,128,128]) -> f32[3,3,128,128] {
  %param_0.49 = f32[3,3,128,128]{1,0,2,3} parameter(0)
  %param_1.694 = f32[3,3,128,128]{3,2,1,0} parameter(1)
  %copy.454 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %param_1.694), metadata={op_name="XLA_Args"}
  %constant_315 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.165 = f32[3,3,128,128]{1,0,2,3} broadcast(f32[] %constant_315), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul"}
  %multiply.30 = f32[3,3,128,128]{1,0,2,3} multiply(f32[3,3,128,128]{1,0,2,3} %copy.454, f32[3,3,128,128]{1,0,2,3} %broadcast.165), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_5_grad/Mul_1"}
  %add.18 = f32[3,3,128,128]{1,0,2,3} add(f32[3,3,128,128]{1,0,2,3} %param_0.49, f32[3,3,128,128]{1,0,2,3} %multiply.30), metadata={op_type="AddN" op_name="tower0/gradients/AddN_193"}
  ROOT %copy.373 = f32[3,3,128,128]{3,2,1,0} copy(f32[3,3,128,128]{1,0,2,3} %add.18), metadata={op_name="XLA_Retvals"}
}

%fused_computation.19 (param_0.52: f32[1,512,1,1], param_1.56: f32[1,512,1,1], param_2.43: f32[512], param_3.23: f32[512]) -> f32[512] {
  %param_3.23 = f32[512]{0} parameter(3)
  %bitcast.244 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.23), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.43 = f32[512]{0} parameter(2)
  %negate.53 = f32[512]{0} negate(f32[512]{0} %param_2.43), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.243 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.53), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.56 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.32 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.243, f32[1,512,1,1]{3,2,1,0} %param_1.56), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.19 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.244, f32[1,512,1,1]{3,2,1,0} %multiply.32), metadata={op_type="AddN" op_name="tower0/gradients/AddN_192"}
  %param_0.52 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.31 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.19, f32[1,512,1,1]{3,2,1,0} %param_0.52), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.242 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.31), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3199 (x.3200: f32[], y.3201: f32[]) -> f32[] {
  %x.3200 = f32[] parameter(0)
  %y.3201 = f32[] parameter(1)
  ROOT %add.3202 = f32[] add(f32[] %x.3200, f32[] %y.3201)
}

%tower0_gradients_tower0_group1_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3209 (x.3210: f32[], y.3211: f32[]) -> f32[] {
  %x.3210 = f32[] parameter(0)
  %y.3211 = f32[] parameter(1)
  ROOT %add.3212 = f32[] add(f32[] %x.3210, f32[] %y.3211)
}

%fused_computation.20 (param_0.643: f32[1,512,100,152], param_1.880: f32[1,512,100,152], param_2.766: f32[1,512,100,152], param_3.491: f32[1,512,100,152]) -> (f32[512], f32[512], f32[1,512,100,152]) {
  %param_3.491 = f32[1,512,100,152]{3,2,1,0} parameter(3)
  %constant_247 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.199.clone.1 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[] %constant_247), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %compare.5.clone.1 = pred[1,512,100,152]{3,2,1,0} compare(f32[1,512,100,152]{3,2,1,0} %param_3.491, f32[1,512,100,152]{3,2,1,0} %broadcast.199.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/output_grad/ReluGrad"}
  %param_1.880 = f32[1,512,100,152]{3,2,1,0} parameter(1)
  %param_2.766 = f32[1,512,100,152]{3,2,1,0} parameter(2)
  %add.127.clone.1 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %param_1.880, f32[1,512,100,152]{3,2,1,0} %param_2.766), metadata={op_type="AddN" op_name="tower0/gradients/AddN_188"}
  %select.5.clone.1 = f32[1,512,100,152]{3,2,1,0} select(pred[1,512,100,152]{3,2,1,0} %compare.5.clone.1, f32[1,512,100,152]{3,2,1,0} %add.127.clone.1, f32[1,512,100,152]{3,2,1,0} %broadcast.199.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/output_grad/ReluGrad"}
  %param_0.643 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %multiply.33 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %select.5.clone.1, f32[1,512,100,152]{3,2,1,0} %param_0.643), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.19 = f32[512]{0} reduce(f32[1,512,100,152]{3,2,1,0} %multiply.33, f32[] %constant_247), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3199, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.68 = f32[512]{0} reduce(f32[1,512,100,152]{3,2,1,0} %select.5.clone.1, f32[] %constant_247), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3209, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.232 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) tuple(f32[512]{0} %reduce.19, f32[512]{0} %reduce.68, f32[1,512,100,152]{3,2,1,0} %select.5.clone.1)
}

%fused_computation.21 (param_0.57: f32[1,1,128,512], param_1.695: f32[1,1,128,512]) -> f32[1,1,128,512] {
  %param_0.57 = f32[1,1,128,512]{1,0,2,3} parameter(0)
  %param_1.695 = f32[1,1,128,512]{3,2,1,0} parameter(1)
  %copy.375 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %param_1.695), metadata={op_name="XLA_Args"}
  %constant_316 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.166 = f32[1,1,128,512]{1,0,2,3} broadcast(f32[] %constant_316), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul"}
  %multiply.34 = f32[1,1,128,512]{1,0,2,3} multiply(f32[1,1,128,512]{1,0,2,3} %copy.375, f32[1,1,128,512]{1,0,2,3} %broadcast.166), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_6_grad/Mul_1"}
  %add.20 = f32[1,1,128,512]{1,0,2,3} add(f32[1,1,128,512]{1,0,2,3} %param_0.57, f32[1,1,128,512]{1,0,2,3} %multiply.34), metadata={op_type="AddN" op_name="tower0/gradients/AddN_191"}
  ROOT %copy.374 = f32[1,1,128,512]{3,2,1,0} copy(f32[1,1,128,512]{1,0,2,3} %add.20), metadata={op_name="XLA_Retvals"}
}

%fused_computation.22 (param_0.60: f32[1,128,1,1], param_1.65: f32[1,128,1,1], param_2.50: f32[128], param_3.27: f32[128]) -> f32[128] {
  %param_3.27 = f32[128]{0} parameter(3)
  %bitcast.247 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.27), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.50 = f32[128]{0} parameter(2)
  %negate.54 = f32[128]{0} negate(f32[128]{0} %param_2.50), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.246 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.54), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.65 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.36 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.246, f32[1,128,1,1]{3,2,1,0} %param_1.65), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.21 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.247, f32[1,128,1,1]{3,2,1,0} %multiply.36), metadata={op_type="AddN" op_name="tower0/gradients/AddN_190"}
  %param_0.60 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.35 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.21, f32[1,128,1,1]{3,2,1,0} %param_0.60), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.245 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.35), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3226 (x.3227: f32[], y.3228: f32[]) -> f32[] {
  %x.3227 = f32[] parameter(0)
  %y.3228 = f32[] parameter(1)
  ROOT %add.3229 = f32[] add(f32[] %x.3227, f32[] %y.3228)
}

%tower0_gradients_tower0_group1_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3236 (x.3237: f32[], y.3238: f32[]) -> f32[] {
  %x.3237 = f32[] parameter(0)
  %y.3238 = f32[] parameter(1)
  ROOT %add.3239 = f32[] add(f32[] %x.3237, f32[] %y.3238)
}

%fused_computation.23 (param_0.642: f32[1,128,100,152], param_1.881: f32[1,128,100,152], param_2.767: f32[1,128,100,152]) -> (f32[128], f32[128], f32[1,128,100,152]) {
  %param_2.767 = f32[1,128,100,152]{3,2,1,0} parameter(2)
  %constant_248 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.200.clone.1 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_248), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.6.clone.1 = pred[1,128,100,152]{3,2,1,0} compare(f32[1,128,100,152]{3,2,1,0} %param_2.767, f32[1,128,100,152]{3,2,1,0} %broadcast.200.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/conv1/Relu_grad/ReluGrad"}
  %param_1.881 = f32[1,128,100,152]{3,2,1,0} parameter(1)
  %select.6.clone.1 = f32[1,128,100,152]{3,2,1,0} select(pred[1,128,100,152]{3,2,1,0} %compare.6.clone.1, f32[1,128,100,152]{3,2,1,0} %param_1.881, f32[1,128,100,152]{3,2,1,0} %broadcast.200.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/conv1/Relu_grad/ReluGrad"}
  %param_0.642 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %multiply.37 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %select.6.clone.1, f32[1,128,100,152]{3,2,1,0} %param_0.642), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.20 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %multiply.37, f32[] %constant_248), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3226, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.86 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %select.6.clone.1, f32[] %constant_248), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3236, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.231 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) tuple(f32[128]{0} %reduce.20, f32[128]{0} %reduce.86, f32[1,128,100,152]{3,2,1,0} %select.6.clone.1)
}

%fused_computation.24 (param_0.65: f32[1,1,512,128], param_1.696: f32[1,1,512,128]) -> f32[1,1,512,128] {
  %param_0.65 = f32[1,1,512,128]{1,0,2,3} parameter(0)
  %param_1.696 = f32[1,1,512,128]{3,2,1,0} parameter(1)
  %copy.377 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %param_1.696), metadata={op_name="XLA_Args"}
  %constant_317 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.167 = f32[1,1,512,128]{1,0,2,3} broadcast(f32[] %constant_317), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %multiply.38 = f32[1,1,512,128]{1,0,2,3} multiply(f32[1,1,512,128]{1,0,2,3} %copy.377, f32[1,1,512,128]{1,0,2,3} %broadcast.167), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_7_grad/Mul_1"}
  %add.22 = f32[1,1,512,128]{1,0,2,3} add(f32[1,1,512,128]{1,0,2,3} %param_0.65, f32[1,1,512,128]{1,0,2,3} %multiply.38), metadata={op_type="AddN" op_name="tower0/gradients/AddN_189"}
  ROOT %copy.376 = f32[1,1,512,128]{3,2,1,0} copy(f32[1,1,512,128]{1,0,2,3} %add.22), metadata={op_name="XLA_Retvals"}
}

%fused_computation.25 (param_0.68: f32[1,128,1,1], param_1.74: f32[1,128,1,1], param_2.57: f32[128], param_3.31: f32[128]) -> f32[128] {
  %param_3.31 = f32[128]{0} parameter(3)
  %bitcast.250 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.31), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.57 = f32[128]{0} parameter(2)
  %negate.55 = f32[128]{0} negate(f32[128]{0} %param_2.57), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.249 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.55), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.74 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.40 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.249, f32[1,128,1,1]{3,2,1,0} %param_1.74), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.23 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.250, f32[1,128,1,1]{3,2,1,0} %multiply.40), metadata={op_type="AddN" op_name="tower0/gradients/AddN_187"}
  %param_0.68 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.39 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.23, f32[1,128,1,1]{3,2,1,0} %param_0.68), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.248 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.39), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3253 (x.3254: f32[], y.3255: f32[]) -> f32[] {
  %x.3254 = f32[] parameter(0)
  %y.3255 = f32[] parameter(1)
  ROOT %add.3256 = f32[] add(f32[] %x.3254, f32[] %y.3255)
}

%tower0_gradients_tower0_group1_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3263 (x.3264: f32[], y.3265: f32[]) -> f32[] {
  %x.3264 = f32[] parameter(0)
  %y.3265 = f32[] parameter(1)
  ROOT %add.3266 = f32[] add(f32[] %x.3264, f32[] %y.3265)
}

%fused_computation.26 (param_0.641: f32[1,128,100,152], param_1.882: f32[1,128,100,152], param_2.768: f32[1,128,100,152]) -> (f32[128], f32[128], f32[1,128,100,152]) {
  %param_2.768 = f32[1,128,100,152]{3,2,1,0} parameter(2)
  %constant_249 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.201.clone.1 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_249), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.7.clone.1 = pred[1,128,100,152]{3,2,1,0} compare(f32[1,128,100,152]{3,2,1,0} %param_2.768, f32[1,128,100,152]{3,2,1,0} %broadcast.201.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/conv2/Relu_grad/ReluGrad"}
  %param_1.882 = f32[1,128,100,152]{3,2,1,0} parameter(1)
  %select.7.clone.1 = f32[1,128,100,152]{3,2,1,0} select(pred[1,128,100,152]{3,2,1,0} %compare.7.clone.1, f32[1,128,100,152]{3,2,1,0} %param_1.882, f32[1,128,100,152]{3,2,1,0} %broadcast.201.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/conv2/Relu_grad/ReluGrad"}
  %param_0.641 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %multiply.41 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %select.7.clone.1, f32[1,128,100,152]{3,2,1,0} %param_0.641), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.21 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %multiply.41, f32[] %constant_249), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3253, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.85 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %select.7.clone.1, f32[] %constant_249), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3263, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.230 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) tuple(f32[128]{0} %reduce.21, f32[128]{0} %reduce.85, f32[1,128,100,152]{3,2,1,0} %select.7.clone.1)
}

%fused_computation.27 (param_0.73: f32[3,3,128,128], param_1.697: f32[3,3,128,128]) -> f32[3,3,128,128] {
  %param_0.73 = f32[3,3,128,128]{1,0,2,3} parameter(0)
  %param_1.697 = f32[3,3,128,128]{3,2,1,0} parameter(1)
  %copy.455 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %param_1.697), metadata={op_name="XLA_Args"}
  %constant_318 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.168 = f32[3,3,128,128]{1,0,2,3} broadcast(f32[] %constant_318), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul"}
  %multiply.42 = f32[3,3,128,128]{1,0,2,3} multiply(f32[3,3,128,128]{1,0,2,3} %copy.455, f32[3,3,128,128]{1,0,2,3} %broadcast.168), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_8_grad/Mul_1"}
  %add.24 = f32[3,3,128,128]{1,0,2,3} add(f32[3,3,128,128]{1,0,2,3} %param_0.73, f32[3,3,128,128]{1,0,2,3} %multiply.42), metadata={op_type="AddN" op_name="tower0/gradients/AddN_186"}
  ROOT %copy.378 = f32[3,3,128,128]{3,2,1,0} copy(f32[3,3,128,128]{1,0,2,3} %add.24), metadata={op_name="XLA_Retvals"}
}

%fused_computation.28 (param_0.76: f32[1,512,1,1], param_1.82: f32[1,512,1,1], param_2.63: f32[512], param_3.35: f32[512]) -> f32[512] {
  %param_3.35 = f32[512]{0} parameter(3)
  %bitcast.253 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.35), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.63 = f32[512]{0} parameter(2)
  %negate.56 = f32[512]{0} negate(f32[512]{0} %param_2.63), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.252 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.56), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.82 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.44 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.252, f32[1,512,1,1]{3,2,1,0} %param_1.82), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.25 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.253, f32[1,512,1,1]{3,2,1,0} %multiply.44), metadata={op_type="AddN" op_name="tower0/gradients/AddN_185"}
  %param_0.76 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.43 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.25, f32[1,512,1,1]{3,2,1,0} %param_0.76), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.251 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.43), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3280 (x.3281: f32[], y.3282: f32[]) -> f32[] {
  %x.3281 = f32[] parameter(0)
  %y.3282 = f32[] parameter(1)
  ROOT %add.3283 = f32[] add(f32[] %x.3281, f32[] %y.3282)
}

%tower0_gradients_tower0_group1_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3290 (x.3291: f32[], y.3292: f32[]) -> f32[] {
  %x.3291 = f32[] parameter(0)
  %y.3292 = f32[] parameter(1)
  ROOT %add.3293 = f32[] add(f32[] %x.3291, f32[] %y.3292)
}

%fused_computation.29 (param_0.640: f32[1,512,100,152], param_1.883: f32[1,512,100,152], param_2.769: f32[1,512,100,152], param_3.492: f32[1,512,100,152]) -> (f32[512], f32[512], f32[1,512,100,152]) {
  %param_3.492 = f32[1,512,100,152]{3,2,1,0} parameter(3)
  %constant_250 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.202.clone.1 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[] %constant_250), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %compare.8.clone.1 = pred[1,512,100,152]{3,2,1,0} compare(f32[1,512,100,152]{3,2,1,0} %param_3.492, f32[1,512,100,152]{3,2,1,0} %broadcast.202.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/output_grad/ReluGrad"}
  %param_1.883 = f32[1,512,100,152]{3,2,1,0} parameter(1)
  %param_2.769 = f32[1,512,100,152]{3,2,1,0} parameter(2)
  %add.128.clone.1 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %param_1.883, f32[1,512,100,152]{3,2,1,0} %param_2.769), metadata={op_type="AddN" op_name="tower0/gradients/AddN_181"}
  %select.8.clone.1 = f32[1,512,100,152]{3,2,1,0} select(pred[1,512,100,152]{3,2,1,0} %compare.8.clone.1, f32[1,512,100,152]{3,2,1,0} %add.128.clone.1, f32[1,512,100,152]{3,2,1,0} %broadcast.202.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/output_grad/ReluGrad"}
  %param_0.640 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %multiply.45 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %select.8.clone.1, f32[1,512,100,152]{3,2,1,0} %param_0.640), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.22 = f32[512]{0} reduce(f32[1,512,100,152]{3,2,1,0} %multiply.45, f32[] %constant_250), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3280, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.72 = f32[512]{0} reduce(f32[1,512,100,152]{3,2,1,0} %select.8.clone.1, f32[] %constant_250), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3290, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.229 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) tuple(f32[512]{0} %reduce.22, f32[512]{0} %reduce.72, f32[1,512,100,152]{3,2,1,0} %select.8.clone.1)
}

%fused_computation.30 (param_0.81: f32[1,1,128,512], param_1.698: f32[1,1,128,512]) -> f32[1,1,128,512] {
  %param_0.81 = f32[1,1,128,512]{1,0,2,3} parameter(0)
  %param_1.698 = f32[1,1,128,512]{3,2,1,0} parameter(1)
  %copy.380 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %param_1.698), metadata={op_name="XLA_Args"}
  %constant_319 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.169 = f32[1,1,128,512]{1,0,2,3} broadcast(f32[] %constant_319), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul"}
  %multiply.46 = f32[1,1,128,512]{1,0,2,3} multiply(f32[1,1,128,512]{1,0,2,3} %copy.380, f32[1,1,128,512]{1,0,2,3} %broadcast.169), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_9_grad/Mul_1"}
  %add.26 = f32[1,1,128,512]{1,0,2,3} add(f32[1,1,128,512]{1,0,2,3} %param_0.81, f32[1,1,128,512]{1,0,2,3} %multiply.46), metadata={op_type="AddN" op_name="tower0/gradients/AddN_184"}
  ROOT %copy.379 = f32[1,1,128,512]{3,2,1,0} copy(f32[1,1,128,512]{1,0,2,3} %add.26), metadata={op_name="XLA_Retvals"}
}

%fused_computation.31 (param_0.84: f32[1,128,1,1], param_1.91: f32[1,128,1,1], param_2.70: f32[128], param_3.39: f32[128]) -> f32[128] {
  %param_3.39 = f32[128]{0} parameter(3)
  %bitcast.256 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.39), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.70 = f32[128]{0} parameter(2)
  %negate.57 = f32[128]{0} negate(f32[128]{0} %param_2.70), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.255 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.57), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.91 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.48 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.255, f32[1,128,1,1]{3,2,1,0} %param_1.91), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.27 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.256, f32[1,128,1,1]{3,2,1,0} %multiply.48), metadata={op_type="AddN" op_name="tower0/gradients/AddN_183"}
  %param_0.84 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.47 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.27, f32[1,128,1,1]{3,2,1,0} %param_0.84), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.254 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.47), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block3_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3307 (x.3308: f32[], y.3309: f32[]) -> f32[] {
  %x.3308 = f32[] parameter(0)
  %y.3309 = f32[] parameter(1)
  ROOT %add.3310 = f32[] add(f32[] %x.3308, f32[] %y.3309)
}

%tower0_gradients_tower0_group1_block3_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3317 (x.3318: f32[], y.3319: f32[]) -> f32[] {
  %x.3318 = f32[] parameter(0)
  %y.3319 = f32[] parameter(1)
  ROOT %add.3320 = f32[] add(f32[] %x.3318, f32[] %y.3319)
}

%fused_computation.32 (param_0.639: f32[1,128,100,152], param_1.884: f32[1,128,100,152], param_2.770: f32[1,128,100,152]) -> (f32[128], f32[128], f32[1,128,100,152]) {
  %param_2.770 = f32[1,128,100,152]{3,2,1,0} parameter(2)
  %constant_251 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.203.clone.1 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_251), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.9.clone.1 = pred[1,128,100,152]{3,2,1,0} compare(f32[1,128,100,152]{3,2,1,0} %param_2.770, f32[1,128,100,152]{3,2,1,0} %broadcast.203.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv1/Relu_grad/ReluGrad"}
  %param_1.884 = f32[1,128,100,152]{3,2,1,0} parameter(1)
  %select.9.clone.1 = f32[1,128,100,152]{3,2,1,0} select(pred[1,128,100,152]{3,2,1,0} %compare.9.clone.1, f32[1,128,100,152]{3,2,1,0} %param_1.884, f32[1,128,100,152]{3,2,1,0} %broadcast.203.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv1/Relu_grad/ReluGrad"}
  %param_0.639 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %multiply.49 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %select.9.clone.1, f32[1,128,100,152]{3,2,1,0} %param_0.639), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.23 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %multiply.49, f32[] %constant_251), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3307, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.87 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %select.9.clone.1, f32[] %constant_251), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3317, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.228 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) tuple(f32[128]{0} %reduce.23, f32[128]{0} %reduce.87, f32[1,128,100,152]{3,2,1,0} %select.9.clone.1)
}

%fused_computation.33 (param_0.89: f32[1,1,512,128], param_1.699: f32[1,1,512,128]) -> f32[1,1,512,128] {
  %param_0.89 = f32[1,1,512,128]{1,0,2,3} parameter(0)
  %param_1.699 = f32[1,1,512,128]{3,2,1,0} parameter(1)
  %copy.382 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %param_1.699), metadata={op_name="XLA_Args"}
  %constant_320 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.170 = f32[1,1,512,128]{1,0,2,3} broadcast(f32[] %constant_320), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %multiply.50 = f32[1,1,512,128]{1,0,2,3} multiply(f32[1,1,512,128]{1,0,2,3} %copy.382, f32[1,1,512,128]{1,0,2,3} %broadcast.170), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul_1"}
  %add.28 = f32[1,1,512,128]{1,0,2,3} add(f32[1,1,512,128]{1,0,2,3} %param_0.89, f32[1,1,512,128]{1,0,2,3} %multiply.50), metadata={op_type="AddN" op_name="tower0/gradients/AddN_182"}
  ROOT %copy.381 = f32[1,1,512,128]{3,2,1,0} copy(f32[1,1,512,128]{1,0,2,3} %add.28), metadata={op_name="XLA_Retvals"}
}

%fused_computation.34 (param_0.92: f32[1,128,1,1], param_1.100: f32[1,128,1,1], param_2.77: f32[128], param_3.43: f32[128]) -> f32[128] {
  %param_3.43 = f32[128]{0} parameter(3)
  %bitcast.259 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.43), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.77 = f32[128]{0} parameter(2)
  %negate.58 = f32[128]{0} negate(f32[128]{0} %param_2.77), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.258 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.58), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.100 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.52 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.258, f32[1,128,1,1]{3,2,1,0} %param_1.100), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.29 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.259, f32[1,128,1,1]{3,2,1,0} %multiply.52), metadata={op_type="AddN" op_name="tower0/gradients/AddN_180"}
  %param_0.92 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.51 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.29, f32[1,128,1,1]{3,2,1,0} %param_0.92), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.257 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.51), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block3_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3334 (x.3335: f32[], y.3336: f32[]) -> f32[] {
  %x.3335 = f32[] parameter(0)
  %y.3336 = f32[] parameter(1)
  ROOT %add.3337 = f32[] add(f32[] %x.3335, f32[] %y.3336)
}

%tower0_gradients_tower0_group1_block3_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3344 (x.3345: f32[], y.3346: f32[]) -> f32[] {
  %x.3345 = f32[] parameter(0)
  %y.3346 = f32[] parameter(1)
  ROOT %add.3347 = f32[] add(f32[] %x.3345, f32[] %y.3346)
}

%fused_computation.35 (param_0.638: f32[1,128,100,152], param_1.885: f32[1,128,100,152], param_2.771: f32[1,128,100,152]) -> (f32[128], f32[128], f32[1,128,100,152]) {
  %param_2.771 = f32[1,128,100,152]{3,2,1,0} parameter(2)
  %constant_252 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.204.clone.1 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_252), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.10.clone.1 = pred[1,128,100,152]{3,2,1,0} compare(f32[1,128,100,152]{3,2,1,0} %param_2.771, f32[1,128,100,152]{3,2,1,0} %broadcast.204.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %param_1.885 = f32[1,128,100,152]{3,2,1,0} parameter(1)
  %select.10.clone.1 = f32[1,128,100,152]{3,2,1,0} select(pred[1,128,100,152]{3,2,1,0} %compare.10.clone.1, f32[1,128,100,152]{3,2,1,0} %param_1.885, f32[1,128,100,152]{3,2,1,0} %broadcast.204.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %param_0.638 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %multiply.53 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %select.10.clone.1, f32[1,128,100,152]{3,2,1,0} %param_0.638), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.24 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %multiply.53, f32[] %constant_252), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3334, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.88 = f32[128]{0} reduce(f32[1,128,100,152]{3,2,1,0} %select.10.clone.1, f32[] %constant_252), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3344, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.227 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) tuple(f32[128]{0} %reduce.24, f32[128]{0} %reduce.88, f32[1,128,100,152]{3,2,1,0} %select.10.clone.1)
}

%fused_computation.36 (param_0.97: f32[3,3,128,128], param_1.700: f32[3,3,128,128]) -> f32[3,3,128,128] {
  %param_0.97 = f32[3,3,128,128]{1,0,2,3} parameter(0)
  %param_1.700 = f32[3,3,128,128]{3,2,1,0} parameter(1)
  %copy.456 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %param_1.700), metadata={op_name="XLA_Args"}
  %constant_321 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.171 = f32[3,3,128,128]{1,0,2,3} broadcast(f32[] %constant_321), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul"}
  %multiply.54 = f32[3,3,128,128]{1,0,2,3} multiply(f32[3,3,128,128]{1,0,2,3} %copy.456, f32[3,3,128,128]{1,0,2,3} %broadcast.171), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul_1"}
  %add.30 = f32[3,3,128,128]{1,0,2,3} add(f32[3,3,128,128]{1,0,2,3} %param_0.97, f32[3,3,128,128]{1,0,2,3} %multiply.54), metadata={op_type="AddN" op_name="tower0/gradients/AddN_179"}
  ROOT %copy.383 = f32[3,3,128,128]{3,2,1,0} copy(f32[3,3,128,128]{1,0,2,3} %add.30), metadata={op_name="XLA_Retvals"}
}

%fused_computation.37 (param_0.100: f32[1,512,1,1], param_1.108: f32[1,512,1,1], param_2.83: f32[512], param_3.47: f32[512]) -> f32[512] {
  %param_3.47 = f32[512]{0} parameter(3)
  %bitcast.262 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.47), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.83 = f32[512]{0} parameter(2)
  %negate.59 = f32[512]{0} negate(f32[512]{0} %param_2.83), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.261 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.59), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.108 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.56 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.261, f32[1,512,1,1]{3,2,1,0} %param_1.108), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.31 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.262, f32[1,512,1,1]{3,2,1,0} %multiply.56), metadata={op_type="AddN" op_name="tower0/gradients/AddN_178"}
  %param_0.100 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.55 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.31, f32[1,512,1,1]{3,2,1,0} %param_0.100), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.260 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.55), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block3_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3361 (x.3362: f32[], y.3363: f32[]) -> f32[] {
  %x.3362 = f32[] parameter(0)
  %y.3363 = f32[] parameter(1)
  ROOT %add.3364 = f32[] add(f32[] %x.3362, f32[] %y.3363)
}

%tower0_gradients_tower0_group1_block3_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3371 (x.3372: f32[], y.3373: f32[]) -> f32[] {
  %x.3372 = f32[] parameter(0)
  %y.3373 = f32[] parameter(1)
  ROOT %add.3374 = f32[] add(f32[] %x.3372, f32[] %y.3373)
}

%fused_computation.38 (param_0.637: f32[1,512,100,152], param_1.886: f32[1,512,100,152], param_2.772: f32[1,512,100,152], param_3.493: f32[1,512,100,152], param_4.242: f32[1,512,100,152]) -> (f32[512], f32[512], f32[1,512,100,152]) {
  %param_4.242 = f32[1,512,100,152]{3,2,1,0} parameter(4)
  %constant_253 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.205.clone.1 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[] %constant_253), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %compare.11.clone.1 = pred[1,512,100,152]{3,2,1,0} compare(f32[1,512,100,152]{3,2,1,0} %param_4.242, f32[1,512,100,152]{3,2,1,0} %broadcast.205.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %param_2.772 = f32[1,512,100,152]{3,2,1,0} parameter(2)
  %param_3.493 = f32[1,512,100,152]{3,2,1,0} parameter(3)
  %add.130.clone.1 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %param_2.772, f32[1,512,100,152]{3,2,1,0} %param_3.493), metadata={op_type="AddN" op_name="tower0/gradients/AddN_174"}
  %param_1.886 = f32[1,512,100,152]{3,2,1,0} parameter(1)
  %add.129.clone.1 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %add.130.clone.1, f32[1,512,100,152]{3,2,1,0} %param_1.886), metadata={op_type="AddN" op_name="tower0/gradients/AddN_174"}
  %select.11.clone.1 = f32[1,512,100,152]{3,2,1,0} select(pred[1,512,100,152]{3,2,1,0} %compare.11.clone.1, f32[1,512,100,152]{3,2,1,0} %add.129.clone.1, f32[1,512,100,152]{3,2,1,0} %broadcast.205.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %param_0.637 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %multiply.57 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %select.11.clone.1, f32[1,512,100,152]{3,2,1,0} %param_0.637), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.25 = f32[512]{0} reduce(f32[1,512,100,152]{3,2,1,0} %multiply.57, f32[] %constant_253), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3361, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.71 = f32[512]{0} reduce(f32[1,512,100,152]{3,2,1,0} %select.11.clone.1, f32[] %constant_253), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3371, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.226 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) tuple(f32[512]{0} %reduce.25, f32[512]{0} %reduce.71, f32[1,512,100,152]{3,2,1,0} %select.11.clone.1)
}

%fused_computation.39 (param_0.105: f32[1,1,128,512], param_1.701: f32[1,1,128,512]) -> f32[1,1,128,512] {
  %param_0.105 = f32[1,1,128,512]{1,0,2,3} parameter(0)
  %param_1.701 = f32[1,1,128,512]{3,2,1,0} parameter(1)
  %copy.385 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %param_1.701), metadata={op_name="XLA_Args"}
  %constant_322 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.172 = f32[1,1,128,512]{1,0,2,3} broadcast(f32[] %constant_322), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul"}
  %multiply.58 = f32[1,1,128,512]{1,0,2,3} multiply(f32[1,1,128,512]{1,0,2,3} %copy.385, f32[1,1,128,512]{1,0,2,3} %broadcast.172), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_12_grad/Mul_1"}
  %add.32 = f32[1,1,128,512]{1,0,2,3} add(f32[1,1,128,512]{1,0,2,3} %param_0.105, f32[1,1,128,512]{1,0,2,3} %multiply.58), metadata={op_type="AddN" op_name="tower0/gradients/AddN_177"}
  ROOT %copy.384 = f32[1,1,128,512]{3,2,1,0} copy(f32[1,1,128,512]{1,0,2,3} %add.32), metadata={op_name="XLA_Retvals"}
}

%fused_computation.40 (param_0.108: f32[1,256,1,1], param_1.117: f32[1,256,1,1], param_2.90: f32[256], param_3.51: f32[256]) -> f32[256] {
  %param_3.51 = f32[256]{0} parameter(3)
  %bitcast.265 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.51), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.90 = f32[256]{0} parameter(2)
  %negate.60 = f32[256]{0} negate(f32[256]{0} %param_2.90), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.264 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.60), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.117 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.60 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.264, f32[1,256,1,1]{3,2,1,0} %param_1.117), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.33 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.265, f32[1,256,1,1]{3,2,1,0} %multiply.60), metadata={op_type="AddN" op_name="tower0/gradients/AddN_176"}
  %param_0.108 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.59 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.33, f32[1,256,1,1]{3,2,1,0} %param_0.108), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.263 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.59), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.2525 (x.2526: f32[], y.2527: f32[]) -> f32[] {
  %x.2526 = f32[] parameter(0)
  %y.2527 = f32[] parameter(1)
  ROOT %add.2528 = f32[] add(f32[] %x.2526, f32[] %y.2527)
}

%tower0_gradients_tower0_group2_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.2535 (x.2536: f32[], y.2537: f32[]) -> f32[] {
  %x.2536 = f32[] parameter(0)
  %y.2537 = f32[] parameter(1)
  ROOT %add.2538 = f32[] add(f32[] %x.2536, f32[] %y.2537)
}

%fused_computation.41 (param_0.636: f32[1,256,100,152], param_1.887: f32[1,256,101,153], param_2.773: f32[1,256,100,152]) -> (f32[256], f32[256], f32[1,256,100,152]) {
  %param_2.773 = f32[1,256,100,152]{3,2,1,0} parameter(2)
  %constant_254 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.206.clone.1 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[] %constant_254), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/Relu_grad/ReluGrad"}
  %compare.12.clone.1 = pred[1,256,100,152]{3,2,1,0} compare(f32[1,256,100,152]{3,2,1,0} %param_2.773, f32[1,256,100,152]{3,2,1,0} %broadcast.206.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/conv1/Relu_grad/ReluGrad"}
  %param_1.887 = f32[1,256,101,153]{3,2,1,0} parameter(1)
  %slice.1.clone.1 = f32[1,256,100,152]{3,2,1,0} slice(f32[1,256,101,153]{3,2,1,0} %param_1.887), slice={[0:1], [0:256], [1:101], [1:153]}, metadata={op_type="Slice" op_name="tower0/gradients/tower0/group2/block0/Pad_grad/Slice_1"}
  %select.12.clone.1 = f32[1,256,100,152]{3,2,1,0} select(pred[1,256,100,152]{3,2,1,0} %compare.12.clone.1, f32[1,256,100,152]{3,2,1,0} %slice.1.clone.1, f32[1,256,100,152]{3,2,1,0} %broadcast.206.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/conv1/Relu_grad/ReluGrad"}
  %param_0.636 = f32[1,256,100,152]{3,2,1,0} parameter(0)
  %multiply.61 = f32[1,256,100,152]{3,2,1,0} multiply(f32[1,256,100,152]{3,2,1,0} %select.12.clone.1, f32[1,256,100,152]{3,2,1,0} %param_0.636), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.26 = f32[256]{0} reduce(f32[1,256,100,152]{3,2,1,0} %multiply.61, f32[] %constant_254), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.2525, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.77 = f32[256]{0} reduce(f32[1,256,100,152]{3,2,1,0} %select.12.clone.1, f32[] %constant_254), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.2535, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.225 = (f32[256]{0}, f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) tuple(f32[256]{0} %reduce.26, f32[256]{0} %reduce.77, f32[1,256,100,152]{3,2,1,0} %select.12.clone.1)
}

%fused_computation.42 (param_0.113: f32[1,1,512,256], param_1.702: f32[1,1,512,256]) -> f32[1,1,512,256] {
  %param_0.113 = f32[1,1,512,256]{1,0,2,3} parameter(0)
  %param_1.702 = f32[1,1,512,256]{3,2,1,0} parameter(1)
  %copy.387 = f32[1,1,512,256]{1,0,2,3} copy(f32[1,1,512,256]{3,2,1,0} %param_1.702), metadata={op_name="XLA_Args"}
  %constant_323 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.173 = f32[1,1,512,256]{1,0,2,3} broadcast(f32[] %constant_323), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_13_grad/Mul"}
  %multiply.62 = f32[1,1,512,256]{1,0,2,3} multiply(f32[1,1,512,256]{1,0,2,3} %copy.387, f32[1,1,512,256]{1,0,2,3} %broadcast.173), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_13_grad/Mul_1"}
  %add.34 = f32[1,1,512,256]{1,0,2,3} add(f32[1,1,512,256]{1,0,2,3} %param_0.113, f32[1,1,512,256]{1,0,2,3} %multiply.62), metadata={op_type="AddN" op_name="tower0/gradients/AddN_175"}
  ROOT %copy.386 = f32[1,1,512,256]{3,2,1,0} copy(f32[1,1,512,256]{1,0,2,3} %add.34), metadata={op_name="XLA_Retvals"}
}

%fused_computation.43 (param_0.116: f32[1,256,1,1], param_1.126: f32[1,256,1,1], param_2.97: f32[256], param_3.55: f32[256]) -> f32[256] {
  %param_3.55 = f32[256]{0} parameter(3)
  %bitcast.268 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.55), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.97 = f32[256]{0} parameter(2)
  %negate.61 = f32[256]{0} negate(f32[256]{0} %param_2.97), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.267 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.61), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.126 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.64 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.267, f32[1,256,1,1]{3,2,1,0} %param_1.126), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.35 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.268, f32[1,256,1,1]{3,2,1,0} %multiply.64), metadata={op_type="AddN" op_name="tower0/gradients/AddN_173"}
  %param_0.116 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.63 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.35, f32[1,256,1,1]{3,2,1,0} %param_0.116), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.266 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.63), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.2552 (x.2553: f32[], y.2554: f32[]) -> f32[] {
  %x.2553 = f32[] parameter(0)
  %y.2554 = f32[] parameter(1)
  ROOT %add.2555 = f32[] add(f32[] %x.2553, f32[] %y.2554)
}

%tower0_gradients_tower0_group2_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.2562 (x.2563: f32[], y.2564: f32[]) -> f32[] {
  %x.2563 = f32[] parameter(0)
  %y.2564 = f32[] parameter(1)
  ROOT %add.2565 = f32[] add(f32[] %x.2563, f32[] %y.2564)
}

%fused_computation.44 (param_0.635: f32[1,256,50,76], param_1.888: f32[1,256,50,76], param_2.774: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.774 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_255 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.207.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_255), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.13.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.774, f32[1,256,50,76]{3,2,1,0} %broadcast.207.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/conv2/Relu_grad/ReluGrad"}
  %param_1.888 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.13.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.13.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.888, f32[1,256,50,76]{3,2,1,0} %broadcast.207.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/conv2/Relu_grad/ReluGrad"}
  %param_0.635 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.65 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.13.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.635), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.27 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.65, f32[] %constant_255), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.2552, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.95 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.13.clone.1, f32[] %constant_255), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.2562, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.224 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.27, f32[256]{0} %reduce.95, f32[1,256,50,76]{3,2,1,0} %select.13.clone.1)
}

%fused_computation.48 (param_0.130: f32[1,1024,1,1], param_1.795: f32[1,1024,1,1], param_2.578: f32[1024], param_3.298: f32[1024], param_4.87: f32[1,1024,1,1], param_5.76: f32[1,1024,1,1], param_6.70: f32[1024]) -> (f32[1024], f32[1024]) {
  %param_2.578 = f32[1024]{0} parameter(2)
  %bitcast.272 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_2.578), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_3.298 = f32[1024]{0} parameter(3)
  %negate.104 = f32[1024]{0} negate(f32[1024]{0} %param_3.298), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/sub_grad/Neg"}
  %bitcast.415 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.104), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/sub_grad/Neg"}
  %param_1.795 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.71 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.415, f32[1,1024,1,1]{3,2,1,0} %param_1.795), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.38 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.272, f32[1,1024,1,1]{3,2,1,0} %multiply.71), metadata={op_type="AddN" op_name="tower0/gradients/AddN_171"}
  %param_0.130 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.70 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.38, f32[1,1024,1,1]{3,2,1,0} %param_0.130), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.271 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.70), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/Reshape_grad/Reshape"}
  %param_6.70 = f32[1024]{0} parameter(6)
  %bitcast.270.clone.1 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_6.70), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_5.76 = f32[1,1024,1,1]{3,2,1,0} parameter(5)
  %multiply.68.clone.1 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.415, f32[1,1024,1,1]{3,2,1,0} %param_5.76), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.37.clone.1 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.270.clone.1, f32[1,1024,1,1]{3,2,1,0} %multiply.68.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_169"}
  %param_4.87 = f32[1,1024,1,1]{3,2,1,0} parameter(4)
  %multiply.67.clone.1 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.37.clone.1, f32[1,1024,1,1]{3,2,1,0} %param_4.87), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.269.clone.1 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.67.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/Reshape_grad/Reshape"}
  ROOT %tuple.185 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %bitcast.271, f32[1024]{0} %bitcast.269.clone.1)
}

%tower0_gradients_tower0_group2_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3388 (x.3389: f32[], y.3390: f32[]) -> f32[] {
  %x.3389 = f32[] parameter(0)
  %y.3390 = f32[] parameter(1)
  ROOT %add.3391 = f32[] add(f32[] %x.3389, f32[] %y.3390)
}

%tower0_gradients_tower0_group2_block0_convshortcut_bn_batchnorm_add_1_grad_Sum_1-reduction.3398 (x.3399: f32[], y.3400: f32[]) -> f32[] {
  %x.3399 = f32[] parameter(0)
  %y.3400 = f32[] parameter(1)
  ROOT %add.3401 = f32[] add(f32[] %x.3399, f32[] %y.3400)
}

%tower0_gradients_tower0_group2_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.2579 (x.2580: f32[], y.2581: f32[]) -> f32[] {
  %x.2580 = f32[] parameter(0)
  %y.2581 = f32[] parameter(1)
  ROOT %add.2582 = f32[] add(f32[] %x.2580, f32[] %y.2581)
}

%fused_computation.50 (param_0.634: f32[1,1024,50,76], param_1.889: f32[1,1024,50,76], param_2.775: f32[1,1024,50,76], param_3.494: f32[1,1024,50,76], param_4.243: f32[1,1024,50,76]) -> (f32[1024], f32[1024], f32[1024], f32[1,1024,50,76]) {
  %param_4.243 = f32[1,1024,50,76]{3,2,1,0} parameter(4)
  %constant_257 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.208.clone.1 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_257), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.14.clone.1 = pred[1,1024,50,76]{3,2,1,0} compare(f32[1,1024,50,76]{3,2,1,0} %param_4.243, f32[1,1024,50,76]{3,2,1,0} %broadcast.208.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/output_grad/ReluGrad"}
  %param_2.775 = f32[1,1024,50,76]{3,2,1,0} parameter(2)
  %param_3.494 = f32[1,1024,50,76]{3,2,1,0} parameter(3)
  %add.131.clone.1 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_2.775, f32[1,1024,50,76]{3,2,1,0} %param_3.494), metadata={op_type="AddN" op_name="tower0/gradients/AddN_165"}
  %select.14.clone.1 = f32[1,1024,50,76]{3,2,1,0} select(pred[1,1024,50,76]{3,2,1,0} %compare.14.clone.1, f32[1,1024,50,76]{3,2,1,0} %add.131.clone.1, f32[1,1024,50,76]{3,2,1,0} %broadcast.208.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/output_grad/ReluGrad"}
  %param_0.634 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %multiply.72 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %select.14.clone.1, f32[1,1024,50,76]{3,2,1,0} %param_0.634), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.29 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %multiply.72, f32[] %constant_257), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3388, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.79 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %select.14.clone.1, f32[] %constant_257), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_convshortcut_bn_batchnorm_add_1_grad_Sum_1-reduction.3398, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/add_1_grad/Sum_1"}
  %param_1.889 = f32[1,1024,50,76]{3,2,1,0} parameter(1)
  %multiply.69.clone.1 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %select.14.clone.1, f32[1,1024,50,76]{3,2,1,0} %param_1.889), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.28.clone.1 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %multiply.69.clone.1, f32[] %constant_257), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.2579, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  ROOT %tuple.223 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) tuple(f32[1024]{0} %reduce.29, f32[1024]{0} %reduce.79, f32[1024]{0} %reduce.28.clone.1, f32[1,1024,50,76]{3,2,1,0} %select.14.clone.1)
}

%fused_computation.51 (param_0.137: f32[1,1,256,1024], param_1.704: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.137 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.704 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.390 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.704), metadata={op_name="XLA_Args"}
  %constant_324 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.174 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_324), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.73 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.390, f32[1,1,256,1024]{1,0,2,3} %broadcast.174), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_15_grad/Mul_1"}
  %add.39 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.137, f32[1,1,256,1024]{1,0,2,3} %multiply.73), metadata={op_type="AddN" op_name="tower0/gradients/AddN_168"}
  ROOT %copy.389 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.39), metadata={op_name="XLA_Retvals"}
}

%fused_computation.52 (param_0.139: f32[1,1,512,1024], param_1.705: f32[1,1,512,1024]) -> f32[1,1,512,1024] {
  %param_0.139 = f32[1,1,512,1024]{1,0,2,3} parameter(0)
  %param_1.705 = f32[1,1,512,1024]{3,2,1,0} parameter(1)
  %copy.459 = f32[1,1,512,1024]{1,0,2,3} copy(f32[1,1,512,1024]{3,2,1,0} %param_1.705), metadata={op_name="XLA_Args"}
  %constant_258 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.69 = f32[1,1,512,1024]{1,0,2,3} broadcast(f32[] %constant_258), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_16_grad/Mul"}
  %multiply.74 = f32[1,1,512,1024]{1,0,2,3} multiply(f32[1,1,512,1024]{1,0,2,3} %copy.459, f32[1,1,512,1024]{1,0,2,3} %broadcast.69), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_16_grad/Mul_1"}
  %add.40 = f32[1,1,512,1024]{1,0,2,3} add(f32[1,1,512,1024]{1,0,2,3} %param_0.139, f32[1,1,512,1024]{1,0,2,3} %multiply.74), metadata={op_type="AddN" op_name="tower0/gradients/AddN_170"}
  ROOT %copy.391 = f32[1,1,512,1024]{3,2,1,0} copy(f32[1,1,512,1024]{1,0,2,3} %add.40), metadata={op_name="XLA_Retvals"}
}

%fused_computation.53 (param_0.142: f32[1,256,1,1], param_1.151: f32[1,256,1,1], param_2.115: f32[256], param_3.63: f32[256]) -> f32[256] {
  %param_3.63 = f32[256]{0} parameter(3)
  %bitcast.276 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.63), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.115 = f32[256]{0} parameter(2)
  %negate.63 = f32[256]{0} negate(f32[256]{0} %param_2.115), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.275 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.63), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.151 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.76 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.275, f32[1,256,1,1]{3,2,1,0} %param_1.151), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.41 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.276, f32[1,256,1,1]{3,2,1,0} %multiply.76), metadata={op_type="AddN" op_name="tower0/gradients/AddN_167"}
  %param_0.142 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.75 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.41, f32[1,256,1,1]{3,2,1,0} %param_0.142), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.274 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.75), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3415 (x.3416: f32[], y.3417: f32[]) -> f32[] {
  %x.3416 = f32[] parameter(0)
  %y.3417 = f32[] parameter(1)
  ROOT %add.3418 = f32[] add(f32[] %x.3416, f32[] %y.3417)
}

%tower0_gradients_tower0_group2_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3425 (x.3426: f32[], y.3427: f32[]) -> f32[] {
  %x.3426 = f32[] parameter(0)
  %y.3427 = f32[] parameter(1)
  ROOT %add.3428 = f32[] add(f32[] %x.3426, f32[] %y.3427)
}

%fused_computation.54 (param_0.633: f32[1,256,50,76], param_1.890: f32[1,256,50,76], param_2.776: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.776 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_259 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.209.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_259), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.15.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.776, f32[1,256,50,76]{3,2,1,0} %broadcast.209.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/conv1/Relu_grad/ReluGrad"}
  %param_1.890 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.15.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.15.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.890, f32[1,256,50,76]{3,2,1,0} %broadcast.209.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/conv1/Relu_grad/ReluGrad"}
  %param_0.633 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.77 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.15.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.633), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.30 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.77, f32[] %constant_259), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3415, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.99 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.15.clone.1, f32[] %constant_259), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3425, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.222 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.30, f32[256]{0} %reduce.99, f32[1,256,50,76]{3,2,1,0} %select.15.clone.1)
}

%fused_computation.55 (param_0.147: f32[1,1,1024,256], param_1.706: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.147 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.706 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.393 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.706), metadata={op_name="XLA_Args"}
  %constant_325 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.175 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_325), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.78 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.393, f32[1,1,1024,256]{1,0,2,3} %broadcast.175), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_17_grad/Mul_1"}
  %add.42 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.147, f32[1,1,1024,256]{1,0,2,3} %multiply.78), metadata={op_type="AddN" op_name="tower0/gradients/AddN_166"}
  ROOT %copy.392 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.42), metadata={op_name="XLA_Retvals"}
}

%fused_computation.56 (param_0.150: f32[1,256,1,1], param_1.160: f32[1,256,1,1], param_2.122: f32[256], param_3.67: f32[256]) -> f32[256] {
  %param_3.67 = f32[256]{0} parameter(3)
  %bitcast.279 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.67), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.122 = f32[256]{0} parameter(2)
  %negate.64 = f32[256]{0} negate(f32[256]{0} %param_2.122), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.278 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.64), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.160 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.80 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.278, f32[1,256,1,1]{3,2,1,0} %param_1.160), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.43 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.279, f32[1,256,1,1]{3,2,1,0} %multiply.80), metadata={op_type="AddN" op_name="tower0/gradients/AddN_164"}
  %param_0.150 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.79 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.43, f32[1,256,1,1]{3,2,1,0} %param_0.150), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.277 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.79), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3442 (x.3443: f32[], y.3444: f32[]) -> f32[] {
  %x.3443 = f32[] parameter(0)
  %y.3444 = f32[] parameter(1)
  ROOT %add.3445 = f32[] add(f32[] %x.3443, f32[] %y.3444)
}

%tower0_gradients_tower0_group2_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3452 (x.3453: f32[], y.3454: f32[]) -> f32[] {
  %x.3453 = f32[] parameter(0)
  %y.3454 = f32[] parameter(1)
  ROOT %add.3455 = f32[] add(f32[] %x.3453, f32[] %y.3454)
}

%fused_computation.57 (param_0.632: f32[1,256,50,76], param_1.891: f32[1,256,50,76], param_2.777: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.777 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_260 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.210.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_260), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.16.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.777, f32[1,256,50,76]{3,2,1,0} %broadcast.210.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/conv2/Relu_grad/ReluGrad"}
  %param_1.891 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.16.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.16.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.891, f32[1,256,50,76]{3,2,1,0} %broadcast.210.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/conv2/Relu_grad/ReluGrad"}
  %param_0.632 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.81 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.16.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.632), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.31 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.81, f32[] %constant_260), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3442, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.98 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.16.clone.1, f32[] %constant_260), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3452, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.221 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.31, f32[256]{0} %reduce.98, f32[1,256,50,76]{3,2,1,0} %select.16.clone.1)
}

%fused_computation.59 (param_0.158: f32[1,1024,1,1], param_1.168: f32[1,1024,1,1], param_2.128: f32[1024], param_3.71: f32[1024]) -> f32[1024] {
  %param_3.71 = f32[1024]{0} parameter(3)
  %bitcast.282 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.71), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.128 = f32[1024]{0} parameter(2)
  %negate.65 = f32[1024]{0} negate(f32[1024]{0} %param_2.128), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.281 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.65), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.168 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.84 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.281, f32[1,1024,1,1]{3,2,1,0} %param_1.168), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.45 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.282, f32[1,1024,1,1]{3,2,1,0} %multiply.84), metadata={op_type="AddN" op_name="tower0/gradients/AddN_162"}
  %param_0.158 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.83 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.45, f32[1,1024,1,1]{3,2,1,0} %param_0.158), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.280 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.83), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3469 (x.3470: f32[], y.3471: f32[]) -> f32[] {
  %x.3470 = f32[] parameter(0)
  %y.3471 = f32[] parameter(1)
  ROOT %add.3472 = f32[] add(f32[] %x.3470, f32[] %y.3471)
}

%tower0_gradients_tower0_group2_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3479 (x.3480: f32[], y.3481: f32[]) -> f32[] {
  %x.3480 = f32[] parameter(0)
  %y.3481 = f32[] parameter(1)
  ROOT %add.3482 = f32[] add(f32[] %x.3480, f32[] %y.3481)
}

%fused_computation.60 (param_0.631: f32[1,1024,50,76], param_1.892: f32[1,1024,50,76], param_2.778: f32[1,1024,50,76], param_3.495: f32[1,1024,50,76]) -> (f32[1024], f32[1024], f32[1,1024,50,76]) {
  %param_3.495 = f32[1,1024,50,76]{3,2,1,0} parameter(3)
  %constant_261 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.211.clone.1 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_261), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.17.clone.1 = pred[1,1024,50,76]{3,2,1,0} compare(f32[1,1024,50,76]{3,2,1,0} %param_3.495, f32[1,1024,50,76]{3,2,1,0} %broadcast.211.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/output_grad/ReluGrad"}
  %param_1.892 = f32[1,1024,50,76]{3,2,1,0} parameter(1)
  %param_2.778 = f32[1,1024,50,76]{3,2,1,0} parameter(2)
  %add.132.clone.1 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_1.892, f32[1,1024,50,76]{3,2,1,0} %param_2.778), metadata={op_type="AddN" op_name="tower0/gradients/AddN_158"}
  %select.17.clone.1 = f32[1,1024,50,76]{3,2,1,0} select(pred[1,1024,50,76]{3,2,1,0} %compare.17.clone.1, f32[1,1024,50,76]{3,2,1,0} %add.132.clone.1, f32[1,1024,50,76]{3,2,1,0} %broadcast.211.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/output_grad/ReluGrad"}
  %param_0.631 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %multiply.85 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %select.17.clone.1, f32[1,1024,50,76]{3,2,1,0} %param_0.631), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.32 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %multiply.85, f32[] %constant_261), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3469, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.78 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %select.17.clone.1, f32[] %constant_261), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3479, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.220 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) tuple(f32[1024]{0} %reduce.32, f32[1024]{0} %reduce.78, f32[1,1024,50,76]{3,2,1,0} %select.17.clone.1)
}

%fused_computation.61 (param_0.163: f32[1,1,256,1024], param_1.708: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.163 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.708 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.396 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.708), metadata={op_name="XLA_Args"}
  %constant_326 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.176 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_326), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.86 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.396, f32[1,1,256,1024]{1,0,2,3} %broadcast.176), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_19_grad/Mul_1"}
  %add.46 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.163, f32[1,1,256,1024]{1,0,2,3} %multiply.86), metadata={op_type="AddN" op_name="tower0/gradients/AddN_161"}
  ROOT %copy.395 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.46), metadata={op_name="XLA_Retvals"}
}

%fused_computation.62 (param_0.166: f32[1,256,1,1], param_1.177: f32[1,256,1,1], param_2.135: f32[256], param_3.75: f32[256]) -> f32[256] {
  %param_3.75 = f32[256]{0} parameter(3)
  %bitcast.285 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.75), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.135 = f32[256]{0} parameter(2)
  %negate.66 = f32[256]{0} negate(f32[256]{0} %param_2.135), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.284 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.66), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.177 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.88 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.284, f32[1,256,1,1]{3,2,1,0} %param_1.177), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.47 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.285, f32[1,256,1,1]{3,2,1,0} %multiply.88), metadata={op_type="AddN" op_name="tower0/gradients/AddN_160"}
  %param_0.166 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.87 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.47, f32[1,256,1,1]{3,2,1,0} %param_0.166), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.283 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.87), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3496 (x.3497: f32[], y.3498: f32[]) -> f32[] {
  %x.3497 = f32[] parameter(0)
  %y.3498 = f32[] parameter(1)
  ROOT %add.3499 = f32[] add(f32[] %x.3497, f32[] %y.3498)
}

%tower0_gradients_tower0_group2_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3506 (x.3507: f32[], y.3508: f32[]) -> f32[] {
  %x.3507 = f32[] parameter(0)
  %y.3508 = f32[] parameter(1)
  ROOT %add.3509 = f32[] add(f32[] %x.3507, f32[] %y.3508)
}

%fused_computation.63 (param_0.630: f32[1,256,50,76], param_1.893: f32[1,256,50,76], param_2.779: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.779 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_262 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.212.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_262), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.18.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.779, f32[1,256,50,76]{3,2,1,0} %broadcast.212.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/conv1/Relu_grad/ReluGrad"}
  %param_1.893 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.18.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.18.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.893, f32[1,256,50,76]{3,2,1,0} %broadcast.212.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/conv1/Relu_grad/ReluGrad"}
  %param_0.630 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.89 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.18.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.630), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.33 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.89, f32[] %constant_262), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3496, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.92 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.18.clone.1, f32[] %constant_262), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3506, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.219 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.33, f32[256]{0} %reduce.92, f32[1,256,50,76]{3,2,1,0} %select.18.clone.1)
}

%fused_computation.64 (param_0.171: f32[1,1,1024,256], param_1.709: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.171 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.709 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.398 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.709), metadata={op_name="XLA_Args"}
  %constant_327 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.177 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_327), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.90 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.398, f32[1,1,1024,256]{1,0,2,3} %broadcast.177), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_20_grad/Mul_1"}
  %add.48 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.171, f32[1,1,1024,256]{1,0,2,3} %multiply.90), metadata={op_type="AddN" op_name="tower0/gradients/AddN_159"}
  ROOT %copy.397 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.48), metadata={op_name="XLA_Retvals"}
}

%fused_computation.65 (param_0.174: f32[1,256,1,1], param_1.186: f32[1,256,1,1], param_2.142: f32[256], param_3.79: f32[256]) -> f32[256] {
  %param_3.79 = f32[256]{0} parameter(3)
  %bitcast.288 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.79), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.142 = f32[256]{0} parameter(2)
  %negate.67 = f32[256]{0} negate(f32[256]{0} %param_2.142), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.287 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.67), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.186 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.92 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.287, f32[1,256,1,1]{3,2,1,0} %param_1.186), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.49 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.288, f32[1,256,1,1]{3,2,1,0} %multiply.92), metadata={op_type="AddN" op_name="tower0/gradients/AddN_157"}
  %param_0.174 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.91 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.49, f32[1,256,1,1]{3,2,1,0} %param_0.174), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.286 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.91), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3523 (x.3524: f32[], y.3525: f32[]) -> f32[] {
  %x.3524 = f32[] parameter(0)
  %y.3525 = f32[] parameter(1)
  ROOT %add.3526 = f32[] add(f32[] %x.3524, f32[] %y.3525)
}

%tower0_gradients_tower0_group2_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3533 (x.3534: f32[], y.3535: f32[]) -> f32[] {
  %x.3534 = f32[] parameter(0)
  %y.3535 = f32[] parameter(1)
  ROOT %add.3536 = f32[] add(f32[] %x.3534, f32[] %y.3535)
}

%fused_computation.66 (param_0.629: f32[1,256,50,76], param_1.894: f32[1,256,50,76], param_2.780: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.780 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_263 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.213.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_263), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.19.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.780, f32[1,256,50,76]{3,2,1,0} %broadcast.213.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/conv2/Relu_grad/ReluGrad"}
  %param_1.894 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.19.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.19.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.894, f32[1,256,50,76]{3,2,1,0} %broadcast.213.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/conv2/Relu_grad/ReluGrad"}
  %param_0.629 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.93 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.19.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.629), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.34 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.93, f32[] %constant_263), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3523, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.91 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.19.clone.1, f32[] %constant_263), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3533, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.218 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.34, f32[256]{0} %reduce.91, f32[1,256,50,76]{3,2,1,0} %select.19.clone.1)
}

%fused_computation.68 (param_0.182: f32[1,1024,1,1], param_1.194: f32[1,1024,1,1], param_2.148: f32[1024], param_3.83: f32[1024]) -> f32[1024] {
  %param_3.83 = f32[1024]{0} parameter(3)
  %bitcast.291 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.83), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.148 = f32[1024]{0} parameter(2)
  %negate.68 = f32[1024]{0} negate(f32[1024]{0} %param_2.148), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.290 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.68), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.194 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.96 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.290, f32[1,1024,1,1]{3,2,1,0} %param_1.194), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.51 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.291, f32[1,1024,1,1]{3,2,1,0} %multiply.96), metadata={op_type="AddN" op_name="tower0/gradients/AddN_155"}
  %param_0.182 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.95 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.51, f32[1,1024,1,1]{3,2,1,0} %param_0.182), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.289 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.95), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3550 (x.3551: f32[], y.3552: f32[]) -> f32[] {
  %x.3551 = f32[] parameter(0)
  %y.3552 = f32[] parameter(1)
  ROOT %add.3553 = f32[] add(f32[] %x.3551, f32[] %y.3552)
}

%tower0_gradients_tower0_group2_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3560 (x.3561: f32[], y.3562: f32[]) -> f32[] {
  %x.3561 = f32[] parameter(0)
  %y.3562 = f32[] parameter(1)
  ROOT %add.3563 = f32[] add(f32[] %x.3561, f32[] %y.3562)
}

%fused_computation.69 (param_0.628: f32[1,1024,50,76], param_1.895: f32[1,1024,50,76], param_2.781: f32[1,1024,50,76], param_3.496: f32[1,1024,50,76]) -> (f32[1024], f32[1024], f32[1,1024,50,76]) {
  %param_3.496 = f32[1,1024,50,76]{3,2,1,0} parameter(3)
  %constant_264 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.214.clone.1 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_264), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.20.clone.1 = pred[1,1024,50,76]{3,2,1,0} compare(f32[1,1024,50,76]{3,2,1,0} %param_3.496, f32[1,1024,50,76]{3,2,1,0} %broadcast.214.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/output_grad/ReluGrad"}
  %param_1.895 = f32[1,1024,50,76]{3,2,1,0} parameter(1)
  %param_2.781 = f32[1,1024,50,76]{3,2,1,0} parameter(2)
  %add.133.clone.1 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_1.895, f32[1,1024,50,76]{3,2,1,0} %param_2.781), metadata={op_type="AddN" op_name="tower0/gradients/AddN_151"}
  %select.20.clone.1 = f32[1,1024,50,76]{3,2,1,0} select(pred[1,1024,50,76]{3,2,1,0} %compare.20.clone.1, f32[1,1024,50,76]{3,2,1,0} %add.133.clone.1, f32[1,1024,50,76]{3,2,1,0} %broadcast.214.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/output_grad/ReluGrad"}
  %param_0.628 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %multiply.97 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %select.20.clone.1, f32[1,1024,50,76]{3,2,1,0} %param_0.628), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.35 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %multiply.97, f32[] %constant_264), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3550, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.73 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %select.20.clone.1, f32[] %constant_264), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3560, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.217 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) tuple(f32[1024]{0} %reduce.35, f32[1024]{0} %reduce.73, f32[1,1024,50,76]{3,2,1,0} %select.20.clone.1)
}

%fused_computation.70 (param_0.187: f32[1,1,256,1024], param_1.711: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.187 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.711 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.401 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.711), metadata={op_name="XLA_Args"}
  %constant_328 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.178 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_328), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.98 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.401, f32[1,1,256,1024]{1,0,2,3} %broadcast.178), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_22_grad/Mul_1"}
  %add.52 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.187, f32[1,1,256,1024]{1,0,2,3} %multiply.98), metadata={op_type="AddN" op_name="tower0/gradients/AddN_154"}
  ROOT %copy.400 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.52), metadata={op_name="XLA_Retvals"}
}

%fused_computation.71 (param_0.190: f32[1,256,1,1], param_1.203: f32[1,256,1,1], param_2.155: f32[256], param_3.87: f32[256]) -> f32[256] {
  %param_3.87 = f32[256]{0} parameter(3)
  %bitcast.294 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.87), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.155 = f32[256]{0} parameter(2)
  %negate.69 = f32[256]{0} negate(f32[256]{0} %param_2.155), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.293 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.69), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.203 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.100 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.293, f32[1,256,1,1]{3,2,1,0} %param_1.203), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.53 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.294, f32[1,256,1,1]{3,2,1,0} %multiply.100), metadata={op_type="AddN" op_name="tower0/gradients/AddN_153"}
  %param_0.190 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.99 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.53, f32[1,256,1,1]{3,2,1,0} %param_0.190), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.292 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.99), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block3_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3577 (x.3578: f32[], y.3579: f32[]) -> f32[] {
  %x.3578 = f32[] parameter(0)
  %y.3579 = f32[] parameter(1)
  ROOT %add.3580 = f32[] add(f32[] %x.3578, f32[] %y.3579)
}

%tower0_gradients_tower0_group2_block3_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3587 (x.3588: f32[], y.3589: f32[]) -> f32[] {
  %x.3588 = f32[] parameter(0)
  %y.3589 = f32[] parameter(1)
  ROOT %add.3590 = f32[] add(f32[] %x.3588, f32[] %y.3589)
}

%fused_computation.72 (param_0.627: f32[1,256,50,76], param_1.896: f32[1,256,50,76], param_2.782: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.782 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_265 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.215.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_265), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.21.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.782, f32[1,256,50,76]{3,2,1,0} %broadcast.215.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/conv1/Relu_grad/ReluGrad"}
  %param_1.896 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.21.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.21.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.896, f32[1,256,50,76]{3,2,1,0} %broadcast.215.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/conv1/Relu_grad/ReluGrad"}
  %param_0.627 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.101 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.21.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.627), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.36 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.101, f32[] %constant_265), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3577, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.93 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.21.clone.1, f32[] %constant_265), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3587, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.216 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.36, f32[256]{0} %reduce.93, f32[1,256,50,76]{3,2,1,0} %select.21.clone.1)
}

%fused_computation.73 (param_0.195: f32[1,1,1024,256], param_1.712: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.195 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.712 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.403 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.712), metadata={op_name="XLA_Args"}
  %constant_329 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.179 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_329), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.102 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.403, f32[1,1,1024,256]{1,0,2,3} %broadcast.179), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_23_grad/Mul_1"}
  %add.54 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.195, f32[1,1,1024,256]{1,0,2,3} %multiply.102), metadata={op_type="AddN" op_name="tower0/gradients/AddN_152"}
  ROOT %copy.402 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.54), metadata={op_name="XLA_Retvals"}
}

%fused_computation.74 (param_0.198: f32[1,256,1,1], param_1.212: f32[1,256,1,1], param_2.162: f32[256], param_3.91: f32[256]) -> f32[256] {
  %param_3.91 = f32[256]{0} parameter(3)
  %bitcast.297 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.91), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.162 = f32[256]{0} parameter(2)
  %negate.70 = f32[256]{0} negate(f32[256]{0} %param_2.162), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.296 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.70), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.212 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.104 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.296, f32[1,256,1,1]{3,2,1,0} %param_1.212), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.55 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.297, f32[1,256,1,1]{3,2,1,0} %multiply.104), metadata={op_type="AddN" op_name="tower0/gradients/AddN_150"}
  %param_0.198 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.103 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.55, f32[1,256,1,1]{3,2,1,0} %param_0.198), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.295 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.103), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block3_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3604 (x.3605: f32[], y.3606: f32[]) -> f32[] {
  %x.3605 = f32[] parameter(0)
  %y.3606 = f32[] parameter(1)
  ROOT %add.3607 = f32[] add(f32[] %x.3605, f32[] %y.3606)
}

%tower0_gradients_tower0_group2_block3_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3614 (x.3615: f32[], y.3616: f32[]) -> f32[] {
  %x.3615 = f32[] parameter(0)
  %y.3616 = f32[] parameter(1)
  ROOT %add.3617 = f32[] add(f32[] %x.3615, f32[] %y.3616)
}

%fused_computation.75 (param_0.626: f32[1,256,50,76], param_1.897: f32[1,256,50,76], param_2.783: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.783 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_266 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.216.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_266), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.22.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.783, f32[1,256,50,76]{3,2,1,0} %broadcast.216.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/conv2/Relu_grad/ReluGrad"}
  %param_1.897 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.22.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.22.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.897, f32[1,256,50,76]{3,2,1,0} %broadcast.216.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/conv2/Relu_grad/ReluGrad"}
  %param_0.626 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.105 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.22.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.626), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.37 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.105, f32[] %constant_266), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3604, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.94 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.22.clone.1, f32[] %constant_266), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3614, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.215 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.37, f32[256]{0} %reduce.94, f32[1,256,50,76]{3,2,1,0} %select.22.clone.1)
}

%fused_computation.77 (param_0.206: f32[1,1024,1,1], param_1.220: f32[1,1024,1,1], param_2.168: f32[1024], param_3.95: f32[1024]) -> f32[1024] {
  %param_3.95 = f32[1024]{0} parameter(3)
  %bitcast.300 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.95), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.168 = f32[1024]{0} parameter(2)
  %negate.71 = f32[1024]{0} negate(f32[1024]{0} %param_2.168), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.299 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.71), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.220 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.108 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.299, f32[1,1024,1,1]{3,2,1,0} %param_1.220), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.57 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.300, f32[1,1024,1,1]{3,2,1,0} %multiply.108), metadata={op_type="AddN" op_name="tower0/gradients/AddN_148"}
  %param_0.206 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.107 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.57, f32[1,1024,1,1]{3,2,1,0} %param_0.206), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.298 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.107), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block3_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3631 (x.3632: f32[], y.3633: f32[]) -> f32[] {
  %x.3632 = f32[] parameter(0)
  %y.3633 = f32[] parameter(1)
  ROOT %add.3634 = f32[] add(f32[] %x.3632, f32[] %y.3633)
}

%tower0_gradients_tower0_group2_block3_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3641 (x.3642: f32[], y.3643: f32[]) -> f32[] {
  %x.3642 = f32[] parameter(0)
  %y.3643 = f32[] parameter(1)
  ROOT %add.3644 = f32[] add(f32[] %x.3642, f32[] %y.3643)
}

%fused_computation.78 (param_0.625: f32[1,1024,50,76], param_1.898: f32[1,1024,50,76], param_2.784: f32[1,1024,50,76], param_3.497: f32[1,1024,50,76]) -> (f32[1024], f32[1024], f32[1,1024,50,76]) {
  %param_3.497 = f32[1,1024,50,76]{3,2,1,0} parameter(3)
  %constant_267 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.217.clone.1 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_267), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.23.clone.1 = pred[1,1024,50,76]{3,2,1,0} compare(f32[1,1024,50,76]{3,2,1,0} %param_3.497, f32[1,1024,50,76]{3,2,1,0} %broadcast.217.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/output_grad/ReluGrad"}
  %param_1.898 = f32[1,1024,50,76]{3,2,1,0} parameter(1)
  %param_2.784 = f32[1,1024,50,76]{3,2,1,0} parameter(2)
  %add.134.clone.1 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_1.898, f32[1,1024,50,76]{3,2,1,0} %param_2.784), metadata={op_type="AddN" op_name="tower0/gradients/AddN_144"}
  %select.23.clone.1 = f32[1,1024,50,76]{3,2,1,0} select(pred[1,1024,50,76]{3,2,1,0} %compare.23.clone.1, f32[1,1024,50,76]{3,2,1,0} %add.134.clone.1, f32[1,1024,50,76]{3,2,1,0} %broadcast.217.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/output_grad/ReluGrad"}
  %param_0.625 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %multiply.109 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %select.23.clone.1, f32[1,1024,50,76]{3,2,1,0} %param_0.625), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.38 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %multiply.109, f32[] %constant_267), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3631, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.76 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %select.23.clone.1, f32[] %constant_267), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3641, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.214 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) tuple(f32[1024]{0} %reduce.38, f32[1024]{0} %reduce.76, f32[1,1024,50,76]{3,2,1,0} %select.23.clone.1)
}

%fused_computation.79 (param_0.211: f32[1,1,256,1024], param_1.714: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.211 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.714 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.406 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.714), metadata={op_name="XLA_Args"}
  %constant_330 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.180 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_330), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.110 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.406, f32[1,1,256,1024]{1,0,2,3} %broadcast.180), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_25_grad/Mul_1"}
  %add.58 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.211, f32[1,1,256,1024]{1,0,2,3} %multiply.110), metadata={op_type="AddN" op_name="tower0/gradients/AddN_147"}
  ROOT %copy.405 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.58), metadata={op_name="XLA_Retvals"}
}

%fused_computation.80 (param_0.214: f32[1,256,1,1], param_1.229: f32[1,256,1,1], param_2.175: f32[256], param_3.99: f32[256]) -> f32[256] {
  %param_3.99 = f32[256]{0} parameter(3)
  %bitcast.303 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.99), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.175 = f32[256]{0} parameter(2)
  %negate.72 = f32[256]{0} negate(f32[256]{0} %param_2.175), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.302 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.72), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.229 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.112 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.302, f32[1,256,1,1]{3,2,1,0} %param_1.229), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.59 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.303, f32[1,256,1,1]{3,2,1,0} %multiply.112), metadata={op_type="AddN" op_name="tower0/gradients/AddN_146"}
  %param_0.214 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.111 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.59, f32[1,256,1,1]{3,2,1,0} %param_0.214), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.301 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.111), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block4_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3658 (x.3659: f32[], y.3660: f32[]) -> f32[] {
  %x.3659 = f32[] parameter(0)
  %y.3660 = f32[] parameter(1)
  ROOT %add.3661 = f32[] add(f32[] %x.3659, f32[] %y.3660)
}

%tower0_gradients_tower0_group2_block4_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3668 (x.3669: f32[], y.3670: f32[]) -> f32[] {
  %x.3669 = f32[] parameter(0)
  %y.3670 = f32[] parameter(1)
  ROOT %add.3671 = f32[] add(f32[] %x.3669, f32[] %y.3670)
}

%fused_computation.81 (param_0.624: f32[1,256,50,76], param_1.899: f32[1,256,50,76], param_2.785: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.785 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_268 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.218.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_268), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.24.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.785, f32[1,256,50,76]{3,2,1,0} %broadcast.218.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/conv1/Relu_grad/ReluGrad"}
  %param_1.899 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.24.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.24.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.899, f32[1,256,50,76]{3,2,1,0} %broadcast.218.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/conv1/Relu_grad/ReluGrad"}
  %param_0.624 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.113 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.24.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.624), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.39 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.113, f32[] %constant_268), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3658, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.96 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.24.clone.1, f32[] %constant_268), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3668, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.213 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.39, f32[256]{0} %reduce.96, f32[1,256,50,76]{3,2,1,0} %select.24.clone.1)
}

%fused_computation.82 (param_0.219: f32[1,1,1024,256], param_1.715: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.219 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.715 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.408 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.715), metadata={op_name="XLA_Args"}
  %constant_331 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.181 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_331), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.114 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.408, f32[1,1,1024,256]{1,0,2,3} %broadcast.181), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_26_grad/Mul_1"}
  %add.60 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.219, f32[1,1,1024,256]{1,0,2,3} %multiply.114), metadata={op_type="AddN" op_name="tower0/gradients/AddN_145"}
  ROOT %copy.407 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.60), metadata={op_name="XLA_Retvals"}
}

%fused_computation.83 (param_0.222: f32[1,256,1,1], param_1.238: f32[1,256,1,1], param_2.182: f32[256], param_3.103: f32[256]) -> f32[256] {
  %param_3.103 = f32[256]{0} parameter(3)
  %bitcast.306 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.103), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.182 = f32[256]{0} parameter(2)
  %negate.73 = f32[256]{0} negate(f32[256]{0} %param_2.182), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.305 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.73), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.238 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.116 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.305, f32[1,256,1,1]{3,2,1,0} %param_1.238), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.61 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.306, f32[1,256,1,1]{3,2,1,0} %multiply.116), metadata={op_type="AddN" op_name="tower0/gradients/AddN_143"}
  %param_0.222 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.115 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.61, f32[1,256,1,1]{3,2,1,0} %param_0.222), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.304 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.115), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block4_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3685 (x.3686: f32[], y.3687: f32[]) -> f32[] {
  %x.3686 = f32[] parameter(0)
  %y.3687 = f32[] parameter(1)
  ROOT %add.3688 = f32[] add(f32[] %x.3686, f32[] %y.3687)
}

%tower0_gradients_tower0_group2_block4_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3695 (x.3696: f32[], y.3697: f32[]) -> f32[] {
  %x.3696 = f32[] parameter(0)
  %y.3697 = f32[] parameter(1)
  ROOT %add.3698 = f32[] add(f32[] %x.3696, f32[] %y.3697)
}

%fused_computation.84 (param_0.623: f32[1,256,50,76], param_1.900: f32[1,256,50,76], param_2.786: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.786 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_269 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.219.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_269), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.25.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.786, f32[1,256,50,76]{3,2,1,0} %broadcast.219.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/conv2/Relu_grad/ReluGrad"}
  %param_1.900 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.25.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.25.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.900, f32[1,256,50,76]{3,2,1,0} %broadcast.219.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/conv2/Relu_grad/ReluGrad"}
  %param_0.623 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.117 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.25.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.623), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.40 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.117, f32[] %constant_269), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3685, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.97 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.25.clone.1, f32[] %constant_269), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3695, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.212 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.40, f32[256]{0} %reduce.97, f32[1,256,50,76]{3,2,1,0} %select.25.clone.1)
}

%fused_computation.85 (param_0.227: f32[3,3,256,256], param_1.716: f32[3,3,256,256], param_2.507: f32[3,3,256,256], param_3.314: f32[3,3,256,256], param_4.81: f32[3,3,256,256], param_5.70: f32[3,3,256,256], param_6.62: f32[3,3,256,256], param_7.72: f32[3,3,256,256], param_8.57: f32[3,3,256,256], param_9.28: f32[3,3,256,256], param_10.20: f32[3,3,256,256], param_11.13: f32[3,3,256,256], param_12.11: f32[3,3,256,256], param_13.12: f32[3,3,256,256], param_14.12: f32[3,3,256,256], param_15.13: f32[3,3,256,256], param_16.13: f32[3,3,256,256], param_17.12: f32[3,3,256,256], param_18.12: f32[3,3,256,256], param_19.13: f32[3,3,256,256], param_20.12: f32[3,3,256,256], param_21.11: f32[3,3,256,256], param_22.8: f32[3,3,256,256], param_23.5: f32[3,3,256,256], param_24.5: f32[3,3,256,256], param_25.5: f32[3,3,256,256], param_26.3: f32[3,3,256,256]) -> (f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256]) {
  %param_0.227 = f32[3,3,256,256]{1,0,2,3} parameter(0)
  %param_2.507 = f32[3,3,256,256]{3,2,1,0} parameter(2)
  %copy.467 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_2.507), metadata={op_name="XLA_Args"}
  %param_1.716 = f32[3,3,256,256]{3,2,1,0} parameter(1)
  %copy.466 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_1.716), metadata={op_name="XLA_Args"}
  %multiply.118 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.467, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_27_grad/Mul_1"}
  %add.62 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_0.227, f32[3,3,256,256]{1,0,2,3} %multiply.118), metadata={op_type="AddN" op_name="tower0/gradients/AddN_142"}
  %copy.409 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.62), metadata={op_name="XLA_Retvals"}
  %param_3.314 = f32[3,3,256,256]{1,0,2,3} parameter(3)
  %param_4.81 = f32[3,3,256,256]{3,2,1,0} parameter(4)
  %copy.458.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_4.81), metadata={op_name="XLA_Args"}
  %multiply.66.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.458.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_14_grad/Mul_1"}
  %add.36.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_3.314, f32[3,3,256,256]{1,0,2,3} %multiply.66.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_172"}
  %copy.388.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.36.clone.1), metadata={op_name="XLA_Retvals"}
  %param_25.5 = f32[3,3,256,256]{1,0,2,3} parameter(25)
  %param_26.3 = f32[3,3,256,256]{3,2,1,0} parameter(26)
  %copy.479.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_26.3), metadata={op_name="XLA_Args"}
  %multiply.181.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.479.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_47_grad/Mul_1"}
  %add.97.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_25.5, f32[3,3,256,256]{1,0,2,3} %multiply.181.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_100"}
  %copy.443.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.97.clone.1), metadata={op_name="XLA_Retvals"}
  %param_23.5 = f32[3,3,256,256]{1,0,2,3} parameter(23)
  %param_24.5 = f32[3,3,256,256]{3,2,1,0} parameter(24)
  %copy.465.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_24.5), metadata={op_name="XLA_Args"}
  %multiply.106.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.465.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_24_grad/Mul_1"}
  %add.56.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_23.5, f32[3,3,256,256]{1,0,2,3} %multiply.106.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_149"}
  %copy.404.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.56.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_21.11 = f32[3,3,256,256]{1,0,2,3} parameter(21)
  %param_22.8 = f32[3,3,256,256]{3,2,1,0} parameter(22)
  %copy.477.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_22.8), metadata={op_name="XLA_Args"}
  %multiply.180.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.477.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_48_grad/Mul_1"}
  %add.96.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_21.11, f32[3,3,256,256]{1,0,2,3} %multiply.180.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_101"}
  %copy.442.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.96.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_19.13 = f32[3,3,256,256]{1,0,2,3} parameter(19)
  %param_20.12 = f32[3,3,256,256]{1,0,2,3} parameter(20)
  %add.103.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_19.13, f32[3,3,256,256]{1,0,2,3} %param_20.12), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %param_18.12 = f32[3,3,256,256]{1,0,2,3} parameter(18)
  %add.102.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %add.103.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %param_18.12), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %param_17.12 = f32[3,3,256,256]{1,0,2,3} parameter(17)
  %add.101.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %add.102.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %param_17.12), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %param_16.13 = f32[3,3,256,256]{1,0,2,3} parameter(16)
  %add.100.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %add.101.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %param_16.13), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %param_15.13 = f32[3,3,256,256]{3,2,1,0} parameter(15)
  %copy.483.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_15.13), metadata={op_name="XLA_Args"}
  %multiply.183.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.483.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_50_grad/Mul_1"}
  %add.99.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %add.100.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %multiply.183.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %copy.445.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.99.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_13.12 = f32[3,3,256,256]{1,0,2,3} parameter(13)
  %param_14.12 = f32[3,3,256,256]{3,2,1,0} parameter(14)
  %copy.461.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_14.12), metadata={op_name="XLA_Args"}
  %multiply.82.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.461.clone.1.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_18_grad/Mul_1"}
  %add.44.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_13.12, f32[3,3,256,256]{1,0,2,3} %multiply.82.clone.1.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_163"}
  %copy.394.clone.1.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.44.clone.1.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_11.13 = f32[3,3,256,256]{1,0,2,3} parameter(11)
  %param_12.11 = f32[3,3,256,256]{3,2,1,0} parameter(12)
  %copy.475.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_12.11), metadata={op_name="XLA_Args"}
  %multiply.179.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.475.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_49_grad/Mul_1"}
  %add.95.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_11.13, f32[3,3,256,256]{1,0,2,3} %multiply.179.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_102"}
  %copy.441.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.95.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_9.28 = f32[3,3,256,256]{1,0,2,3} parameter(9)
  %param_10.20 = f32[3,3,256,256]{3,2,1,0} parameter(10)
  %copy.481.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_10.20), metadata={op_name="XLA_Args"}
  %multiply.182.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.481.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_46_grad/Mul_1"}
  %add.98.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_9.28, f32[3,3,256,256]{1,0,2,3} %multiply.182.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_99"}
  %copy.444.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.98.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_7.72 = f32[3,3,256,256]{1,0,2,3} parameter(7)
  %param_8.57 = f32[3,3,256,256]{3,2,1,0} parameter(8)
  %copy.469.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_8.57), metadata={op_name="XLA_Args"}
  %multiply.130.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.469.clone.1.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_30_grad/Mul_1"}
  %add.68.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_7.72, f32[3,3,256,256]{1,0,2,3} %multiply.130.clone.1.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_135"}
  %copy.414.clone.1.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.68.clone.1.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_5.70 = f32[3,3,256,256]{1,0,2,3} parameter(5)
  %param_6.62 = f32[3,3,256,256]{3,2,1,0} parameter(6)
  %copy.463.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_6.62), metadata={op_name="XLA_Args"}
  %multiply.94.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.463.clone.1.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_21_grad/Mul_1"}
  %add.50.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_5.70, f32[3,3,256,256]{1,0,2,3} %multiply.94.clone.1.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_156"}
  %copy.399.clone.1.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.50.clone.1.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  ROOT %tuple.178 = (f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) tuple(f32[3,3,256,256]{3,2,1,0} %copy.409, f32[3,3,256,256]{3,2,1,0} %copy.388.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.443.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.404.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.442.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.445.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.394.clone.1.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.441.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.444.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.414.clone.1.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.399.clone.1.clone.1.clone.1)
}

%fused_computation.86 (param_0.230: f32[1,1024,1,1], param_1.246: f32[1,1024,1,1], param_2.188: f32[1024], param_3.107: f32[1024]) -> f32[1024] {
  %param_3.107 = f32[1024]{0} parameter(3)
  %bitcast.309 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.107), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.188 = f32[1024]{0} parameter(2)
  %negate.74 = f32[1024]{0} negate(f32[1024]{0} %param_2.188), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.308 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.74), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.246 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.120 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.308, f32[1,1024,1,1]{3,2,1,0} %param_1.246), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.63 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.309, f32[1,1024,1,1]{3,2,1,0} %multiply.120), metadata={op_type="AddN" op_name="tower0/gradients/AddN_141"}
  %param_0.230 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.119 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.63, f32[1,1024,1,1]{3,2,1,0} %param_0.230), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.307 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.119), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block4_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3712 (x.3713: f32[], y.3714: f32[]) -> f32[] {
  %x.3713 = f32[] parameter(0)
  %y.3714 = f32[] parameter(1)
  ROOT %add.3715 = f32[] add(f32[] %x.3713, f32[] %y.3714)
}

%tower0_gradients_tower0_group2_block4_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3722 (x.3723: f32[], y.3724: f32[]) -> f32[] {
  %x.3723 = f32[] parameter(0)
  %y.3724 = f32[] parameter(1)
  ROOT %add.3725 = f32[] add(f32[] %x.3723, f32[] %y.3724)
}

%fused_computation.87 (param_0.622: f32[1,1024,50,76], param_1.901: f32[1,1024,50,76], param_2.787: f32[1,1024,50,76], param_3.498: f32[1,1024,50,76]) -> (f32[1024], f32[1024], f32[1,1024,50,76]) {
  %param_3.498 = f32[1,1024,50,76]{3,2,1,0} parameter(3)
  %constant_270 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.220.clone.1 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_270), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.26.clone.1 = pred[1,1024,50,76]{3,2,1,0} compare(f32[1,1024,50,76]{3,2,1,0} %param_3.498, f32[1,1024,50,76]{3,2,1,0} %broadcast.220.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/output_grad/ReluGrad"}
  %param_1.901 = f32[1,1024,50,76]{3,2,1,0} parameter(1)
  %param_2.787 = f32[1,1024,50,76]{3,2,1,0} parameter(2)
  %add.135.clone.1 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_1.901, f32[1,1024,50,76]{3,2,1,0} %param_2.787), metadata={op_type="AddN" op_name="tower0/gradients/AddN_137"}
  %select.26.clone.1 = f32[1,1024,50,76]{3,2,1,0} select(pred[1,1024,50,76]{3,2,1,0} %compare.26.clone.1, f32[1,1024,50,76]{3,2,1,0} %add.135.clone.1, f32[1,1024,50,76]{3,2,1,0} %broadcast.220.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/output_grad/ReluGrad"}
  %param_0.622 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %multiply.121 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %select.26.clone.1, f32[1,1024,50,76]{3,2,1,0} %param_0.622), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.41 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %multiply.121, f32[] %constant_270), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3712, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.75 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %select.26.clone.1, f32[] %constant_270), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3722, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.211 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) tuple(f32[1024]{0} %reduce.41, f32[1024]{0} %reduce.75, f32[1,1024,50,76]{3,2,1,0} %select.26.clone.1)
}

%fused_computation.88 (param_0.235: f32[1,1,256,1024], param_1.717: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.235 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.717 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.411 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.717), metadata={op_name="XLA_Args"}
  %constant_332 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.182 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_332), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.122 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.411, f32[1,1,256,1024]{1,0,2,3} %broadcast.182), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_28_grad/Mul_1"}
  %add.64 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.235, f32[1,1,256,1024]{1,0,2,3} %multiply.122), metadata={op_type="AddN" op_name="tower0/gradients/AddN_140"}
  ROOT %copy.410 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.64), metadata={op_name="XLA_Retvals"}
}

%fused_computation.89 (param_0.238: f32[1,256,1,1], param_1.255: f32[1,256,1,1], param_2.195: f32[256], param_3.111: f32[256]) -> f32[256] {
  %param_3.111 = f32[256]{0} parameter(3)
  %bitcast.312 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.111), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.195 = f32[256]{0} parameter(2)
  %negate.75 = f32[256]{0} negate(f32[256]{0} %param_2.195), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.311 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.75), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.255 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.124 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.311, f32[1,256,1,1]{3,2,1,0} %param_1.255), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.65 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.312, f32[1,256,1,1]{3,2,1,0} %multiply.124), metadata={op_type="AddN" op_name="tower0/gradients/AddN_139"}
  %param_0.238 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.123 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.65, f32[1,256,1,1]{3,2,1,0} %param_0.238), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.310 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.123), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block5_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3739 (x.3740: f32[], y.3741: f32[]) -> f32[] {
  %x.3740 = f32[] parameter(0)
  %y.3741 = f32[] parameter(1)
  ROOT %add.3742 = f32[] add(f32[] %x.3740, f32[] %y.3741)
}

%tower0_gradients_tower0_group2_block5_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3749 (x.3750: f32[], y.3751: f32[]) -> f32[] {
  %x.3750 = f32[] parameter(0)
  %y.3751 = f32[] parameter(1)
  ROOT %add.3752 = f32[] add(f32[] %x.3750, f32[] %y.3751)
}

%fused_computation.90 (param_0.621: f32[1,256,50,76], param_1.902: f32[1,256,50,76], param_2.788: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.788 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_271 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.221.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_271), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.27.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.788, f32[1,256,50,76]{3,2,1,0} %broadcast.221.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/conv1/Relu_grad/ReluGrad"}
  %param_1.902 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.27.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.27.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.902, f32[1,256,50,76]{3,2,1,0} %broadcast.221.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/conv1/Relu_grad/ReluGrad"}
  %param_0.621 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.125 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.27.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.621), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.42 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.125, f32[] %constant_271), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3739, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.101 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.27.clone.1, f32[] %constant_271), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3749, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.210 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.42, f32[256]{0} %reduce.101, f32[1,256,50,76]{3,2,1,0} %select.27.clone.1)
}

%fused_computation.91 (param_0.243: f32[1,1,1024,256], param_1.718: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.243 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.718 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.413 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.718), metadata={op_name="XLA_Args"}
  %constant_333 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.183 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_333), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.126 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.413, f32[1,1,1024,256]{1,0,2,3} %broadcast.183), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul_1"}
  %add.66 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.243, f32[1,1,1024,256]{1,0,2,3} %multiply.126), metadata={op_type="AddN" op_name="tower0/gradients/AddN_138"}
  ROOT %copy.412 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.66), metadata={op_name="XLA_Retvals"}
}

%fused_computation.92 (param_0.246: f32[1,256,1,1], param_1.264: f32[1,256,1,1], param_2.202: f32[256], param_3.115: f32[256]) -> f32[256] {
  %param_3.115 = f32[256]{0} parameter(3)
  %bitcast.315 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.115), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.202 = f32[256]{0} parameter(2)
  %negate.76 = f32[256]{0} negate(f32[256]{0} %param_2.202), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.314 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.76), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.264 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.128 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.314, f32[1,256,1,1]{3,2,1,0} %param_1.264), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.67 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.315, f32[1,256,1,1]{3,2,1,0} %multiply.128), metadata={op_type="AddN" op_name="tower0/gradients/AddN_136"}
  %param_0.246 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.127 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.67, f32[1,256,1,1]{3,2,1,0} %param_0.246), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.313 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.127), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block5_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3766 (x.3767: f32[], y.3768: f32[]) -> f32[] {
  %x.3767 = f32[] parameter(0)
  %y.3768 = f32[] parameter(1)
  ROOT %add.3769 = f32[] add(f32[] %x.3767, f32[] %y.3768)
}

%tower0_gradients_tower0_group2_block5_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3776 (x.3777: f32[], y.3778: f32[]) -> f32[] {
  %x.3777 = f32[] parameter(0)
  %y.3778 = f32[] parameter(1)
  ROOT %add.3779 = f32[] add(f32[] %x.3777, f32[] %y.3778)
}

%fused_computation.93 (param_0.620: f32[1,256,50,76], param_1.903: f32[1,256,50,76], param_2.789: f32[1,256,50,76]) -> (f32[256], f32[256], f32[1,256,50,76]) {
  %param_2.789 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_272 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.222.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_272), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.28.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.789, f32[1,256,50,76]{3,2,1,0} %broadcast.222.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/conv2/Relu_grad/ReluGrad"}
  %param_1.903 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %select.28.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.28.clone.1, f32[1,256,50,76]{3,2,1,0} %param_1.903, f32[1,256,50,76]{3,2,1,0} %broadcast.222.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/conv2/Relu_grad/ReluGrad"}
  %param_0.620 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %multiply.129 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %select.28.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.620), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.43 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %multiply.129, f32[] %constant_272), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3766, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.100 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.28.clone.1, f32[] %constant_272), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3776, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.209 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.43, f32[256]{0} %reduce.100, f32[1,256,50,76]{3,2,1,0} %select.28.clone.1)
}

%fused_computation.95 (param_0.254: f32[1,1024,1,1], param_1.272: f32[1,1024,1,1], param_2.208: f32[1024], param_3.119: f32[1024]) -> f32[1024] {
  %param_3.119 = f32[1024]{0} parameter(3)
  %bitcast.318 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.119), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.208 = f32[1024]{0} parameter(2)
  %negate.77 = f32[1024]{0} negate(f32[1024]{0} %param_2.208), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.317 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.77), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.272 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.132 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.317, f32[1,1024,1,1]{3,2,1,0} %param_1.272), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.69 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.318, f32[1,1024,1,1]{3,2,1,0} %multiply.132), metadata={op_type="AddN" op_name="tower0/gradients/AddN_134"}
  %param_0.254 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.131 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.69, f32[1,1024,1,1]{3,2,1,0} %param_0.254), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.316 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.131), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block5_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3793 (x.3794: f32[], y.3795: f32[]) -> f32[] {
  %x.3794 = f32[] parameter(0)
  %y.3795 = f32[] parameter(1)
  ROOT %add.3796 = f32[] add(f32[] %x.3794, f32[] %y.3795)
}

%tower0_gradients_tower0_group2_block5_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3803 (x.3804: f32[], y.3805: f32[]) -> f32[] {
  %x.3804 = f32[] parameter(0)
  %y.3805 = f32[] parameter(1)
  ROOT %add.3806 = f32[] add(f32[] %x.3804, f32[] %y.3805)
}

%fused_computation.96 (param_0.619: f32[1,1024,50,76], param_1.904: f32[1,1024,50,76], param_2.790: f32[1,1024,50,76], param_3.499: f32[1,1024,50,76], param_4.244: f32[1,1024,50,76]) -> (f32[1024], f32[1024], f32[1,1024,50,76]) {
  %param_4.244 = f32[1,1024,50,76]{3,2,1,0} parameter(4)
  %constant_273 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.223.clone.1 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_273), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.29.clone.1 = pred[1,1024,50,76]{3,2,1,0} compare(f32[1,1024,50,76]{3,2,1,0} %param_4.244, f32[1,1024,50,76]{3,2,1,0} %broadcast.223.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %param_2.790 = f32[1,1024,50,76]{3,2,1,0} parameter(2)
  %param_3.499 = f32[1,1024,50,76]{3,2,1,0} parameter(3)
  %add.137.clone.1 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_2.790, f32[1,1024,50,76]{3,2,1,0} %param_3.499), metadata={op_type="AddN" op_name="tower0/gradients/AddN_130"}
  %param_1.904 = f32[1,1024,50,76]{3,2,1,0} parameter(1)
  %add.136.clone.1 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %add.137.clone.1, f32[1,1024,50,76]{3,2,1,0} %param_1.904), metadata={op_type="AddN" op_name="tower0/gradients/AddN_130"}
  %select.29.clone.1 = f32[1,1024,50,76]{3,2,1,0} select(pred[1,1024,50,76]{3,2,1,0} %compare.29.clone.1, f32[1,1024,50,76]{3,2,1,0} %add.136.clone.1, f32[1,1024,50,76]{3,2,1,0} %broadcast.223.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %param_0.619 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %multiply.133 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %select.29.clone.1, f32[1,1024,50,76]{3,2,1,0} %param_0.619), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.44 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %multiply.133, f32[] %constant_273), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3793, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.74 = f32[1024]{0} reduce(f32[1,1024,50,76]{3,2,1,0} %select.29.clone.1, f32[] %constant_273), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3803, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.208 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) tuple(f32[1024]{0} %reduce.44, f32[1024]{0} %reduce.74, f32[1,1024,50,76]{3,2,1,0} %select.29.clone.1)
}

%fused_computation.97 (param_0.259: f32[1,1,256,1024], param_1.720: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.259 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.720 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.416 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.720), metadata={op_name="XLA_Args"}
  %constant_334 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.184 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_334), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.134 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.416, f32[1,1,256,1024]{1,0,2,3} %broadcast.184), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul_1"}
  %add.70 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.259, f32[1,1,256,1024]{1,0,2,3} %multiply.134), metadata={op_type="AddN" op_name="tower0/gradients/AddN_133"}
  ROOT %copy.415 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.70), metadata={op_name="XLA_Retvals"}
}

%fused_computation.98 (param_0.262: f32[1,512,1,1], param_1.281: f32[1,512,1,1], param_2.215: f32[512], param_3.123: f32[512]) -> f32[512] {
  %param_3.123 = f32[512]{0} parameter(3)
  %bitcast.321 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.123), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.215 = f32[512]{0} parameter(2)
  %negate.78 = f32[512]{0} negate(f32[512]{0} %param_2.215), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.320 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.78), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.281 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.136 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.320, f32[1,512,1,1]{3,2,1,0} %param_1.281), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.71 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.321, f32[1,512,1,1]{3,2,1,0} %multiply.136), metadata={op_type="AddN" op_name="tower0/gradients/AddN_132"}
  %param_0.262 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.135 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.71, f32[1,512,1,1]{3,2,1,0} %param_0.262), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.319 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.135), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.1849 (x.1850: f32[], y.1851: f32[]) -> f32[] {
  %x.1850 = f32[] parameter(0)
  %y.1851 = f32[] parameter(1)
  ROOT %add.1852 = f32[] add(f32[] %x.1850, f32[] %y.1851)
}

%tower0_gradients_tower0_group3_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.1859 (x.1860: f32[], y.1861: f32[]) -> f32[] {
  %x.1860 = f32[] parameter(0)
  %y.1861 = f32[] parameter(1)
  ROOT %add.1862 = f32[] add(f32[] %x.1860, f32[] %y.1861)
}

%fused_computation.99 (param_0.618: f32[1,512,50,76], param_1.905: f32[1,512,51,77], param_2.791: f32[1,512,50,76]) -> (f32[512], f32[512], f32[1,512,50,76]) {
  %param_2.791 = f32[1,512,50,76]{3,2,1,0} parameter(2)
  %constant_274 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.115.clone.1 = f32[1,512,50,76]{3,2,1,0} broadcast(f32[] %constant_274), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv1/Relu_grad/ReluGrad"}
  %compare.30.clone.1 = pred[1,512,50,76]{3,2,1,0} compare(f32[1,512,50,76]{3,2,1,0} %param_2.791, f32[1,512,50,76]{3,2,1,0} %broadcast.115.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv1/Relu_grad/ReluGrad"}
  %param_1.905 = f32[1,512,51,77]{3,2,1,0} parameter(1)
  %slice.2.clone.1 = f32[1,512,50,76]{3,2,1,0} slice(f32[1,512,51,77]{3,2,1,0} %param_1.905), slice={[0:1], [0:512], [1:51], [1:77]}, metadata={op_type="Slice" op_name="tower0/gradients/tower0/group3/block0/Pad_grad/Slice_1"}
  %select.30.clone.1 = f32[1,512,50,76]{3,2,1,0} select(pred[1,512,50,76]{3,2,1,0} %compare.30.clone.1, f32[1,512,50,76]{3,2,1,0} %slice.2.clone.1, f32[1,512,50,76]{3,2,1,0} %broadcast.115.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv1/Relu_grad/ReluGrad"}
  %param_0.618 = f32[1,512,50,76]{3,2,1,0} parameter(0)
  %multiply.137 = f32[1,512,50,76]{3,2,1,0} multiply(f32[1,512,50,76]{3,2,1,0} %select.30.clone.1, f32[1,512,50,76]{3,2,1,0} %param_0.618), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.45 = f32[512]{0} reduce(f32[1,512,50,76]{3,2,1,0} %multiply.137, f32[] %constant_274), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.1849, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.81 = f32[512]{0} reduce(f32[1,512,50,76]{3,2,1,0} %select.30.clone.1, f32[] %constant_274), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.1859, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.207 = (f32[512]{0}, f32[512]{0}, f32[1,512,50,76]{3,2,1,0}) tuple(f32[512]{0} %reduce.45, f32[512]{0} %reduce.81, f32[1,512,50,76]{3,2,1,0} %select.30.clone.1)
}

%fused_computation.100 (param_0.267: f32[1,1,1024,512], param_1.288: f32[1,1,1024,512]) -> f32[1,1,1024,512] {
  %param_0.267 = f32[1,1,1024,512]{1,0,2,3} parameter(0)
  %param_1.288 = f32[1,1,1024,512]{3,2,1,0} parameter(1)
  %copy.418 = f32[1,1,1024,512]{1,0,2,3} copy(f32[1,1,1024,512]{3,2,1,0} %param_1.288), metadata={op_name="XLA_Args"}
  %constant_275 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.70 = f32[1,1,1024,512]{1,0,2,3} broadcast(f32[] %constant_275), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_32_grad/Mul"}
  %multiply.138 = f32[1,1,1024,512]{1,0,2,3} multiply(f32[1,1,1024,512]{1,0,2,3} %copy.418, f32[1,1,1024,512]{1,0,2,3} %broadcast.70), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_32_grad/Mul_1"}
  %add.72 = f32[1,1,1024,512]{1,0,2,3} add(f32[1,1,1024,512]{1,0,2,3} %param_0.267, f32[1,1,1024,512]{1,0,2,3} %multiply.138), metadata={op_type="AddN" op_name="tower0/gradients/AddN_131"}
  ROOT %copy.417 = f32[1,1,1024,512]{3,2,1,0} copy(f32[1,1,1024,512]{1,0,2,3} %add.72), metadata={op_name="XLA_Retvals"}
}

%fused_computation.101 (param_0.270: f32[1,512,1,1], param_1.291: f32[1,512,1,1], param_2.223: f32[512], param_3.127: f32[512]) -> f32[512] {
  %param_3.127 = f32[512]{0} parameter(3)
  %bitcast.324 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.127), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.223 = f32[512]{0} parameter(2)
  %negate.79 = f32[512]{0} negate(f32[512]{0} %param_2.223), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.323 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.79), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.291 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.140 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.323, f32[1,512,1,1]{3,2,1,0} %param_1.291), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.73 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.324, f32[1,512,1,1]{3,2,1,0} %multiply.140), metadata={op_type="AddN" op_name="tower0/gradients/AddN_129"}
  %param_0.270 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.139 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.73, f32[1,512,1,1]{3,2,1,0} %param_0.270), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.322 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.139), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.1876 (x.1877: f32[], y.1878: f32[]) -> f32[] {
  %x.1877 = f32[] parameter(0)
  %y.1878 = f32[] parameter(1)
  ROOT %add.1879 = f32[] add(f32[] %x.1877, f32[] %y.1878)
}

%tower0_gradients_tower0_group3_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.1886 (x.1887: f32[], y.1888: f32[]) -> f32[] {
  %x.1887 = f32[] parameter(0)
  %y.1888 = f32[] parameter(1)
  ROOT %add.1889 = f32[] add(f32[] %x.1887, f32[] %y.1888)
}

%fused_computation.102 (param_0.617: f32[1,512,25,38], param_1.906: f32[1,512,25,38], param_2.792: f32[1,512,25,38]) -> (f32[512], f32[512], f32[1,512,25,38]) {
  %param_2.792 = f32[1,512,25,38]{3,2,1,0} parameter(2)
  %constant_276 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.224.clone.1 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_276), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.31.clone.1 = pred[1,512,25,38]{3,2,1,0} compare(f32[1,512,25,38]{3,2,1,0} %param_2.792, f32[1,512,25,38]{3,2,1,0} %broadcast.224.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv2/Relu_grad/ReluGrad"}
  %param_1.906 = f32[1,512,25,38]{3,2,1,0} parameter(1)
  %select.31.clone.1 = f32[1,512,25,38]{3,2,1,0} select(pred[1,512,25,38]{3,2,1,0} %compare.31.clone.1, f32[1,512,25,38]{3,2,1,0} %param_1.906, f32[1,512,25,38]{3,2,1,0} %broadcast.224.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv2/Relu_grad/ReluGrad"}
  %param_0.617 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %multiply.141 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %select.31.clone.1, f32[1,512,25,38]{3,2,1,0} %param_0.617), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.46 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %multiply.141, f32[] %constant_276), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.1876, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.102 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %select.31.clone.1, f32[] %constant_276), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.1886, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.206 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) tuple(f32[512]{0} %reduce.46, f32[512]{0} %reduce.102, f32[1,512,25,38]{3,2,1,0} %select.31.clone.1)
}

%fused_computation.103 (param_0.275: f32[3,3,512,512], param_1.721: f32[3,3,512,512]) -> f32[3,3,512,512] {
  %param_0.275 = f32[3,3,512,512]{1,0,2,3} parameter(0)
  %param_1.721 = f32[3,3,512,512]{3,2,1,0} parameter(1)
  %copy.470 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %param_1.721), metadata={op_name="XLA_Args"}
  %constant_335 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.185 = f32[3,3,512,512]{1,0,2,3} broadcast(f32[] %constant_335), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_33_grad/Mul"}
  %multiply.142 = f32[3,3,512,512]{1,0,2,3} multiply(f32[3,3,512,512]{1,0,2,3} %copy.470, f32[3,3,512,512]{1,0,2,3} %broadcast.185), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_33_grad/Mul_1"}
  %add.74 = f32[3,3,512,512]{1,0,2,3} add(f32[3,3,512,512]{1,0,2,3} %param_0.275, f32[3,3,512,512]{1,0,2,3} %multiply.142), metadata={op_type="AddN" op_name="tower0/gradients/AddN_128"}
  ROOT %copy.419 = f32[3,3,512,512]{3,2,1,0} copy(f32[3,3,512,512]{1,0,2,3} %add.74), metadata={op_name="XLA_Retvals"}
}

%fused_computation.106 (param_0.284: f32[1,2048,1,1], param_1.793: f32[1,2048,1,1], param_2.576: f32[2048], param_3.294: f32[2048], param_4.84: f32[1,2048,1,1], param_5.73: f32[1,2048,1,1], param_6.66: f32[2048]) -> (f32[2048], f32[2048]) {
  %param_2.576 = f32[2048]{0} parameter(2)
  %bitcast.328 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_2.576), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_3.294 = f32[2048]{0} parameter(3)
  %negate.100 = f32[2048]{0} negate(f32[2048]{0} %param_3.294), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.411 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %negate.100), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.793 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %multiply.147 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.411, f32[1,2048,1,1]{3,2,1,0} %param_1.793), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.76 = f32[1,2048,1,1]{3,2,1,0} add(f32[1,2048,1,1]{3,2,1,0} %bitcast.328, f32[1,2048,1,1]{3,2,1,0} %multiply.147), metadata={op_type="AddN" op_name="tower0/gradients/AddN_125"}
  %param_0.284 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  %multiply.146 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %add.76, f32[1,2048,1,1]{3,2,1,0} %param_0.284), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.327 = f32[2048]{0} bitcast(f32[1,2048,1,1]{3,2,1,0} %multiply.146), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/Reshape_grad/Reshape"}
  %param_6.66 = f32[2048]{0} parameter(6)
  %bitcast.326.clone.1 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_6.66), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_5.73 = f32[1,2048,1,1]{3,2,1,0} parameter(5)
  %multiply.144.clone.1 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.411, f32[1,2048,1,1]{3,2,1,0} %param_5.73), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.75.clone.1 = f32[1,2048,1,1]{3,2,1,0} add(f32[1,2048,1,1]{3,2,1,0} %bitcast.326.clone.1, f32[1,2048,1,1]{3,2,1,0} %multiply.144.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_127"}
  %param_4.84 = f32[1,2048,1,1]{3,2,1,0} parameter(4)
  %multiply.143.clone.1 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %add.75.clone.1, f32[1,2048,1,1]{3,2,1,0} %param_4.84), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.325.clone.1 = f32[2048]{0} bitcast(f32[1,2048,1,1]{3,2,1,0} %multiply.143.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/Reshape_grad/Reshape"}
  ROOT %tuple.184 = (f32[2048]{0}, f32[2048]{0}) tuple(f32[2048]{0} %bitcast.327, f32[2048]{0} %bitcast.325.clone.1)
}

%tower0_gradients_tower0_group3_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.1903 (x.1904: f32[], y.1905: f32[]) -> f32[] {
  %x.1904 = f32[] parameter(0)
  %y.1905 = f32[] parameter(1)
  ROOT %add.1906 = f32[] add(f32[] %x.1904, f32[] %y.1905)
}

%tower0_gradients_tower0_group3_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3820 (x.3821: f32[], y.3822: f32[]) -> f32[] {
  %x.3821 = f32[] parameter(0)
  %y.3822 = f32[] parameter(1)
  ROOT %add.3823 = f32[] add(f32[] %x.3821, f32[] %y.3822)
}

%tower0_gradients_tower0_group3_block0_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.1913 (x.1914: f32[], y.1915: f32[]) -> f32[] {
  %x.1914 = f32[] parameter(0)
  %y.1915 = f32[] parameter(1)
  ROOT %add.1916 = f32[] add(f32[] %x.1914, f32[] %y.1915)
}

%fused_computation.108 (param_0.616: f32[1,2048,25,38], param_1.842: f32[1,2048,25,38], param_2.793: f32[1,2048,25,38], param_3.500: f32[1,2048,25,38], param_4.245: f32[1,2048,25,38]) -> (f32[2048], f32[2048], f32[2048], f32[1,2048,25,38]) {
  %param_4.245 = f32[1,2048,25,38]{3,2,1,0} parameter(4)
  %constant_278 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.225.clone.1 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[] %constant_278), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %compare.32.clone.1 = pred[1,2048,25,38]{3,2,1,0} compare(f32[1,2048,25,38]{3,2,1,0} %param_4.245, f32[1,2048,25,38]{3,2,1,0} %broadcast.225.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/output_grad/ReluGrad"}
  %param_2.793 = f32[1,2048,25,38]{3,2,1,0} parameter(2)
  %param_3.500 = f32[1,2048,25,38]{3,2,1,0} parameter(3)
  %add.138.clone.1 = f32[1,2048,25,38]{3,2,1,0} add(f32[1,2048,25,38]{3,2,1,0} %param_2.793, f32[1,2048,25,38]{3,2,1,0} %param_3.500), metadata={op_type="AddN" op_name="tower0/gradients/AddN_121"}
  %select.32.clone.1 = f32[1,2048,25,38]{3,2,1,0} select(pred[1,2048,25,38]{3,2,1,0} %compare.32.clone.1, f32[1,2048,25,38]{3,2,1,0} %add.138.clone.1, f32[1,2048,25,38]{3,2,1,0} %broadcast.225.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/output_grad/ReluGrad"}
  %param_0.616 = f32[1,2048,25,38]{3,2,1,0} parameter(0)
  %multiply.148 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %select.32.clone.1, f32[1,2048,25,38]{3,2,1,0} %param_0.616), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.48 = f32[2048]{0} reduce(f32[1,2048,25,38]{3,2,1,0} %multiply.148, f32[] %constant_278), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.1903, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %param_1.842 = f32[1,2048,25,38]{3,2,1,0} parameter(1)
  %multiply.145.clone.1 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %select.32.clone.1, f32[1,2048,25,38]{3,2,1,0} %param_1.842), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.47.clone.1 = f32[2048]{0} reduce(f32[1,2048,25,38]{3,2,1,0} %multiply.145.clone.1, f32[] %constant_278), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3820, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.89 = f32[2048]{0} reduce(f32[1,2048,25,38]{3,2,1,0} %select.32.clone.1, f32[] %constant_278), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.1913, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.205 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) tuple(f32[2048]{0} %reduce.48, f32[2048]{0} %reduce.47.clone.1, f32[2048]{0} %reduce.89, f32[1,2048,25,38]{3,2,1,0} %select.32.clone.1)
}

%fused_computation.109 (param_0.291: f32[1,1,1024,2048], param_1.722: f32[1,1,1024,2048]) -> f32[1,1,1024,2048] {
  %param_0.291 = f32[1,1,1024,2048]{1,0,2,3} parameter(0)
  %param_1.722 = f32[1,1,1024,2048]{3,2,1,0} parameter(1)
  %copy.471 = f32[1,1,1024,2048]{1,0,2,3} copy(f32[1,1,1024,2048]{3,2,1,0} %param_1.722), metadata={op_name="XLA_Args"}
  %constant_279 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.71 = f32[1,1,1024,2048]{1,0,2,3} broadcast(f32[] %constant_279), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_35_grad/Mul"}
  %multiply.149 = f32[1,1,1024,2048]{1,0,2,3} multiply(f32[1,1,1024,2048]{1,0,2,3} %copy.471, f32[1,1,1024,2048]{1,0,2,3} %broadcast.71), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_35_grad/Mul_1"}
  %add.77 = f32[1,1,1024,2048]{1,0,2,3} add(f32[1,1,1024,2048]{1,0,2,3} %param_0.291, f32[1,1,1024,2048]{1,0,2,3} %multiply.149), metadata={op_type="AddN" op_name="tower0/gradients/AddN_126"}
  ROOT %copy.420 = f32[1,1,1024,2048]{3,2,1,0} copy(f32[1,1,1024,2048]{1,0,2,3} %add.77), metadata={op_name="XLA_Retvals"}
}

%fused_computation.110 (param_0.293: f32[1,1,512,2048], param_1.723: f32[1,1,512,2048]) -> f32[1,1,512,2048] {
  %param_0.293 = f32[1,1,512,2048]{1,0,2,3} parameter(0)
  %param_1.723 = f32[1,1,512,2048]{3,2,1,0} parameter(1)
  %copy.422 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %param_1.723), metadata={op_name="XLA_Args"}
  %constant_336 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.186 = f32[1,1,512,2048]{1,0,2,3} broadcast(f32[] %constant_336), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_41_grad/Mul"}
  %multiply.150 = f32[1,1,512,2048]{1,0,2,3} multiply(f32[1,1,512,2048]{1,0,2,3} %copy.422, f32[1,1,512,2048]{1,0,2,3} %broadcast.186), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_34_grad/Mul_1"}
  %add.78 = f32[1,1,512,2048]{1,0,2,3} add(f32[1,1,512,2048]{1,0,2,3} %param_0.293, f32[1,1,512,2048]{1,0,2,3} %multiply.150), metadata={op_type="AddN" op_name="tower0/gradients/AddN_124"}
  ROOT %copy.421 = f32[1,1,512,2048]{3,2,1,0} copy(f32[1,1,512,2048]{1,0,2,3} %add.78), metadata={op_name="XLA_Retvals"}
}

%fused_computation.111 (param_0.296: f32[1,512,1,1], param_1.316: f32[1,512,1,1], param_2.241: f32[512], param_3.135: f32[512]) -> f32[512] {
  %param_3.135 = f32[512]{0} parameter(3)
  %bitcast.332 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.135), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.241 = f32[512]{0} parameter(2)
  %negate.81 = f32[512]{0} negate(f32[512]{0} %param_2.241), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.331 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.81), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.316 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.152 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.331, f32[1,512,1,1]{3,2,1,0} %param_1.316), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.79 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.332, f32[1,512,1,1]{3,2,1,0} %multiply.152), metadata={op_type="AddN" op_name="tower0/gradients/AddN_123"}
  %param_0.296 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.151 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.79, f32[1,512,1,1]{3,2,1,0} %param_0.296), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.330 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.151), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3847 (x.3848: f32[], y.3849: f32[]) -> f32[] {
  %x.3848 = f32[] parameter(0)
  %y.3849 = f32[] parameter(1)
  ROOT %add.3850 = f32[] add(f32[] %x.3848, f32[] %y.3849)
}

%tower0_gradients_tower0_group3_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3857 (x.3858: f32[], y.3859: f32[]) -> f32[] {
  %x.3858 = f32[] parameter(0)
  %y.3859 = f32[] parameter(1)
  ROOT %add.3860 = f32[] add(f32[] %x.3858, f32[] %y.3859)
}

%fused_computation.112 (param_0.615: f32[1,512,25,38], param_1.907: f32[1,512,25,38], param_2.794: f32[1,512,25,38]) -> (f32[512], f32[512], f32[1,512,25,38]) {
  %param_2.794 = f32[1,512,25,38]{3,2,1,0} parameter(2)
  %constant_280 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.226.clone.1 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_280), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.33.clone.1 = pred[1,512,25,38]{3,2,1,0} compare(f32[1,512,25,38]{3,2,1,0} %param_2.794, f32[1,512,25,38]{3,2,1,0} %broadcast.226.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/conv1/Relu_grad/ReluGrad"}
  %param_1.907 = f32[1,512,25,38]{3,2,1,0} parameter(1)
  %select.33.clone.1 = f32[1,512,25,38]{3,2,1,0} select(pred[1,512,25,38]{3,2,1,0} %compare.33.clone.1, f32[1,512,25,38]{3,2,1,0} %param_1.907, f32[1,512,25,38]{3,2,1,0} %broadcast.226.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/conv1/Relu_grad/ReluGrad"}
  %param_0.615 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %multiply.153 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %select.33.clone.1, f32[1,512,25,38]{3,2,1,0} %param_0.615), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.49 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %multiply.153, f32[] %constant_280), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3847, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.104 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %select.33.clone.1, f32[] %constant_280), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3857, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.204 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) tuple(f32[512]{0} %reduce.49, f32[512]{0} %reduce.104, f32[1,512,25,38]{3,2,1,0} %select.33.clone.1)
}

%fused_computation.113 (param_0.301: f32[1,1,2048,512], param_1.724: f32[1,1,2048,512]) -> f32[1,1,2048,512] {
  %param_0.301 = f32[1,1,2048,512]{1,0,2,3} parameter(0)
  %param_1.724 = f32[1,1,2048,512]{3,2,1,0} parameter(1)
  %copy.424 = f32[1,1,2048,512]{1,0,2,3} copy(f32[1,1,2048,512]{3,2,1,0} %param_1.724), metadata={op_name="XLA_Args"}
  %constant_337 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.187 = f32[1,1,2048,512]{1,0,2,3} broadcast(f32[] %constant_337), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_39_grad/Mul"}
  %multiply.154 = f32[1,1,2048,512]{1,0,2,3} multiply(f32[1,1,2048,512]{1,0,2,3} %copy.424, f32[1,1,2048,512]{1,0,2,3} %broadcast.187), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_36_grad/Mul_1"}
  %add.80 = f32[1,1,2048,512]{1,0,2,3} add(f32[1,1,2048,512]{1,0,2,3} %param_0.301, f32[1,1,2048,512]{1,0,2,3} %multiply.154), metadata={op_type="AddN" op_name="tower0/gradients/AddN_122"}
  ROOT %copy.423 = f32[1,1,2048,512]{3,2,1,0} copy(f32[1,1,2048,512]{1,0,2,3} %add.80), metadata={op_name="XLA_Retvals"}
}

%fused_computation.114 (param_0.304: f32[1,512,1,1], param_1.325: f32[1,512,1,1], param_2.248: f32[512], param_3.139: f32[512]) -> f32[512] {
  %param_3.139 = f32[512]{0} parameter(3)
  %bitcast.335 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.139), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.248 = f32[512]{0} parameter(2)
  %negate.82 = f32[512]{0} negate(f32[512]{0} %param_2.248), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.334 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.82), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.325 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.156 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.334, f32[1,512,1,1]{3,2,1,0} %param_1.325), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.81 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.335, f32[1,512,1,1]{3,2,1,0} %multiply.156), metadata={op_type="AddN" op_name="tower0/gradients/AddN_120"}
  %param_0.304 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.155 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.81, f32[1,512,1,1]{3,2,1,0} %param_0.304), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.333 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.155), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3874 (x.3875: f32[], y.3876: f32[]) -> f32[] {
  %x.3875 = f32[] parameter(0)
  %y.3876 = f32[] parameter(1)
  ROOT %add.3877 = f32[] add(f32[] %x.3875, f32[] %y.3876)
}

%tower0_gradients_tower0_group3_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3884 (x.3885: f32[], y.3886: f32[]) -> f32[] {
  %x.3885 = f32[] parameter(0)
  %y.3886 = f32[] parameter(1)
  ROOT %add.3887 = f32[] add(f32[] %x.3885, f32[] %y.3886)
}

%fused_computation.115 (param_0.614: f32[1,512,25,38], param_1.908: f32[1,512,25,38], param_2.795: f32[1,512,25,38]) -> (f32[512], f32[512], f32[1,512,25,38]) {
  %param_2.795 = f32[1,512,25,38]{3,2,1,0} parameter(2)
  %constant_281 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.227.clone.1 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_281), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.34.clone.1 = pred[1,512,25,38]{3,2,1,0} compare(f32[1,512,25,38]{3,2,1,0} %param_2.795, f32[1,512,25,38]{3,2,1,0} %broadcast.227.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/conv2/Relu_grad/ReluGrad"}
  %param_1.908 = f32[1,512,25,38]{3,2,1,0} parameter(1)
  %select.34.clone.1 = f32[1,512,25,38]{3,2,1,0} select(pred[1,512,25,38]{3,2,1,0} %compare.34.clone.1, f32[1,512,25,38]{3,2,1,0} %param_1.908, f32[1,512,25,38]{3,2,1,0} %broadcast.227.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/conv2/Relu_grad/ReluGrad"}
  %param_0.614 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %multiply.157 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %select.34.clone.1, f32[1,512,25,38]{3,2,1,0} %param_0.614), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.50 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %multiply.157, f32[] %constant_281), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3874, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.103 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %select.34.clone.1, f32[] %constant_281), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3884, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.203 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) tuple(f32[512]{0} %reduce.50, f32[512]{0} %reduce.103, f32[1,512,25,38]{3,2,1,0} %select.34.clone.1)
}

%fused_computation.116 (param_0.309: f32[3,3,512,512], param_1.725: f32[3,3,512,512]) -> f32[3,3,512,512] {
  %param_0.309 = f32[3,3,512,512]{1,0,2,3} parameter(0)
  %param_1.725 = f32[3,3,512,512]{3,2,1,0} parameter(1)
  %copy.472 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %param_1.725), metadata={op_name="XLA_Args"}
  %constant_338 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.188 = f32[3,3,512,512]{1,0,2,3} broadcast(f32[] %constant_338), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_33_grad/Mul"}
  %multiply.158 = f32[3,3,512,512]{1,0,2,3} multiply(f32[3,3,512,512]{1,0,2,3} %copy.472, f32[3,3,512,512]{1,0,2,3} %broadcast.188), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_37_grad/Mul_1"}
  %add.82 = f32[3,3,512,512]{1,0,2,3} add(f32[3,3,512,512]{1,0,2,3} %param_0.309, f32[3,3,512,512]{1,0,2,3} %multiply.158), metadata={op_type="AddN" op_name="tower0/gradients/AddN_119"}
  ROOT %copy.425 = f32[3,3,512,512]{3,2,1,0} copy(f32[3,3,512,512]{1,0,2,3} %add.82), metadata={op_name="XLA_Retvals"}
}

%fused_computation.117 (param_0.312: f32[1,2048,1,1], param_1.333: f32[1,2048,1,1], param_2.254: f32[2048], param_3.143: f32[2048]) -> f32[2048] {
  %param_3.143 = f32[2048]{0} parameter(3)
  %bitcast.338 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_3.143), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.254 = f32[2048]{0} parameter(2)
  %negate.83 = f32[2048]{0} negate(f32[2048]{0} %param_2.254), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.337 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %negate.83), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.333 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %multiply.160 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.337, f32[1,2048,1,1]{3,2,1,0} %param_1.333), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.83 = f32[1,2048,1,1]{3,2,1,0} add(f32[1,2048,1,1]{3,2,1,0} %bitcast.338, f32[1,2048,1,1]{3,2,1,0} %multiply.160), metadata={op_type="AddN" op_name="tower0/gradients/AddN_118"}
  %param_0.312 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  %multiply.159 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %add.83, f32[1,2048,1,1]{3,2,1,0} %param_0.312), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.336 = f32[2048]{0} bitcast(f32[1,2048,1,1]{3,2,1,0} %multiply.159), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3901 (x.3902: f32[], y.3903: f32[]) -> f32[] {
  %x.3902 = f32[] parameter(0)
  %y.3903 = f32[] parameter(1)
  ROOT %add.3904 = f32[] add(f32[] %x.3902, f32[] %y.3903)
}

%tower0_gradients_tower0_group3_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3911 (x.3912: f32[], y.3913: f32[]) -> f32[] {
  %x.3912 = f32[] parameter(0)
  %y.3913 = f32[] parameter(1)
  ROOT %add.3914 = f32[] add(f32[] %x.3912, f32[] %y.3913)
}

%fused_computation.118 (param_0.613: f32[1,2048,25,38], param_1.909: f32[1,2048,25,38], param_2.796: f32[1,2048,25,38], param_3.501: f32[1,2048,25,38]) -> (f32[2048], f32[2048], f32[1,2048,25,38]) {
  %param_3.501 = f32[1,2048,25,38]{3,2,1,0} parameter(3)
  %constant_282 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.228.clone.1 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[] %constant_282), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %compare.35.clone.1 = pred[1,2048,25,38]{3,2,1,0} compare(f32[1,2048,25,38]{3,2,1,0} %param_3.501, f32[1,2048,25,38]{3,2,1,0} %broadcast.228.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/output_grad/ReluGrad"}
  %param_1.909 = f32[1,2048,25,38]{3,2,1,0} parameter(1)
  %param_2.796 = f32[1,2048,25,38]{3,2,1,0} parameter(2)
  %add.139.clone.1 = f32[1,2048,25,38]{3,2,1,0} add(f32[1,2048,25,38]{3,2,1,0} %param_1.909, f32[1,2048,25,38]{3,2,1,0} %param_2.796), metadata={op_type="AddN" op_name="tower0/gradients/AddN_114"}
  %select.35.clone.1 = f32[1,2048,25,38]{3,2,1,0} select(pred[1,2048,25,38]{3,2,1,0} %compare.35.clone.1, f32[1,2048,25,38]{3,2,1,0} %add.139.clone.1, f32[1,2048,25,38]{3,2,1,0} %broadcast.228.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/output_grad/ReluGrad"}
  %param_0.613 = f32[1,2048,25,38]{3,2,1,0} parameter(0)
  %multiply.161 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %select.35.clone.1, f32[1,2048,25,38]{3,2,1,0} %param_0.613), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.51 = f32[2048]{0} reduce(f32[1,2048,25,38]{3,2,1,0} %multiply.161, f32[] %constant_282), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3901, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.90 = f32[2048]{0} reduce(f32[1,2048,25,38]{3,2,1,0} %select.35.clone.1, f32[] %constant_282), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3911, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.202 = (f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) tuple(f32[2048]{0} %reduce.51, f32[2048]{0} %reduce.90, f32[1,2048,25,38]{3,2,1,0} %select.35.clone.1)
}

%fused_computation.119 (param_0.317: f32[1,1,512,2048], param_1.726: f32[1,1,512,2048]) -> f32[1,1,512,2048] {
  %param_0.317 = f32[1,1,512,2048]{1,0,2,3} parameter(0)
  %param_1.726 = f32[1,1,512,2048]{3,2,1,0} parameter(1)
  %copy.427 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %param_1.726), metadata={op_name="XLA_Args"}
  %constant_339 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.189 = f32[1,1,512,2048]{1,0,2,3} broadcast(f32[] %constant_339), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_41_grad/Mul"}
  %multiply.162 = f32[1,1,512,2048]{1,0,2,3} multiply(f32[1,1,512,2048]{1,0,2,3} %copy.427, f32[1,1,512,2048]{1,0,2,3} %broadcast.189), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_38_grad/Mul_1"}
  %add.84 = f32[1,1,512,2048]{1,0,2,3} add(f32[1,1,512,2048]{1,0,2,3} %param_0.317, f32[1,1,512,2048]{1,0,2,3} %multiply.162), metadata={op_type="AddN" op_name="tower0/gradients/AddN_117"}
  ROOT %copy.426 = f32[1,1,512,2048]{3,2,1,0} copy(f32[1,1,512,2048]{1,0,2,3} %add.84), metadata={op_name="XLA_Retvals"}
}

%fused_computation.120 (param_0.320: f32[1,512,1,1], param_1.342: f32[1,512,1,1], param_2.261: f32[512], param_3.147: f32[512]) -> f32[512] {
  %param_3.147 = f32[512]{0} parameter(3)
  %bitcast.341 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.147), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.261 = f32[512]{0} parameter(2)
  %negate.84 = f32[512]{0} negate(f32[512]{0} %param_2.261), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.340 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.84), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.342 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.164 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.340, f32[1,512,1,1]{3,2,1,0} %param_1.342), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.85 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.341, f32[1,512,1,1]{3,2,1,0} %multiply.164), metadata={op_type="AddN" op_name="tower0/gradients/AddN_116"}
  %param_0.320 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.163 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.85, f32[1,512,1,1]{3,2,1,0} %param_0.320), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.339 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.163), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3928 (x.3929: f32[], y.3930: f32[]) -> f32[] {
  %x.3929 = f32[] parameter(0)
  %y.3930 = f32[] parameter(1)
  ROOT %add.3931 = f32[] add(f32[] %x.3929, f32[] %y.3930)
}

%tower0_gradients_tower0_group3_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3938 (x.3939: f32[], y.3940: f32[]) -> f32[] {
  %x.3939 = f32[] parameter(0)
  %y.3940 = f32[] parameter(1)
  ROOT %add.3941 = f32[] add(f32[] %x.3939, f32[] %y.3940)
}

%fused_computation.121 (param_0.612: f32[1,512,25,38], param_1.910: f32[1,512,25,38], param_2.797: f32[1,512,25,38]) -> (f32[512], f32[512], f32[1,512,25,38]) {
  %param_2.797 = f32[1,512,25,38]{3,2,1,0} parameter(2)
  %constant_283 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.229.clone.1 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_283), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.36.clone.1 = pred[1,512,25,38]{3,2,1,0} compare(f32[1,512,25,38]{3,2,1,0} %param_2.797, f32[1,512,25,38]{3,2,1,0} %broadcast.229.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv1/Relu_grad/ReluGrad"}
  %param_1.910 = f32[1,512,25,38]{3,2,1,0} parameter(1)
  %select.36.clone.1 = f32[1,512,25,38]{3,2,1,0} select(pred[1,512,25,38]{3,2,1,0} %compare.36.clone.1, f32[1,512,25,38]{3,2,1,0} %param_1.910, f32[1,512,25,38]{3,2,1,0} %broadcast.229.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv1/Relu_grad/ReluGrad"}
  %param_0.612 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %multiply.165 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %select.36.clone.1, f32[1,512,25,38]{3,2,1,0} %param_0.612), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.52 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %multiply.165, f32[] %constant_283), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3928, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.106 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %select.36.clone.1, f32[] %constant_283), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3938, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.201 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) tuple(f32[512]{0} %reduce.52, f32[512]{0} %reduce.106, f32[1,512,25,38]{3,2,1,0} %select.36.clone.1)
}

%fused_computation.122 (param_0.325: f32[1,1,2048,512], param_1.727: f32[1,1,2048,512]) -> f32[1,1,2048,512] {
  %param_0.325 = f32[1,1,2048,512]{1,0,2,3} parameter(0)
  %param_1.727 = f32[1,1,2048,512]{3,2,1,0} parameter(1)
  %copy.429 = f32[1,1,2048,512]{1,0,2,3} copy(f32[1,1,2048,512]{3,2,1,0} %param_1.727), metadata={op_name="XLA_Args"}
  %constant_340 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.190 = f32[1,1,2048,512]{1,0,2,3} broadcast(f32[] %constant_340), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_39_grad/Mul"}
  %multiply.166 = f32[1,1,2048,512]{1,0,2,3} multiply(f32[1,1,2048,512]{1,0,2,3} %copy.429, f32[1,1,2048,512]{1,0,2,3} %broadcast.190), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_39_grad/Mul_1"}
  %add.86 = f32[1,1,2048,512]{1,0,2,3} add(f32[1,1,2048,512]{1,0,2,3} %param_0.325, f32[1,1,2048,512]{1,0,2,3} %multiply.166), metadata={op_type="AddN" op_name="tower0/gradients/AddN_115"}
  ROOT %copy.428 = f32[1,1,2048,512]{3,2,1,0} copy(f32[1,1,2048,512]{1,0,2,3} %add.86), metadata={op_name="XLA_Retvals"}
}

%fused_computation.123 (param_0.328: f32[1,512,1,1], param_1.351: f32[1,512,1,1], param_2.268: f32[512], param_3.151: f32[512]) -> f32[512] {
  %param_3.151 = f32[512]{0} parameter(3)
  %bitcast.344 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.151), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.268 = f32[512]{0} parameter(2)
  %negate.85 = f32[512]{0} negate(f32[512]{0} %param_2.268), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.343 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.85), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.351 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.168 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.343, f32[1,512,1,1]{3,2,1,0} %param_1.351), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.87 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.344, f32[1,512,1,1]{3,2,1,0} %multiply.168), metadata={op_type="AddN" op_name="tower0/gradients/AddN_113"}
  %param_0.328 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.167 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.87, f32[1,512,1,1]{3,2,1,0} %param_0.328), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.342 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.167), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3955 (x.3956: f32[], y.3957: f32[]) -> f32[] {
  %x.3956 = f32[] parameter(0)
  %y.3957 = f32[] parameter(1)
  ROOT %add.3958 = f32[] add(f32[] %x.3956, f32[] %y.3957)
}

%tower0_gradients_tower0_group3_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3965 (x.3966: f32[], y.3967: f32[]) -> f32[] {
  %x.3966 = f32[] parameter(0)
  %y.3967 = f32[] parameter(1)
  ROOT %add.3968 = f32[] add(f32[] %x.3966, f32[] %y.3967)
}

%fused_computation.124 (param_0.611: f32[1,512,25,38], param_1.911: f32[1,512,25,38], param_2.798: f32[1,512,25,38]) -> (f32[512], f32[512], f32[1,512,25,38]) {
  %param_2.798 = f32[1,512,25,38]{3,2,1,0} parameter(2)
  %constant_284 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.230.clone.1 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_284), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.37.clone.1 = pred[1,512,25,38]{3,2,1,0} compare(f32[1,512,25,38]{3,2,1,0} %param_2.798, f32[1,512,25,38]{3,2,1,0} %broadcast.230.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %param_1.911 = f32[1,512,25,38]{3,2,1,0} parameter(1)
  %select.37.clone.1 = f32[1,512,25,38]{3,2,1,0} select(pred[1,512,25,38]{3,2,1,0} %compare.37.clone.1, f32[1,512,25,38]{3,2,1,0} %param_1.911, f32[1,512,25,38]{3,2,1,0} %broadcast.230.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %param_0.611 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %multiply.169 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %select.37.clone.1, f32[1,512,25,38]{3,2,1,0} %param_0.611), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.53 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %multiply.169, f32[] %constant_284), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3955, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.105 = f32[512]{0} reduce(f32[1,512,25,38]{3,2,1,0} %select.37.clone.1, f32[] %constant_284), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3965, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.200 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) tuple(f32[512]{0} %reduce.53, f32[512]{0} %reduce.105, f32[1,512,25,38]{3,2,1,0} %select.37.clone.1)
}

%fused_computation.125 (param_0.333: f32[3,3,512,512], param_1.728: f32[3,3,512,512]) -> f32[3,3,512,512] {
  %param_0.333 = f32[3,3,512,512]{1,0,2,3} parameter(0)
  %param_1.728 = f32[3,3,512,512]{3,2,1,0} parameter(1)
  %copy.473 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %param_1.728), metadata={op_name="XLA_Args"}
  %constant_341 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.191 = f32[3,3,512,512]{1,0,2,3} broadcast(f32[] %constant_341), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_33_grad/Mul"}
  %multiply.170 = f32[3,3,512,512]{1,0,2,3} multiply(f32[3,3,512,512]{1,0,2,3} %copy.473, f32[3,3,512,512]{1,0,2,3} %broadcast.191), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_40_grad/Mul_1"}
  %add.88 = f32[3,3,512,512]{1,0,2,3} add(f32[3,3,512,512]{1,0,2,3} %param_0.333, f32[3,3,512,512]{1,0,2,3} %multiply.170), metadata={op_type="AddN" op_name="tower0/gradients/AddN_112"}
  ROOT %copy.430 = f32[3,3,512,512]{3,2,1,0} copy(f32[3,3,512,512]{1,0,2,3} %add.88), metadata={op_name="XLA_Retvals"}
}

%fused_computation.126 (param_0.336: f32[1,2048,1,1], param_1.359: f32[1,2048,1,1], param_2.274: f32[2048], param_3.155: f32[2048]) -> f32[2048] {
  %param_3.155 = f32[2048]{0} parameter(3)
  %bitcast.347 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_3.155), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.274 = f32[2048]{0} parameter(2)
  %negate.86 = f32[2048]{0} negate(f32[2048]{0} %param_2.274), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.346 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %negate.86), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.359 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %multiply.172 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.346, f32[1,2048,1,1]{3,2,1,0} %param_1.359), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.89 = f32[1,2048,1,1]{3,2,1,0} add(f32[1,2048,1,1]{3,2,1,0} %bitcast.347, f32[1,2048,1,1]{3,2,1,0} %multiply.172), metadata={op_type="AddN" op_name="tower0/gradients/AddN_111"}
  %param_0.336 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  %multiply.171 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %add.89, f32[1,2048,1,1]{3,2,1,0} %param_0.336), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.345 = f32[2048]{0} bitcast(f32[1,2048,1,1]{3,2,1,0} %multiply.171), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3982 (x.3983: f32[], y.3984: f32[]) -> f32[] {
  %x.3983 = f32[] parameter(0)
  %y.3984 = f32[] parameter(1)
  ROOT %add.3985 = f32[] add(f32[] %x.3983, f32[] %y.3984)
}

%tower0_gradients_tower0_group3_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3992 (x.3993: f32[], y.3994: f32[]) -> f32[] {
  %x.3993 = f32[] parameter(0)
  %y.3994 = f32[] parameter(1)
  ROOT %add.3995 = f32[] add(f32[] %x.3993, f32[] %y.3994)
}

%fused_computation.127 (param_0.610: f32[1,2048,25,38], param_1.912: f32[1,2048,25,38], param_2.799: f32[1,2048,25,38]) -> (f32[2048], f32[2048], f32[1,2048,25,38]) {
  %param_2.799 = f32[1,2048,25,38]{3,2,1,0} parameter(2)
  %constant_285 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.231.clone.1 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[] %constant_285), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %compare.38.clone.1 = pred[1,2048,25,38]{3,2,1,0} compare(f32[1,2048,25,38]{3,2,1,0} %param_2.799, f32[1,2048,25,38]{3,2,1,0} %broadcast.231.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %param_1.912 = f32[1,2048,25,38]{3,2,1,0} parameter(1)
  %select.38.clone.1 = f32[1,2048,25,38]{3,2,1,0} select(pred[1,2048,25,38]{3,2,1,0} %compare.38.clone.1, f32[1,2048,25,38]{3,2,1,0} %param_1.912, f32[1,2048,25,38]{3,2,1,0} %broadcast.231.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %param_0.610 = f32[1,2048,25,38]{3,2,1,0} parameter(0)
  %multiply.173 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %select.38.clone.1, f32[1,2048,25,38]{3,2,1,0} %param_0.610), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.54 = f32[2048]{0} reduce(f32[1,2048,25,38]{3,2,1,0} %multiply.173, f32[] %constant_285), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3982, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.80 = f32[2048]{0} reduce(f32[1,2048,25,38]{3,2,1,0} %select.38.clone.1, f32[] %constant_285), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3992, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.199 = (f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) tuple(f32[2048]{0} %reduce.54, f32[2048]{0} %reduce.80, f32[1,2048,25,38]{3,2,1,0} %select.38.clone.1)
}

%fused_computation.128 (param_0.341: f32[1,1,512,2048], param_1.729: f32[1,1,512,2048]) -> f32[1,1,512,2048] {
  %param_0.341 = f32[1,1,512,2048]{1,0,2,3} parameter(0)
  %param_1.729 = f32[1,1,512,2048]{3,2,1,0} parameter(1)
  %copy.432 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %param_1.729), metadata={op_name="XLA_Args"}
  %constant_342 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.192 = f32[1,1,512,2048]{1,0,2,3} broadcast(f32[] %constant_342), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_41_grad/Mul"}
  %multiply.174 = f32[1,1,512,2048]{1,0,2,3} multiply(f32[1,1,512,2048]{1,0,2,3} %copy.432, f32[1,1,512,2048]{1,0,2,3} %broadcast.192), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_41_grad/Mul_1"}
  %add.90 = f32[1,1,512,2048]{1,0,2,3} add(f32[1,1,512,2048]{1,0,2,3} %param_0.341, f32[1,1,512,2048]{1,0,2,3} %multiply.174), metadata={op_type="AddN" op_name="tower0/gradients/AddN_110"}
  ROOT %copy.431 = f32[1,1,512,2048]{3,2,1,0} copy(f32[1,1,512,2048]{1,0,2,3} %add.90), metadata={op_name="XLA_Retvals"}
}

%fused_computation.129 (param_0.343: f32[1,1,2048,256], param_1.369: f32[1,1,2048,256]) -> f32[1,1,2048,256] {
  %param_0.343 = f32[1,1,2048,256]{1,0,2,3} parameter(0)
  %param_1.369 = f32[1,1,2048,256]{3,2,1,0} parameter(1)
  %copy.434 = f32[1,1,2048,256]{1,0,2,3} copy(f32[1,1,2048,256]{3,2,1,0} %param_1.369), metadata={op_name="XLA_Args"}
  %constant_286 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.72 = f32[1,1,2048,256]{1,0,2,3} broadcast(f32[] %constant_286), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_45_grad/Mul"}
  %multiply.175 = f32[1,1,2048,256]{1,0,2,3} multiply(f32[1,1,2048,256]{1,0,2,3} %copy.434, f32[1,1,2048,256]{1,0,2,3} %broadcast.72), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_45_grad/Mul_1"}
  %add.91 = f32[1,1,2048,256]{1,0,2,3} add(f32[1,1,2048,256]{1,0,2,3} %param_0.343, f32[1,1,2048,256]{1,0,2,3} %multiply.175), metadata={op_type="AddN" op_name="tower0/gradients/AddN_109"}
  ROOT %copy.433 = f32[1,1,2048,256]{3,2,1,0} copy(f32[1,1,2048,256]{1,0,2,3} %add.91), metadata={op_name="XLA_Retvals"}
}

%fused_computation.130 (param_0.345: f32[1,1,1024,256], param_1.730: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.345 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.730 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.436 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.730), metadata={op_name="XLA_Args"}
  %constant_343 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.193 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_343), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.176 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.436, f32[1,1,1024,256]{1,0,2,3} %broadcast.193), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_44_grad/Mul_1"}
  %add.92 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.345, f32[1,1,1024,256]{1,0,2,3} %multiply.176), metadata={op_type="AddN" op_name="tower0/gradients/AddN_107"}
  ROOT %copy.435 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.92), metadata={op_name="XLA_Retvals"}
}

%fused_computation.131 (param_0.347: f32[1,1,512,256], param_1.731: f32[1,1,512,256]) -> f32[1,1,512,256] {
  %param_0.347 = f32[1,1,512,256]{1,0,2,3} parameter(0)
  %param_1.731 = f32[1,1,512,256]{3,2,1,0} parameter(1)
  %copy.438 = f32[1,1,512,256]{1,0,2,3} copy(f32[1,1,512,256]{3,2,1,0} %param_1.731), metadata={op_name="XLA_Args"}
  %constant_344 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.194 = f32[1,1,512,256]{1,0,2,3} broadcast(f32[] %constant_344), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_13_grad/Mul"}
  %multiply.177 = f32[1,1,512,256]{1,0,2,3} multiply(f32[1,1,512,256]{1,0,2,3} %copy.438, f32[1,1,512,256]{1,0,2,3} %broadcast.194), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_43_grad/Mul_1"}
  %add.93 = f32[1,1,512,256]{1,0,2,3} add(f32[1,1,512,256]{1,0,2,3} %param_0.347, f32[1,1,512,256]{1,0,2,3} %multiply.177), metadata={op_type="AddN" op_name="tower0/gradients/AddN_105"}
  ROOT %copy.437 = f32[1,1,512,256]{3,2,1,0} copy(f32[1,1,512,256]{1,0,2,3} %add.93), metadata={op_name="XLA_Retvals"}
}

%fused_computation.132 (param_0.349: f32[1,1,256,256], param_1.379: f32[1,1,256,256]) -> f32[1,1,256,256] {
  %param_0.349 = f32[1,1,256,256]{1,0,2,3} parameter(0)
  %param_1.379 = f32[1,1,256,256]{3,2,1,0} parameter(1)
  %copy.440 = f32[1,1,256,256]{1,0,2,3} copy(f32[1,1,256,256]{3,2,1,0} %param_1.379), metadata={op_name="XLA_Args"}
  %constant_287 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.73 = f32[1,1,256,256]{1,0,2,3} broadcast(f32[] %constant_287), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_42_grad/Mul"}
  %multiply.178 = f32[1,1,256,256]{1,0,2,3} multiply(f32[1,1,256,256]{1,0,2,3} %copy.440, f32[1,1,256,256]{1,0,2,3} %broadcast.73), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_42_grad/Mul_1"}
  %add.94 = f32[1,1,256,256]{1,0,2,3} add(f32[1,1,256,256]{1,0,2,3} %param_0.349, f32[1,1,256,256]{1,0,2,3} %multiply.178), metadata={op_type="AddN" op_name="tower0/gradients/AddN_103"}
  ROOT %copy.439 = f32[1,1,256,256]{3,2,1,0} copy(f32[1,1,256,256]{1,0,2,3} %add.94), metadata={op_name="XLA_Retvals"}
}

%fused_computation.138 (param_0.362: f32[256], param_1.393: f32[256], param_2.297: f32[256], param_3.160: f32[256], param_4.3: f32[256]) -> f32[256] {
  %param_3.160 = f32[256]{0} parameter(3)
  %param_4.3 = f32[256]{0} parameter(4)
  %add.107 = f32[256]{0} add(f32[256]{0} %param_3.160, f32[256]{0} %param_4.3), metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
  %param_2.297 = f32[256]{0} parameter(2)
  %add.106 = f32[256]{0} add(f32[256]{0} %add.107, f32[256]{0} %param_2.297), metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
  %param_1.393 = f32[256]{0} parameter(1)
  %add.105 = f32[256]{0} add(f32[256]{0} %add.106, f32[256]{0} %param_1.393), metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
  %param_0.362 = f32[256]{0} parameter(0)
  ROOT %add.104 = f32[256]{0} add(f32[256]{0} %add.105, f32[256]{0} %param_0.362), metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
}

%fused_computation.139 (param_0.366: f32[1,1,256,12], param_1.398: f32[1,1,256,12], param_2.303: f32[1,1,256,12], param_3.164: f32[1,1,256,12], param_4.7: f32[1,1,256,12], param_5.4: f32[1,1,256,12]) -> f32[1,1,256,12] {
  %param_4.7 = f32[1,1,256,12]{1,0,2,3} parameter(4)
  %param_5.4 = f32[1,1,256,12]{1,0,2,3} parameter(5)
  %add.112 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %param_4.7, f32[1,1,256,12]{1,0,2,3} %param_5.4), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  %param_3.164 = f32[1,1,256,12]{1,0,2,3} parameter(3)
  %add.111 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %add.112, f32[1,1,256,12]{1,0,2,3} %param_3.164), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  %param_2.303 = f32[1,1,256,12]{1,0,2,3} parameter(2)
  %add.110 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %add.111, f32[1,1,256,12]{1,0,2,3} %param_2.303), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  %param_1.398 = f32[1,1,256,12]{1,0,2,3} parameter(1)
  %add.109 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %add.110, f32[1,1,256,12]{1,0,2,3} %param_1.398), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  %param_0.366 = f32[1,1,256,12]{3,2,1,0} parameter(0)
  %copy.447 = f32[1,1,256,12]{1,0,2,3} copy(f32[1,1,256,12]{3,2,1,0} %param_0.366), metadata={op_name="XLA_Args"}
  %constant_288 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.74 = f32[1,1,256,12]{1,0,2,3} broadcast(f32[] %constant_288), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_52_grad/Mul"}
  %multiply.184 = f32[1,1,256,12]{1,0,2,3} multiply(f32[1,1,256,12]{1,0,2,3} %copy.447, f32[1,1,256,12]{1,0,2,3} %broadcast.74), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_52_grad/Mul_1"}
  %add.108 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %add.109, f32[1,1,256,12]{1,0,2,3} %multiply.184), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  ROOT %copy.446 = f32[1,1,256,12]{3,2,1,0} copy(f32[1,1,256,12]{1,0,2,3} %add.108), metadata={op_name="XLA_Retvals"}
}

%add_float_.1454 (x.1455: f32[], y.1456: f32[]) -> f32[] {
  %x.1455 = f32[] parameter(0)
  %y.1456 = f32[] parameter(1)
  ROOT %add.1457 = f32[] add(f32[] %x.1455, f32[] %y.1456)
}

%add_float_.1266 (x.1267: f32[], y.1268: f32[]) -> f32[] {
  %x.1267 = f32[] parameter(0)
  %y.1268 = f32[] parameter(1)
  ROOT %add.1269 = f32[] add(f32[] %x.1267, f32[] %y.1268)
}

%add_float_.1195 (x.1196: f32[], y.1197: f32[]) -> f32[] {
  %x.1196 = f32[] parameter(0)
  %y.1197 = f32[] parameter(1)
  ROOT %add.1198 = f32[] add(f32[] %x.1196, f32[] %y.1197)
}

%fused_computation.140 (param_0.369: f32[12], param_1.404: f32[12]) -> f32[12] {
  %constant_289 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.77 = f32[1,200,304,12]{3,2,1,0} broadcast(f32[] %constant_289), dimensions={}
  %reduce.57 = f32[12]{0} reduce(f32[1,200,304,12]{3,2,1,0} %broadcast.77, f32[] %constant_289), dimensions={0,1,2}, to_apply=%add_float_.1195, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/box/BiasAdd_grad/BiasAddGrad"}
  %broadcast.76 = f32[1,100,152,12]{3,2,1,0} broadcast(f32[] %constant_289), dimensions={}
  %reduce.56 = f32[12]{0} reduce(f32[1,100,152,12]{3,2,1,0} %broadcast.76, f32[] %constant_289), dimensions={0,1,2}, to_apply=%add_float_.1266, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/box/BiasAdd_grad/BiasAddGrad"}
  %add.116 = f32[12]{0} add(f32[12]{0} %reduce.57, f32[12]{0} %reduce.56), metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
  %param_1.404 = f32[12]{0} parameter(1)
  %add.115 = f32[12]{0} add(f32[12]{0} %add.116, f32[12]{0} %param_1.404), metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
  %param_0.369 = f32[12]{0} parameter(0)
  %add.114 = f32[12]{0} add(f32[12]{0} %add.115, f32[12]{0} %param_0.369), metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
  %broadcast.75 = f32[1,13,19,12]{3,2,1,0} broadcast(f32[] %constant_289), dimensions={}
  %reduce.55 = f32[12]{0} reduce(f32[1,13,19,12]{3,2,1,0} %broadcast.75, f32[] %constant_289), dimensions={0,1,2}, to_apply=%add_float_.1454, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/box/BiasAdd_grad/BiasAddGrad"}
  ROOT %add.113 = f32[12]{0} add(f32[12]{0} %add.114, f32[12]{0} %reduce.55), metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
}

%fused_computation.141 (param_0.372: f32[1,1,256,3], param_1.409: f32[1,1,256,3], param_2.312: f32[1,1,256,3], param_3.170: f32[1,1,256,3], param_4.11: f32[1,1,256,3], param_5.5: f32[1,1,256,3]) -> f32[1,1,256,3] {
  %param_4.11 = f32[1,1,256,3]{1,0,2,3} parameter(4)
  %param_5.5 = f32[1,1,256,3]{1,0,2,3} parameter(5)
  %add.121 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %param_4.11, f32[1,1,256,3]{1,0,2,3} %param_5.5), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  %param_3.170 = f32[1,1,256,3]{1,0,2,3} parameter(3)
  %add.120 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %add.121, f32[1,1,256,3]{1,0,2,3} %param_3.170), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  %param_2.312 = f32[1,1,256,3]{1,0,2,3} parameter(2)
  %add.119 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %add.120, f32[1,1,256,3]{1,0,2,3} %param_2.312), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  %param_1.409 = f32[1,1,256,3]{1,0,2,3} parameter(1)
  %add.118 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %add.119, f32[1,1,256,3]{1,0,2,3} %param_1.409), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  %param_0.372 = f32[1,1,256,3]{3,2,1,0} parameter(0)
  %copy.449 = f32[1,1,256,3]{1,0,2,3} copy(f32[1,1,256,3]{3,2,1,0} %param_0.372), metadata={op_name="XLA_Args"}
  %constant_290 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.78 = f32[1,1,256,3]{1,0,2,3} broadcast(f32[] %constant_290), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_51_grad/Mul"}
  %multiply.185 = f32[1,1,256,3]{1,0,2,3} multiply(f32[1,1,256,3]{1,0,2,3} %copy.449, f32[1,1,256,3]{1,0,2,3} %broadcast.78), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_51_grad/Mul_1"}
  %add.117 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %add.118, f32[1,1,256,3]{1,0,2,3} %multiply.185), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  ROOT %copy.448 = f32[1,1,256,3]{3,2,1,0} copy(f32[1,1,256,3]{1,0,2,3} %add.117), metadata={op_name="XLA_Retvals"}
}

%add_float_.1475 (x.1476: f32[], y.1477: f32[]) -> f32[] {
  %x.1476 = f32[] parameter(0)
  %y.1477 = f32[] parameter(1)
  ROOT %add.1478 = f32[] add(f32[] %x.1476, f32[] %y.1477)
}

%fused_computation.142 (param_0.375: f32[3], param_1.415: f32[3], param_2.317: f32[3], param_3.173: f32[3]) -> f32[3] {
  %param_2.317 = f32[3]{0} parameter(2)
  %param_3.173 = f32[3]{0} parameter(3)
  %add.125 = f32[3]{0} add(f32[3]{0} %param_2.317, f32[3]{0} %param_3.173), metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
  %param_1.415 = f32[3]{0} parameter(1)
  %add.124 = f32[3]{0} add(f32[3]{0} %add.125, f32[3]{0} %param_1.415), metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
  %param_0.375 = f32[3]{0} parameter(0)
  %add.123 = f32[3]{0} add(f32[3]{0} %add.124, f32[3]{0} %param_0.375), metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
  %constant_291 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.79 = f32[1,13,19,3]{3,2,1,0} broadcast(f32[] %constant_291), dimensions={}
  %reduce.58 = f32[3]{0} reduce(f32[1,13,19,3]{3,2,1,0} %broadcast.79, f32[] %constant_291), dimensions={0,1,2}, to_apply=%add_float_.1475, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/class/BiasAdd_grad/BiasAddGrad"}
  ROOT %add.122 = f32[3]{0} add(f32[3]{0} %add.123, f32[3]{0} %reduce.58), metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
}

%fused_computation.144 (param_0.377: f32[1,128,200,304], param_1.421: f32[1,128,1,1]) -> f32[1,128,200,304] {
  %param_0.377 = f32[1,128,200,304]{3,2,1,0} parameter(0)
  %param_1.421 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.349 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.421), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.81 = f32[1,128,200,304]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.349), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.187 = f32[1,128,200,304]{3,2,1,0} multiply(f32[1,128,200,304]{3,2,1,0} %param_0.377, f32[1,128,200,304]{3,2,1,0} %broadcast.81), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.146 (param_0.381: f32[1,128,100,152], param_1.428: f32[1,128,1,1]) -> f32[1,128,100,152] {
  %param_0.381 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %param_1.428 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.350 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.428), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.83 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.350), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.188 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %param_0.381, f32[1,128,100,152]{3,2,1,0} %broadcast.83), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.148 (param_0.384: f32[1,512,100,152], param_1.433: f32[1,512,1,1], param_2.583: f32[1,512,1,1]) -> (f32[1,512,100,152], f32[1,512,100,152]) {
  %param_0.384 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %param_1.433 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.351 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.433), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.84 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.351), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.189 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %param_0.384, f32[1,512,100,152]{3,2,1,0} %broadcast.84), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %param_2.583 = f32[1,512,1,1]{3,2,1,0} parameter(2)
  %bitcast.348.clone.1 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_2.583), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.80.clone.1 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.348.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.186.clone.1 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %param_0.384, f32[1,512,100,152]{3,2,1,0} %broadcast.80.clone.1), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %tuple.129 = (f32[1,512,100,152]{3,2,1,0}, f32[1,512,100,152]{3,2,1,0}) tuple(f32[1,512,100,152]{3,2,1,0} %multiply.189, f32[1,512,100,152]{3,2,1,0} %multiply.186.clone.1)
}

%fused_computation.150 (param_0.387: f32[1,128,100,152], param_1.439: f32[1,128,1,1]) -> f32[1,128,100,152] {
  %param_0.387 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %param_1.439 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.352 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.439), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.85 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.352), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.190 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %param_0.387, f32[1,128,100,152]{3,2,1,0} %broadcast.85), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.152 (param_0.390: f32[1,128,100,152], param_1.444: f32[1,128,1,1]) -> f32[1,128,100,152] {
  %param_0.390 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %param_1.444 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.353 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.444), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.86 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.353), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.191 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %param_0.390, f32[1,128,100,152]{3,2,1,0} %broadcast.86), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.154 (param_0.393: f32[1,512,100,152], param_1.449: f32[1,512,1,1]) -> f32[1,512,100,152] {
  %param_0.393 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %param_1.449 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.354 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.449), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.87 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.354), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.192 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %param_0.393, f32[1,512,100,152]{3,2,1,0} %broadcast.87), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.156 (param_0.396: f32[1,128,100,152], param_1.455: f32[1,128,1,1]) -> f32[1,128,100,152] {
  %param_0.396 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %param_1.455 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.355 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.455), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.88 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.355), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.193 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %param_0.396, f32[1,128,100,152]{3,2,1,0} %broadcast.88), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.158 (param_0.399: f32[1,128,100,152], param_1.460: f32[1,128,1,1]) -> f32[1,128,100,152] {
  %param_0.399 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %param_1.460 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.356 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.460), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.89 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.356), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.194 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %param_0.399, f32[1,128,100,152]{3,2,1,0} %broadcast.89), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.160 (param_0.402: f32[1,512,100,152], param_1.465: f32[1,512,1,1]) -> f32[1,512,100,152] {
  %param_0.402 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %param_1.465 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.357 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.465), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.90 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.357), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.195 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %param_0.402, f32[1,512,100,152]{3,2,1,0} %broadcast.90), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.162 (param_0.405: f32[1,128,100,152], param_1.471: f32[1,128,1,1]) -> f32[1,128,100,152] {
  %param_0.405 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %param_1.471 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.358 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.471), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.91 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.358), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.196 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %param_0.405, f32[1,128,100,152]{3,2,1,0} %broadcast.91), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.164 (param_0.408: f32[1,128,100,152], param_1.476: f32[1,128,1,1]) -> f32[1,128,100,152] {
  %param_0.408 = f32[1,128,100,152]{3,2,1,0} parameter(0)
  %param_1.476 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.359 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.476), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.92 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.359), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.197 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %param_0.408, f32[1,128,100,152]{3,2,1,0} %broadcast.92), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.166 (param_0.411: f32[1,512,100,152], param_1.481: f32[1,512,1,1]) -> f32[1,512,100,152] {
  %param_0.411 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %param_1.481 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.360 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.481), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.93 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.360), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.198 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %param_0.411, f32[1,512,100,152]{3,2,1,0} %broadcast.93), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.169 (param_0.415: f32[1,256,100,152], param_1.490: f32[1,256,1,1]) -> f32[1,256,100,152] {
  %param_0.415 = f32[1,256,100,152]{3,2,1,0} parameter(0)
  %param_1.490 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.362 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.490), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.95 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.362), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.200 = f32[1,256,100,152]{3,2,1,0} multiply(f32[1,256,100,152]{3,2,1,0} %param_0.415, f32[1,256,100,152]{3,2,1,0} %broadcast.95), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.171 (param_0.418: f32[1,256,50,76], param_1.496: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.418 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.496 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.363 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.496), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.96 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.363), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.201 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.418, f32[1,256,50,76]{3,2,1,0} %broadcast.96), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.173 (param_0.421: f32[1,1024,50,76], param_1.501: f32[1,1024,1,1], param_2.592: f32[1,1024,1,1]) -> (f32[1,1024,50,76], f32[1,1024,50,76]) {
  %param_0.421 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_1.501 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.364 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.501), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.97 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.364), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.202 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %param_0.421, f32[1,1024,50,76]{3,2,1,0} %broadcast.97), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %param_2.592 = f32[1,1024,1,1]{3,2,1,0} parameter(2)
  %bitcast.361.clone.1 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_2.592), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.94.clone.1 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.361.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.199.clone.1 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %param_0.421, f32[1,1024,50,76]{3,2,1,0} %broadcast.94.clone.1), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %tuple.137 = (f32[1,1024,50,76]{3,2,1,0}, f32[1,1024,50,76]{3,2,1,0}) tuple(f32[1,1024,50,76]{3,2,1,0} %multiply.202, f32[1,1024,50,76]{3,2,1,0} %multiply.199.clone.1)
}

%fused_computation.175 (param_0.424: f32[1,256,50,76], param_1.507: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.424 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.507 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.365 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.507), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.98 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.365), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.203 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.424, f32[1,256,50,76]{3,2,1,0} %broadcast.98), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.177 (param_0.427: f32[1,256,50,76], param_1.512: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.427 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.512 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.366 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.512), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.99 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.366), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.204 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.427, f32[1,256,50,76]{3,2,1,0} %broadcast.99), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.179 (param_0.430: f32[1,1024,50,76], param_1.517: f32[1,1024,1,1]) -> f32[1,1024,50,76] {
  %param_0.430 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_1.517 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.367 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.517), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.100 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.367), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.205 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %param_0.430, f32[1,1024,50,76]{3,2,1,0} %broadcast.100), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.181 (param_0.433: f32[1,256,50,76], param_1.523: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.433 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.523 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.368 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.523), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.101 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.368), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.206 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.433, f32[1,256,50,76]{3,2,1,0} %broadcast.101), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.183 (param_0.436: f32[1,256,50,76], param_1.528: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.436 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.528 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.369 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.528), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.102 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.369), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.207 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.436, f32[1,256,50,76]{3,2,1,0} %broadcast.102), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.185 (param_0.439: f32[1,1024,50,76], param_1.533: f32[1,1024,1,1]) -> f32[1,1024,50,76] {
  %param_0.439 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_1.533 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.370 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.533), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.103 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.370), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.208 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %param_0.439, f32[1,1024,50,76]{3,2,1,0} %broadcast.103), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.187 (param_0.442: f32[1,256,50,76], param_1.539: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.442 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.539 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.371 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.539), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.104 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.371), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.209 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.442, f32[1,256,50,76]{3,2,1,0} %broadcast.104), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.189 (param_0.445: f32[1,256,50,76], param_1.544: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.445 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.544 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.372 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.544), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.105 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.372), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.210 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.445, f32[1,256,50,76]{3,2,1,0} %broadcast.105), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.191 (param_0.448: f32[1,1024,50,76], param_1.549: f32[1,1024,1,1]) -> f32[1,1024,50,76] {
  %param_0.448 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_1.549 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.373 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.549), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.106 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.373), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.211 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %param_0.448, f32[1,1024,50,76]{3,2,1,0} %broadcast.106), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.193 (param_0.451: f32[1,256,50,76], param_1.555: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.451 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.555 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.374 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.555), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.107 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.374), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.212 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.451, f32[1,256,50,76]{3,2,1,0} %broadcast.107), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.195 (param_0.454: f32[1,256,50,76], param_1.560: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.454 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.560 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.375 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.560), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.108 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.375), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.213 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.454, f32[1,256,50,76]{3,2,1,0} %broadcast.108), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.197 (param_0.457: f32[1,1024,50,76], param_1.565: f32[1,1024,1,1]) -> f32[1,1024,50,76] {
  %param_0.457 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_1.565 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.376 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.565), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.109 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.376), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.214 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %param_0.457, f32[1,1024,50,76]{3,2,1,0} %broadcast.109), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.199 (param_0.460: f32[1,256,50,76], param_1.571: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.460 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.571 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.377 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.571), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.110 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.377), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.215 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.460, f32[1,256,50,76]{3,2,1,0} %broadcast.110), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.201 (param_0.463: f32[1,256,50,76], param_1.576: f32[1,256,1,1]) -> f32[1,256,50,76] {
  %param_0.463 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.576 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.378 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.576), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.111 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.378), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.216 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %param_0.463, f32[1,256,50,76]{3,2,1,0} %broadcast.111), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.203 (param_0.466: f32[1,1024,50,76], param_1.581: f32[1,1024,1,1]) -> f32[1,1024,50,76] {
  %param_0.466 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_1.581 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.379 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.581), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.112 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.379), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.217 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %param_0.466, f32[1,1024,50,76]{3,2,1,0} %broadcast.112), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.206 (param_0.470: f32[1,512,50,76], param_1.590: f32[1,512,1,1]) -> f32[1,512,50,76] {
  %param_0.470 = f32[1,512,50,76]{3,2,1,0} parameter(0)
  %param_1.590 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.381 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.590), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.114 = f32[1,512,50,76]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.381), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.219 = f32[1,512,50,76]{3,2,1,0} multiply(f32[1,512,50,76]{3,2,1,0} %param_0.470, f32[1,512,50,76]{3,2,1,0} %broadcast.114), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.208 (param_0.474: f32[1,512,25,38], param_1.597: f32[1,512,1,1]) -> f32[1,512,25,38] {
  %param_0.474 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %param_1.597 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.382 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.597), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.116 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.382), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.220 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %param_0.474, f32[1,512,25,38]{3,2,1,0} %broadcast.116), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.210 (param_0.477: f32[1,2048,25,38], param_1.602: f32[1,2048,1,1], param_2.612: f32[1,2048,1,1]) -> (f32[1,2048,25,38], f32[1,2048,25,38]) {
  %param_0.477 = f32[1,2048,25,38]{3,2,1,0} parameter(0)
  %param_1.602 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %bitcast.383 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_1.602), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.117 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.383), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.221 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %param_0.477, f32[1,2048,25,38]{3,2,1,0} %broadcast.117), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %param_2.612 = f32[1,2048,1,1]{3,2,1,0} parameter(2)
  %bitcast.380.clone.1 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_2.612), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.113.clone.1 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.380.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.218.clone.1 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %param_0.477, f32[1,2048,25,38]{3,2,1,0} %broadcast.113.clone.1), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %tuple.156 = (f32[1,2048,25,38]{3,2,1,0}, f32[1,2048,25,38]{3,2,1,0}) tuple(f32[1,2048,25,38]{3,2,1,0} %multiply.221, f32[1,2048,25,38]{3,2,1,0} %multiply.218.clone.1)
}

%fused_computation.212 (param_0.480: f32[1,512,25,38], param_1.608: f32[1,512,1,1]) -> f32[1,512,25,38] {
  %param_0.480 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %param_1.608 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.384 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.608), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.118 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.384), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.222 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %param_0.480, f32[1,512,25,38]{3,2,1,0} %broadcast.118), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.214 (param_0.483: f32[1,512,25,38], param_1.613: f32[1,512,1,1]) -> f32[1,512,25,38] {
  %param_0.483 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %param_1.613 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.385 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.613), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.119 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.385), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.223 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %param_0.483, f32[1,512,25,38]{3,2,1,0} %broadcast.119), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.216 (param_0.486: f32[1,2048,25,38], param_1.618: f32[1,2048,1,1]) -> f32[1,2048,25,38] {
  %param_0.486 = f32[1,2048,25,38]{3,2,1,0} parameter(0)
  %param_1.618 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %bitcast.386 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_1.618), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.120 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.386), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.224 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %param_0.486, f32[1,2048,25,38]{3,2,1,0} %broadcast.120), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.218 (param_0.489: f32[1,512,25,38], param_1.624: f32[1,512,1,1]) -> f32[1,512,25,38] {
  %param_0.489 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %param_1.624 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.387 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.624), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.121 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.387), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.225 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %param_0.489, f32[1,512,25,38]{3,2,1,0} %broadcast.121), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.220 (param_0.492: f32[1,512,25,38], param_1.629: f32[1,512,1,1]) -> f32[1,512,25,38] {
  %param_0.492 = f32[1,512,25,38]{3,2,1,0} parameter(0)
  %param_1.629 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.388 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.629), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.122 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.388), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.226 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %param_0.492, f32[1,512,25,38]{3,2,1,0} %broadcast.122), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.222 (param_0.495: f32[1,2048,25,38], param_1.634: f32[1,2048,1,1]) -> f32[1,2048,25,38] {
  %param_0.495 = f32[1,2048,25,38]{3,2,1,0} parameter(0)
  %param_1.634 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %bitcast.389 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_1.634), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.123 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.389), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.227 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %param_0.495, f32[1,2048,25,38]{3,2,1,0} %broadcast.123), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%scatter-combiner.1098 (p0.1099: f32[], p1.1100: f32[]) -> f32[] {
  %p0.1099 = f32[] parameter(0)
  %p1.1100 = f32[] parameter(1)
  ROOT %add.1101 = f32[] add(f32[] %p0.1099, f32[] %p1.1100)
}

%fused_computation.228 (param_0.508: s64[6], param_1.654: f32[6], param_2.432: pred[6], param_3.218: f32[6], param_4.24: f32[], param_5.12: pred[]) -> f32[2850] {
  %constant_298 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.131 = f32[2850]{0} broadcast(f32[] %constant_298), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %param_0.508 = s64[6]{0} parameter(0)
  %param_2.432 = pred[6]{0} parameter(2)
  %constant_297 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %broadcast.130 = f32[6]{0} broadcast(f32[] %constant_297), dimensions={}, metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Log1p_grad/Reciprocal"}
  %param_3.218 = f32[6]{0} parameter(3)
  %add.149 = f32[6]{0} add(f32[6]{0} %broadcast.130, f32[6]{0} %param_3.218), metadata={op_type="Add" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Log1p_grad/add"}
  %divide.0 = f32[6]{0} divide(f32[6]{0} %broadcast.130, f32[6]{0} %add.149), metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Log1p_grad/Reciprocal"}
  %param_5.12 = pred[] parameter(5)
  %select.44 = f32[] select(pred[] %param_5.12, f32[] %constant_298, f32[] %constant_297), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/label_loss_grad/Select_1"}
  %param_4.24 = f32[] parameter(4)
  %multiply.232 = f32[] multiply(f32[] %select.44, f32[] %param_4.24), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/mul_grad/Mul"}
  %broadcast.129 = f32[6]{0} broadcast(f32[] %multiply.232), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss_grad/Reshape_1"}
  %multiply.231 = f32[6]{0} multiply(f32[6]{0} %divide.0, f32[6]{0} %broadcast.129), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Log1p_grad/mul"}
  %multiply.230 = f32[6]{0} multiply(f32[6]{0} %multiply.231, f32[6]{0} %param_3.218), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Exp_grad/mul"}
  %broadcast.128 = f32[6]{0} broadcast(f32[] %constant_298), dimensions={}, metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Select_1_grad/zeros_like"}
  %select.43 = f32[6]{0} select(pred[6]{0} %param_2.432, f32[6]{0} %multiply.230, f32[6]{0} %broadcast.128), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Select_1_grad/Select"}
  %negate.88 = f32[6]{0} negate(f32[6]{0} %select.43), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Neg_grad/Neg"}
  %select.42 = f32[6]{0} select(pred[6]{0} %param_2.432, f32[6]{0} %broadcast.128, f32[6]{0} %multiply.230), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Select_1_grad/Select_1"}
  %add.148 = f32[6]{0} add(f32[6]{0} %negate.88, f32[6]{0} %select.42), metadata={op_type="AddN" op_name="tower0/gradients/AddN_13"}
  %select.41 = f32[6]{0} select(pred[6]{0} %param_2.432, f32[6]{0} %broadcast.129, f32[6]{0} %broadcast.128), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Select_grad/Select"}
  %add.147 = f32[6]{0} add(f32[6]{0} %add.148, f32[6]{0} %select.41), metadata={op_type="AddN" op_name="tower0/gradients/AddN_13"}
  %negate.87 = f32[] negate(f32[] %multiply.232), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/sub_grad/Neg"}
  %broadcast.127 = f32[6]{0} broadcast(f32[] %negate.87), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/sub_grad/Reshape_1"}
  %param_1.654 = f32[6]{0} parameter(1)
  %multiply.229 = f32[6]{0} multiply(f32[6]{0} %broadcast.127, f32[6]{0} %param_1.654), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/mul_grad/Mul"}
  %add.146 = f32[6]{0} add(f32[6]{0} %add.147, f32[6]{0} %multiply.229), metadata={op_type="AddN" op_name="tower0/gradients/AddN_13"}
  ROOT %scatter.0 = f32[2850]{0} scatter(f32[2850]{0} %broadcast.131, s64[6]{0} %param_0.508, f32[6]{0} %add.146), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1098, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_1/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.1073 (p0.1074: f32[], p1.1075: f32[]) -> f32[] {
  %p0.1074 = f32[] parameter(0)
  %p1.1075 = f32[] parameter(1)
  ROOT %add.1076 = f32[] add(f32[] %p0.1074, f32[] %p1.1075)
}

%fused_computation.229 (param_0.510: s64[6], param_1.656: f32[6,4], param_2.438: f32[], param_3.227: f32[6,4], param_4.33: f32[], param_5.25: f32[], param_6.18: pred[], param_7.20: f32[6,4]) -> f32[2850,4] {
  %constant_300 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.137 = f32[2850,4]{1,0} broadcast(f32[] %constant_300), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %param_0.510 = s64[6]{0} parameter(0)
  %param_7.20 = f32[6,4]{1,0} parameter(7)
  %param_2.438 = f32[] parameter(2)
  %broadcast.136 = f32[6,4]{1,0} broadcast(f32[] %param_2.438), dimensions={}, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Minimum_grad/LessEqual"}
  %compare.42 = pred[6,4]{1,0} compare(f32[6,4]{1,0} %param_7.20, f32[6,4]{1,0} %broadcast.136), direction=LE, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Minimum_grad/LessEqual"}
  %param_4.33 = f32[] parameter(4)
  %param_5.25 = f32[] parameter(5)
  %param_6.18 = pred[] parameter(6)
  %constant_299 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %select.47 = f32[] select(pred[] %param_6.18, f32[] %constant_300, f32[] %constant_299), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/box_loss_grad/Select_1"}
  %multiply.237 = f32[] multiply(f32[] %param_5.25, f32[] %select.47), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/truediv_grad/RealDiv"}
  %multiply.236 = f32[] multiply(f32[] %param_4.33, f32[] %multiply.237), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Mul_1_grad/Mul_1"}
  %broadcast.135 = f32[6,4]{1,0} broadcast(f32[] %multiply.236), dimensions={}
  %param_3.227 = f32[6,4]{1,0} parameter(3)
  %multiply.235 = f32[6,4]{1,0} multiply(f32[6,4]{1,0} %broadcast.135, f32[6,4]{1,0} %param_3.227), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Mul_grad/Mul_1"}
  %add.152 = f32[6,4]{1,0} add(f32[6,4]{1,0} %multiply.235, f32[6,4]{1,0} %multiply.235), metadata={op_type="AddN" op_name="tower0/gradients/AddN_5"}
  %multiply.234 = f32[] multiply(f32[] %multiply.237, f32[] %param_2.438), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Mul_2_grad/Mul_1"}
  %negate.89 = f32[] negate(f32[] %multiply.234), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Sub_1_grad/Neg"}
  %broadcast.134 = f32[6,4]{1,0} broadcast(f32[] %negate.89), dimensions={}
  %add.151 = f32[6,4]{1,0} add(f32[6,4]{1,0} %add.152, f32[6,4]{1,0} %broadcast.134), metadata={op_type="AddN" op_name="tower0/gradients/AddN_5"}
  %broadcast.133 = f32[6,4]{1,0} broadcast(f32[] %constant_300), dimensions={}, metadata={op_type="Fill" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Minimum_grad/zeros"}
  %select.46 = f32[6,4]{1,0} select(pred[6,4]{1,0} %compare.42, f32[6,4]{1,0} %add.151, f32[6,4]{1,0} %broadcast.133), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Minimum_grad/Select"}
  %broadcast.132 = f32[6,4]{1,0} broadcast(f32[] %multiply.234), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Mul_2_grad/Reshape_1"}
  %add.150 = f32[6,4]{1,0} add(f32[6,4]{1,0} %select.46, f32[6,4]{1,0} %broadcast.132), metadata={op_type="AddN" op_name="tower0/gradients/AddN_18"}
  %param_1.656 = f32[6,4]{1,0} parameter(1)
  %compare.41 = pred[6,4]{1,0} compare(f32[6,4]{1,0} %param_1.656, f32[6,4]{1,0} %param_1.656), direction=NE, metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Abs_grad/Sign"}
  %sign.0 = f32[6,4]{1,0} sign(f32[6,4]{1,0} %param_1.656), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Abs_grad/Sign"}
  %select.45 = f32[6,4]{1,0} select(pred[6,4]{1,0} %compare.41, f32[6,4]{1,0} %broadcast.133, f32[6,4]{1,0} %sign.0), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Abs_grad/Sign"}
  %multiply.233 = f32[6,4]{1,0} multiply(f32[6,4]{1,0} %add.150, f32[6,4]{1,0} %select.45), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Abs_grad/mul"}
  ROOT %scatter.1 = f32[2850,4]{1,0} scatter(f32[2850,4]{1,0} %broadcast.137, s64[6]{0} %param_0.510, f32[6,4]{1,0} %multiply.233), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1073, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_3/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.1011 (p0.1012: f32[], p1.1013: f32[]) -> f32[] {
  %p0.1012 = f32[] parameter(0)
  %p1.1013 = f32[] parameter(1)
  ROOT %add.1014 = f32[] add(f32[] %p0.1012, f32[] %p1.1013)
}

%fused_computation.233 (param_0.517: s64[14], param_1.668: f32[14], param_2.450: pred[14], param_3.241: f32[14], param_4.41: f32[], param_5.32: pred[]) -> f32[11400] {
  %constant_303 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.143 = f32[11400]{0} broadcast(f32[] %constant_303), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %param_0.517 = s64[14]{0} parameter(0)
  %param_2.450 = pred[14]{0} parameter(2)
  %constant_302 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %broadcast.142 = f32[14]{0} broadcast(f32[] %constant_302), dimensions={}, metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Log1p_grad/Reciprocal"}
  %param_3.241 = f32[14]{0} parameter(3)
  %add.160 = f32[14]{0} add(f32[14]{0} %broadcast.142, f32[14]{0} %param_3.241), metadata={op_type="Add" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Log1p_grad/add"}
  %divide.1 = f32[14]{0} divide(f32[14]{0} %broadcast.142, f32[14]{0} %add.160), metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Log1p_grad/Reciprocal"}
  %param_5.32 = pred[] parameter(5)
  %select.52 = f32[] select(pred[] %param_5.32, f32[] %constant_303, f32[] %constant_302), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/label_loss_grad/Select_1"}
  %param_4.41 = f32[] parameter(4)
  %multiply.242 = f32[] multiply(f32[] %select.52, f32[] %param_4.41), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/mul_grad/Mul"}
  %broadcast.141 = f32[14]{0} broadcast(f32[] %multiply.242), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss_grad/Reshape_1"}
  %multiply.241 = f32[14]{0} multiply(f32[14]{0} %divide.1, f32[14]{0} %broadcast.141), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Log1p_grad/mul"}
  %multiply.240 = f32[14]{0} multiply(f32[14]{0} %multiply.241, f32[14]{0} %param_3.241), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Exp_grad/mul"}
  %broadcast.140 = f32[14]{0} broadcast(f32[] %constant_303), dimensions={}, metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Select_1_grad/zeros_like"}
  %select.51 = f32[14]{0} select(pred[14]{0} %param_2.450, f32[14]{0} %multiply.240, f32[14]{0} %broadcast.140), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Select_1_grad/Select"}
  %negate.91 = f32[14]{0} negate(f32[14]{0} %select.51), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Neg_grad/Neg"}
  %select.50 = f32[14]{0} select(pred[14]{0} %param_2.450, f32[14]{0} %broadcast.140, f32[14]{0} %multiply.240), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Select_1_grad/Select_1"}
  %add.159 = f32[14]{0} add(f32[14]{0} %negate.91, f32[14]{0} %select.50), metadata={op_type="AddN" op_name="tower0/gradients/AddN_12"}
  %select.49 = f32[14]{0} select(pred[14]{0} %param_2.450, f32[14]{0} %broadcast.141, f32[14]{0} %broadcast.140), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Select_grad/Select"}
  %add.158 = f32[14]{0} add(f32[14]{0} %add.159, f32[14]{0} %select.49), metadata={op_type="AddN" op_name="tower0/gradients/AddN_12"}
  %negate.90 = f32[] negate(f32[] %multiply.242), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/sub_grad/Neg"}
  %broadcast.139 = f32[14]{0} broadcast(f32[] %negate.90), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/sub_grad/Reshape_1"}
  %param_1.668 = f32[14]{0} parameter(1)
  %multiply.239 = f32[14]{0} multiply(f32[14]{0} %broadcast.139, f32[14]{0} %param_1.668), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/mul_grad/Mul"}
  %add.157 = f32[14]{0} add(f32[14]{0} %add.158, f32[14]{0} %multiply.239), metadata={op_type="AddN" op_name="tower0/gradients/AddN_12"}
  ROOT %scatter.2 = f32[11400]{0} scatter(f32[11400]{0} %broadcast.143, s64[14]{0} %param_0.517, f32[14]{0} %add.157), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1011, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_1/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.986 (p0.987: f32[], p1.988: f32[]) -> f32[] {
  %p0.987 = f32[] parameter(0)
  %p1.988 = f32[] parameter(1)
  ROOT %add.989 = f32[] add(f32[] %p0.987, f32[] %p1.988)
}

%fused_computation.234 (param_0.519: s64[3], param_1.670: f32[3,4], param_2.456: f32[], param_3.250: f32[3,4], param_4.50: f32[], param_5.45: f32[], param_6.35: pred[], param_7.41: f32[3,4]) -> f32[11400,4] {
  %constant_305 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.149 = f32[11400,4]{1,0} broadcast(f32[] %constant_305), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %param_0.519 = s64[3]{0} parameter(0)
  %param_7.41 = f32[3,4]{1,0} parameter(7)
  %param_2.456 = f32[] parameter(2)
  %broadcast.148 = f32[3,4]{1,0} broadcast(f32[] %param_2.456), dimensions={}, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Minimum_grad/LessEqual"}
  %compare.45 = pred[3,4]{1,0} compare(f32[3,4]{1,0} %param_7.41, f32[3,4]{1,0} %broadcast.148), direction=LE, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Minimum_grad/LessEqual"}
  %param_4.50 = f32[] parameter(4)
  %param_5.45 = f32[] parameter(5)
  %param_6.35 = pred[] parameter(6)
  %constant_304 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %select.55 = f32[] select(pred[] %param_6.35, f32[] %constant_305, f32[] %constant_304), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/box_loss_grad/Select_1"}
  %multiply.247 = f32[] multiply(f32[] %param_5.45, f32[] %select.55), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/truediv_grad/RealDiv"}
  %multiply.246 = f32[] multiply(f32[] %param_4.50, f32[] %multiply.247), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Mul_1_grad/Mul_1"}
  %broadcast.147 = f32[3,4]{1,0} broadcast(f32[] %multiply.246), dimensions={}
  %param_3.250 = f32[3,4]{1,0} parameter(3)
  %multiply.245 = f32[3,4]{1,0} multiply(f32[3,4]{1,0} %broadcast.147, f32[3,4]{1,0} %param_3.250), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Mul_grad/Mul_1"}
  %add.163 = f32[3,4]{1,0} add(f32[3,4]{1,0} %multiply.245, f32[3,4]{1,0} %multiply.245), metadata={op_type="AddN" op_name="tower0/gradients/AddN_4"}
  %multiply.244 = f32[] multiply(f32[] %multiply.247, f32[] %param_2.456), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Mul_2_grad/Mul_1"}
  %negate.92 = f32[] negate(f32[] %multiply.244), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Sub_1_grad/Neg"}
  %broadcast.146 = f32[3,4]{1,0} broadcast(f32[] %negate.92), dimensions={}
  %add.162 = f32[3,4]{1,0} add(f32[3,4]{1,0} %add.163, f32[3,4]{1,0} %broadcast.146), metadata={op_type="AddN" op_name="tower0/gradients/AddN_4"}
  %broadcast.145 = f32[3,4]{1,0} broadcast(f32[] %constant_305), dimensions={}, metadata={op_type="Fill" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Minimum_grad/zeros"}
  %select.54 = f32[3,4]{1,0} select(pred[3,4]{1,0} %compare.45, f32[3,4]{1,0} %add.162, f32[3,4]{1,0} %broadcast.145), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Minimum_grad/Select"}
  %broadcast.144 = f32[3,4]{1,0} broadcast(f32[] %multiply.244), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Mul_2_grad/Reshape_1"}
  %add.161 = f32[3,4]{1,0} add(f32[3,4]{1,0} %select.54, f32[3,4]{1,0} %broadcast.144), metadata={op_type="AddN" op_name="tower0/gradients/AddN_17"}
  %param_1.670 = f32[3,4]{1,0} parameter(1)
  %compare.44 = pred[3,4]{1,0} compare(f32[3,4]{1,0} %param_1.670, f32[3,4]{1,0} %param_1.670), direction=NE, metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Abs_grad/Sign"}
  %sign.1 = f32[3,4]{1,0} sign(f32[3,4]{1,0} %param_1.670), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Abs_grad/Sign"}
  %select.53 = f32[3,4]{1,0} select(pred[3,4]{1,0} %compare.44, f32[3,4]{1,0} %broadcast.145, f32[3,4]{1,0} %sign.1), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Abs_grad/Sign"}
  %multiply.243 = f32[3,4]{1,0} multiply(f32[3,4]{1,0} %add.161, f32[3,4]{1,0} %select.53), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Abs_grad/mul"}
  ROOT %scatter.3 = f32[11400,4]{1,0} scatter(f32[11400,4]{1,0} %broadcast.149, s64[3]{0} %param_0.519, f32[3,4]{1,0} %multiply.243), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.986, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_3/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.924 (p0.925: f32[], p1.926: f32[]) -> f32[] {
  %p0.925 = f32[] parameter(0)
  %p1.926 = f32[] parameter(1)
  ROOT %add.927 = f32[] add(f32[] %p0.925, f32[] %p1.926)
}

%fused_computation.238 (param_0.526: s64[50], param_1.682: f32[50], param_2.468: pred[50], param_3.264: f32[50], param_4.58: f32[], param_5.52: pred[]) -> f32[45600] {
  %constant_308 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.155 = f32[45600]{0} broadcast(f32[] %constant_308), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level3/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %param_0.526 = s64[50]{0} parameter(0)
  %param_2.468 = pred[50]{0} parameter(2)
  %constant_307 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %broadcast.154 = f32[50]{0} broadcast(f32[] %constant_307), dimensions={}, metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Log1p_grad/Reciprocal"}
  %param_3.264 = f32[50]{0} parameter(3)
  %add.171 = f32[50]{0} add(f32[50]{0} %broadcast.154, f32[50]{0} %param_3.264), metadata={op_type="Add" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Log1p_grad/add"}
  %divide.2 = f32[50]{0} divide(f32[50]{0} %broadcast.154, f32[50]{0} %add.171), metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Log1p_grad/Reciprocal"}
  %param_5.52 = pred[] parameter(5)
  %select.60 = f32[] select(pred[] %param_5.52, f32[] %constant_308, f32[] %constant_307), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/label_loss_grad/Select_1"}
  %param_4.58 = f32[] parameter(4)
  %multiply.252 = f32[] multiply(f32[] %select.60, f32[] %param_4.58), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/mul_grad/Mul"}
  %broadcast.153 = f32[50]{0} broadcast(f32[] %multiply.252), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss_grad/Reshape_1"}
  %multiply.251 = f32[50]{0} multiply(f32[50]{0} %divide.2, f32[50]{0} %broadcast.153), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Log1p_grad/mul"}
  %multiply.250 = f32[50]{0} multiply(f32[50]{0} %multiply.251, f32[50]{0} %param_3.264), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Exp_grad/mul"}
  %broadcast.152 = f32[50]{0} broadcast(f32[] %constant_308), dimensions={}, metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Select_1_grad/zeros_like"}
  %select.59 = f32[50]{0} select(pred[50]{0} %param_2.468, f32[50]{0} %multiply.250, f32[50]{0} %broadcast.152), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Select_1_grad/Select"}
  %negate.94 = f32[50]{0} negate(f32[50]{0} %select.59), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Neg_grad/Neg"}
  %select.58 = f32[50]{0} select(pred[50]{0} %param_2.468, f32[50]{0} %broadcast.152, f32[50]{0} %multiply.250), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Select_1_grad/Select_1"}
  %add.170 = f32[50]{0} add(f32[50]{0} %negate.94, f32[50]{0} %select.58), metadata={op_type="AddN" op_name="tower0/gradients/AddN_11"}
  %select.57 = f32[50]{0} select(pred[50]{0} %param_2.468, f32[50]{0} %broadcast.153, f32[50]{0} %broadcast.152), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Select_grad/Select"}
  %add.169 = f32[50]{0} add(f32[50]{0} %add.170, f32[50]{0} %select.57), metadata={op_type="AddN" op_name="tower0/gradients/AddN_11"}
  %negate.93 = f32[] negate(f32[] %multiply.252), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/sub_grad/Neg"}
  %broadcast.151 = f32[50]{0} broadcast(f32[] %negate.93), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/sub_grad/Reshape_1"}
  %param_1.682 = f32[50]{0} parameter(1)
  %multiply.249 = f32[50]{0} multiply(f32[50]{0} %broadcast.151, f32[50]{0} %param_1.682), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/mul_grad/Mul"}
  %add.168 = f32[50]{0} add(f32[50]{0} %add.169, f32[50]{0} %multiply.249), metadata={op_type="AddN" op_name="tower0/gradients/AddN_11"}
  ROOT %scatter.4 = f32[45600]{0} scatter(f32[45600]{0} %broadcast.155, s64[50]{0} %param_0.526, f32[50]{0} %add.168), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.924, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level3/boolean_mask_1/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.838 (p0.839: f32[], p1.840: f32[]) -> f32[] {
  %p0.839 = f32[] parameter(0)
  %p1.840 = f32[] parameter(1)
  ROOT %add.841 = f32[] add(f32[] %p0.839, f32[] %p1.840)
}

%fused_computation.241 (param_0.533: s64[186], param_1.690: f32[186], param_2.477: pred[186], param_3.274: f32[186], param_4.66: f32[], param_5.59: pred[]) -> f32[182400] {
  %constant_311 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.161 = f32[182400]{0} broadcast(f32[] %constant_311), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level2/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %param_0.533 = s64[186]{0} parameter(0)
  %param_2.477 = pred[186]{0} parameter(2)
  %constant_310 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %broadcast.160 = f32[186]{0} broadcast(f32[] %constant_310), dimensions={}, metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Log1p_grad/Reciprocal"}
  %param_3.274 = f32[186]{0} parameter(3)
  %add.178 = f32[186]{0} add(f32[186]{0} %broadcast.160, f32[186]{0} %param_3.274), metadata={op_type="Add" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Log1p_grad/add"}
  %divide.3 = f32[186]{0} divide(f32[186]{0} %broadcast.160, f32[186]{0} %add.178), metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Log1p_grad/Reciprocal"}
  %param_5.59 = pred[] parameter(5)
  %select.65 = f32[] select(pred[] %param_5.59, f32[] %constant_311, f32[] %constant_310), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %param_4.66 = f32[] parameter(4)
  %multiply.256 = f32[] multiply(f32[] %select.65, f32[] %param_4.66), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/mul_grad/Mul"}
  %broadcast.159 = f32[186]{0} broadcast(f32[] %multiply.256), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss_grad/Reshape_1"}
  %multiply.255 = f32[186]{0} multiply(f32[186]{0} %divide.3, f32[186]{0} %broadcast.159), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Log1p_grad/mul"}
  %multiply.254 = f32[186]{0} multiply(f32[186]{0} %multiply.255, f32[186]{0} %param_3.274), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Exp_grad/mul"}
  %broadcast.158 = f32[186]{0} broadcast(f32[] %constant_311), dimensions={}, metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_1_grad/zeros_like"}
  %select.64 = f32[186]{0} select(pred[186]{0} %param_2.477, f32[186]{0} %multiply.254, f32[186]{0} %broadcast.158), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_1_grad/Select"}
  %negate.96 = f32[186]{0} negate(f32[186]{0} %select.64), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Neg_grad/Neg"}
  %select.63 = f32[186]{0} select(pred[186]{0} %param_2.477, f32[186]{0} %broadcast.158, f32[186]{0} %multiply.254), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_1_grad/Select_1"}
  %add.177 = f32[186]{0} add(f32[186]{0} %negate.96, f32[186]{0} %select.63), metadata={op_type="AddN" op_name="tower0/gradients/AddN_10"}
  %select.62 = f32[186]{0} select(pred[186]{0} %param_2.477, f32[186]{0} %broadcast.159, f32[186]{0} %broadcast.158), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/Select"}
  %add.176 = f32[186]{0} add(f32[186]{0} %add.177, f32[186]{0} %select.62), metadata={op_type="AddN" op_name="tower0/gradients/AddN_10"}
  %negate.95 = f32[] negate(f32[] %multiply.256), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/sub_grad/Neg"}
  %broadcast.157 = f32[186]{0} broadcast(f32[] %negate.95), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/sub_grad/Reshape_1"}
  %param_1.690 = f32[186]{0} parameter(1)
  %multiply.253 = f32[186]{0} multiply(f32[186]{0} %broadcast.157, f32[186]{0} %param_1.690), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/mul_grad/Mul"}
  %add.175 = f32[186]{0} add(f32[186]{0} %add.176, f32[186]{0} %multiply.253), metadata={op_type="AddN" op_name="tower0/gradients/AddN_10"}
  ROOT %scatter.5 = f32[182400]{0} scatter(f32[182400]{0} %broadcast.161, s64[186]{0} %param_0.533, f32[186]{0} %add.175), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.838, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level2/boolean_mask_1/Reshape_grad/Reshape/tensor"}
}

%add_float_.1340 (x.1341: f32[], y.1342: f32[]) -> f32[] {
  %x.1341 = f32[] parameter(0)
  %y.1342 = f32[] parameter(1)
  ROOT %add.1343 = f32[] add(f32[] %x.1341, f32[] %y.1342)
}

%fused_computation.242 (param_0.537: f32[11400,4]) -> f32[12] {
  %param_0.537 = f32[11400,4]{1,0} parameter(0)
  %reshape.257 = f32[1,50,76,12]{2,1,3,0} reshape(f32[11400,4]{1,0} %param_0.537), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_2/Reshape_grad/Reshape"}
  %constant_345 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.62 = f32[12]{0} reduce(f32[1,50,76,12]{2,1,3,0} %reshape.257, f32[] %constant_345), dimensions={0,1,2}, to_apply=%add_float_.1340, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/box/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1414 (x.1415: f32[], y.1416: f32[]) -> f32[] {
  %x.1415 = f32[] parameter(0)
  %y.1416 = f32[] parameter(1)
  ROOT %add.1417 = f32[] add(f32[] %x.1415, f32[] %y.1416)
}

%fused_computation.243 (param_0.540: f32[2850,4]) -> f32[12] {
  %param_0.540 = f32[2850,4]{1,0} parameter(0)
  %reshape.258 = f32[1,25,38,12]{2,1,3,0} reshape(f32[2850,4]{1,0} %param_0.540), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_3/Reshape_grad/Reshape"}
  %constant_346 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.63 = f32[12]{0} reduce(f32[1,25,38,12]{2,1,3,0} %reshape.258, f32[] %constant_346), dimensions={0,1,2}, to_apply=%add_float_.1414, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/box/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1278 (x.1279: f32[], y.1280: f32[]) -> f32[] {
  %x.1279 = f32[] parameter(0)
  %y.1280 = f32[] parameter(1)
  ROOT %add.1281 = f32[] add(f32[] %x.1279, f32[] %y.1280)
}

%fused_computation.244 (param_0.543: f32[45600]) -> f32[3] {
  %param_0.543 = f32[45600]{0} parameter(0)
  %reshape.259 = f32[1,100,152,3]{2,1,3,0} reshape(f32[45600]{0} %param_0.543), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_1/Squeeze_grad/Reshape"}
  %constant_347 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.64 = f32[3]{0} reduce(f32[1,100,152,3]{2,1,3,0} %reshape.259, f32[] %constant_347), dimensions={0,1,2}, to_apply=%add_float_.1278, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/class/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1207 (x.1208: f32[], y.1209: f32[]) -> f32[] {
  %x.1208 = f32[] parameter(0)
  %y.1209 = f32[] parameter(1)
  ROOT %add.1210 = f32[] add(f32[] %x.1208, f32[] %y.1209)
}

%fused_computation.245 (param_0.546: f32[182400]) -> f32[3] {
  %param_0.546 = f32[182400]{0} parameter(0)
  %reshape.260 = f32[1,200,304,3]{2,1,3,0} reshape(f32[182400]{0} %param_0.546), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn/Squeeze_grad/Reshape"}
  %constant_348 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.65 = f32[3]{0} reduce(f32[1,200,304,3]{2,1,3,0} %reshape.260, f32[] %constant_348), dimensions={0,1,2}, to_apply=%add_float_.1207, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/class/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1352 (x.1353: f32[], y.1354: f32[]) -> f32[] {
  %x.1353 = f32[] parameter(0)
  %y.1354 = f32[] parameter(1)
  ROOT %add.1355 = f32[] add(f32[] %x.1353, f32[] %y.1354)
}

%fused_computation.246 (param_0.549: f32[11400]) -> f32[3] {
  %param_0.549 = f32[11400]{0} parameter(0)
  %reshape.261 = f32[1,50,76,3]{2,1,3,0} reshape(f32[11400]{0} %param_0.549), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_2/Squeeze_grad/Reshape"}
  %constant_349 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.66 = f32[3]{0} reduce(f32[1,50,76,3]{2,1,3,0} %reshape.261, f32[] %constant_349), dimensions={0,1,2}, to_apply=%add_float_.1352, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/class/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1426 (x.1427: f32[], y.1428: f32[]) -> f32[] {
  %x.1427 = f32[] parameter(0)
  %y.1428 = f32[] parameter(1)
  ROOT %add.1429 = f32[] add(f32[] %x.1427, f32[] %y.1428)
}

%fused_computation.247 (param_0.552: f32[2850]) -> f32[3] {
  %param_0.552 = f32[2850]{0} parameter(0)
  %reshape.262 = f32[1,25,38,3]{2,1,3,0} reshape(f32[2850]{0} %param_0.552), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_3/Squeeze_grad/Reshape"}
  %constant_350 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.67 = f32[3]{0} reduce(f32[1,25,38,3]{2,1,3,0} %reshape.262, f32[] %constant_350), dimensions={0,1,2}, to_apply=%add_float_.1426, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/class/BiasAdd_grad/BiasAddGrad"}
}

%fused_computation.248 (param_0.567: f32[2850]) -> f32[1,3,25,38] {
  %param_0.567 = f32[2850]{0} parameter(0)
  %reshape.263 = f32[1,25,38,3]{2,1,3,0} reshape(f32[2850]{0} %param_0.567), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_3/Squeeze_grad/Reshape"}
  ROOT %bitcast.400 = f32[1,3,25,38]{3,2,1,0} bitcast(f32[1,25,38,3]{2,1,3,0} %reshape.263), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_3/transpose_grad/transpose"}
}

%fused_computation.249 (param_0.569: f32[2850,4]) -> f32[1,12,25,38] {
  %param_0.569 = f32[2850,4]{1,0} parameter(0)
  %reshape.264 = f32[1,25,38,12]{2,1,3,0} reshape(f32[2850,4]{1,0} %param_0.569), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_3/Reshape_grad/Reshape"}
  ROOT %bitcast.401 = f32[1,12,25,38]{3,2,1,0} bitcast(f32[1,25,38,12]{2,1,3,0} %reshape.264), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_3/transpose_1_grad/transpose"}
}

%fused_computation.250 (param_0.572: f32[11400]) -> f32[1,3,50,76] {
  %param_0.572 = f32[11400]{0} parameter(0)
  %reshape.265 = f32[1,50,76,3]{2,1,3,0} reshape(f32[11400]{0} %param_0.572), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_2/Squeeze_grad/Reshape"}
  ROOT %bitcast.403 = f32[1,3,50,76]{3,2,1,0} bitcast(f32[1,50,76,3]{2,1,3,0} %reshape.265), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_2/transpose_grad/transpose"}
}

%fused_computation.251 (param_0.574: f32[11400,4]) -> f32[1,12,50,76] {
  %param_0.574 = f32[11400,4]{1,0} parameter(0)
  %reshape.266 = f32[1,50,76,12]{2,1,3,0} reshape(f32[11400,4]{1,0} %param_0.574), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_2/Reshape_grad/Reshape"}
  ROOT %bitcast.404 = f32[1,12,50,76]{3,2,1,0} bitcast(f32[1,50,76,12]{2,1,3,0} %reshape.266), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_2/transpose_1_grad/transpose"}
}

%fused_computation.252 (param_0.577: f32[45600]) -> f32[1,3,100,152] {
  %param_0.577 = f32[45600]{0} parameter(0)
  %reshape.267 = f32[1,100,152,3]{2,1,3,0} reshape(f32[45600]{0} %param_0.577), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_1/Squeeze_grad/Reshape"}
  ROOT %bitcast.406 = f32[1,3,100,152]{3,2,1,0} bitcast(f32[1,100,152,3]{2,1,3,0} %reshape.267), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_1/transpose_grad/transpose"}
}

%fused_computation.253 (param_0.579: f32[182400]) -> f32[1,3,200,304] {
  %param_0.579 = f32[182400]{0} parameter(0)
  %reshape.268 = f32[1,200,304,3]{2,1,3,0} reshape(f32[182400]{0} %param_0.579), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn/Squeeze_grad/Reshape"}
  ROOT %bitcast.407 = f32[1,3,200,304]{3,2,1,0} bitcast(f32[1,200,304,3]{2,1,3,0} %reshape.268), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn/transpose_grad/transpose"}
}

%add_float_.1500 (x.1501: f32[], y.1502: f32[]) -> f32[] {
  %x.1501 = f32[] parameter(0)
  %y.1502 = f32[] parameter(1)
  ROOT %add.1503 = f32[] add(f32[] %x.1501, f32[] %y.1502)
}

%fused_computation.254 (param_0.660: f32[1,256,13,19], param_1.924: f32[1,256,13,19], param_2.811: f32[1,256,13,19]) -> (f32[256], f32[1,256,13,19]) {
  %param_2.811 = f32[1,256,13,19]{3,2,1,0} parameter(2)
  %constant_295_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.125.clone.1 = f32[1,256,13,19]{3,2,1,0} broadcast(f32[] %constant_295_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/Relu_grad/ReluGrad"}
  %compare.39.clone.1 = pred[1,256,13,19]{3,2,1,0} compare(f32[1,256,13,19]{3,2,1,0} %param_2.811, f32[1,256,13,19]{3,2,1,0} %broadcast.125.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/Relu_grad/ReluGrad"}
  %param_0.660 = f32[1,256,13,19]{3,2,1,0} parameter(0)
  %param_1.924 = f32[1,256,13,19]{3,2,1,0} parameter(1)
  %add.144.clone.1 = f32[1,256,13,19]{3,2,1,0} add(f32[1,256,13,19]{3,2,1,0} %param_0.660, f32[1,256,13,19]{3,2,1,0} %param_1.924), metadata={op_type="AddN" op_name="tower0/gradients/AddN_91"}
  %select.39.clone.1 = f32[1,256,13,19]{3,2,1,0} select(pred[1,256,13,19]{3,2,1,0} %compare.39.clone.1, f32[1,256,13,19]{3,2,1,0} %add.144.clone.1, f32[1,256,13,19]{3,2,1,0} %broadcast.125.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/Relu_grad/ReluGrad"}
  %reduce.107 = f32[256]{0} reduce(f32[1,256,13,19]{3,2,1,0} %select.39.clone.1, f32[] %constant_295_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1500, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.187 = (f32[256]{0}, f32[1,256,13,19]{3,2,1,0}) tuple(f32[256]{0} %reduce.107, f32[1,256,13,19]{3,2,1,0} %select.39.clone.1)
}

%add_float_.1442 (x.1443: f32[], y.1444: f32[]) -> f32[] {
  %x.1443 = f32[] parameter(0)
  %y.1444 = f32[] parameter(1)
  ROOT %add.1445 = f32[] add(f32[] %x.1443, f32[] %y.1444)
}

%fused_computation.255 (param_0.659: f32[1,256,25,38], param_1.923: f32[1,256,25,38], param_2.810: f32[1,256,25,38]) -> (f32[256], f32[1,256,25,38]) {
  %param_2.810 = f32[1,256,25,38]{3,2,1,0} parameter(2)
  %constant_296_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.126.clone.1 = f32[1,256,25,38]{3,2,1,0} broadcast(f32[] %constant_296_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/Relu_grad/ReluGrad"}
  %compare.40.clone.1 = pred[1,256,25,38]{3,2,1,0} compare(f32[1,256,25,38]{3,2,1,0} %param_2.810, f32[1,256,25,38]{3,2,1,0} %broadcast.126.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/Relu_grad/ReluGrad"}
  %param_0.659 = f32[1,256,25,38]{3,2,1,0} parameter(0)
  %param_1.923 = f32[1,256,25,38]{3,2,1,0} parameter(1)
  %add.145.clone.1 = f32[1,256,25,38]{3,2,1,0} add(f32[1,256,25,38]{3,2,1,0} %param_0.659, f32[1,256,25,38]{3,2,1,0} %param_1.923), metadata={op_type="AddN" op_name="tower0/gradients/AddN_90"}
  %select.40.clone.1 = f32[1,256,25,38]{3,2,1,0} select(pred[1,256,25,38]{3,2,1,0} %compare.40.clone.1, f32[1,256,25,38]{3,2,1,0} %add.145.clone.1, f32[1,256,25,38]{3,2,1,0} %broadcast.126.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/Relu_grad/ReluGrad"}
  %reduce.108 = f32[256]{0} reduce(f32[1,256,25,38]{3,2,1,0} %select.40.clone.1, f32[] %constant_296_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1442, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.188 = (f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) tuple(f32[256]{0} %reduce.108, f32[1,256,25,38]{3,2,1,0} %select.40.clone.1)
}

%add_float_.1368 (x.1369: f32[], y.1370: f32[]) -> f32[] {
  %x.1369 = f32[] parameter(0)
  %y.1370 = f32[] parameter(1)
  ROOT %add.1371 = f32[] add(f32[] %x.1369, f32[] %y.1370)
}

%fused_computation.256 (param_0.658: f32[1,256,50,76], param_1.922: f32[1,256,50,76], param_2.809: f32[1,256,50,76]) -> (f32[256], f32[1,256,50,76]) {
  %param_2.809 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %constant_388_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.232.clone.1 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_388_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.43.clone.1 = pred[1,256,50,76]{3,2,1,0} compare(f32[1,256,50,76]{3,2,1,0} %param_2.809, f32[1,256,50,76]{3,2,1,0} %broadcast.232.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %param_0.658 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.922 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %add.156.clone.1 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %param_0.658, f32[1,256,50,76]{3,2,1,0} %param_1.922), metadata={op_type="AddN" op_name="tower0/gradients/AddN_89"}
  %select.48.clone.1 = f32[1,256,50,76]{3,2,1,0} select(pred[1,256,50,76]{3,2,1,0} %compare.43.clone.1, f32[1,256,50,76]{3,2,1,0} %add.156.clone.1, f32[1,256,50,76]{3,2,1,0} %broadcast.232.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %reduce.109 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %select.48.clone.1, f32[] %constant_388_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1368, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.189 = (f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.109, f32[1,256,50,76]{3,2,1,0} %select.48.clone.1)
}

%add_float_.1223 (x.1224: f32[], y.1225: f32[]) -> f32[] {
  %x.1224 = f32[] parameter(0)
  %y.1225 = f32[] parameter(1)
  ROOT %add.1226 = f32[] add(f32[] %x.1224, f32[] %y.1225)
}

%fused_computation.257 (param_0.657: f32[1,256,200,304], param_1.921: f32[1,256,200,304], param_2.808: f32[1,256,200,304]) -> (f32[256], f32[1,256,200,304]) {
  %param_2.808 = f32[1,256,200,304]{3,2,1,0} parameter(2)
  %constant_309_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.156.clone.1 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[] %constant_309_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn/conv0/Relu_grad/ReluGrad"}
  %compare.47.clone.1 = pred[1,256,200,304]{3,2,1,0} compare(f32[1,256,200,304]{3,2,1,0} %param_2.808, f32[1,256,200,304]{3,2,1,0} %broadcast.156.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn/conv0/Relu_grad/ReluGrad"}
  %param_0.657 = f32[1,256,200,304]{3,2,1,0} parameter(0)
  %param_1.921 = f32[1,256,200,304]{3,2,1,0} parameter(1)
  %add.174.clone.1 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %param_0.657, f32[1,256,200,304]{3,2,1,0} %param_1.921), metadata={op_type="AddN" op_name="tower0/gradients/AddN_87"}
  %select.61.clone.1 = f32[1,256,200,304]{3,2,1,0} select(pred[1,256,200,304]{3,2,1,0} %compare.47.clone.1, f32[1,256,200,304]{3,2,1,0} %add.174.clone.1, f32[1,256,200,304]{3,2,1,0} %broadcast.156.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn/conv0/Relu_grad/ReluGrad"}
  %reduce.110 = f32[256]{0} reduce(f32[1,256,200,304]{3,2,1,0} %select.61.clone.1, f32[] %constant_309_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1223, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.190 = (f32[256]{0}, f32[1,256,200,304]{3,2,1,0}) tuple(f32[256]{0} %reduce.110, f32[1,256,200,304]{3,2,1,0} %select.61.clone.1)
}

%add_float_.1294 (x.1295: f32[], y.1296: f32[]) -> f32[] {
  %x.1295 = f32[] parameter(0)
  %y.1296 = f32[] parameter(1)
  ROOT %add.1297 = f32[] add(f32[] %x.1295, f32[] %y.1296)
}

%fused_computation.258 (param_0.656: f32[1,256,100,152], param_1.920: f32[1,256,100,152], param_2.807: f32[1,256,100,152]) -> (f32[256], f32[1,256,100,152]) {
  %param_2.807 = f32[1,256,100,152]{3,2,1,0} parameter(2)
  %constant_389_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.233.clone.1 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[] %constant_389_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/Relu_grad/ReluGrad"}
  %compare.46.clone.1 = pred[1,256,100,152]{3,2,1,0} compare(f32[1,256,100,152]{3,2,1,0} %param_2.807, f32[1,256,100,152]{3,2,1,0} %broadcast.233.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/Relu_grad/ReluGrad"}
  %param_0.656 = f32[1,256,100,152]{3,2,1,0} parameter(0)
  %param_1.920 = f32[1,256,100,152]{3,2,1,0} parameter(1)
  %add.167.clone.1 = f32[1,256,100,152]{3,2,1,0} add(f32[1,256,100,152]{3,2,1,0} %param_0.656, f32[1,256,100,152]{3,2,1,0} %param_1.920), metadata={op_type="AddN" op_name="tower0/gradients/AddN_88"}
  %select.56.clone.1 = f32[1,256,100,152]{3,2,1,0} select(pred[1,256,100,152]{3,2,1,0} %compare.46.clone.1, f32[1,256,100,152]{3,2,1,0} %add.167.clone.1, f32[1,256,100,152]{3,2,1,0} %broadcast.233.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/Relu_grad/ReluGrad"}
  %reduce.111 = f32[256]{0} reduce(f32[1,256,100,152]{3,2,1,0} %select.56.clone.1, f32[] %constant_389_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1294, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.191 = (f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) tuple(f32[256]{0} %reduce.111, f32[1,256,100,152]{3,2,1,0} %select.56.clone.1)
}

%add_float_.1236 (x.1237: f32[], y.1238: f32[]) -> f32[] {
  %x.1237 = f32[] parameter(0)
  %y.1238 = f32[] parameter(1)
  ROOT %add.1239 = f32[] add(f32[] %x.1237, f32[] %y.1238)
}

%fused_computation.259 (param_0.652: f32[1,256,200,304], param_1.916: f32[1,256,200,304], param_2.803: f32[1,256,200,304]) -> (f32[256], f32[1,256,200,304]) {
  %param_1.916 = f32[1,256,200,304]{3,2,1,0} parameter(1)
  %param_2.803 = f32[1,256,200,304]{3,2,1,0} parameter(2)
  %add.173.clone.1 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %param_1.916, f32[1,256,200,304]{3,2,1,0} %param_2.803), metadata={op_type="AddN" op_name="tower0/gradients/AddN_94"}
  %param_0.652 = f32[1,256,200,304]{3,2,1,0} parameter(0)
  %add.172.clone.1 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %add.173.clone.1, f32[1,256,200,304]{3,2,1,0} %param_0.652), metadata={op_type="AddN" op_name="tower0/gradients/AddN_94"}
  %constant_432 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.112 = f32[256]{0} reduce(f32[1,256,200,304]{3,2,1,0} %add.172.clone.1, f32[] %constant_432), dimensions={0,2,3}, to_apply=%add_float_.1236, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p2/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.192 = (f32[256]{0}, f32[1,256,200,304]{3,2,1,0}) tuple(f32[256]{0} %reduce.112, f32[1,256,200,304]{3,2,1,0} %add.172.clone.1)
}

%add_float_.1307 (x.1308: f32[], y.1309: f32[]) -> f32[] {
  %x.1308 = f32[] parameter(0)
  %y.1309 = f32[] parameter(1)
  ROOT %add.1310 = f32[] add(f32[] %x.1308, f32[] %y.1309)
}

%fused_computation.260 (param_0.653: f32[1,256,100,152], param_1.917: f32[1,256,100,152], param_2.804: f32[1,256,100,152]) -> (f32[256], f32[1,256,100,152]) {
  %param_1.917 = f32[1,256,100,152]{3,2,1,0} parameter(1)
  %param_2.804 = f32[1,256,100,152]{3,2,1,0} parameter(2)
  %add.166.clone.1 = f32[1,256,100,152]{3,2,1,0} add(f32[1,256,100,152]{3,2,1,0} %param_1.917, f32[1,256,100,152]{3,2,1,0} %param_2.804), metadata={op_type="AddN" op_name="tower0/gradients/AddN_95"}
  %param_0.653 = f32[1,256,100,152]{3,2,1,0} parameter(0)
  %add.165.clone.1 = f32[1,256,100,152]{3,2,1,0} add(f32[1,256,100,152]{3,2,1,0} %add.166.clone.1, f32[1,256,100,152]{3,2,1,0} %param_0.653), metadata={op_type="AddN" op_name="tower0/gradients/AddN_95"}
  %constant_433 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.113 = f32[256]{0} reduce(f32[1,256,100,152]{3,2,1,0} %add.165.clone.1, f32[] %constant_433), dimensions={0,2,3}, to_apply=%add_float_.1307, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p3/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.193 = (f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) tuple(f32[256]{0} %reduce.113, f32[1,256,100,152]{3,2,1,0} %add.165.clone.1)
}

%add_float_.1381 (x.1382: f32[], y.1383: f32[]) -> f32[] {
  %x.1382 = f32[] parameter(0)
  %y.1383 = f32[] parameter(1)
  ROOT %add.1384 = f32[] add(f32[] %x.1382, f32[] %y.1383)
}

%fused_computation.261 (param_0.654: f32[1,256,50,76], param_1.918: f32[1,256,50,76], param_2.805: f32[1,256,50,76]) -> (f32[256], f32[1,256,50,76]) {
  %param_1.918 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %param_2.805 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %add.155.clone.1 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %param_1.918, f32[1,256,50,76]{3,2,1,0} %param_2.805), metadata={op_type="AddN" op_name="tower0/gradients/AddN_96"}
  %param_0.654 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %add.154.clone.1 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %add.155.clone.1, f32[1,256,50,76]{3,2,1,0} %param_0.654), metadata={op_type="AddN" op_name="tower0/gradients/AddN_96"}
  %constant_434 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.114 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %add.154.clone.1, f32[] %constant_434), dimensions={0,2,3}, to_apply=%add_float_.1381, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p4/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.194 = (f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.114, f32[1,256,50,76]{3,2,1,0} %add.154.clone.1)
}

%add_float_.1539 (x.1540: f32[], y.1541: f32[]) -> f32[] {
  %x.1540 = f32[] parameter(0)
  %y.1541 = f32[] parameter(1)
  ROOT %add.1542 = f32[] add(f32[] %x.1540, f32[] %y.1541)
}

%fused_computation.262 (param_0.655: f32[1,256,25,38], param_1.919: f32[1,256,25,38], param_2.806: f32[1,256,25,38], param_3.502: f32[1,256,25,38]) -> (f32[256], f32[1,256,25,38]) {
  %param_2.806 = f32[1,256,25,38]{3,2,1,0} parameter(2)
  %param_3.502 = f32[1,256,25,38]{3,2,1,0} parameter(3)
  %add.143.clone.1 = f32[1,256,25,38]{3,2,1,0} add(f32[1,256,25,38]{3,2,1,0} %param_2.806, f32[1,256,25,38]{3,2,1,0} %param_3.502), metadata={op_type="AddN" op_name="tower0/gradients/AddN_98"}
  %param_1.919 = f32[1,256,25,38]{3,2,1,0} parameter(1)
  %add.142.clone.1 = f32[1,256,25,38]{3,2,1,0} add(f32[1,256,25,38]{3,2,1,0} %add.143.clone.1, f32[1,256,25,38]{3,2,1,0} %param_1.919), metadata={op_type="AddN" op_name="tower0/gradients/AddN_98"}
  %param_0.655 = f32[1,256,25,38]{3,2,1,0} parameter(0)
  %add.141.clone.1 = f32[1,256,25,38]{3,2,1,0} add(f32[1,256,25,38]{3,2,1,0} %add.142.clone.1, f32[1,256,25,38]{3,2,1,0} %param_0.655), metadata={op_type="AddN" op_name="tower0/gradients/AddN_98"}
  %constant_435 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.115 = f32[256]{0} reduce(f32[1,256,25,38]{3,2,1,0} %add.141.clone.1, f32[] %constant_435), dimensions={0,2,3}, to_apply=%add_float_.1539, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p5/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.195 = (f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) tuple(f32[256]{0} %reduce.115, f32[1,256,25,38]{3,2,1,0} %add.141.clone.1)
}

%add_float_.1321 (x.1322: f32[], y.1323: f32[]) -> f32[] {
  %x.1322 = f32[] parameter(0)
  %y.1323 = f32[] parameter(1)
  ROOT %add.1324 = f32[] add(f32[] %x.1322, f32[] %y.1323)
}

%scalar_add_computation (scalar_lhs: f32[], scalar_rhs: f32[]) -> f32[] {
  %scalar_lhs = f32[] parameter(0)
  %scalar_rhs = f32[] parameter(1)
  ROOT %add.6 = f32[] add(f32[] %scalar_lhs, f32[] %scalar_rhs)
}

%fused_computation.263 (param_0.651: f32[1,256,100,152], param_1.915: f32[1,4], param_2.802: f32[1,256,200,304]) -> (f32[256], f32[1,256,100,152]) {
  %param_0.651 = f32[1,256,100,152]{3,2,1,0} parameter(0)
  %param_2.802 = f32[1,256,200,304]{3,2,1,0} parameter(2)
  %bitcast.398.clone.1 = f32[1,256,100,152,2,2]{5,3,4,2,1,0} bitcast(f32[1,256,200,304]{3,2,1,0} %param_2.802), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat3/transpose_grad/transpose"}
  %copy.452.clone.1 = f32[1,256,100,152,2,2]{5,4,3,2,1,0} copy(f32[1,256,100,152,2,2]{5,3,4,2,1,0} %bitcast.398.clone.1), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat3/transpose_grad/transpose"}
  %bitcast.397.clone.1 = f32[3891200,4]{1,0} bitcast(f32[1,256,100,152,2,2]{5,4,3,2,1,0} %copy.452.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat3/Tensordot_grad/Reshape"}
  %param_1.915 = f32[1,4]{1,0} parameter(1)
  %bitcast.405.clone.1 = f32[4]{0} bitcast(f32[1,4]{1,0} %param_1.915)
  %broadcast.150.clone.1 = f32[3891200,4]{1,0} broadcast(f32[4]{0} %bitcast.405.clone.1), dimensions={1}
  %multiply.248.clone.1 = f32[3891200,4]{1,0} multiply(f32[3891200,4]{1,0} %bitcast.397.clone.1, f32[3891200,4]{1,0} %broadcast.150.clone.1)
  %constant_306_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.61.clone.1 = f32[3891200]{0} reduce(f32[3891200,4]{1,0} %multiply.248.clone.1, f32[] %constant_306_clone_1), dimensions={1}, to_apply=%scalar_add_computation
  %bitcast.396.clone.1 = f32[1,256,100,152]{3,2,1,0} bitcast(f32[3891200]{0} %reduce.61.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat3/ExpandDims_grad/Reshape"}
  %add.164.clone.1 = f32[1,256,100,152]{3,2,1,0} add(f32[1,256,100,152]{3,2,1,0} %param_0.651, f32[1,256,100,152]{3,2,1,0} %bitcast.396.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_104"}
  %reduce.116 = f32[256]{0} reduce(f32[1,256,100,152]{3,2,1,0} %add.164.clone.1, f32[] %constant_306_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1321, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c3/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.196 = (f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) tuple(f32[256]{0} %reduce.116, f32[1,256,100,152]{3,2,1,0} %add.164.clone.1)
}

%add_float_.1395 (x.1396: f32[], y.1397: f32[]) -> f32[] {
  %x.1396 = f32[] parameter(0)
  %y.1397 = f32[] parameter(1)
  ROOT %add.1398 = f32[] add(f32[] %x.1396, f32[] %y.1397)
}

%fused_computation.264 (param_0.650: f32[1,256,50,76], param_1.914: f32[1,4], param_2.801: f32[1,256,100,152]) -> (f32[256], f32[1,256,50,76]) {
  %param_0.650 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_2.801 = f32[1,256,100,152]{3,2,1,0} parameter(2)
  %bitcast.395.clone.1 = f32[1,256,50,76,2,2]{5,3,4,2,1,0} bitcast(f32[1,256,100,152]{3,2,1,0} %param_2.801), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat4/transpose_grad/transpose"}
  %copy.451.clone.1 = f32[1,256,50,76,2,2]{5,4,3,2,1,0} copy(f32[1,256,50,76,2,2]{5,3,4,2,1,0} %bitcast.395.clone.1), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat4/transpose_grad/transpose"}
  %bitcast.394.clone.1 = f32[972800,4]{1,0} bitcast(f32[1,256,50,76,2,2]{5,4,3,2,1,0} %copy.451.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat4/Tensordot_grad/Reshape"}
  %param_1.914 = f32[1,4]{1,0} parameter(1)
  %bitcast.402.clone.1 = f32[4]{0} bitcast(f32[1,4]{1,0} %param_1.914)
  %broadcast.138.clone.1 = f32[972800,4]{1,0} broadcast(f32[4]{0} %bitcast.402.clone.1), dimensions={1}
  %multiply.238.clone.1 = f32[972800,4]{1,0} multiply(f32[972800,4]{1,0} %bitcast.394.clone.1, f32[972800,4]{1,0} %broadcast.138.clone.1)
  %constant_301_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.60.clone.1 = f32[972800]{0} reduce(f32[972800,4]{1,0} %multiply.238.clone.1, f32[] %constant_301_clone_1), dimensions={1}, to_apply=%scalar_add_computation
  %bitcast.393.clone.1 = f32[1,256,50,76]{3,2,1,0} bitcast(f32[972800]{0} %reduce.60.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat4/ExpandDims_grad/Reshape"}
  %add.153.clone.1 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %param_0.650, f32[1,256,50,76]{3,2,1,0} %bitcast.393.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_106"}
  %reduce.117 = f32[256]{0} reduce(f32[1,256,50,76]{3,2,1,0} %add.153.clone.1, f32[] %constant_301_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1395, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c4/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.197 = (f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) tuple(f32[256]{0} %reduce.117, f32[1,256,50,76]{3,2,1,0} %add.153.clone.1)
}

%add_float_.1552 (x.1553: f32[], y.1554: f32[]) -> f32[] {
  %x.1553 = f32[] parameter(0)
  %y.1554 = f32[] parameter(1)
  ROOT %add.1555 = f32[] add(f32[] %x.1553, f32[] %y.1554)
}

%fused_computation.265 (param_0.649: f32[1,256,25,38], param_1.913: f32[1,4], param_2.800: f32[1,256,50,76]) -> (f32[256], f32[1,256,25,38]) {
  %param_0.649 = f32[1,256,25,38]{3,2,1,0} parameter(0)
  %param_2.800 = f32[1,256,50,76]{3,2,1,0} parameter(2)
  %bitcast.392.clone.1 = f32[1,256,25,38,2,2]{5,3,4,2,1,0} bitcast(f32[1,256,50,76]{3,2,1,0} %param_2.800), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat5/transpose_grad/transpose"}
  %copy.450.clone.1 = f32[1,256,25,38,2,2]{5,4,3,2,1,0} copy(f32[1,256,25,38,2,2]{5,3,4,2,1,0} %bitcast.392.clone.1), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat5/transpose_grad/transpose"}
  %bitcast.391.clone.1 = f32[243200,4]{1,0} bitcast(f32[1,256,25,38,2,2]{5,4,3,2,1,0} %copy.450.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat5/Tensordot_grad/Reshape"}
  %param_1.913 = f32[1,4]{1,0} parameter(1)
  %bitcast.399.clone.1 = f32[4]{0} bitcast(f32[1,4]{1,0} %param_1.913)
  %broadcast.124.clone.1 = f32[243200,4]{1,0} broadcast(f32[4]{0} %bitcast.399.clone.1), dimensions={1}
  %multiply.228.clone.1 = f32[243200,4]{1,0} multiply(f32[243200,4]{1,0} %bitcast.391.clone.1, f32[243200,4]{1,0} %broadcast.124.clone.1)
  %constant_294_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.59.clone.1 = f32[243200]{0} reduce(f32[243200,4]{1,0} %multiply.228.clone.1, f32[] %constant_294_clone_1), dimensions={1}, to_apply=%scalar_add_computation
  %bitcast.390.clone.1 = f32[1,256,25,38]{3,2,1,0} bitcast(f32[243200]{0} %reduce.59.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat5/ExpandDims_grad/Reshape"}
  %add.140.clone.1 = f32[1,256,25,38]{3,2,1,0} add(f32[1,256,25,38]{3,2,1,0} %param_0.649, f32[1,256,25,38]{3,2,1,0} %bitcast.390.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_108"}
  %reduce.118 = f32[256]{0} reduce(f32[1,256,25,38]{3,2,1,0} %add.140.clone.1, f32[] %constant_294_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1552, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c5/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.198 = (f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) tuple(f32[256]{0} %reduce.118, f32[1,256,25,38]{3,2,1,0} %add.140.clone.1)
}

ENTRY %cluster_4__XlaCompiledKernel_true__XlaNumConstantArgs_723__XlaNumResourceArgs_0_.4154 (arg0.1: f32[1,1,1024,512], arg1.2: f32[1,1,256,1024], arg2.3: f32[3,3,256,256], arg3.4: f32[3,3,256,256], arg4.5: f32[1,1,1024,256], arg5.6: f32[1,1,256,1024], arg6.7: f32[3,3,256,256], arg7.8: f32[1,1,1024,256], arg8.9: f32[1,1,256,1024], arg9.10: f32[3,3,256,256], arg10.11: f32[1,1,1024,256], arg11.12: f32[1,1,256,1024], arg12.13: f32[3,3,256,256], arg13.14: f32[1,1,1024,256], arg14.15: f32[1,1,256,1024], arg15.16: f32[3,3,256,256], arg16.17: f32[1,1,1024,256], arg17.18: f32[1,1,512,1024], arg18.19: f32[1,1,128,512], arg19.20: f32[3,3,128,128], arg20.21: f32[1,1,512,128], arg21.22: f32[1,1,128,512], arg22.23: f32[3,3,128,128], arg23.24: f32[1,1,512,128], arg24.25: f32[1,1,128,512], arg25.26: f32[3,3,128,128], arg26.27: f32[1,1,512,128], arg27.28: f32[1,1,256,512], arg28.29: f32[1,1,128,512], arg29.30: f32[3,3,128,128], arg30.31: f32[1,1,256,128], arg31.32: f32[1,1,256,1024], arg32.33: f32[3,3,256,256], arg33.34: f32[1,1,512,256], arg34.35: f32[3,3,512,512], arg35.36: f32[1,1,512,2048], arg36.37: f32[1,1,1024,2048], arg37.38: f32[1,1,2048,512], arg38.39: f32[3,3,512,512], arg39.40: f32[1,1,512,2048], arg40.41: f32[1,1,2048,512], arg41.42: f32[3,3,512,512], arg42.43: f32[1,1,512,2048], arg43.44: f32[1,1,2048,256], arg44.45: f32[1,1,1024,256], arg45.46: f32[1,1,512,256], arg46.47: f32[1,1,256,256], arg47.48: f32[3,3,256,256], arg48.49: f32[3,3,256,256], arg49.50: f32[1,1,256,3], arg50.51: f32[1,1,256,12], arg51.52: f32[3,3,256,256], arg52.53: f32[3,3,256,256], arg53.54: f32[3,3,256,256], arg54.55: pred[], arg55.56: pred[], arg56.57: pred[], arg57.58: pred[], arg58.59: f32[], arg59.60: f32[], arg60.61: f32[6], arg61.62: f32[6], arg62.63: pred[], arg63.64: pred[], arg64.65: f32[6,4], arg65.66: f32[6,4], arg66.67: f32[], arg67.68: f32[14], arg68.69: f32[6], arg69.70: f32[14], arg70.71: pred[6], arg71.72: f32[6], arg72.73: pred[], arg73.74: pred[], arg74.75: f32[3,4], arg75.76: s64[6], arg76.77: f32[], arg77.78: f32[3,4], arg78.79: f32[1,256,25,38], arg79.80: f32[6,4], arg80.81: f32[50], arg81.82: f32[14], arg82.83: f32[1,256,13,19], arg83.84: f32[50], arg84.85: pred[14], arg85.86: f32[14], arg86.87: pred[], arg87.88: pred[], arg88.89: s64[14], arg89.90: f32[1,256,50,76], arg90.91: f32[3,4], arg91.92: s64[6], arg92.93: f32[186], arg93.94: f32[50], arg94.95: f32[186], arg95.96: pred[50], arg96.97: f32[50], arg97.98: f32[1,256,25,38], arg98.99: f32[1,256,13,19], arg99.100: s64[50], arg100.101: f32[1,256,100,152], arg101.102: s64[3], arg102.103: f32[186], arg103.104: pred[186], arg104.105: f32[186], arg105.106: f32[1,256,50,76], arg106.107: s64[186], arg107.108: f32[1,256,200,304], arg108.109: f32[1,256,100,152], arg109.110: f32[1,256,200,304], arg110.111: f32[1,256,200,304], arg111.112: f32[1,256,200,304], arg112.113: f32[1,256,100,152], arg113.114: f32[1,256,100,152], arg114.115: f32[1,256,50,76], arg115.116: f32[1,256,50,76], arg116.117: f32[1,256,25,38], arg117.118: f32[1,256,25,38], arg118.119: f32[1,256,200,304], arg119.120: f32[1,256,100,152], arg120.121: f32[1,256,50,76], arg121.122: f32[1,256,25,38], arg122.123: f32[1,256,200,304], arg123.124: f32[1,4], arg124.125: f32[1,512,100,152], arg125.126: f32[1,1024,50,76], arg126.127: f32[1,2048,25,38], arg127.128: f32[1,2048,1,1], arg128.129: f32[1,2048,25,38], arg129.130: f32[1,2048,1,1], arg130.131: f32[1,512,25,38], arg131.132: f32[1,2048,1,1], arg132.133: f32[1,512,1,1], arg133.134: f32[1,512,25,38], arg134.135: f32[1,512,1,1], arg135.136: f32[1,512,25,38], arg136.137: f32[1,512,1,1], arg137.138: f32[1,512,1,1], arg138.139: f32[1,512,25,38], arg139.140: f32[1,512,1,1], arg140.141: f32[1,2048,25,38], arg141.142: f32[1,512,1,1], arg142.143: f32[1,2048,1,1], arg143.144: f32[1,2048,25,38], arg144.145: f32[1,2048,1,1], arg145.146: f32[1,512,25,38], arg146.147: f32[1,2048,1,1], arg147.148: f32[1,512,1,1], arg148.149: f32[1,512,25,38], arg149.150: f32[1,512,1,1], arg150.151: f32[1,512,25,38], arg151.152: f32[1,512,1,1], arg152.153: f32[1,512,1,1], arg153.154: f32[1,512,25,38], arg154.155: f32[1,512,1,1], arg155.156: f32[1,2048,25,38], arg156.157: f32[1,512,1,1], arg157.158: f32[1,2048,1,1], arg158.159: f32[1,2048,25,38], arg159.160: f32[1,2048,1,1], arg160.161: f32[1,2048,25,38], arg161.162: f32[1,2048,1,1], arg162.163: f32[1,2048,1,1], arg163.164: f32[1,512,25,38], arg164.165: f32[1,2048,1,1], arg165.166: f32[1,2048,1,1], arg166.167: f32[1,512,1,1], arg167.168: f32[1,512,25,38], arg168.169: f32[1,512,1,1], arg169.170: f32[1,512,51,77], arg170.171: f32[1,512,1,1], arg171.172: f32[1,512,50,76], arg172.173: f32[1,512,50,76], arg173.174: f32[1,512,1,1], arg174.175: f32[1,512,1,1], arg175.176: f32[1,512,1,1], arg176.177: f32[1,1024,50,76], arg177.178: f32[1,1024,1,1], arg178.179: f32[1,1024,1,1], arg179.180: f32[1,256,50,76], arg180.181: f32[1,1024,1,1], arg181.182: f32[1,256,50,76], arg182.183: f32[1,256,1,1], arg183.184: f32[1,256,1,1], arg184.185: f32[1,256,50,76], arg185.186: f32[1,256,1,1], arg186.187: f32[1,256,1,1], arg187.188: f32[1,256,50,76], arg188.189: f32[1,256,1,1], arg189.190: f32[1,1024,50,76], arg190.191: f32[1,256,1,1], arg191.192: f32[1,1024,1,1], arg192.193: f32[1,1024,50,76], arg193.194: f32[1,1024,1,1], arg194.195: f32[1,256,50,76], arg195.196: f32[1,1024,1,1], arg196.197: f32[1,256,1,1], arg197.198: f32[1,256,50,76], arg198.199: f32[1,256,1,1], arg199.200: f32[1,256,50,76], arg200.201: f32[1,256,1,1], arg201.202: f32[1,256,1,1], arg202.203: f32[1,256,50,76], arg203.204: f32[1,256,1,1], arg204.205: f32[1,1024,50,76], arg205.206: f32[1,256,1,1], arg206.207: f32[1,1024,1,1], arg207.208: f32[1,1024,50,76], arg208.209: f32[1,1024,1,1], arg209.210: f32[1,256,50,76], arg210.211: f32[1,1024,1,1], arg211.212: f32[1,256,1,1], arg212.213: f32[1,256,50,76], arg213.214: f32[1,256,1,1], arg214.215: f32[1,256,50,76], arg215.216: f32[1,256,1,1], arg216.217: f32[1,256,1,1], arg217.218: f32[1,256,50,76], arg218.219: f32[1,256,1,1], arg219.220: f32[1,1024,50,76], arg220.221: f32[1,256,1,1], arg221.222: f32[1,1024,1,1], arg222.223: f32[1,1024,50,76], arg223.224: f32[1,1024,1,1], arg224.225: f32[1,256,50,76], arg225.226: f32[1,1024,1,1], arg226.227: f32[1,256,1,1], arg227.228: f32[1,256,50,76], arg228.229: f32[1,256,1,1], arg229.230: f32[1,256,50,76], arg230.231: f32[1,256,1,1], arg231.232: f32[1,256,1,1], arg232.233: f32[1,256,50,76], arg233.234: f32[1,256,1,1], arg234.235: f32[1,1024,50,76], arg235.236: f32[1,256,1,1], arg236.237: f32[1,1024,1,1], arg237.238: f32[1,1024,50,76], arg238.239: f32[1,1024,1,1], arg239.240: f32[1,256,50,76], arg240.241: f32[1,1024,1,1], arg241.242: f32[1,256,1,1], arg242.243: f32[1,256,50,76], arg243.244: f32[1,256,1,1], arg244.245: f32[1,256,50,76], arg245.246: f32[1,256,1,1], arg246.247: f32[1,256,1,1], arg247.248: f32[1,256,50,76], arg248.249: f32[1,256,1,1], arg249.250: f32[1,1024,50,76], arg250.251: f32[1,256,1,1], arg251.252: f32[1,1024,1,1], arg252.253: f32[1,1024,50,76], arg253.254: f32[1,1024,1,1], arg254.255: f32[1,1024,50,76], arg255.256: f32[1,1024,1,1], arg256.257: f32[1,1024,1,1], arg257.258: f32[1,256,50,76], arg258.259: f32[1,1024,1,1], arg259.260: f32[1,1024,1,1], arg260.261: f32[1,256,1,1], arg261.262: f32[1,256,50,76], arg262.263: f32[1,256,1,1], arg263.264: f32[1,256,101,153], arg264.265: f32[1,256,1,1], arg265.266: f32[1,256,100,152], arg266.267: f32[1,256,1,1], arg267.268: f32[1,256,100,152], arg268.269: f32[1,256,1,1], arg269.270: f32[1,256,1,1], arg270.271: f32[1,512,1,1], arg271.272: f32[1,512,100,152], arg272.273: f32[1,512,1,1], arg273.274: f32[1,128,100,152], arg274.275: f32[1,512,1,1], arg275.276: f32[1,128,1,1], arg276.277: f32[1,128,100,152], arg277.278: f32[1,128,1,1], arg278.279: f32[1,128,100,152], arg279.280: f32[1,128,1,1], arg280.281: f32[1,128,1,1], arg281.282: f32[1,128,100,152], arg282.283: f32[1,128,1,1], arg283.284: f32[1,512,100,152], arg284.285: f32[1,128,1,1], arg285.286: f32[1,512,1,1], arg286.287: f32[1,512,100,152], arg287.288: f32[1,512,1,1], arg288.289: f32[1,128,100,152], arg289.290: f32[1,512,1,1], arg290.291: f32[1,128,1,1], arg291.292: f32[1,128,100,152], arg292.293: f32[1,128,1,1], arg293.294: f32[1,128,100,152], arg294.295: f32[1,128,1,1], arg295.296: f32[1,128,1,1], arg296.297: f32[1,128,100,152], arg297.298: f32[1,128,1,1], arg298.299: f32[1,512,100,152], arg299.300: f32[1,128,1,1], arg300.301: f32[1,512,1,1], arg301.302: f32[1,512,100,152], arg302.303: f32[1,512,1,1], arg303.304: f32[1,128,100,152], arg304.305: f32[1,512,1,1], arg305.306: f32[1,128,1,1], arg306.307: f32[1,128,100,152], arg307.308: f32[1,128,1,1], arg308.309: f32[1,128,100,152], arg309.310: f32[1,128,1,1], arg310.311: f32[1,128,1,1], arg311.312: f32[1,128,100,152], arg312.313: f32[1,128,1,1], arg313.314: f32[1,512,100,152], arg314.315: f32[1,128,1,1], arg315.316: f32[1,512,1,1], arg316.317: f32[1,512,100,152], arg317.318: f32[1,512,100,152], arg318.319: f32[1,512,1,1], arg319.320: f32[1,512,1,1], arg320.321: f32[1,512,1,1], arg321.322: f32[1,128,100,152], arg322.323: f32[1,512,1,1], arg323.324: f32[1,512,1,1], arg324.325: f32[1,128,100,152], arg325.326: f32[1,128,1,1], arg326.327: f32[1,128,1,1], arg327.328: f32[1,128,201,305], arg328.329: f32[1,128,1,1], arg329.330: f32[1,128,200,304], arg330.331: f32[1,128,200,304], arg331.332: f32[1,128,1,1], arg332.333: f32[1,128,1,1], arg333.334: f32[1,128,1,1]) -> (f32[3], f32[1,1,256,3], f32[12], f32[1,1,256,12], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[256], f32[256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[256], f32[1,1,256,256], f32[256], f32[1,1,512,256], f32[256], f32[1,1,1024,256], f32[256], f32[1,1,2048,256], f32[2048], f32[1,1,512,2048], f32[2048], f32[512], f32[3,3,512,512], f32[512], f32[512], f32[1,1,2048,512], f32[512], f32[2048], f32[1,1,512,2048], f32[2048], f32[512], f32[3,3,512,512], f32[512], f32[512], f32[1,1,2048,512], f32[512], f32[2048], f32[2048], f32[1,1,512,2048], f32[1,1,1024,2048], f32[2048], f32[2048], f32[512], f32[3,3,512,512], f32[512], f32[512], f32[1,1,1024,512], f32[512], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1024], f32[1,1,512,1024], f32[1,1,256,1024], f32[1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,512,256], f32[256], f32[512], f32[1,1,128,512], f32[512], f32[128], f32[3,3,128,128], f32[128], f32[128], f32[1,1,512,128], f32[128], f32[512], f32[1,1,128,512], f32[512], f32[128], f32[3,3,128,128], f32[128], f32[128], f32[1,1,512,128], f32[128], f32[512], f32[1,1,128,512], f32[512], f32[128], f32[3,3,128,128], f32[128], f32[128], f32[1,1,512,128], f32[128], f32[512], f32[512], f32[1,1,256,512], f32[1,1,128,512], f32[512], f32[512], f32[128], f32[3,3,128,128], f32[128], f32[128], f32[1,1,256,128], f32[128]) {
  %arg56.57 = pred[] parameter(56), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg57.58 = pred[] parameter(57), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg60.61 = f32[6]{0} parameter(60), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg61.62 = f32[6]{0} parameter(61), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg67.68 = f32[14]{0} parameter(67), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg69.70 = f32[14]{0} parameter(69), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg73.74 = pred[] parameter(73), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg80.81 = f32[50]{0} parameter(80), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg83.84 = f32[50]{0} parameter(83), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg87.88 = pred[] parameter(87), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg92.93 = f32[186]{0} parameter(92), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg94.95 = f32[186]{0} parameter(94), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg75.76 = s64[6]{0} parameter(75), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg71.72 = f32[6]{0} parameter(71), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg70.71 = pred[6]{0} parameter(70), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg68.69 = f32[6]{0} parameter(68), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg58.59 = f32[] parameter(58), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg54.55 = pred[] parameter(54), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.228 = f32[2850]{0} fusion(s64[6]{0} %arg75.76, f32[6]{0} %arg71.72, pred[6]{0} %arg70.71, f32[6]{0} %arg68.69, f32[] %arg58.59, pred[] %arg54.55), kind=kInput, calls=%fused_computation.228, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %fusion.247 = f32[3]{0} fusion(f32[2850]{0} %fusion.228), kind=kInput, calls=%fused_computation.247, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/class/BiasAdd_grad/BiasAddGrad"}
  %arg88.89 = s64[14]{0} parameter(88), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg85.86 = f32[14]{0} parameter(85), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg84.85 = pred[14]{0} parameter(84), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg81.82 = f32[14]{0} parameter(81), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg62.63 = pred[] parameter(62), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.233 = f32[11400]{0} fusion(s64[14]{0} %arg88.89, f32[14]{0} %arg85.86, pred[14]{0} %arg84.85, f32[14]{0} %arg81.82, f32[] %arg58.59, pred[] %arg62.63), kind=kInput, calls=%fused_computation.233, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %fusion.246 = f32[3]{0} fusion(f32[11400]{0} %fusion.233), kind=kInput, calls=%fused_computation.246, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/class/BiasAdd_grad/BiasAddGrad"}
  %arg106.107 = s64[186]{0} parameter(106), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg104.105 = f32[186]{0} parameter(104), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg103.104 = pred[186]{0} parameter(103), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg102.103 = f32[186]{0} parameter(102), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg86.87 = pred[] parameter(86), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.241 = f32[182400]{0} fusion(s64[186]{0} %arg106.107, f32[186]{0} %arg104.105, pred[186]{0} %arg103.104, f32[186]{0} %arg102.103, f32[] %arg58.59, pred[] %arg86.87), kind=kInput, calls=%fused_computation.241, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level2/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %fusion.245 = f32[3]{0} fusion(f32[182400]{0} %fusion.241), kind=kInput, calls=%fused_computation.245, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/class/BiasAdd_grad/BiasAddGrad"}
  %arg99.100 = s64[50]{0} parameter(99), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg96.97 = f32[50]{0} parameter(96), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg95.96 = pred[50]{0} parameter(95), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg93.94 = f32[50]{0} parameter(93), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg72.73 = pred[] parameter(72), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.238 = f32[45600]{0} fusion(s64[50]{0} %arg99.100, f32[50]{0} %arg96.97, pred[50]{0} %arg95.96, f32[50]{0} %arg93.94, f32[] %arg58.59, pred[] %arg72.73), kind=kInput, calls=%fused_computation.238, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level3/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %fusion.244 = f32[3]{0} fusion(f32[45600]{0} %fusion.238), kind=kInput, calls=%fused_computation.244, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/class/BiasAdd_grad/BiasAddGrad"}
  %fusion.142 = f32[3]{0} fusion(f32[3]{0} %fusion.247, f32[3]{0} %fusion.246, f32[3]{0} %fusion.245, f32[3]{0} %fusion.244), kind=kLoop, calls=%fused_computation.142, metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
  %arg49.50 = f32[1,1,256,3]{3,2,1,0} parameter(49), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg82.83 = f32[1,256,13,19]{3,2,1,0} parameter(82), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %constant_758 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.56 = f32[1,3,13,19]{3,2,1,0} broadcast(f32[] %constant_758), dimensions={}
  %custom-call.164 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,13,19]{3,2,1,0} %arg82.83, f32[1,3,13,19]{3,2,1,0} %broadcast.56), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_4/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.460 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.164), index=0
  %arg78.79 = f32[1,256,25,38]{3,2,1,0} parameter(78), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.248 = f32[1,3,25,38]{3,2,1,0} fusion(f32[2850]{0} %fusion.228), kind=kLoop, calls=%fused_computation.248, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_3/transpose_grad/transpose"}
  %custom-call.158 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %arg78.79, f32[1,3,25,38]{3,2,1,0} %fusion.248), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_3/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.454 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.158), index=0
  %arg89.90 = f32[1,256,50,76]{3,2,1,0} parameter(89), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.250 = f32[1,3,50,76]{3,2,1,0} fusion(f32[11400]{0} %fusion.233), kind=kLoop, calls=%fused_computation.250, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_2/transpose_grad/transpose"}
  %custom-call.148 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg89.90, f32[1,3,50,76]{3,2,1,0} %fusion.250), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_2/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.444 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.148), index=0
  %arg107.108 = f32[1,256,200,304]{3,2,1,0} parameter(107), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.253 = f32[1,3,200,304]{3,2,1,0} fusion(f32[182400]{0} %fusion.241), kind=kLoop, calls=%fused_computation.253, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn/transpose_grad/transpose"}
  %custom-call.129 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %arg107.108, f32[1,3,200,304]{3,2,1,0} %fusion.253), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.425 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.129), index=0
  %arg100.101 = f32[1,256,100,152]{3,2,1,0} parameter(100), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.252 = f32[1,3,100,152]{3,2,1,0} fusion(f32[45600]{0} %fusion.238), kind=kLoop, calls=%fused_computation.252, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_1/transpose_grad/transpose"}
  %custom-call.138 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %arg100.101, f32[1,3,100,152]{3,2,1,0} %fusion.252), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_1/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.434 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.138), index=0
  %fusion.141 = f32[1,1,256,3]{3,2,1,0} fusion(f32[1,1,256,3]{3,2,1,0} %arg49.50, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.460, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.454, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.444, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.425, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.434), kind=kLoop, calls=%fused_computation.141, metadata={op_name="XLA_Retvals"}
  %arg91.92 = s64[6]{0} parameter(91), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg64.65 = f32[6,4]{1,0} parameter(64), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg66.67 = f32[] parameter(66), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg79.80 = f32[6,4]{1,0} parameter(79), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg76.77 = f32[] parameter(76), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg59.60 = f32[] parameter(59), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg55.56 = pred[] parameter(55), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg65.66 = f32[6,4]{1,0} parameter(65), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.229 = f32[2850,4]{1,0} fusion(s64[6]{0} %arg91.92, f32[6,4]{1,0} %arg64.65, f32[] %arg66.67, f32[6,4]{1,0} %arg79.80, f32[] %arg76.77, f32[] %arg59.60, pred[] %arg55.56, f32[6,4]{1,0} %arg65.66), kind=kInput, calls=%fused_computation.229, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %fusion.243 = f32[12]{0} fusion(f32[2850,4]{1,0} %fusion.229), kind=kInput, calls=%fused_computation.243, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/box/BiasAdd_grad/BiasAddGrad"}
  %arg101.102 = s64[3]{0} parameter(101), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg74.75 = f32[3,4]{1,0} parameter(74), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg90.91 = f32[3,4]{1,0} parameter(90), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg63.64 = pred[] parameter(63), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg77.78 = f32[3,4]{1,0} parameter(77), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.234 = f32[11400,4]{1,0} fusion(s64[3]{0} %arg101.102, f32[3,4]{1,0} %arg74.75, f32[] %arg66.67, f32[3,4]{1,0} %arg90.91, f32[] %arg76.77, f32[] %arg59.60, pred[] %arg63.64, f32[3,4]{1,0} %arg77.78), kind=kInput, calls=%fused_computation.234, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %fusion.242 = f32[12]{0} fusion(f32[11400,4]{1,0} %fusion.234), kind=kInput, calls=%fused_computation.242, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/box/BiasAdd_grad/BiasAddGrad"}
  %fusion.140 = f32[12]{0} fusion(f32[12]{0} %fusion.243, f32[12]{0} %fusion.242), kind=kLoop, calls=%fused_computation.140, metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
  %arg50.51 = f32[1,1,256,12]{3,2,1,0} parameter(50), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %broadcast.63 = f32[1,12,13,19]{3,2,1,0} broadcast(f32[] %constant_758), dimensions={}
  %custom-call.162 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,13,19]{3,2,1,0} %arg82.83, f32[1,12,13,19]{3,2,1,0} %broadcast.63), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_4/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.458 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.162), index=0
  %fusion.249 = f32[1,12,25,38]{3,2,1,0} fusion(f32[2850,4]{1,0} %fusion.229), kind=kLoop, calls=%fused_computation.249, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_3/transpose_1_grad/transpose"}
  %custom-call.156 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %arg78.79, f32[1,12,25,38]{3,2,1,0} %fusion.249), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_3/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.452 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.156), index=0
  %fusion.251 = f32[1,12,50,76]{3,2,1,0} fusion(f32[11400,4]{1,0} %fusion.234), kind=kLoop, calls=%fused_computation.251, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_2/transpose_1_grad/transpose"}
  %custom-call.146 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg89.90, f32[1,12,50,76]{3,2,1,0} %fusion.251), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_2/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.442 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.146), index=0
  %broadcast.57 = f32[1,12,200,304]{3,2,1,0} broadcast(f32[] %constant_758), dimensions={}
  %custom-call.127 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %arg107.108, f32[1,12,200,304]{3,2,1,0} %broadcast.57), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.423 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.127), index=0
  %broadcast.58 = f32[1,12,100,152]{3,2,1,0} broadcast(f32[] %constant_758), dimensions={}
  %custom-call.136 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %arg100.101, f32[1,12,100,152]{3,2,1,0} %broadcast.58), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_1/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.432 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.136), index=0
  %fusion.139 = f32[1,1,256,12]{3,2,1,0} fusion(f32[1,1,256,12]{3,2,1,0} %arg50.51, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.458, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.452, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.442, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.423, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.432), kind=kLoop, calls=%fused_computation.139, metadata={op_name="XLA_Retvals"}
  %bitcast.6 = f32[1,1,256,12]{1,0,3,2} bitcast(f32[1,1,256,12]{3,2,1,0} %arg50.51), metadata={op_name="XLA_Args"}
  %custom-call.163 = (f32[1,256,13,19]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,12,13,19]{3,2,1,0} %broadcast.63, f32[1,1,256,12]{1,0,3,2} %bitcast.6), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_4/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.459 = f32[1,256,13,19]{3,2,1,0} get-tuple-element((f32[1,256,13,19]{3,2,1,0}, u8[0]{0}) %custom-call.163), index=0
  %bitcast.7 = f32[1,1,256,3]{1,0,3,2} bitcast(f32[1,1,256,3]{3,2,1,0} %arg49.50), metadata={op_name="XLA_Args"}
  %custom-call.165 = (f32[1,256,13,19]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,3,13,19]{3,2,1,0} %broadcast.56, f32[1,1,256,3]{1,0,3,2} %bitcast.7), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_4/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.461 = f32[1,256,13,19]{3,2,1,0} get-tuple-element((f32[1,256,13,19]{3,2,1,0}, u8[0]{0}) %custom-call.165), index=0
  %fusion.254 = (f32[256]{0}, f32[1,256,13,19]{3,2,1,0}) fusion(f32[1,256,13,19]{3,2,1,0} %get-tuple-element.459, f32[1,256,13,19]{3,2,1,0} %get-tuple-element.461, f32[1,256,13,19]{3,2,1,0} %arg82.83), kind=kInput, calls=%fused_computation.254, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.668 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,13,19]{3,2,1,0}) %fusion.254), index=0
  %custom-call.157 = (f32[1,256,25,38]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,12,25,38]{3,2,1,0} %fusion.249, f32[1,1,256,12]{1,0,3,2} %bitcast.6), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_3/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.453 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[1,256,25,38]{3,2,1,0}, u8[0]{0}) %custom-call.157), index=0
  %custom-call.159 = (f32[1,256,25,38]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,3,25,38]{3,2,1,0} %fusion.248, f32[1,1,256,3]{1,0,3,2} %bitcast.7), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_3/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.455 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[1,256,25,38]{3,2,1,0}, u8[0]{0}) %custom-call.159), index=0
  %fusion.255 = (f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) fusion(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.453, f32[1,256,25,38]{3,2,1,0} %get-tuple-element.455, f32[1,256,25,38]{3,2,1,0} %arg78.79), kind=kInput, calls=%fused_computation.255, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.670 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) %fusion.255), index=0
  %custom-call.147 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,12,50,76]{3,2,1,0} %fusion.251, f32[1,1,256,12]{1,0,3,2} %bitcast.6), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_2/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.443 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.147), index=0
  %custom-call.149 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,3,50,76]{3,2,1,0} %fusion.250, f32[1,1,256,3]{1,0,3,2} %bitcast.7), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_2/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.445 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.149), index=0
  %fusion.256 = (f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.443, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.445, f32[1,256,50,76]{3,2,1,0} %arg89.90), kind=kInput, calls=%fused_computation.256, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.672 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.256), index=0
  %custom-call.128 = (f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,12,200,304]{3,2,1,0} %broadcast.57, f32[1,1,256,12]{1,0,3,2} %bitcast.6), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.424 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.128), index=0
  %custom-call.130 = (f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,3,200,304]{3,2,1,0} %fusion.253, f32[1,1,256,3]{1,0,3,2} %bitcast.7), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.426 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.130), index=0
  %fusion.257 = (f32[256]{0}, f32[1,256,200,304]{3,2,1,0}) fusion(f32[1,256,200,304]{3,2,1,0} %get-tuple-element.424, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.426, f32[1,256,200,304]{3,2,1,0} %arg107.108), kind=kInput, calls=%fused_computation.257, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.674 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,200,304]{3,2,1,0}) %fusion.257), index=0
  %custom-call.137 = (f32[1,256,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,12,100,152]{3,2,1,0} %broadcast.58, f32[1,1,256,12]{1,0,3,2} %bitcast.6), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_1/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.433 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[1,256,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.137), index=0
  %custom-call.139 = (f32[1,256,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,3,100,152]{3,2,1,0} %fusion.252, f32[1,1,256,3]{1,0,3,2} %bitcast.7), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_1/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.435 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[1,256,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.139), index=0
  %fusion.258 = (f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) fusion(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.433, f32[1,256,100,152]{3,2,1,0} %get-tuple-element.435, f32[1,256,100,152]{3,2,1,0} %arg100.101), kind=kInput, calls=%fused_computation.258, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.676 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) %fusion.258), index=0
  %fusion.138 = f32[256]{0} fusion(f32[256]{0} %get-tuple-element.668, f32[256]{0} %get-tuple-element.670, f32[256]{0} %get-tuple-element.672, f32[256]{0} %get-tuple-element.674, f32[256]{0} %get-tuple-element.676), kind=kLoop, calls=%fused_computation.138, metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
  %arg199.200 = f32[1,256,50,76]{3,2,1,0} parameter(199), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg197.198 = f32[1,256,50,76]{3,2,1,0} parameter(197), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg192.193 = f32[1,1024,50,76]{3,2,1,0} parameter(192), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg187.188 = f32[1,256,50,76]{3,2,1,0} parameter(187), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg181.182 = f32[1,256,50,76]{3,2,1,0} parameter(181), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg176.177 = f32[1,1024,50,76]{3,2,1,0} parameter(176), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg158.159 = f32[1,2048,25,38]{3,2,1,0} parameter(158), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg160.161 = f32[1,2048,25,38]{3,2,1,0} parameter(160), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg153.154 = f32[1,512,25,38]{3,2,1,0} parameter(153), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg148.149 = f32[1,512,25,38]{3,2,1,0} parameter(148), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg143.144 = f32[1,2048,25,38]{3,2,1,0} parameter(143), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg138.139 = f32[1,512,25,38]{3,2,1,0} parameter(138), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg133.134 = f32[1,512,25,38]{3,2,1,0} parameter(133), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg128.129 = f32[1,2048,25,38]{3,2,1,0} parameter(128), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.671 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) %fusion.255), index=1
  %arg47.48 = f32[3,3,256,256]{3,2,1,0} parameter(47), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.196 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg47.48), metadata={op_name="XLA_Args"}
  %custom-call.161 = (f32[1,256,25,38]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.671, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_3/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.457 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[1,256,25,38]{3,2,1,0}, u8[6554624]{0}) %custom-call.161), index=0
  %arg117.118 = f32[1,256,25,38]{3,2,1,0} parameter(117), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg97.98 = f32[1,256,25,38]{3,2,1,0} parameter(97), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.669 = f32[1,256,13,19]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,13,19]{3,2,1,0}) %fusion.254), index=1
  %custom-call.167 = (f32[1,256,13,19]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,13,19]{3,2,1,0} %get-tuple-element.669, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_4/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.463 = f32[1,256,13,19]{3,2,1,0} get-tuple-element((f32[1,256,13,19]{3,2,1,0}, u8[6554624]{0}) %custom-call.167), index=0
  %select-and-scatter.1533 = f32[1,256,25,38]{3,2,1,0} select-and-scatter(f32[1,256,25,38]{3,2,1,0} %arg97.98, f32[1,256,13,19]{3,2,1,0} %get-tuple-element.463, f32[] %constant_758), window={size=1x1x1x1 stride=1x1x2x2}, select=%ge_F32.1525, scatter=%add_F32.1529, metadata={op_type="MaxPoolGrad" op_name="tower0/gradients/tower0/fpn/maxpool_p6/MaxPool_grad/MaxPoolGrad"}
  %arg116.117 = f32[1,256,25,38]{3,2,1,0} parameter(116), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.262 = (f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) fusion(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.457, f32[1,256,25,38]{3,2,1,0} %arg117.118, f32[1,256,25,38]{3,2,1,0} %select-and-scatter.1533, f32[1,256,25,38]{3,2,1,0} %arg116.117), kind=kInput, calls=%fused_computation.262, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p5/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.685 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) %fusion.262), index=1
  %arg51.52 = f32[3,3,256,256]{3,2,1,0} parameter(51), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.217 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg51.52), metadata={op_name="XLA_Args"}
  %custom-call.169 = (f32[1,256,25,38]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.685, f32[3,3,256,256]{1,0,2,3} %copy.217), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p5/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.465 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[1,256,25,38]{3,2,1,0}, u8[6554624]{0}) %custom-call.169), index=0
  %arg123.124 = f32[1,4]{1,0} parameter(123), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.673 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.256), index=1
  %custom-call.151 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.673, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_2/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.447 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.151), index=0
  %arg114.115 = f32[1,256,50,76]{3,2,1,0} parameter(114), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg115.116 = f32[1,256,50,76]{3,2,1,0} parameter(115), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.261 = (f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.447, f32[1,256,50,76]{3,2,1,0} %arg114.115, f32[1,256,50,76]{3,2,1,0} %arg115.116), kind=kInput, calls=%fused_computation.261, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p4/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.683 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.261), index=1
  %arg52.53 = f32[3,3,256,256]{3,2,1,0} parameter(52), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.214 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg52.53), metadata={op_name="XLA_Args"}
  %custom-call.153 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.683, f32[3,3,256,256]{1,0,2,3} %copy.214), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p4/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.449 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.153), index=0
  %get-tuple-element.677 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) %fusion.258), index=1
  %custom-call.141 = (f32[1,256,100,152]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.677, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_1/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.437 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[1,256,100,152]{3,2,1,0}, u8[6554624]{0}) %custom-call.141), index=0
  %arg112.113 = f32[1,256,100,152]{3,2,1,0} parameter(112), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg113.114 = f32[1,256,100,152]{3,2,1,0} parameter(113), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.260 = (f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) fusion(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.437, f32[1,256,100,152]{3,2,1,0} %arg112.113, f32[1,256,100,152]{3,2,1,0} %arg113.114), kind=kInput, calls=%fused_computation.260, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p3/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.681 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) %fusion.260), index=1
  %arg53.54 = f32[3,3,256,256]{3,2,1,0} parameter(53), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.211 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg53.54), metadata={op_name="XLA_Args"}
  %custom-call.143 = (f32[1,256,100,152]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.681, f32[3,3,256,256]{1,0,2,3} %copy.211), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.439 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[1,256,100,152]{3,2,1,0}, u8[6554624]{0}) %custom-call.143), index=0
  %get-tuple-element.675 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,200,304]{3,2,1,0}) %fusion.257), index=1
  %custom-call.132 = (f32[1,256,200,304]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %get-tuple-element.675, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.428 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[6554624]{0}) %custom-call.132), index=0
  %arg110.111 = f32[1,256,200,304]{3,2,1,0} parameter(110), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg111.112 = f32[1,256,200,304]{3,2,1,0} parameter(111), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.259 = (f32[256]{0}, f32[1,256,200,304]{3,2,1,0}) fusion(f32[1,256,200,304]{3,2,1,0} %get-tuple-element.428, f32[1,256,200,304]{3,2,1,0} %arg110.111, f32[1,256,200,304]{3,2,1,0} %arg111.112), kind=kInput, calls=%fused_computation.259, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p2/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.679 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,200,304]{3,2,1,0}) %fusion.259), index=1
  %arg48.49 = f32[3,3,256,256]{3,2,1,0} parameter(48), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.209 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg48.49), metadata={op_name="XLA_Args"}
  %custom-call.134 = (f32[1,256,200,304]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %get-tuple-element.679, f32[3,3,256,256]{1,0,2,3} %copy.209), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.430 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[6554624]{0}) %custom-call.134), index=0
  %fusion.263 = (f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) fusion(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.439, f32[1,4]{1,0} %arg123.124, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.430), kind=kInput, calls=%fused_computation.263, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c3/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.687 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) %fusion.263), index=1
  %fusion.264 = (f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.449, f32[1,4]{1,0} %arg123.124, f32[1,256,100,152]{3,2,1,0} %get-tuple-element.687), kind=kInput, calls=%fused_computation.264, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c4/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.689 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.264), index=1
  %fusion.265 = (f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) fusion(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.465, f32[1,4]{1,0} %arg123.124, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.689), kind=kInput, calls=%fused_computation.265, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c5/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.691 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) %fusion.265), index=1
  %arg43.44 = f32[1,1,2048,256]{3,2,1,0} parameter(43), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.29 = f32[1,1,2048,256]{1,0,3,2} bitcast(f32[1,1,2048,256]{3,2,1,0} %arg43.44), metadata={op_name="XLA_Args"}
  %custom-call.171 = (f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.691, f32[1,1,2048,256]{1,0,3,2} %bitcast.29), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c5/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.467 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.171), index=0
  %arg126.127 = f32[1,2048,25,38]{3,2,1,0} parameter(126), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.127 = (f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) fusion(f32[1,2048,25,38]{3,2,1,0} %arg128.129, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.467, f32[1,2048,25,38]{3,2,1,0} %arg126.127), kind=kInput, calls=%fused_computation.127, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.692 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.127), index=2
  %arg127.128 = f32[1,2048,1,1]{3,2,1,0} parameter(127), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.222 = f32[1,2048,25,38]{3,2,1,0} fusion(f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.692, f32[1,2048,1,1]{3,2,1,0} %arg127.128), kind=kLoop, calls=%fused_computation.222, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg42.43 = f32[1,1,512,2048]{3,2,1,0} parameter(42), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.34 = f32[1,1,512,2048]{1,0,3,2} bitcast(f32[1,1,512,2048]{3,2,1,0} %arg42.43), metadata={op_name="XLA_Args"}
  %custom-call.173 = (f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %fusion.222, f32[1,1,512,2048]{1,0,3,2} %bitcast.34), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block2/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.469 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.173), index=0
  %arg130.131 = f32[1,512,25,38]{3,2,1,0} parameter(130), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.124 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) fusion(f32[1,512,25,38]{3,2,1,0} %arg133.134, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.469, f32[1,512,25,38]{3,2,1,0} %arg130.131), kind=kInput, calls=%fused_computation.124, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.693 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.124), index=2
  %arg132.133 = f32[1,512,1,1]{3,2,1,0} parameter(132), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.220 = f32[1,512,25,38]{3,2,1,0} fusion(f32[1,512,25,38]{3,2,1,0} %get-tuple-element.693, f32[1,512,1,1]{3,2,1,0} %arg132.133), kind=kLoop, calls=%fused_computation.220, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg41.42 = f32[3,3,512,512]{3,2,1,0} parameter(41), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.224 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg41.42), metadata={op_name="XLA_Args"}
  %custom-call.175 = (f32[1,512,25,38]{3,2,1,0}, u8[26216448]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.220, f32[3,3,512,512]{1,0,2,3} %copy.224), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block2/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.471 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[26216448]{0}) %custom-call.175), index=0
  %arg135.136 = f32[1,512,25,38]{3,2,1,0} parameter(135), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.121 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) fusion(f32[1,512,25,38]{3,2,1,0} %arg138.139, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.471, f32[1,512,25,38]{3,2,1,0} %arg135.136), kind=kInput, calls=%fused_computation.121, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.694 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.121), index=2
  %arg137.138 = f32[1,512,1,1]{3,2,1,0} parameter(137), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.218 = f32[1,512,25,38]{3,2,1,0} fusion(f32[1,512,25,38]{3,2,1,0} %get-tuple-element.694, f32[1,512,1,1]{3,2,1,0} %arg137.138), kind=kLoop, calls=%fused_computation.218, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg40.41 = f32[1,1,2048,512]{3,2,1,0} parameter(40), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.43 = f32[1,1,2048,512]{1,0,3,2} bitcast(f32[1,1,2048,512]{3,2,1,0} %arg40.41), metadata={op_name="XLA_Args"}
  %custom-call.177 = (f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.218, f32[1,1,2048,512]{1,0,3,2} %bitcast.43), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block2/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.473 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.177), index=0
  %arg140.141 = f32[1,2048,25,38]{3,2,1,0} parameter(140), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.118 = (f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) fusion(f32[1,2048,25,38]{3,2,1,0} %arg143.144, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.473, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.692, f32[1,2048,25,38]{3,2,1,0} %arg140.141), kind=kInput, calls=%fused_computation.118, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.695 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.118), index=2
  %arg142.143 = f32[1,2048,1,1]{3,2,1,0} parameter(142), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.216 = f32[1,2048,25,38]{3,2,1,0} fusion(f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.695, f32[1,2048,1,1]{3,2,1,0} %arg142.143), kind=kLoop, calls=%fused_computation.216, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg39.40 = f32[1,1,512,2048]{3,2,1,0} parameter(39), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.48 = f32[1,1,512,2048]{1,0,3,2} bitcast(f32[1,1,512,2048]{3,2,1,0} %arg39.40), metadata={op_name="XLA_Args"}
  %custom-call.179 = (f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %fusion.216, f32[1,1,512,2048]{1,0,3,2} %bitcast.48), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block1/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.475 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.179), index=0
  %arg145.146 = f32[1,512,25,38]{3,2,1,0} parameter(145), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.115 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) fusion(f32[1,512,25,38]{3,2,1,0} %arg148.149, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.475, f32[1,512,25,38]{3,2,1,0} %arg145.146), kind=kInput, calls=%fused_computation.115, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.696 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.115), index=2
  %arg147.148 = f32[1,512,1,1]{3,2,1,0} parameter(147), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.214 = f32[1,512,25,38]{3,2,1,0} fusion(f32[1,512,25,38]{3,2,1,0} %get-tuple-element.696, f32[1,512,1,1]{3,2,1,0} %arg147.148), kind=kLoop, calls=%fused_computation.214, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg38.39 = f32[3,3,512,512]{3,2,1,0} parameter(38), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.230 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg38.39), metadata={op_name="XLA_Args"}
  %custom-call.181 = (f32[1,512,25,38]{3,2,1,0}, u8[26216448]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.214, f32[3,3,512,512]{1,0,2,3} %copy.230), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block1/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.477 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[26216448]{0}) %custom-call.181), index=0
  %arg150.151 = f32[1,512,25,38]{3,2,1,0} parameter(150), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.112 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) fusion(f32[1,512,25,38]{3,2,1,0} %arg153.154, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.477, f32[1,512,25,38]{3,2,1,0} %arg150.151), kind=kInput, calls=%fused_computation.112, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.697 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.112), index=2
  %arg152.153 = f32[1,512,1,1]{3,2,1,0} parameter(152), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.212 = f32[1,512,25,38]{3,2,1,0} fusion(f32[1,512,25,38]{3,2,1,0} %get-tuple-element.697, f32[1,512,1,1]{3,2,1,0} %arg152.153), kind=kLoop, calls=%fused_computation.212, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg37.38 = f32[1,1,2048,512]{3,2,1,0} parameter(37), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.57 = f32[1,1,2048,512]{1,0,3,2} bitcast(f32[1,1,2048,512]{3,2,1,0} %arg37.38), metadata={op_name="XLA_Args"}
  %custom-call.183 = (f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.212, f32[1,1,2048,512]{1,0,3,2} %bitcast.57), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block1/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.479 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.183), index=0
  %arg155.156 = f32[1,2048,25,38]{3,2,1,0} parameter(155), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.108 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) fusion(f32[1,2048,25,38]{3,2,1,0} %arg158.159, f32[1,2048,25,38]{3,2,1,0} %arg160.161, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.479, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.695, f32[1,2048,25,38]{3,2,1,0} %arg155.156), kind=kInput, calls=%fused_computation.108, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.698 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.108), index=3
  %arg157.158 = f32[1,2048,1,1]{3,2,1,0} parameter(157), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg159.160 = f32[1,2048,1,1]{3,2,1,0} parameter(159), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.210 = (f32[1,2048,25,38]{3,2,1,0}, f32[1,2048,25,38]{3,2,1,0}) fusion(f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.698, f32[1,2048,1,1]{3,2,1,0} %arg157.158, f32[1,2048,1,1]{3,2,1,0} %arg159.160), kind=kLoop, calls=%fused_computation.210, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %get-tuple-element.602 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[1,2048,25,38]{3,2,1,0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.210), index=1
  %arg36.37 = f32[1,1,1024,2048]{3,2,1,0} parameter(36), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.241 = f32[1,1,1024,2048]{1,0,2,3} copy(f32[1,1,1024,2048]{3,2,1,0} %arg36.37), metadata={op_name="XLA_Args"}
  %custom-call.191 = (f32[1,1024,50,76]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.602, f32[1,1,1024,2048]{1,0,2,3} %copy.241), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block0/convshortcut/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.487 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[0]{0}) %custom-call.191), index=0
  %arg44.45 = f32[1,1,1024,256]{3,2,1,0} parameter(44), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.74 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg44.45), metadata={op_name="XLA_Args"}
  %custom-call.155 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.689, f32[1,1,1024,256]{1,0,3,2} %bitcast.74), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c4/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.451 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.155), index=0
  %arg172.173 = f32[1,512,50,76]{3,2,1,0} parameter(172), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg167.168 = f32[1,512,25,38]{3,2,1,0} parameter(167), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.601 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[1,2048,25,38]{3,2,1,0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.210), index=0
  %arg35.36 = f32[1,1,512,2048]{3,2,1,0} parameter(35), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.65 = f32[1,1,512,2048]{1,0,3,2} bitcast(f32[1,1,512,2048]{3,2,1,0} %arg35.36), metadata={op_name="XLA_Args"}
  %custom-call.185 = (f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.601, f32[1,1,512,2048]{1,0,3,2} %bitcast.65), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block0/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.481 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.185), index=0
  %arg163.164 = f32[1,512,25,38]{3,2,1,0} parameter(163), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.102 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) fusion(f32[1,512,25,38]{3,2,1,0} %arg167.168, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.481, f32[1,512,25,38]{3,2,1,0} %arg163.164), kind=kInput, calls=%fused_computation.102, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.699 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.102), index=2
  %arg166.167 = f32[1,512,1,1]{3,2,1,0} parameter(166), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.208 = f32[1,512,25,38]{3,2,1,0} fusion(f32[1,512,25,38]{3,2,1,0} %get-tuple-element.699, f32[1,512,1,1]{3,2,1,0} %arg166.167), kind=kLoop, calls=%fused_computation.208, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg34.35 = f32[3,3,512,512]{3,2,1,0} parameter(34), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.237 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg34.35), metadata={op_name="XLA_Args"}
  %custom-call.187 = (f32[1,512,51,77]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.208, f32[3,3,512,512]{1,0,2,3} %copy.237), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block0/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.483 = f32[1,512,51,77]{3,2,1,0} get-tuple-element((f32[1,512,51,77]{3,2,1,0}, u8[0]{0}) %custom-call.187), index=0
  %arg171.172 = f32[1,512,50,76]{3,2,1,0} parameter(171), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.99 = (f32[512]{0}, f32[512]{0}, f32[1,512,50,76]{3,2,1,0}) fusion(f32[1,512,50,76]{3,2,1,0} %arg172.173, f32[1,512,51,77]{3,2,1,0} %get-tuple-element.483, f32[1,512,50,76]{3,2,1,0} %arg171.172), kind=kInput, calls=%fused_computation.99, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.700 = f32[1,512,50,76]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,50,76]{3,2,1,0}) %fusion.99), index=2
  %arg173.174 = f32[1,512,1,1]{3,2,1,0} parameter(173), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.206 = f32[1,512,50,76]{3,2,1,0} fusion(f32[1,512,50,76]{3,2,1,0} %get-tuple-element.700, f32[1,512,1,1]{3,2,1,0} %arg173.174), kind=kLoop, calls=%fused_computation.206, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg0.1 = f32[1,1,1024,512]{3,2,1,0} parameter(0), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.75 = f32[1,1,1024,512]{1,0,3,2} bitcast(f32[1,1,1024,512]{3,2,1,0} %arg0.1), metadata={op_name="XLA_Args"}
  %custom-call.189 = (f32[1,1024,50,76]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,512,50,76]{3,2,1,0} %fusion.206, f32[1,1,1024,512]{1,0,3,2} %bitcast.75), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block0/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.485 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[0]{0}) %custom-call.189), index=0
  %arg125.126 = f32[1,1024,50,76]{3,2,1,0} parameter(125), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.96 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) fusion(f32[1,1024,50,76]{3,2,1,0} %arg176.177, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.487, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.451, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.485, f32[1,1024,50,76]{3,2,1,0} %arg125.126), kind=kInput, calls=%fused_computation.96, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.701 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.96), index=2
  %arg177.178 = f32[1,1024,1,1]{3,2,1,0} parameter(177), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.203 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.701, f32[1,1024,1,1]{3,2,1,0} %arg177.178), kind=kLoop, calls=%fused_computation.203, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg1.2 = f32[1,1,256,1024]{3,2,1,0} parameter(1), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.80 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg1.2), metadata={op_name="XLA_Args"}
  %custom-call.193 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.203, f32[1,1,256,1024]{1,0,3,2} %bitcast.80), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block5/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.489 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.193), index=0
  %arg179.180 = f32[1,256,50,76]{3,2,1,0} parameter(179), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.93 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg181.182, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.489, f32[1,256,50,76]{3,2,1,0} %arg179.180), kind=kInput, calls=%fused_computation.93, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.702 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.93), index=2
  %arg182.183 = f32[1,256,1,1]{3,2,1,0} parameter(182), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.201 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.702, f32[1,256,1,1]{3,2,1,0} %arg182.183), kind=kLoop, calls=%fused_computation.201, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg2.3 = f32[3,3,256,256]{3,2,1,0} parameter(2), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.246 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg2.3), metadata={op_name="XLA_Args"}
  %custom-call.195 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.201, f32[3,3,256,256]{1,0,2,3} %copy.246), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block5/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.491 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.195), index=0
  %arg184.185 = f32[1,256,50,76]{3,2,1,0} parameter(184), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.90 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg187.188, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.491, f32[1,256,50,76]{3,2,1,0} %arg184.185), kind=kInput, calls=%fused_computation.90, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.703 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.90), index=2
  %arg186.187 = f32[1,256,1,1]{3,2,1,0} parameter(186), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.199 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.703, f32[1,256,1,1]{3,2,1,0} %arg186.187), kind=kLoop, calls=%fused_computation.199, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg4.5 = f32[1,1,1024,256]{3,2,1,0} parameter(4), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.89 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg4.5), metadata={op_name="XLA_Args"}
  %custom-call.197 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.199, f32[1,1,1024,256]{1,0,3,2} %bitcast.89), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block5/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.493 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.197), index=0
  %arg189.190 = f32[1,1024,50,76]{3,2,1,0} parameter(189), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.87 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) fusion(f32[1,1024,50,76]{3,2,1,0} %arg192.193, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.493, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.701, f32[1,1024,50,76]{3,2,1,0} %arg189.190), kind=kInput, calls=%fused_computation.87, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.704 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.87), index=2
  %arg191.192 = f32[1,1024,1,1]{3,2,1,0} parameter(191), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.197 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.704, f32[1,1024,1,1]{3,2,1,0} %arg191.192), kind=kLoop, calls=%fused_computation.197, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg5.6 = f32[1,1,256,1024]{3,2,1,0} parameter(5), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.94 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg5.6), metadata={op_name="XLA_Args"}
  %custom-call.199 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.197, f32[1,1,256,1024]{1,0,3,2} %bitcast.94), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block4/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.495 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.199), index=0
  %arg194.195 = f32[1,256,50,76]{3,2,1,0} parameter(194), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.84 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg197.198, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.495, f32[1,256,50,76]{3,2,1,0} %arg194.195), kind=kInput, calls=%fused_computation.84, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.705 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.84), index=2
  %arg196.197 = f32[1,256,1,1]{3,2,1,0} parameter(196), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.195 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.705, f32[1,256,1,1]{3,2,1,0} %arg196.197), kind=kLoop, calls=%fused_computation.195, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.200 = (f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg199.200, f32[1,256,50,76]{3,2,1,0} %fusion.195), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block4/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.496 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) %custom-call.200), index=0
  %arg3.4 = f32[3,3,256,256]{3,2,1,0} parameter(3), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg6.7 = f32[3,3,256,256]{3,2,1,0} parameter(6), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg263.264 = f32[1,256,101,153]{3,2,1,0} parameter(263), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg261.262 = f32[1,256,50,76]{3,2,1,0} parameter(261), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg252.253 = f32[1,1024,50,76]{3,2,1,0} parameter(252), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg254.255 = f32[1,1024,50,76]{3,2,1,0} parameter(254), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg247.248 = f32[1,256,50,76]{3,2,1,0} parameter(247), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg242.243 = f32[1,256,50,76]{3,2,1,0} parameter(242), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg237.238 = f32[1,1024,50,76]{3,2,1,0} parameter(237), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg232.233 = f32[1,256,50,76]{3,2,1,0} parameter(232), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg227.228 = f32[1,256,50,76]{3,2,1,0} parameter(227), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg222.223 = f32[1,1024,50,76]{3,2,1,0} parameter(222), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg217.218 = f32[1,256,50,76]{3,2,1,0} parameter(217), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg212.213 = f32[1,256,50,76]{3,2,1,0} parameter(212), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg207.208 = f32[1,1024,50,76]{3,2,1,0} parameter(207), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg202.203 = f32[1,256,50,76]{3,2,1,0} parameter(202), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.253 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg6.7), metadata={op_name="XLA_Args"}
  %custom-call.201 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.195, f32[3,3,256,256]{1,0,2,3} %copy.253), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block4/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.497 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.201), index=0
  %fusion.81 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg202.203, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.497, f32[1,256,50,76]{3,2,1,0} %arg199.200), kind=kInput, calls=%fused_computation.81, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.706 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.81), index=2
  %arg201.202 = f32[1,256,1,1]{3,2,1,0} parameter(201), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.193 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.706, f32[1,256,1,1]{3,2,1,0} %arg201.202), kind=kLoop, calls=%fused_computation.193, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg7.8 = f32[1,1,1024,256]{3,2,1,0} parameter(7), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.103 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg7.8), metadata={op_name="XLA_Args"}
  %custom-call.203 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.193, f32[1,1,1024,256]{1,0,3,2} %bitcast.103), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block4/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.499 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.203), index=0
  %arg204.205 = f32[1,1024,50,76]{3,2,1,0} parameter(204), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.78 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) fusion(f32[1,1024,50,76]{3,2,1,0} %arg207.208, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.499, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.704, f32[1,1024,50,76]{3,2,1,0} %arg204.205), kind=kInput, calls=%fused_computation.78, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.707 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.78), index=2
  %arg206.207 = f32[1,1024,1,1]{3,2,1,0} parameter(206), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.191 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.707, f32[1,1024,1,1]{3,2,1,0} %arg206.207), kind=kLoop, calls=%fused_computation.191, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg8.9 = f32[1,1,256,1024]{3,2,1,0} parameter(8), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.108 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg8.9), metadata={op_name="XLA_Args"}
  %custom-call.205 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.191, f32[1,1,256,1024]{1,0,3,2} %bitcast.108), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block3/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.501 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.205), index=0
  %arg209.210 = f32[1,256,50,76]{3,2,1,0} parameter(209), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.75 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg212.213, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.501, f32[1,256,50,76]{3,2,1,0} %arg209.210), kind=kInput, calls=%fused_computation.75, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.708 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.75), index=2
  %arg211.212 = f32[1,256,1,1]{3,2,1,0} parameter(211), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.189 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.708, f32[1,256,1,1]{3,2,1,0} %arg211.212), kind=kLoop, calls=%fused_computation.189, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg9.10 = f32[3,3,256,256]{3,2,1,0} parameter(9), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.260 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg9.10), metadata={op_name="XLA_Args"}
  %custom-call.207 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.189, f32[3,3,256,256]{1,0,2,3} %copy.260), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block3/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.503 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.207), index=0
  %arg214.215 = f32[1,256,50,76]{3,2,1,0} parameter(214), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.72 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg217.218, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.503, f32[1,256,50,76]{3,2,1,0} %arg214.215), kind=kInput, calls=%fused_computation.72, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.709 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.72), index=2
  %arg216.217 = f32[1,256,1,1]{3,2,1,0} parameter(216), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.187 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.709, f32[1,256,1,1]{3,2,1,0} %arg216.217), kind=kLoop, calls=%fused_computation.187, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg10.11 = f32[1,1,1024,256]{3,2,1,0} parameter(10), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.117 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg10.11), metadata={op_name="XLA_Args"}
  %custom-call.209 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.187, f32[1,1,1024,256]{1,0,3,2} %bitcast.117), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block3/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.505 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.209), index=0
  %arg219.220 = f32[1,1024,50,76]{3,2,1,0} parameter(219), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.69 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) fusion(f32[1,1024,50,76]{3,2,1,0} %arg222.223, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.505, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.707, f32[1,1024,50,76]{3,2,1,0} %arg219.220), kind=kInput, calls=%fused_computation.69, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.710 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.69), index=2
  %arg221.222 = f32[1,1024,1,1]{3,2,1,0} parameter(221), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.185 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.710, f32[1,1024,1,1]{3,2,1,0} %arg221.222), kind=kLoop, calls=%fused_computation.185, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg11.12 = f32[1,1,256,1024]{3,2,1,0} parameter(11), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.122 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg11.12), metadata={op_name="XLA_Args"}
  %custom-call.211 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.185, f32[1,1,256,1024]{1,0,3,2} %bitcast.122), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block2/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.507 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.211), index=0
  %arg224.225 = f32[1,256,50,76]{3,2,1,0} parameter(224), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.66 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg227.228, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.507, f32[1,256,50,76]{3,2,1,0} %arg224.225), kind=kInput, calls=%fused_computation.66, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.711 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.66), index=2
  %arg226.227 = f32[1,256,1,1]{3,2,1,0} parameter(226), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.183 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.711, f32[1,256,1,1]{3,2,1,0} %arg226.227), kind=kLoop, calls=%fused_computation.183, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg12.13 = f32[3,3,256,256]{3,2,1,0} parameter(12), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.267 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg12.13), metadata={op_name="XLA_Args"}
  %custom-call.213 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.183, f32[3,3,256,256]{1,0,2,3} %copy.267), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block2/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.509 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.213), index=0
  %arg229.230 = f32[1,256,50,76]{3,2,1,0} parameter(229), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.63 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg232.233, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.509, f32[1,256,50,76]{3,2,1,0} %arg229.230), kind=kInput, calls=%fused_computation.63, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.712 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.63), index=2
  %arg231.232 = f32[1,256,1,1]{3,2,1,0} parameter(231), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.181 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.712, f32[1,256,1,1]{3,2,1,0} %arg231.232), kind=kLoop, calls=%fused_computation.181, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg13.14 = f32[1,1,1024,256]{3,2,1,0} parameter(13), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.131 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg13.14), metadata={op_name="XLA_Args"}
  %custom-call.215 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.181, f32[1,1,1024,256]{1,0,3,2} %bitcast.131), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block2/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.511 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.215), index=0
  %arg234.235 = f32[1,1024,50,76]{3,2,1,0} parameter(234), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.60 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) fusion(f32[1,1024,50,76]{3,2,1,0} %arg237.238, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.511, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.710, f32[1,1024,50,76]{3,2,1,0} %arg234.235), kind=kInput, calls=%fused_computation.60, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.713 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.60), index=2
  %arg236.237 = f32[1,1024,1,1]{3,2,1,0} parameter(236), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.179 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.713, f32[1,1024,1,1]{3,2,1,0} %arg236.237), kind=kLoop, calls=%fused_computation.179, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg14.15 = f32[1,1,256,1024]{3,2,1,0} parameter(14), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.136 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg14.15), metadata={op_name="XLA_Args"}
  %custom-call.217 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.179, f32[1,1,256,1024]{1,0,3,2} %bitcast.136), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block1/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.513 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.217), index=0
  %arg239.240 = f32[1,256,50,76]{3,2,1,0} parameter(239), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.57 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg242.243, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.513, f32[1,256,50,76]{3,2,1,0} %arg239.240), kind=kInput, calls=%fused_computation.57, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.714 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.57), index=2
  %arg241.242 = f32[1,256,1,1]{3,2,1,0} parameter(241), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.177 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.714, f32[1,256,1,1]{3,2,1,0} %arg241.242), kind=kLoop, calls=%fused_computation.177, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg15.16 = f32[3,3,256,256]{3,2,1,0} parameter(15), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.274 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg15.16), metadata={op_name="XLA_Args"}
  %custom-call.219 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.177, f32[3,3,256,256]{1,0,2,3} %copy.274), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block1/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.515 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.219), index=0
  %arg244.245 = f32[1,256,50,76]{3,2,1,0} parameter(244), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.54 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg247.248, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.515, f32[1,256,50,76]{3,2,1,0} %arg244.245), kind=kInput, calls=%fused_computation.54, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.715 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.54), index=2
  %arg246.247 = f32[1,256,1,1]{3,2,1,0} parameter(246), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.175 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.715, f32[1,256,1,1]{3,2,1,0} %arg246.247), kind=kLoop, calls=%fused_computation.175, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg16.17 = f32[1,1,1024,256]{3,2,1,0} parameter(16), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.145 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg16.17), metadata={op_name="XLA_Args"}
  %custom-call.221 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.175, f32[1,1,1024,256]{1,0,3,2} %bitcast.145), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block1/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.517 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.221), index=0
  %arg249.250 = f32[1,1024,50,76]{3,2,1,0} parameter(249), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.50 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) fusion(f32[1,1024,50,76]{3,2,1,0} %arg252.253, f32[1,1024,50,76]{3,2,1,0} %arg254.255, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.517, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.713, f32[1,1024,50,76]{3,2,1,0} %arg249.250), kind=kInput, calls=%fused_computation.50, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.716 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.50), index=3
  %arg253.254 = f32[1,1024,1,1]{3,2,1,0} parameter(253), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg251.252 = f32[1,1024,1,1]{3,2,1,0} parameter(251), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.173 = (f32[1,1024,50,76]{3,2,1,0}, f32[1,1024,50,76]{3,2,1,0}) fusion(f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.716, f32[1,1024,1,1]{3,2,1,0} %arg253.254, f32[1,1024,1,1]{3,2,1,0} %arg251.252), kind=kLoop, calls=%fused_computation.173, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %get-tuple-element.565 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.173), index=0
  %arg31.32 = f32[1,1,256,1024]{3,2,1,0} parameter(31), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.153 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg31.32), metadata={op_name="XLA_Args"}
  %custom-call.223 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.565, f32[1,1,256,1024]{1,0,3,2} %bitcast.153), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block0/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.519 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.223), index=0
  %arg257.258 = f32[1,256,50,76]{3,2,1,0} parameter(257), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.44 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) fusion(f32[1,256,50,76]{3,2,1,0} %arg261.262, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.519, f32[1,256,50,76]{3,2,1,0} %arg257.258), kind=kInput, calls=%fused_computation.44, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.717 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.44), index=2
  %arg260.261 = f32[1,256,1,1]{3,2,1,0} parameter(260), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.171 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.717, f32[1,256,1,1]{3,2,1,0} %arg260.261), kind=kLoop, calls=%fused_computation.171, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.224 = (f32[3,3,256,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,101,153]{3,2,1,0} %arg263.264, f32[1,256,50,76]{3,2,1,0} %fusion.171), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block0/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.520 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[0]{0}) %custom-call.224), index=0
  %arg32.33 = f32[3,3,256,256]{3,2,1,0} parameter(32), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.212 = (f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg229.230, f32[1,256,50,76]{3,2,1,0} %fusion.183), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block2/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.508 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) %custom-call.212), index=0
  %custom-call.194 = (f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg184.185, f32[1,256,50,76]{3,2,1,0} %fusion.201), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block5/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.490 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) %custom-call.194), index=0
  %arg118.119 = f32[1,256,200,304]{3,2,1,0} parameter(118), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.133 = (f32[3,3,256,256]{1,0,2,3}, u8[289603584]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %arg118.119, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.679), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.429 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[289603584]{0}) %custom-call.133), index=0
  %arg121.122 = f32[1,256,25,38]{3,2,1,0} parameter(121), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.168 = (f32[3,3,256,256]{1,0,2,3}, u8[14598144]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %arg121.122, f32[1,256,25,38]{3,2,1,0} %get-tuple-element.685), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p5/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.464 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[14598144]{0}) %custom-call.168), index=0
  %custom-call.218 = (f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg244.245, f32[1,256,50,76]{3,2,1,0} %fusion.177), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block1/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.514 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) %custom-call.218), index=0
  %arg98.99 = f32[1,256,13,19]{3,2,1,0} parameter(98), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.166 = (f32[3,3,256,256]{1,0,2,3}, u8[10911744]{0}) custom-call(f32[1,256,13,19]{3,2,1,0} %arg98.99, f32[1,256,13,19]{3,2,1,0} %get-tuple-element.669), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_4/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.462 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[10911744]{0}) %custom-call.166), index=0
  %custom-call.160 = (f32[3,3,256,256]{1,0,2,3}, u8[14598144]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %arg97.98, f32[1,256,25,38]{3,2,1,0} %get-tuple-element.671), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_3/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.456 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[14598144]{0}) %custom-call.160), index=0
  %arg105.106 = f32[1,256,50,76]{3,2,1,0} parameter(105), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.150 = (f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg105.106, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.673), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_2/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.446 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) %custom-call.150), index=0
  %arg109.110 = f32[1,256,200,304]{3,2,1,0} parameter(109), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.131 = (f32[3,3,256,256]{1,0,2,3}, u8[289603584]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %arg109.110, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.675), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.427 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[289603584]{0}) %custom-call.131), index=0
  %arg108.109 = f32[1,256,100,152]{3,2,1,0} parameter(108), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.140 = (f32[3,3,256,256]{1,0,2,3}, u8[79478784]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %arg108.109, f32[1,256,100,152]{3,2,1,0} %get-tuple-element.677), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_1/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.436 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[79478784]{0}) %custom-call.140), index=0
  %arg120.121 = f32[1,256,50,76]{3,2,1,0} parameter(120), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.152 = (f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg120.121, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.683), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p4/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.448 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) %custom-call.152), index=0
  %custom-call.206 = (f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg214.215, f32[1,256,50,76]{3,2,1,0} %fusion.189), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block3/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.502 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[27648000]{0}) %custom-call.206), index=0
  %arg119.120 = f32[1,256,100,152]{3,2,1,0} parameter(119), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.142 = (f32[3,3,256,256]{1,0,2,3}, u8[79478784]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %arg119.120, f32[1,256,100,152]{3,2,1,0} %get-tuple-element.681), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.438 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[79478784]{0}) %custom-call.142), index=0
  %fusion.85 = (f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) fusion(f32[3,3,256,256]{1,0,2,3} %get-tuple-element.496, f32[3,3,256,256]{3,2,1,0} %arg3.4, f32[3,3,256,256]{3,2,1,0} %arg6.7, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.520, f32[3,3,256,256]{3,2,1,0} %arg32.33, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.508, f32[3,3,256,256]{3,2,1,0} %arg12.13, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.490, f32[3,3,256,256]{3,2,1,0} %arg2.3, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.429, f32[3,3,256,256]{3,2,1,0} %arg48.49, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.464, f32[3,3,256,256]{3,2,1,0} %arg51.52, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.514, f32[3,3,256,256]{3,2,1,0} %arg15.16, f32[3,3,256,256]{3,2,1,0} %arg47.48, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.462, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.456, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.446, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.427, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.436, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.448, f32[3,3,256,256]{3,2,1,0} %arg52.53, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.502, f32[3,3,256,256]{3,2,1,0} %arg9.10, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.438, f32[3,3,256,256]{3,2,1,0} %arg53.54), kind=kLoop, calls=%fused_computation.85, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.646 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=5
  %get-tuple-element.678 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,200,304]{3,2,1,0}) %fusion.259), index=0
  %get-tuple-element.680 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) %fusion.260), index=0
  %get-tuple-element.682 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.261), index=0
  %get-tuple-element.684 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) %fusion.262), index=0
  %get-tuple-element.649 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=8
  %get-tuple-element.643 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=2
  %get-tuple-element.645 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=4
  %get-tuple-element.648 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=7
  %reduce.1253 = f32[256]{0} reduce(f32[1,256,200,304]{3,2,1,0} %get-tuple-element.430, f32[] %constant_758), dimensions={0,2,3}, to_apply=%add_float_.1249, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c2/BiasAdd_grad/BiasAddGrad"}
  %arg122.123 = f32[1,256,200,304]{3,2,1,0} parameter(122), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.135 = (f32[1,1,256,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %arg122.123, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.430), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.431 = f32[1,1,256,256]{1,0,2,3} get-tuple-element((f32[1,1,256,256]{1,0,2,3}, u8[0]{0}) %custom-call.135), index=0
  %arg46.47 = f32[1,1,256,256]{3,2,1,0} parameter(46), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.132 = f32[1,1,256,256]{3,2,1,0} fusion(f32[1,1,256,256]{1,0,2,3} %get-tuple-element.431, f32[1,1,256,256]{3,2,1,0} %arg46.47), kind=kLoop, calls=%fused_computation.132, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.686 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) %fusion.263), index=0
  %arg124.125 = f32[1,512,100,152]{3,2,1,0} parameter(124), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.144 = (f32[1,1,512,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %arg124.125, f32[1,256,100,152]{3,2,1,0} %get-tuple-element.687), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.440 = f32[1,1,512,256]{1,0,2,3} get-tuple-element((f32[1,1,512,256]{1,0,2,3}, u8[0]{0}) %custom-call.144), index=0
  %arg45.46 = f32[1,1,512,256]{3,2,1,0} parameter(45), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.131 = f32[1,1,512,256]{3,2,1,0} fusion(f32[1,1,512,256]{1,0,2,3} %get-tuple-element.440, f32[1,1,512,256]{3,2,1,0} %arg45.46), kind=kLoop, calls=%fused_computation.131, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.688 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.264), index=0
  %custom-call.154 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %arg125.126, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.689), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c4/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.450 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.154), index=0
  %fusion.130 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.450, f32[1,1,1024,256]{3,2,1,0} %arg44.45), kind=kLoop, calls=%fused_computation.130, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.690 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,25,38]{3,2,1,0}) %fusion.265), index=0
  %custom-call.170 = (f32[1,1,2048,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %arg126.127, f32[1,256,25,38]{3,2,1,0} %get-tuple-element.691), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c5/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.466 = f32[1,1,2048,256]{1,0,2,3} get-tuple-element((f32[1,1,2048,256]{1,0,2,3}, u8[0]{0}) %custom-call.170), index=0
  %fusion.129 = f32[1,1,2048,256]{3,2,1,0} fusion(f32[1,1,2048,256]{1,0,2,3} %get-tuple-element.466, f32[1,1,2048,256]{3,2,1,0} %arg43.44), kind=kLoop, calls=%fused_computation.129, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.581 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.127), index=1
  %custom-call.172 = (f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %arg130.131, f32[1,2048,25,38]{3,2,1,0} %fusion.222), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block2/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.468 = f32[1,1,512,2048]{1,0,2,3} get-tuple-element((f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) %custom-call.172), index=0
  %fusion.128 = f32[1,1,512,2048]{3,2,1,0} fusion(f32[1,1,512,2048]{1,0,2,3} %get-tuple-element.468, f32[1,1,512,2048]{3,2,1,0} %arg42.43), kind=kLoop, calls=%fused_computation.128, metadata={op_name="XLA_Retvals"}
  %arg131.132 = f32[1,2048,1,1]{3,2,1,0} parameter(131), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg129.130 = f32[1,2048,1,1]{3,2,1,0} parameter(129), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.580 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.127), index=0
  %fusion.126 = f32[2048]{0} fusion(f32[1,2048,1,1]{3,2,1,0} %arg131.132, f32[1,2048,1,1]{3,2,1,0} %arg129.130, f32[2048]{0} %get-tuple-element.581, f32[2048]{0} %get-tuple-element.580), kind=kLoop, calls=%fused_computation.126, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.659 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.124), index=1
  %custom-call.174 = (f32[3,3,512,512]{1,0,2,3}, u8[48070656]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %arg135.136, f32[1,512,25,38]{3,2,1,0} %fusion.220), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block2/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.470 = f32[3,3,512,512]{1,0,2,3} get-tuple-element((f32[3,3,512,512]{1,0,2,3}, u8[48070656]{0}) %custom-call.174), index=0
  %fusion.125 = f32[3,3,512,512]{3,2,1,0} fusion(f32[3,3,512,512]{1,0,2,3} %get-tuple-element.470, f32[3,3,512,512]{3,2,1,0} %arg41.42), kind=kLoop, calls=%fused_computation.125, metadata={op_name="XLA_Retvals"}
  %arg136.137 = f32[1,512,1,1]{3,2,1,0} parameter(136), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg134.135 = f32[1,512,1,1]{3,2,1,0} parameter(134), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.658 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.124), index=0
  %fusion.123 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg136.137, f32[1,512,1,1]{3,2,1,0} %arg134.135, f32[512]{0} %get-tuple-element.659, f32[512]{0} %get-tuple-element.658), kind=kLoop, calls=%fused_computation.123, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.661 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.121), index=1
  %custom-call.176 = (f32[1,1,2048,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %arg140.141, f32[1,512,25,38]{3,2,1,0} %fusion.218), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block2/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.472 = f32[1,1,2048,512]{1,0,2,3} get-tuple-element((f32[1,1,2048,512]{1,0,2,3}, u8[0]{0}) %custom-call.176), index=0
  %fusion.122 = f32[1,1,2048,512]{3,2,1,0} fusion(f32[1,1,2048,512]{1,0,2,3} %get-tuple-element.472, f32[1,1,2048,512]{3,2,1,0} %arg40.41), kind=kLoop, calls=%fused_computation.122, metadata={op_name="XLA_Retvals"}
  %arg141.142 = f32[1,512,1,1]{3,2,1,0} parameter(141), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg139.140 = f32[1,512,1,1]{3,2,1,0} parameter(139), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.660 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.121), index=0
  %fusion.120 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg141.142, f32[1,512,1,1]{3,2,1,0} %arg139.140, f32[512]{0} %get-tuple-element.661, f32[512]{0} %get-tuple-element.660), kind=kLoop, calls=%fused_computation.120, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.604 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.118), index=1
  %custom-call.178 = (f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %arg145.146, f32[1,2048,25,38]{3,2,1,0} %fusion.216), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block1/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.474 = f32[1,1,512,2048]{1,0,2,3} get-tuple-element((f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) %custom-call.178), index=0
  %fusion.119 = f32[1,1,512,2048]{3,2,1,0} fusion(f32[1,1,512,2048]{1,0,2,3} %get-tuple-element.474, f32[1,1,512,2048]{3,2,1,0} %arg39.40), kind=kLoop, calls=%fused_computation.119, metadata={op_name="XLA_Retvals"}
  %arg146.147 = f32[1,2048,1,1]{3,2,1,0} parameter(146), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg144.145 = f32[1,2048,1,1]{3,2,1,0} parameter(144), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.603 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.118), index=0
  %fusion.117 = f32[2048]{0} fusion(f32[1,2048,1,1]{3,2,1,0} %arg146.147, f32[1,2048,1,1]{3,2,1,0} %arg144.145, f32[2048]{0} %get-tuple-element.604, f32[2048]{0} %get-tuple-element.603), kind=kLoop, calls=%fused_computation.117, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.655 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.115), index=1
  %custom-call.180 = (f32[3,3,512,512]{1,0,2,3}, u8[48070656]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %arg150.151, f32[1,512,25,38]{3,2,1,0} %fusion.214), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block1/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.476 = f32[3,3,512,512]{1,0,2,3} get-tuple-element((f32[3,3,512,512]{1,0,2,3}, u8[48070656]{0}) %custom-call.180), index=0
  %fusion.116 = f32[3,3,512,512]{3,2,1,0} fusion(f32[3,3,512,512]{1,0,2,3} %get-tuple-element.476, f32[3,3,512,512]{3,2,1,0} %arg38.39), kind=kLoop, calls=%fused_computation.116, metadata={op_name="XLA_Retvals"}
  %arg151.152 = f32[1,512,1,1]{3,2,1,0} parameter(151), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg149.150 = f32[1,512,1,1]{3,2,1,0} parameter(149), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.654 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.115), index=0
  %fusion.114 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg151.152, f32[1,512,1,1]{3,2,1,0} %arg149.150, f32[512]{0} %get-tuple-element.655, f32[512]{0} %get-tuple-element.654), kind=kLoop, calls=%fused_computation.114, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.657 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.112), index=1
  %custom-call.182 = (f32[1,1,2048,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %arg155.156, f32[1,512,25,38]{3,2,1,0} %fusion.212), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block1/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.478 = f32[1,1,2048,512]{1,0,2,3} get-tuple-element((f32[1,1,2048,512]{1,0,2,3}, u8[0]{0}) %custom-call.182), index=0
  %fusion.113 = f32[1,1,2048,512]{3,2,1,0} fusion(f32[1,1,2048,512]{1,0,2,3} %get-tuple-element.478, f32[1,1,2048,512]{3,2,1,0} %arg37.38), kind=kLoop, calls=%fused_computation.113, metadata={op_name="XLA_Retvals"}
  %arg156.157 = f32[1,512,1,1]{3,2,1,0} parameter(156), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg154.155 = f32[1,512,1,1]{3,2,1,0} parameter(154), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.656 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.112), index=0
  %fusion.111 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg156.157, f32[1,512,1,1]{3,2,1,0} %arg154.155, f32[512]{0} %get-tuple-element.657, f32[512]{0} %get-tuple-element.656), kind=kLoop, calls=%fused_computation.111, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.600 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.108), index=2
  %copy.484 = f32[2048]{0} copy(f32[2048]{0} %get-tuple-element.600)
  %custom-call.184 = (f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %arg163.164, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.601), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block0/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.480 = f32[1,1,512,2048]{1,0,2,3} get-tuple-element((f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) %custom-call.184), index=0
  %fusion.110 = f32[1,1,512,2048]{3,2,1,0} fusion(f32[1,1,512,2048]{1,0,2,3} %get-tuple-element.480, f32[1,1,512,2048]{3,2,1,0} %arg35.36), kind=kLoop, calls=%fused_computation.110, metadata={op_name="XLA_Retvals"}
  %custom-call.190 = (f32[1,1,1024,2048]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %arg125.126, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.602), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block0/convshortcut/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.486 = f32[1,1,1024,2048]{1,0,2,3} get-tuple-element((f32[1,1,1024,2048]{1,0,2,3}, u8[0]{0}) %custom-call.190), index=0
  %fusion.109 = f32[1,1,1024,2048]{3,2,1,0} fusion(f32[1,1,1024,2048]{1,0,2,3} %get-tuple-element.486, f32[1,1,1024,2048]{3,2,1,0} %arg36.37), kind=kLoop, calls=%fused_computation.109, metadata={op_name="XLA_Retvals"}
  %arg164.165 = f32[1,2048,1,1]{3,2,1,0} parameter(164), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg161.162 = f32[1,2048,1,1]{3,2,1,0} parameter(161), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.598 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.108), index=0
  %arg165.166 = f32[1,2048,1,1]{3,2,1,0} parameter(165), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg162.163 = f32[1,2048,1,1]{3,2,1,0} parameter(162), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.599 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,38]{3,2,1,0}) %fusion.108), index=1
  %fusion.106 = (f32[2048]{0}, f32[2048]{0}) fusion(f32[1,2048,1,1]{3,2,1,0} %arg164.165, f32[1,2048,1,1]{3,2,1,0} %arg161.162, f32[2048]{0} %get-tuple-element.598, f32[2048]{0} %get-tuple-element.600, f32[1,2048,1,1]{3,2,1,0} %arg165.166, f32[1,2048,1,1]{3,2,1,0} %arg162.163, f32[2048]{0} %get-tuple-element.599), kind=kLoop, calls=%fused_computation.106, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.662 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}) %fusion.106), index=0
  %get-tuple-element.663 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}) %fusion.106), index=1
  %get-tuple-element.653 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.102), index=1
  %arg169.170 = f32[1,512,51,77]{3,2,1,0} parameter(169), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.186 = (f32[3,3,512,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,51,77]{3,2,1,0} %arg169.170, f32[1,512,25,38]{3,2,1,0} %fusion.208), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block0/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.482 = f32[3,3,512,512]{1,0,2,3} get-tuple-element((f32[3,3,512,512]{1,0,2,3}, u8[0]{0}) %custom-call.186), index=0
  %fusion.103 = f32[3,3,512,512]{3,2,1,0} fusion(f32[3,3,512,512]{1,0,2,3} %get-tuple-element.482, f32[3,3,512,512]{3,2,1,0} %arg34.35), kind=kLoop, calls=%fused_computation.103, metadata={op_name="XLA_Retvals"}
  %arg170.171 = f32[1,512,1,1]{3,2,1,0} parameter(170), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg168.169 = f32[1,512,1,1]{3,2,1,0} parameter(168), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.652 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,38]{3,2,1,0}) %fusion.102), index=0
  %fusion.101 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg170.171, f32[1,512,1,1]{3,2,1,0} %arg168.169, f32[512]{0} %get-tuple-element.653, f32[512]{0} %get-tuple-element.652), kind=kLoop, calls=%fused_computation.101, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.583 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,50,76]{3,2,1,0}) %fusion.99), index=1
  %custom-call.188 = (f32[1,1,1024,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %arg125.126, f32[1,512,50,76]{3,2,1,0} %fusion.206), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block0/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.484 = f32[1,1,1024,512]{1,0,2,3} get-tuple-element((f32[1,1,1024,512]{1,0,2,3}, u8[0]{0}) %custom-call.188), index=0
  %fusion.100 = f32[1,1,1024,512]{3,2,1,0} fusion(f32[1,1,1024,512]{1,0,2,3} %get-tuple-element.484, f32[1,1,1024,512]{3,2,1,0} %arg0.1), kind=kLoop, calls=%fused_computation.100, metadata={op_name="XLA_Retvals"}
  %arg175.176 = f32[1,512,1,1]{3,2,1,0} parameter(175), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg174.175 = f32[1,512,1,1]{3,2,1,0} parameter(174), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.582 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,50,76]{3,2,1,0}) %fusion.99), index=0
  %fusion.98 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg175.176, f32[1,512,1,1]{3,2,1,0} %arg174.175, f32[512]{0} %get-tuple-element.583, f32[512]{0} %get-tuple-element.582), kind=kLoop, calls=%fused_computation.98, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.568 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.96), index=1
  %custom-call.192 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg179.180, f32[1,1024,50,76]{3,2,1,0} %fusion.203), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block5/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.488 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.192), index=0
  %fusion.97 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.488, f32[1,1,256,1024]{3,2,1,0} %arg1.2), kind=kLoop, calls=%fused_computation.97, metadata={op_name="XLA_Retvals"}
  %arg180.181 = f32[1,1024,1,1]{3,2,1,0} parameter(180), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg178.179 = f32[1,1024,1,1]{3,2,1,0} parameter(178), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.567 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.96), index=0
  %fusion.95 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg180.181, f32[1,1024,1,1]{3,2,1,0} %arg178.179, f32[1024]{0} %get-tuple-element.568, f32[1024]{0} %get-tuple-element.567), kind=kLoop, calls=%fused_computation.95, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.624 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.93), index=1
  %get-tuple-element.650 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=9
  %arg185.186 = f32[1,256,1,1]{3,2,1,0} parameter(185), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg183.184 = f32[1,256,1,1]{3,2,1,0} parameter(183), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.623 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.93), index=0
  %fusion.92 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg185.186, f32[1,256,1,1]{3,2,1,0} %arg183.184, f32[256]{0} %get-tuple-element.624, f32[256]{0} %get-tuple-element.623), kind=kLoop, calls=%fused_computation.92, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.626 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.90), index=1
  %custom-call.196 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %arg189.190, f32[1,256,50,76]{3,2,1,0} %fusion.199), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block5/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.492 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.196), index=0
  %fusion.91 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.492, f32[1,1,1024,256]{3,2,1,0} %arg4.5), kind=kLoop, calls=%fused_computation.91, metadata={op_name="XLA_Retvals"}
  %arg190.191 = f32[1,256,1,1]{3,2,1,0} parameter(190), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg188.189 = f32[1,256,1,1]{3,2,1,0} parameter(188), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.625 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.90), index=0
  %fusion.89 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg190.191, f32[1,256,1,1]{3,2,1,0} %arg188.189, f32[256]{0} %get-tuple-element.626, f32[256]{0} %get-tuple-element.625), kind=kLoop, calls=%fused_computation.89, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.570 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.87), index=1
  %custom-call.198 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg194.195, f32[1,1024,50,76]{3,2,1,0} %fusion.197), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block4/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.494 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.198), index=0
  %fusion.88 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.494, f32[1,1,256,1024]{3,2,1,0} %arg5.6), kind=kLoop, calls=%fused_computation.88, metadata={op_name="XLA_Retvals"}
  %arg195.196 = f32[1,1024,1,1]{3,2,1,0} parameter(195), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg193.194 = f32[1,1024,1,1]{3,2,1,0} parameter(193), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.569 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.87), index=0
  %fusion.86 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg195.196, f32[1,1024,1,1]{3,2,1,0} %arg193.194, f32[1024]{0} %get-tuple-element.570, f32[1024]{0} %get-tuple-element.569), kind=kLoop, calls=%fused_computation.86, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.618 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.84), index=1
  %get-tuple-element.631 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=0
  %arg200.201 = f32[1,256,1,1]{3,2,1,0} parameter(200), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg198.199 = f32[1,256,1,1]{3,2,1,0} parameter(198), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.617 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.84), index=0
  %fusion.83 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg200.201, f32[1,256,1,1]{3,2,1,0} %arg198.199, f32[256]{0} %get-tuple-element.618, f32[256]{0} %get-tuple-element.617), kind=kLoop, calls=%fused_computation.83, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.616 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.81), index=1
  %custom-call.202 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %arg204.205, f32[1,256,50,76]{3,2,1,0} %fusion.193), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block4/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.498 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.202), index=0
  %fusion.82 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.498, f32[1,1,1024,256]{3,2,1,0} %arg7.8), kind=kLoop, calls=%fused_computation.82, metadata={op_name="XLA_Retvals"}
  %arg205.206 = f32[1,256,1,1]{3,2,1,0} parameter(205), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg203.204 = f32[1,256,1,1]{3,2,1,0} parameter(203), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.615 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.81), index=0
  %fusion.80 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg205.206, f32[1,256,1,1]{3,2,1,0} %arg203.204, f32[256]{0} %get-tuple-element.616, f32[256]{0} %get-tuple-element.615), kind=kLoop, calls=%fused_computation.80, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.572 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.78), index=1
  %custom-call.204 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg209.210, f32[1,1024,50,76]{3,2,1,0} %fusion.191), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block3/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.500 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.204), index=0
  %fusion.79 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.500, f32[1,1,256,1024]{3,2,1,0} %arg8.9), kind=kLoop, calls=%fused_computation.79, metadata={op_name="XLA_Retvals"}
  %arg210.211 = f32[1,1024,1,1]{3,2,1,0} parameter(210), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg208.209 = f32[1,1024,1,1]{3,2,1,0} parameter(208), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.571 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.78), index=0
  %fusion.77 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg210.211, f32[1,1024,1,1]{3,2,1,0} %arg208.209, f32[1024]{0} %get-tuple-element.572, f32[1024]{0} %get-tuple-element.571), kind=kLoop, calls=%fused_computation.77, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.612 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.75), index=1
  %get-tuple-element.644 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=3
  %arg215.216 = f32[1,256,1,1]{3,2,1,0} parameter(215), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg213.214 = f32[1,256,1,1]{3,2,1,0} parameter(213), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.611 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.75), index=0
  %fusion.74 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg215.216, f32[1,256,1,1]{3,2,1,0} %arg213.214, f32[256]{0} %get-tuple-element.612, f32[256]{0} %get-tuple-element.611), kind=kLoop, calls=%fused_computation.74, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.610 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.72), index=1
  %custom-call.208 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %arg219.220, f32[1,256,50,76]{3,2,1,0} %fusion.187), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block3/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.504 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.208), index=0
  %fusion.73 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.504, f32[1,1,1024,256]{3,2,1,0} %arg10.11), kind=kLoop, calls=%fused_computation.73, metadata={op_name="XLA_Retvals"}
  %arg220.221 = f32[1,256,1,1]{3,2,1,0} parameter(220), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg218.219 = f32[1,256,1,1]{3,2,1,0} parameter(218), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.609 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.72), index=0
  %fusion.71 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg220.221, f32[1,256,1,1]{3,2,1,0} %arg218.219, f32[256]{0} %get-tuple-element.610, f32[256]{0} %get-tuple-element.609), kind=kLoop, calls=%fused_computation.71, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.564 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.69), index=1
  %custom-call.210 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg224.225, f32[1,1024,50,76]{3,2,1,0} %fusion.185), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block2/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.506 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.210), index=0
  %fusion.70 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.506, f32[1,1,256,1024]{3,2,1,0} %arg11.12), kind=kLoop, calls=%fused_computation.70, metadata={op_name="XLA_Retvals"}
  %arg225.226 = f32[1,1024,1,1]{3,2,1,0} parameter(225), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg223.224 = f32[1,1024,1,1]{3,2,1,0} parameter(223), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.563 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.69), index=0
  %fusion.68 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg225.226, f32[1,1024,1,1]{3,2,1,0} %arg223.224, f32[1024]{0} %get-tuple-element.564, f32[1024]{0} %get-tuple-element.563), kind=kLoop, calls=%fused_computation.68, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.606 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.66), index=1
  %get-tuple-element.651 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=10
  %arg230.231 = f32[1,256,1,1]{3,2,1,0} parameter(230), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg228.229 = f32[1,256,1,1]{3,2,1,0} parameter(228), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.605 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.66), index=0
  %fusion.65 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg230.231, f32[1,256,1,1]{3,2,1,0} %arg228.229, f32[256]{0} %get-tuple-element.606, f32[256]{0} %get-tuple-element.605), kind=kLoop, calls=%fused_computation.65, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.608 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.63), index=1
  %custom-call.214 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %arg234.235, f32[1,256,50,76]{3,2,1,0} %fusion.181), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block2/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.510 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.214), index=0
  %fusion.64 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.510, f32[1,1,1024,256]{3,2,1,0} %arg13.14), kind=kLoop, calls=%fused_computation.64, metadata={op_name="XLA_Retvals"}
  %arg235.236 = f32[1,256,1,1]{3,2,1,0} parameter(235), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg233.234 = f32[1,256,1,1]{3,2,1,0} parameter(233), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.607 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.63), index=0
  %fusion.62 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg235.236, f32[1,256,1,1]{3,2,1,0} %arg233.234, f32[256]{0} %get-tuple-element.608, f32[256]{0} %get-tuple-element.607), kind=kLoop, calls=%fused_computation.62, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.576 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.60), index=1
  %custom-call.216 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg239.240, f32[1,1024,50,76]{3,2,1,0} %fusion.179), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block1/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.512 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.216), index=0
  %fusion.61 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.512, f32[1,1,256,1024]{3,2,1,0} %arg14.15), kind=kLoop, calls=%fused_computation.61, metadata={op_name="XLA_Retvals"}
  %arg240.241 = f32[1,1024,1,1]{3,2,1,0} parameter(240), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg238.239 = f32[1,1024,1,1]{3,2,1,0} parameter(238), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.575 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.60), index=0
  %fusion.59 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg240.241, f32[1,1024,1,1]{3,2,1,0} %arg238.239, f32[1024]{0} %get-tuple-element.576, f32[1024]{0} %get-tuple-element.575), kind=kLoop, calls=%fused_computation.59, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.620 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.57), index=1
  %get-tuple-element.647 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=6
  %arg245.246 = f32[1,256,1,1]{3,2,1,0} parameter(245), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg243.244 = f32[1,256,1,1]{3,2,1,0} parameter(243), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.619 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.57), index=0
  %fusion.56 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg245.246, f32[1,256,1,1]{3,2,1,0} %arg243.244, f32[256]{0} %get-tuple-element.620, f32[256]{0} %get-tuple-element.619), kind=kLoop, calls=%fused_computation.56, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.622 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.54), index=1
  %custom-call.220 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %arg249.250, f32[1,256,50,76]{3,2,1,0} %fusion.175), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block1/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.516 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.220), index=0
  %fusion.55 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.516, f32[1,1,1024,256]{3,2,1,0} %arg16.17), kind=kLoop, calls=%fused_computation.55, metadata={op_name="XLA_Retvals"}
  %arg250.251 = f32[1,256,1,1]{3,2,1,0} parameter(250), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg248.249 = f32[1,256,1,1]{3,2,1,0} parameter(248), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.621 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.54), index=0
  %fusion.53 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg250.251, f32[1,256,1,1]{3,2,1,0} %arg248.249, f32[256]{0} %get-tuple-element.622, f32[256]{0} %get-tuple-element.621), kind=kLoop, calls=%fused_computation.53, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.578 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.50), index=1
  %copy.485 = f32[1024]{0} copy(f32[1024]{0} %get-tuple-element.578)
  %get-tuple-element.566 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.173), index=1
  %custom-call.228 = (f32[1,1,512,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %arg124.125, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.566), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block0/convshortcut/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.524 = f32[1,1,512,1024]{1,0,2,3} get-tuple-element((f32[1,1,512,1024]{1,0,2,3}, u8[0]{0}) %custom-call.228), index=0
  %arg17.18 = f32[1,1,512,1024]{3,2,1,0} parameter(17), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.52 = f32[1,1,512,1024]{3,2,1,0} fusion(f32[1,1,512,1024]{1,0,2,3} %get-tuple-element.524, f32[1,1,512,1024]{3,2,1,0} %arg17.18), kind=kLoop, calls=%fused_computation.52, metadata={op_name="XLA_Retvals"}
  %custom-call.222 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %arg257.258, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.565), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block0/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.518 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.222), index=0
  %fusion.51 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.518, f32[1,1,256,1024]{3,2,1,0} %arg31.32), kind=kLoop, calls=%fused_computation.51, metadata={op_name="XLA_Retvals"}
  %arg258.259 = f32[1,1024,1,1]{3,2,1,0} parameter(258), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg255.256 = f32[1,1024,1,1]{3,2,1,0} parameter(255), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.577 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.50), index=0
  %arg259.260 = f32[1,1024,1,1]{3,2,1,0} parameter(259), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg256.257 = f32[1,1024,1,1]{3,2,1,0} parameter(256), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.579 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,76]{3,2,1,0}) %fusion.50), index=2
  %fusion.48 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1,1024,1,1]{3,2,1,0} %arg258.259, f32[1,1024,1,1]{3,2,1,0} %arg255.256, f32[1024]{0} %get-tuple-element.577, f32[1024]{0} %get-tuple-element.578, f32[1,1024,1,1]{3,2,1,0} %arg259.260, f32[1,1024,1,1]{3,2,1,0} %arg256.257, f32[1024]{0} %get-tuple-element.579), kind=kLoop, calls=%fused_computation.48, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/Reshape_grad/Reshape"}
  %get-tuple-element.664 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.48), index=0
  %get-tuple-element.665 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.48), index=1
  %get-tuple-element.614 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.44), index=1
  %get-tuple-element.632 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=1
  %arg264.265 = f32[1,256,1,1]{3,2,1,0} parameter(264), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg262.263 = f32[1,256,1,1]{3,2,1,0} parameter(262), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.613 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,76]{3,2,1,0}) %fusion.44), index=0
  %fusion.43 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg264.265, f32[1,256,1,1]{3,2,1,0} %arg262.263, f32[256]{0} %get-tuple-element.614, f32[256]{0} %get-tuple-element.613), kind=kLoop, calls=%fused_computation.43, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/Reshape_grad/Reshape"}
  %arg267.268 = f32[1,256,100,152]{3,2,1,0} parameter(267), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.282 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg32.33), metadata={op_name="XLA_Args"}
  %custom-call.225 = (f32[1,256,101,153]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.171, f32[3,3,256,256]{1,0,2,3} %copy.282), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block0/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.521 = f32[1,256,101,153]{3,2,1,0} get-tuple-element((f32[1,256,101,153]{3,2,1,0}, u8[0]{0}) %custom-call.225), index=0
  %arg265.266 = f32[1,256,100,152]{3,2,1,0} parameter(265), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.41 = (f32[256]{0}, f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) fusion(f32[1,256,100,152]{3,2,1,0} %arg267.268, f32[1,256,101,153]{3,2,1,0} %get-tuple-element.521, f32[1,256,100,152]{3,2,1,0} %arg265.266), kind=kInput, calls=%fused_computation.41, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.574 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) %fusion.41), index=1
  %get-tuple-element.718 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) %fusion.41), index=2
  %arg266.267 = f32[1,256,1,1]{3,2,1,0} parameter(266), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.169 = f32[1,256,100,152]{3,2,1,0} fusion(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.718, f32[1,256,1,1]{3,2,1,0} %arg266.267), kind=kLoop, calls=%fused_computation.169, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.226 = (f32[1,1,512,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %arg124.125, f32[1,256,100,152]{3,2,1,0} %fusion.169), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block0/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.522 = f32[1,1,512,256]{1,0,2,3} get-tuple-element((f32[1,1,512,256]{1,0,2,3}, u8[0]{0}) %custom-call.226), index=0
  %arg33.34 = f32[1,1,512,256]{3,2,1,0} parameter(33), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.42 = f32[1,1,512,256]{3,2,1,0} fusion(f32[1,1,512,256]{1,0,2,3} %get-tuple-element.522, f32[1,1,512,256]{3,2,1,0} %arg33.34), kind=kLoop, calls=%fused_computation.42, metadata={op_name="XLA_Retvals"}
  %arg269.270 = f32[1,256,1,1]{3,2,1,0} parameter(269), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg268.269 = f32[1,256,1,1]{3,2,1,0} parameter(268), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.573 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,100,152]{3,2,1,0}) %fusion.41), index=0
  %fusion.40 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg269.270, f32[1,256,1,1]{3,2,1,0} %arg268.269, f32[256]{0} %get-tuple-element.574, f32[256]{0} %get-tuple-element.573), kind=kLoop, calls=%fused_computation.40, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/Reshape_grad/Reshape"}
  %arg271.272 = f32[1,512,100,152]{3,2,1,0} parameter(271), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.286 = f32[1,1,512,1024]{1,0,2,3} copy(f32[1,1,512,1024]{3,2,1,0} %arg17.18), metadata={op_name="XLA_Args"}
  %custom-call.229 = (f32[1,512,100,152]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.566, f32[1,1,512,1024]{1,0,2,3} %copy.286), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block0/convshortcut/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.525 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[0]{0}) %custom-call.229), index=0
  %bitcast.162 = f32[1,1,512,256]{1,0,3,2} bitcast(f32[1,1,512,256]{3,2,1,0} %arg45.46), metadata={op_name="XLA_Args"}
  %custom-call.145 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.687, f32[1,1,512,256]{1,0,3,2} %bitcast.162), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.441 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.145), index=0
  %bitcast.163 = f32[1,1,512,256]{1,0,3,2} bitcast(f32[1,1,512,256]{3,2,1,0} %arg33.34), metadata={op_name="XLA_Args"}
  %custom-call.227 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %fusion.169, f32[1,1,512,256]{1,0,3,2} %bitcast.163), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block0/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.523 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.227), index=0
  %fusion.38 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) fusion(f32[1,512,100,152]{3,2,1,0} %arg271.272, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.525, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.441, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.523, f32[1,512,100,152]{3,2,1,0} %arg124.125), kind=kInput, calls=%fused_computation.38, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.560 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.38), index=1
  %arg273.274 = f32[1,128,100,152]{3,2,1,0} parameter(273), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.719 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.38), index=2
  %arg270.271 = f32[1,512,1,1]{3,2,1,0} parameter(270), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.166 = f32[1,512,100,152]{3,2,1,0} fusion(f32[1,512,100,152]{3,2,1,0} %get-tuple-element.719, f32[1,512,1,1]{3,2,1,0} %arg270.271), kind=kLoop, calls=%fused_computation.166, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.230 = (f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %arg273.274, f32[1,512,100,152]{3,2,1,0} %fusion.166), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block3/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.526 = f32[1,1,128,512]{1,0,2,3} get-tuple-element((f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) %custom-call.230), index=0
  %arg18.19 = f32[1,1,128,512]{3,2,1,0} parameter(18), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.39 = f32[1,1,128,512]{3,2,1,0} fusion(f32[1,1,128,512]{1,0,2,3} %get-tuple-element.526, f32[1,1,128,512]{3,2,1,0} %arg18.19), kind=kLoop, calls=%fused_computation.39, metadata={op_name="XLA_Retvals"}
  %arg274.275 = f32[1,512,1,1]{3,2,1,0} parameter(274), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg272.273 = f32[1,512,1,1]{3,2,1,0} parameter(272), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.559 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.38), index=0
  %fusion.37 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg274.275, f32[1,512,1,1]{3,2,1,0} %arg272.273, f32[512]{0} %get-tuple-element.560, f32[512]{0} %get-tuple-element.559), kind=kLoop, calls=%fused_computation.37, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/Reshape_grad/Reshape"}
  %arg276.277 = f32[1,128,100,152]{3,2,1,0} parameter(276), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.168 = f32[1,1,128,512]{1,0,3,2} bitcast(f32[1,1,128,512]{3,2,1,0} %arg18.19), metadata={op_name="XLA_Args"}
  %custom-call.231 = (f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %fusion.166, f32[1,1,128,512]{1,0,3,2} %bitcast.168), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block3/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.527 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.231), index=0
  %fusion.35 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) fusion(f32[1,128,100,152]{3,2,1,0} %arg276.277, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.527, f32[1,128,100,152]{3,2,1,0} %arg273.274), kind=kInput, calls=%fused_computation.35, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.597 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.35), index=1
  %arg278.279 = f32[1,128,100,152]{3,2,1,0} parameter(278), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.720 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.35), index=2
  %arg275.276 = f32[1,128,1,1]{3,2,1,0} parameter(275), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.164 = f32[1,128,100,152]{3,2,1,0} fusion(f32[1,128,100,152]{3,2,1,0} %get-tuple-element.720, f32[1,128,1,1]{3,2,1,0} %arg275.276), kind=kLoop, calls=%fused_computation.164, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.232 = (f32[3,3,128,128]{1,0,2,3}, u8[37380096]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %arg278.279, f32[1,128,100,152]{3,2,1,0} %fusion.164), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block3/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.528 = f32[3,3,128,128]{1,0,2,3} get-tuple-element((f32[3,3,128,128]{1,0,2,3}, u8[37380096]{0}) %custom-call.232), index=0
  %arg19.20 = f32[3,3,128,128]{3,2,1,0} parameter(19), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.36 = f32[3,3,128,128]{3,2,1,0} fusion(f32[3,3,128,128]{1,0,2,3} %get-tuple-element.528, f32[3,3,128,128]{3,2,1,0} %arg19.20), kind=kLoop, calls=%fused_computation.36, metadata={op_name="XLA_Retvals"}
  %arg279.280 = f32[1,128,1,1]{3,2,1,0} parameter(279), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg277.278 = f32[1,128,1,1]{3,2,1,0} parameter(277), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.596 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.35), index=0
  %fusion.34 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg279.280, f32[1,128,1,1]{3,2,1,0} %arg277.278, f32[128]{0} %get-tuple-element.597, f32[128]{0} %get-tuple-element.596), kind=kLoop, calls=%fused_computation.34, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/Reshape_grad/Reshape"}
  %arg281.282 = f32[1,128,100,152]{3,2,1,0} parameter(281), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.290 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg19.20), metadata={op_name="XLA_Args"}
  %custom-call.233 = (f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.164, f32[3,3,128,128]{1,0,2,3} %copy.290), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block3/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.529 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) %custom-call.233), index=0
  %fusion.32 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) fusion(f32[1,128,100,152]{3,2,1,0} %arg281.282, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.529, f32[1,128,100,152]{3,2,1,0} %arg278.279), kind=kInput, calls=%fused_computation.32, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.595 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.32), index=1
  %arg283.284 = f32[1,512,100,152]{3,2,1,0} parameter(283), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.721 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.32), index=2
  %arg280.281 = f32[1,128,1,1]{3,2,1,0} parameter(280), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.162 = f32[1,128,100,152]{3,2,1,0} fusion(f32[1,128,100,152]{3,2,1,0} %get-tuple-element.721, f32[1,128,1,1]{3,2,1,0} %arg280.281), kind=kLoop, calls=%fused_computation.162, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.234 = (f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %arg283.284, f32[1,128,100,152]{3,2,1,0} %fusion.162), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block3/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.530 = f32[1,1,512,128]{1,0,2,3} get-tuple-element((f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) %custom-call.234), index=0
  %arg20.21 = f32[1,1,512,128]{3,2,1,0} parameter(20), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.33 = f32[1,1,512,128]{3,2,1,0} fusion(f32[1,1,512,128]{1,0,2,3} %get-tuple-element.530, f32[1,1,512,128]{3,2,1,0} %arg20.21), kind=kLoop, calls=%fused_computation.33, metadata={op_name="XLA_Retvals"}
  %arg284.285 = f32[1,128,1,1]{3,2,1,0} parameter(284), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg282.283 = f32[1,128,1,1]{3,2,1,0} parameter(282), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.594 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.32), index=0
  %fusion.31 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg284.285, f32[1,128,1,1]{3,2,1,0} %arg282.283, f32[128]{0} %get-tuple-element.595, f32[128]{0} %get-tuple-element.594), kind=kLoop, calls=%fused_computation.31, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/Reshape_grad/Reshape"}
  %arg286.287 = f32[1,512,100,152]{3,2,1,0} parameter(286), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.177 = f32[1,1,512,128]{1,0,3,2} bitcast(f32[1,1,512,128]{3,2,1,0} %arg20.21), metadata={op_name="XLA_Args"}
  %custom-call.235 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.162, f32[1,1,512,128]{1,0,3,2} %bitcast.177), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block3/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.531 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.235), index=0
  %fusion.29 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) fusion(f32[1,512,100,152]{3,2,1,0} %arg286.287, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.531, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.719, f32[1,512,100,152]{3,2,1,0} %arg283.284), kind=kInput, calls=%fused_computation.29, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.562 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.29), index=1
  %arg288.289 = f32[1,128,100,152]{3,2,1,0} parameter(288), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.722 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.29), index=2
  %arg285.286 = f32[1,512,1,1]{3,2,1,0} parameter(285), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.160 = f32[1,512,100,152]{3,2,1,0} fusion(f32[1,512,100,152]{3,2,1,0} %get-tuple-element.722, f32[1,512,1,1]{3,2,1,0} %arg285.286), kind=kLoop, calls=%fused_computation.160, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.236 = (f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %arg288.289, f32[1,512,100,152]{3,2,1,0} %fusion.160), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block2/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.532 = f32[1,1,128,512]{1,0,2,3} get-tuple-element((f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) %custom-call.236), index=0
  %arg21.22 = f32[1,1,128,512]{3,2,1,0} parameter(21), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.30 = f32[1,1,128,512]{3,2,1,0} fusion(f32[1,1,128,512]{1,0,2,3} %get-tuple-element.532, f32[1,1,128,512]{3,2,1,0} %arg21.22), kind=kLoop, calls=%fused_computation.30, metadata={op_name="XLA_Retvals"}
  %arg289.290 = f32[1,512,1,1]{3,2,1,0} parameter(289), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg287.288 = f32[1,512,1,1]{3,2,1,0} parameter(287), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.561 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.29), index=0
  %fusion.28 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg289.290, f32[1,512,1,1]{3,2,1,0} %arg287.288, f32[512]{0} %get-tuple-element.562, f32[512]{0} %get-tuple-element.561), kind=kLoop, calls=%fused_computation.28, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/Reshape_grad/Reshape"}
  %arg291.292 = f32[1,128,100,152]{3,2,1,0} parameter(291), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.182 = f32[1,1,128,512]{1,0,3,2} bitcast(f32[1,1,128,512]{3,2,1,0} %arg21.22), metadata={op_name="XLA_Args"}
  %custom-call.237 = (f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %fusion.160, f32[1,1,128,512]{1,0,3,2} %bitcast.182), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block2/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.533 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.237), index=0
  %fusion.26 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) fusion(f32[1,128,100,152]{3,2,1,0} %arg291.292, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.533, f32[1,128,100,152]{3,2,1,0} %arg288.289), kind=kInput, calls=%fused_computation.26, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.591 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.26), index=1
  %arg293.294 = f32[1,128,100,152]{3,2,1,0} parameter(293), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.723 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.26), index=2
  %arg290.291 = f32[1,128,1,1]{3,2,1,0} parameter(290), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.158 = f32[1,128,100,152]{3,2,1,0} fusion(f32[1,128,100,152]{3,2,1,0} %get-tuple-element.723, f32[1,128,1,1]{3,2,1,0} %arg290.291), kind=kLoop, calls=%fused_computation.158, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.238 = (f32[3,3,128,128]{1,0,2,3}, u8[37380096]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %arg293.294, f32[1,128,100,152]{3,2,1,0} %fusion.158), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block2/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.534 = f32[3,3,128,128]{1,0,2,3} get-tuple-element((f32[3,3,128,128]{1,0,2,3}, u8[37380096]{0}) %custom-call.238), index=0
  %arg22.23 = f32[3,3,128,128]{3,2,1,0} parameter(22), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.27 = f32[3,3,128,128]{3,2,1,0} fusion(f32[3,3,128,128]{1,0,2,3} %get-tuple-element.534, f32[3,3,128,128]{3,2,1,0} %arg22.23), kind=kLoop, calls=%fused_computation.27, metadata={op_name="XLA_Retvals"}
  %arg294.295 = f32[1,128,1,1]{3,2,1,0} parameter(294), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg292.293 = f32[1,128,1,1]{3,2,1,0} parameter(292), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.590 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.26), index=0
  %fusion.25 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg294.295, f32[1,128,1,1]{3,2,1,0} %arg292.293, f32[128]{0} %get-tuple-element.591, f32[128]{0} %get-tuple-element.590), kind=kLoop, calls=%fused_computation.25, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/Reshape_grad/Reshape"}
  %arg296.297 = f32[1,128,100,152]{3,2,1,0} parameter(296), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.296 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg22.23), metadata={op_name="XLA_Args"}
  %custom-call.239 = (f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.158, f32[3,3,128,128]{1,0,2,3} %copy.296), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block2/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.535 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) %custom-call.239), index=0
  %fusion.23 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) fusion(f32[1,128,100,152]{3,2,1,0} %arg296.297, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.535, f32[1,128,100,152]{3,2,1,0} %arg293.294), kind=kInput, calls=%fused_computation.23, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.593 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.23), index=1
  %arg298.299 = f32[1,512,100,152]{3,2,1,0} parameter(298), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.724 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.23), index=2
  %arg295.296 = f32[1,128,1,1]{3,2,1,0} parameter(295), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.156 = f32[1,128,100,152]{3,2,1,0} fusion(f32[1,128,100,152]{3,2,1,0} %get-tuple-element.724, f32[1,128,1,1]{3,2,1,0} %arg295.296), kind=kLoop, calls=%fused_computation.156, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.240 = (f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %arg298.299, f32[1,128,100,152]{3,2,1,0} %fusion.156), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block2/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.536 = f32[1,1,512,128]{1,0,2,3} get-tuple-element((f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) %custom-call.240), index=0
  %arg23.24 = f32[1,1,512,128]{3,2,1,0} parameter(23), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.24 = f32[1,1,512,128]{3,2,1,0} fusion(f32[1,1,512,128]{1,0,2,3} %get-tuple-element.536, f32[1,1,512,128]{3,2,1,0} %arg23.24), kind=kLoop, calls=%fused_computation.24, metadata={op_name="XLA_Retvals"}
  %arg299.300 = f32[1,128,1,1]{3,2,1,0} parameter(299), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg297.298 = f32[1,128,1,1]{3,2,1,0} parameter(297), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.592 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.23), index=0
  %fusion.22 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg299.300, f32[1,128,1,1]{3,2,1,0} %arg297.298, f32[128]{0} %get-tuple-element.593, f32[128]{0} %get-tuple-element.592), kind=kLoop, calls=%fused_computation.22, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/Reshape_grad/Reshape"}
  %arg301.302 = f32[1,512,100,152]{3,2,1,0} parameter(301), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.191 = f32[1,1,512,128]{1,0,3,2} bitcast(f32[1,1,512,128]{3,2,1,0} %arg23.24), metadata={op_name="XLA_Args"}
  %custom-call.241 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.156, f32[1,1,512,128]{1,0,3,2} %bitcast.191), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block2/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.537 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.241), index=0
  %fusion.20 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) fusion(f32[1,512,100,152]{3,2,1,0} %arg301.302, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.537, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.722, f32[1,512,100,152]{3,2,1,0} %arg298.299), kind=kInput, calls=%fused_computation.20, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.553 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.20), index=1
  %arg303.304 = f32[1,128,100,152]{3,2,1,0} parameter(303), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.725 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.20), index=2
  %arg300.301 = f32[1,512,1,1]{3,2,1,0} parameter(300), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.154 = f32[1,512,100,152]{3,2,1,0} fusion(f32[1,512,100,152]{3,2,1,0} %get-tuple-element.725, f32[1,512,1,1]{3,2,1,0} %arg300.301), kind=kLoop, calls=%fused_computation.154, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.242 = (f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %arg303.304, f32[1,512,100,152]{3,2,1,0} %fusion.154), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block1/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.538 = f32[1,1,128,512]{1,0,2,3} get-tuple-element((f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) %custom-call.242), index=0
  %arg24.25 = f32[1,1,128,512]{3,2,1,0} parameter(24), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.21 = f32[1,1,128,512]{3,2,1,0} fusion(f32[1,1,128,512]{1,0,2,3} %get-tuple-element.538, f32[1,1,128,512]{3,2,1,0} %arg24.25), kind=kLoop, calls=%fused_computation.21, metadata={op_name="XLA_Retvals"}
  %arg304.305 = f32[1,512,1,1]{3,2,1,0} parameter(304), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg302.303 = f32[1,512,1,1]{3,2,1,0} parameter(302), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.552 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.20), index=0
  %fusion.19 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg304.305, f32[1,512,1,1]{3,2,1,0} %arg302.303, f32[512]{0} %get-tuple-element.553, f32[512]{0} %get-tuple-element.552), kind=kLoop, calls=%fused_computation.19, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/Reshape_grad/Reshape"}
  %arg306.307 = f32[1,128,100,152]{3,2,1,0} parameter(306), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.196 = f32[1,1,128,512]{1,0,3,2} bitcast(f32[1,1,128,512]{3,2,1,0} %arg24.25), metadata={op_name="XLA_Args"}
  %custom-call.243 = (f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %fusion.154, f32[1,1,128,512]{1,0,3,2} %bitcast.196), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block1/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.539 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.243), index=0
  %fusion.17 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) fusion(f32[1,128,100,152]{3,2,1,0} %arg306.307, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.539, f32[1,128,100,152]{3,2,1,0} %arg303.304), kind=kInput, calls=%fused_computation.17, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.589 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.17), index=1
  %arg308.309 = f32[1,128,100,152]{3,2,1,0} parameter(308), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.726 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.17), index=2
  %arg305.306 = f32[1,128,1,1]{3,2,1,0} parameter(305), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.152 = f32[1,128,100,152]{3,2,1,0} fusion(f32[1,128,100,152]{3,2,1,0} %get-tuple-element.726, f32[1,128,1,1]{3,2,1,0} %arg305.306), kind=kLoop, calls=%fused_computation.152, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.244 = (f32[3,3,128,128]{1,0,2,3}, u8[37380096]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %arg308.309, f32[1,128,100,152]{3,2,1,0} %fusion.152), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block1/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.540 = f32[3,3,128,128]{1,0,2,3} get-tuple-element((f32[3,3,128,128]{1,0,2,3}, u8[37380096]{0}) %custom-call.244), index=0
  %arg25.26 = f32[3,3,128,128]{3,2,1,0} parameter(25), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.18 = f32[3,3,128,128]{3,2,1,0} fusion(f32[3,3,128,128]{1,0,2,3} %get-tuple-element.540, f32[3,3,128,128]{3,2,1,0} %arg25.26), kind=kLoop, calls=%fused_computation.18, metadata={op_name="XLA_Retvals"}
  %arg309.310 = f32[1,128,1,1]{3,2,1,0} parameter(309), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg307.308 = f32[1,128,1,1]{3,2,1,0} parameter(307), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.588 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.17), index=0
  %fusion.16 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg309.310, f32[1,128,1,1]{3,2,1,0} %arg307.308, f32[128]{0} %get-tuple-element.589, f32[128]{0} %get-tuple-element.588), kind=kLoop, calls=%fused_computation.16, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/Reshape_grad/Reshape"}
  %arg311.312 = f32[1,128,100,152]{3,2,1,0} parameter(311), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.302 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg25.26), metadata={op_name="XLA_Args"}
  %custom-call.245 = (f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.152, f32[3,3,128,128]{1,0,2,3} %copy.302), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block1/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.541 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) %custom-call.245), index=0
  %fusion.14 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) fusion(f32[1,128,100,152]{3,2,1,0} %arg311.312, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.541, f32[1,128,100,152]{3,2,1,0} %arg308.309), kind=kInput, calls=%fused_computation.14, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.587 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.14), index=1
  %arg313.314 = f32[1,512,100,152]{3,2,1,0} parameter(313), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.727 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.14), index=2
  %arg310.311 = f32[1,128,1,1]{3,2,1,0} parameter(310), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.150 = f32[1,128,100,152]{3,2,1,0} fusion(f32[1,128,100,152]{3,2,1,0} %get-tuple-element.727, f32[1,128,1,1]{3,2,1,0} %arg310.311), kind=kLoop, calls=%fused_computation.150, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.246 = (f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %arg313.314, f32[1,128,100,152]{3,2,1,0} %fusion.150), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block1/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.542 = f32[1,1,512,128]{1,0,2,3} get-tuple-element((f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) %custom-call.246), index=0
  %arg26.27 = f32[1,1,512,128]{3,2,1,0} parameter(26), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.15 = f32[1,1,512,128]{3,2,1,0} fusion(f32[1,1,512,128]{1,0,2,3} %get-tuple-element.542, f32[1,1,512,128]{3,2,1,0} %arg26.27), kind=kLoop, calls=%fused_computation.15, metadata={op_name="XLA_Retvals"}
  %arg314.315 = f32[1,128,1,1]{3,2,1,0} parameter(314), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg312.313 = f32[1,128,1,1]{3,2,1,0} parameter(312), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.586 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.14), index=0
  %fusion.13 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg314.315, f32[1,128,1,1]{3,2,1,0} %arg312.313, f32[128]{0} %get-tuple-element.587, f32[128]{0} %get-tuple-element.586), kind=kLoop, calls=%fused_computation.13, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/Reshape_grad/Reshape"}
  %arg316.317 = f32[1,512,100,152]{3,2,1,0} parameter(316), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg317.318 = f32[1,512,100,152]{3,2,1,0} parameter(317), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.205 = f32[1,1,512,128]{1,0,3,2} bitcast(f32[1,1,512,128]{3,2,1,0} %arg26.27), metadata={op_name="XLA_Args"}
  %custom-call.247 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.150, f32[1,1,512,128]{1,0,3,2} %bitcast.205), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block1/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.543 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.247), index=0
  %fusion.10 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) fusion(f32[1,512,100,152]{3,2,1,0} %arg316.317, f32[1,512,100,152]{3,2,1,0} %arg317.318, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.543, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.725, f32[1,512,100,152]{3,2,1,0} %arg313.314), kind=kInput, calls=%fused_computation.10, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.555 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.10), index=1
  %copy.486 = f32[512]{0} copy(f32[512]{0} %get-tuple-element.555)
  %get-tuple-element.728 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.10), index=3
  %arg318.319 = f32[1,512,1,1]{3,2,1,0} parameter(318), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg315.316 = f32[1,512,1,1]{3,2,1,0} parameter(315), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.148 = (f32[1,512,100,152]{3,2,1,0}, f32[1,512,100,152]{3,2,1,0}) fusion(f32[1,512,100,152]{3,2,1,0} %get-tuple-element.728, f32[1,512,1,1]{3,2,1,0} %arg318.319, f32[1,512,1,1]{3,2,1,0} %arg315.316), kind=kLoop, calls=%fused_computation.148, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %get-tuple-element.551 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, f32[1,512,100,152]{3,2,1,0}) %fusion.148), index=1
  %custom-call.253 = (f32[1,1,256,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %arg122.123, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.551), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block0/convshortcut/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.549 = f32[1,1,256,512]{1,0,2,3} get-tuple-element((f32[1,1,256,512]{1,0,2,3}, u8[0]{0}) %custom-call.253), index=0
  %arg27.28 = f32[1,1,256,512]{3,2,1,0} parameter(27), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.12 = f32[1,1,256,512]{3,2,1,0} fusion(f32[1,1,256,512]{1,0,2,3} %get-tuple-element.549, f32[1,1,256,512]{3,2,1,0} %arg27.28), kind=kLoop, calls=%fused_computation.12, metadata={op_name="XLA_Retvals"}
  %arg321.322 = f32[1,128,100,152]{3,2,1,0} parameter(321), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.550 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, f32[1,512,100,152]{3,2,1,0}) %fusion.148), index=0
  %custom-call.248 = (f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %arg321.322, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.550), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block0/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.544 = f32[1,1,128,512]{1,0,2,3} get-tuple-element((f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) %custom-call.248), index=0
  %arg28.29 = f32[1,1,128,512]{3,2,1,0} parameter(28), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.11 = f32[1,1,128,512]{3,2,1,0} fusion(f32[1,1,128,512]{1,0,2,3} %get-tuple-element.544, f32[1,1,128,512]{3,2,1,0} %arg28.29), kind=kLoop, calls=%fused_computation.11, metadata={op_name="XLA_Retvals"}
  %arg322.323 = f32[1,512,1,1]{3,2,1,0} parameter(322), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg319.320 = f32[1,512,1,1]{3,2,1,0} parameter(319), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.554 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.10), index=0
  %arg323.324 = f32[1,512,1,1]{3,2,1,0} parameter(323), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg320.321 = f32[1,512,1,1]{3,2,1,0} parameter(320), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.558 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,152]{3,2,1,0}) %fusion.10), index=2
  %fusion.8 = (f32[512]{0}, f32[512]{0}) fusion(f32[1,512,1,1]{3,2,1,0} %arg322.323, f32[1,512,1,1]{3,2,1,0} %arg319.320, f32[512]{0} %get-tuple-element.554, f32[512]{0} %get-tuple-element.555, f32[1,512,1,1]{3,2,1,0} %arg323.324, f32[1,512,1,1]{3,2,1,0} %arg320.321, f32[512]{0} %get-tuple-element.558), kind=kLoop, calls=%fused_computation.8, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/Reshape_grad/Reshape"}
  %get-tuple-element.666 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}) %fusion.8), index=0
  %get-tuple-element.667 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}) %fusion.8), index=1
  %arg324.325 = f32[1,128,100,152]{3,2,1,0} parameter(324), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.213 = f32[1,1,128,512]{1,0,3,2} bitcast(f32[1,1,128,512]{3,2,1,0} %arg28.29), metadata={op_name="XLA_Args"}
  %custom-call.249 = (f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %get-tuple-element.550, f32[1,1,128,512]{1,0,3,2} %bitcast.213), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block0/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.545 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.249), index=0
  %fusion.4 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) fusion(f32[1,128,100,152]{3,2,1,0} %arg324.325, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.545, f32[1,128,100,152]{3,2,1,0} %arg321.322), kind=kInput, calls=%fused_computation.4, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.585 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.4), index=1
  %arg327.328 = f32[1,128,201,305]{3,2,1,0} parameter(327), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.729 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.4), index=2
  %arg325.326 = f32[1,128,1,1]{3,2,1,0} parameter(325), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.146 = f32[1,128,100,152]{3,2,1,0} fusion(f32[1,128,100,152]{3,2,1,0} %get-tuple-element.729, f32[1,128,1,1]{3,2,1,0} %arg325.326), kind=kLoop, calls=%fused_computation.146, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.250 = (f32[3,3,128,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,201,305]{3,2,1,0} %arg327.328, f32[1,128,100,152]{3,2,1,0} %fusion.146), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block0/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.546 = f32[3,3,128,128]{1,0,2,3} get-tuple-element((f32[3,3,128,128]{1,0,2,3}, u8[0]{0}) %custom-call.250), index=0
  %arg29.30 = f32[3,3,128,128]{3,2,1,0} parameter(29), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.5 = f32[3,3,128,128]{3,2,1,0} fusion(f32[3,3,128,128]{1,0,2,3} %get-tuple-element.546, f32[3,3,128,128]{3,2,1,0} %arg29.30), kind=kLoop, calls=%fused_computation.5, metadata={op_name="XLA_Retvals"}
  %arg328.329 = f32[1,128,1,1]{3,2,1,0} parameter(328), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg326.327 = f32[1,128,1,1]{3,2,1,0} parameter(326), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.584 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,152]{3,2,1,0}) %fusion.4), index=0
  %fusion.3 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg328.329, f32[1,128,1,1]{3,2,1,0} %arg326.327, f32[128]{0} %get-tuple-element.585, f32[128]{0} %get-tuple-element.584), kind=kLoop, calls=%fused_computation.3, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/Reshape_grad/Reshape"}
  %arg330.331 = f32[1,128,200,304]{3,2,1,0} parameter(330), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.309 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg29.30), metadata={op_name="XLA_Args"}
  %custom-call.251 = (f32[1,128,201,305]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.146, f32[3,3,128,128]{1,0,2,3} %copy.309), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block0/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.547 = f32[1,128,201,305]{3,2,1,0} get-tuple-element((f32[1,128,201,305]{3,2,1,0}, u8[0]{0}) %custom-call.251), index=0
  %arg329.330 = f32[1,128,200,304]{3,2,1,0} parameter(329), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.1 = (f32[128]{0}, f32[128]{0}, f32[1,128,200,304]{3,2,1,0}) fusion(f32[1,128,200,304]{3,2,1,0} %arg330.331, f32[1,128,201,305]{3,2,1,0} %get-tuple-element.547, f32[1,128,200,304]{3,2,1,0} %arg329.330), kind=kInput, calls=%fused_computation.1, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.557 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,200,304]{3,2,1,0}) %fusion.1), index=1
  %get-tuple-element.730 = f32[1,128,200,304]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,200,304]{3,2,1,0}) %fusion.1), index=2
  %arg331.332 = f32[1,128,1,1]{3,2,1,0} parameter(331), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.144 = f32[1,128,200,304]{3,2,1,0} fusion(f32[1,128,200,304]{3,2,1,0} %get-tuple-element.730, f32[1,128,1,1]{3,2,1,0} %arg331.332), kind=kLoop, calls=%fused_computation.144, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.252 = (f32[1,1,256,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %arg122.123, f32[1,128,200,304]{3,2,1,0} %fusion.144), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block0/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.548 = f32[1,1,256,128]{1,0,2,3} get-tuple-element((f32[1,1,256,128]{1,0,2,3}, u8[0]{0}) %custom-call.252), index=0
  %arg30.31 = f32[1,1,256,128]{3,2,1,0} parameter(30), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.2 = f32[1,1,256,128]{3,2,1,0} fusion(f32[1,1,256,128]{1,0,2,3} %get-tuple-element.548, f32[1,1,256,128]{3,2,1,0} %arg30.31), kind=kLoop, calls=%fused_computation.2, metadata={op_name="XLA_Retvals"}
  %arg333.334 = f32[1,128,1,1]{3,2,1,0} parameter(333), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg332.333 = f32[1,128,1,1]{3,2,1,0} parameter(332), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.556 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,200,304]{3,2,1,0}) %fusion.1), index=0
  %fusion = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg333.334, f32[1,128,1,1]{3,2,1,0} %arg332.333, f32[128]{0} %get-tuple-element.557, f32[128]{0} %get-tuple-element.556), kind=kLoop, calls=%fused_computation, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/Reshape_grad/Reshape"}
  ROOT %tuple.238 = (f32[3]{0}, f32[1,1,256,3]{3,2,1,0}, f32[12]{0}, f32[1,1,256,12]{3,2,1,0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[1,1,256,256]{3,2,1,0}, f32[256]{0}, f32[1,1,512,256]{3,2,1,0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1,1,2048,256]{3,2,1,0}, f32[2048]{0}, f32[1,1,512,2048]{3,2,1,0}, f32[2048]{0}, f32[512]{0}, f32[3,3,512,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,2048,512]{3,2,1,0}, f32[512]{0}, f32[2048]{0}, f32[1,1,512,2048]{3,2,1,0}, f32[2048]{0}, f32[512]{0}, f32[3,3,512,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,2048,512]{3,2,1,0}, f32[512]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,1,512,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[2048]{0}, f32[2048]{0}, f32[512]{0}, f32[3,3,512,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,1024,512]{3,2,1,0}, f32[512]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,512,256]{3,2,1,0}, f32[256]{0}, f32[512]{0}, f32[1,1,128,512]{3,2,1,0}, f32[512]{0}, f32[128]{0}, f32[3,3,128,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,512,128]{3,2,1,0}, f32[128]{0}, f32[512]{0}, f32[1,1,128,512]{3,2,1,0}, f32[512]{0}, f32[128]{0}, f32[3,3,128,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,512,128]{3,2,1,0}, f32[128]{0}, f32[512]{0}, f32[1,1,128,512]{3,2,1,0}, f32[512]{0}, f32[128]{0}, f32[3,3,128,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,512,128]{3,2,1,0}, f32[128]{0}, f32[512]{0}, f32[512]{0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,128,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[128]{0}, f32[3,3,128,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,256,128]{3,2,1,0}, f32[128]{0}) tuple(f32[3]{0} %fusion.142, f32[1,1,256,3]{3,2,1,0} %fusion.141, f32[12]{0} %fusion.140, f32[1,1,256,12]{3,2,1,0} %fusion.139, f32[256]{0} %fusion.138, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.646, f32[256]{0} %get-tuple-element.678, f32[256]{0} %get-tuple-element.680, f32[256]{0} %get-tuple-element.682, f32[256]{0} %get-tuple-element.684, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.649, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.643, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.645, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.648, f32[256]{0} %reduce.1253, f32[1,1,256,256]{3,2,1,0} %fusion.132, f32[256]{0} %get-tuple-element.686, f32[1,1,512,256]{3,2,1,0} %fusion.131, f32[256]{0} %get-tuple-element.688, f32[1,1,1024,256]{3,2,1,0} %fusion.130, f32[256]{0} %get-tuple-element.690, f32[1,1,2048,256]{3,2,1,0} %fusion.129, f32[2048]{0} %get-tuple-element.581, f32[1,1,512,2048]{3,2,1,0} %fusion.128, f32[2048]{0} %fusion.126, f32[512]{0} %get-tuple-element.659, f32[3,3,512,512]{3,2,1,0} %fusion.125, f32[512]{0} %fusion.123, f32[512]{0} %get-tuple-element.661, f32[1,1,2048,512]{3,2,1,0} %fusion.122, f32[512]{0} %fusion.120, f32[2048]{0} %get-tuple-element.604, f32[1,1,512,2048]{3,2,1,0} %fusion.119, f32[2048]{0} %fusion.117, f32[512]{0} %get-tuple-element.655, f32[3,3,512,512]{3,2,1,0} %fusion.116, f32[512]{0} %fusion.114, f32[512]{0} %get-tuple-element.657, f32[1,1,2048,512]{3,2,1,0} %fusion.113, f32[512]{0} %fusion.111, f32[2048]{0} %get-tuple-element.600, f32[2048]{0} %copy.484, f32[1,1,512,2048]{3,2,1,0} %fusion.110, f32[1,1,1024,2048]{3,2,1,0} %fusion.109, f32[2048]{0} %get-tuple-element.662, f32[2048]{0} %get-tuple-element.663, f32[512]{0} %get-tuple-element.653, f32[3,3,512,512]{3,2,1,0} %fusion.103, f32[512]{0} %fusion.101, f32[512]{0} %get-tuple-element.583, f32[1,1,1024,512]{3,2,1,0} %fusion.100, f32[512]{0} %fusion.98, f32[1024]{0} %get-tuple-element.568, f32[1,1,256,1024]{3,2,1,0} %fusion.97, f32[1024]{0} %fusion.95, f32[256]{0} %get-tuple-element.624, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.650, f32[256]{0} %fusion.92, f32[256]{0} %get-tuple-element.626, f32[1,1,1024,256]{3,2,1,0} %fusion.91, f32[256]{0} %fusion.89, f32[1024]{0} %get-tuple-element.570, f32[1,1,256,1024]{3,2,1,0} %fusion.88, f32[1024]{0} %fusion.86, f32[256]{0} %get-tuple-element.618, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.631, f32[256]{0} %fusion.83, f32[256]{0} %get-tuple-element.616, f32[1,1,1024,256]{3,2,1,0} %fusion.82, f32[256]{0} %fusion.80, f32[1024]{0} %get-tuple-element.572, f32[1,1,256,1024]{3,2,1,0} %fusion.79, f32[1024]{0} %fusion.77, f32[256]{0} %get-tuple-element.612, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.644, f32[256]{0} %fusion.74, f32[256]{0} %get-tuple-element.610, f32[1,1,1024,256]{3,2,1,0} %fusion.73, f32[256]{0} %fusion.71, f32[1024]{0} %get-tuple-element.564, f32[1,1,256,1024]{3,2,1,0} %fusion.70, f32[1024]{0} %fusion.68, f32[256]{0} %get-tuple-element.606, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.651, f32[256]{0} %fusion.65, f32[256]{0} %get-tuple-element.608, f32[1,1,1024,256]{3,2,1,0} %fusion.64, f32[256]{0} %fusion.62, f32[1024]{0} %get-tuple-element.576, f32[1,1,256,1024]{3,2,1,0} %fusion.61, f32[1024]{0} %fusion.59, f32[256]{0} %get-tuple-element.620, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.647, f32[256]{0} %fusion.56, f32[256]{0} %get-tuple-element.622, f32[1,1,1024,256]{3,2,1,0} %fusion.55, f32[256]{0} %fusion.53, f32[1024]{0} %get-tuple-element.578, f32[1024]{0} %copy.485, f32[1,1,512,1024]{3,2,1,0} %fusion.52, f32[1,1,256,1024]{3,2,1,0} %fusion.51, f32[1024]{0} %get-tuple-element.664, f32[1024]{0} %get-tuple-element.665, f32[256]{0} %get-tuple-element.614, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.632, f32[256]{0} %fusion.43, f32[256]{0} %get-tuple-element.574, f32[1,1,512,256]{3,2,1,0} %fusion.42, f32[256]{0} %fusion.40, f32[512]{0} %get-tuple-element.560, f32[1,1,128,512]{3,2,1,0} %fusion.39, f32[512]{0} %fusion.37, f32[128]{0} %get-tuple-element.597, f32[3,3,128,128]{3,2,1,0} %fusion.36, f32[128]{0} %fusion.34, f32[128]{0} %get-tuple-element.595, f32[1,1,512,128]{3,2,1,0} %fusion.33, f32[128]{0} %fusion.31, f32[512]{0} %get-tuple-element.562, f32[1,1,128,512]{3,2,1,0} %fusion.30, f32[512]{0} %fusion.28, f32[128]{0} %get-tuple-element.591, f32[3,3,128,128]{3,2,1,0} %fusion.27, f32[128]{0} %fusion.25, f32[128]{0} %get-tuple-element.593, f32[1,1,512,128]{3,2,1,0} %fusion.24, f32[128]{0} %fusion.22, f32[512]{0} %get-tuple-element.553, f32[1,1,128,512]{3,2,1,0} %fusion.21, f32[512]{0} %fusion.19, f32[128]{0} %get-tuple-element.589, f32[3,3,128,128]{3,2,1,0} %fusion.18, f32[128]{0} %fusion.16, f32[128]{0} %get-tuple-element.587, f32[1,1,512,128]{3,2,1,0} %fusion.15, f32[128]{0} %fusion.13, f32[512]{0} %get-tuple-element.555, f32[512]{0} %copy.486, f32[1,1,256,512]{3,2,1,0} %fusion.12, f32[1,1,128,512]{3,2,1,0} %fusion.11, f32[512]{0} %get-tuple-element.666, f32[512]{0} %get-tuple-element.667, f32[128]{0} %get-tuple-element.585, f32[3,3,128,128]{3,2,1,0} %fusion.5, f32[128]{0} %fusion.3, f32[128]{0} %get-tuple-element.557, f32[1,1,256,128]{3,2,1,0} %fusion.2, f32[128]{0} %fusion)
}

