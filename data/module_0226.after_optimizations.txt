HloModule cluster_16__XlaCompiledKernel_true__XlaNumConstantArgs_723__XlaNumResourceArgs_0_.4191

%add_float_.1286 (x.1287: f32[], y.1288: f32[]) -> f32[] {
  %x.1287 = f32[] parameter(0)
  %y.1288 = f32[] parameter(1)
  ROOT %add.1289 = f32[] add(f32[] %x.1287, f32[] %y.1288)
}

%ge_F32.1562 (lhs.1563: f32[], rhs.1564: f32[]) -> pred[] {
  %lhs.1563 = f32[] parameter(0)
  %rhs.1564 = f32[] parameter(1)
  ROOT %compare.1565 = pred[] compare(f32[] %lhs.1563, f32[] %rhs.1564), direction=GE
}

%add_F32.1566 (lhs.1567: f32[], rhs.1568: f32[]) -> f32[] {
  %lhs.1567 = f32[] parameter(0)
  %rhs.1568 = f32[] parameter(1)
  ROOT %add.1569 = f32[] add(f32[] %lhs.1567, f32[] %rhs.1568)
}

%fused_computation (param_0.2: f32[1,128,1,1], param_1.2: f32[1,128,1,1], param_2.3: f32[128], param_3.3: f32[128]) -> f32[128] {
  %param_3.3 = f32[128]{0} parameter(3)
  %bitcast.231 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.3), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.3 = f32[128]{0} parameter(2)
  %negate.52 = f32[128]{0} negate(f32[128]{0} %param_2.3), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.230 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.52), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.2 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.14 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.230, f32[1,128,1,1]{3,2,1,0} %param_1.2), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.11 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.231, f32[1,128,1,1]{3,2,1,0} %multiply.14), metadata={op_type="AddN" op_name="tower0/gradients/AddN_205"}
  %param_0.2 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.13 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.11, f32[1,128,1,1]{3,2,1,0} %param_0.2), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.229 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.13), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3048 (x.3049: f32[], y.3050: f32[]) -> f32[] {
  %x.3049 = f32[] parameter(0)
  %y.3050 = f32[] parameter(1)
  ROOT %add.3051 = f32[] add(f32[] %x.3049, f32[] %y.3050)
}

%tower0_gradients_tower0_group1_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3058 (x.3059: f32[], y.3060: f32[]) -> f32[] {
  %x.3059 = f32[] parameter(0)
  %y.3060 = f32[] parameter(1)
  ROOT %add.3061 = f32[] add(f32[] %x.3059, f32[] %y.3060)
}

%fused_computation.1 (param_0.674: f32[1,128,200,200], param_1.885: f32[1,128,201,201], param_2.780: f32[1,128,200,200]) -> (f32[128], f32[128], f32[1,128,200,200]) {
  %param_2.780 = f32[1,128,200,200]{3,2,1,0} parameter(2)
  %constant_140 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.109.clone.1 = f32[1,128,200,200]{3,2,1,0} broadcast(f32[] %constant_140), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv1/Relu_grad/ReluGrad"}
  %compare.0.clone.1 = pred[1,128,200,200]{3,2,1,0} compare(f32[1,128,200,200]{3,2,1,0} %param_2.780, f32[1,128,200,200]{3,2,1,0} %broadcast.109.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv1/Relu_grad/ReluGrad"}
  %param_1.885 = f32[1,128,201,201]{3,2,1,0} parameter(1)
  %slice.0.clone.1 = f32[1,128,200,200]{3,2,1,0} slice(f32[1,128,201,201]{3,2,1,0} %param_1.885), slice={[0:1], [0:128], [1:201], [1:201]}, metadata={op_type="Slice" op_name="tower0/gradients/tower0/group1/block0/Pad_grad/Slice_1"}
  %select.0.clone.1 = f32[1,128,200,200]{3,2,1,0} select(pred[1,128,200,200]{3,2,1,0} %compare.0.clone.1, f32[1,128,200,200]{3,2,1,0} %slice.0.clone.1, f32[1,128,200,200]{3,2,1,0} %broadcast.109.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv1/Relu_grad/ReluGrad"}
  %param_0.674 = f32[1,128,200,200]{3,2,1,0} parameter(0)
  %multiply.15 = f32[1,128,200,200]{3,2,1,0} multiply(f32[1,128,200,200]{3,2,1,0} %select.0.clone.1, f32[1,128,200,200]{3,2,1,0} %param_0.674), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.13 = f32[128]{0} reduce(f32[1,128,200,200]{3,2,1,0} %multiply.15, f32[] %constant_140), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3048, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.70 = f32[128]{0} reduce(f32[1,128,200,200]{3,2,1,0} %select.0.clone.1, f32[] %constant_140), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3058, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.237 = (f32[128]{0}, f32[128]{0}, f32[1,128,200,200]{3,2,1,0}) tuple(f32[128]{0} %reduce.13, f32[128]{0} %reduce.70, f32[1,128,200,200]{3,2,1,0} %select.0.clone.1)
}

%fused_computation.2 (param_0.7: f32[1,1,256,128], param_1.9: f32[1,1,256,128]) -> f32[1,1,256,128] {
  %param_0.7 = f32[1,1,256,128]{1,0,2,3} parameter(0)
  %param_1.9 = f32[1,1,256,128]{3,2,1,0} parameter(1)
  %copy.365 = f32[1,1,256,128]{1,0,2,3} copy(f32[1,1,256,128]{3,2,1,0} %param_1.9), metadata={op_name="XLA_Args"}
  %constant_141 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.98 = f32[1,1,256,128]{1,0,2,3} broadcast(f32[] %constant_141), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_grad/Mul"}
  %multiply.16 = f32[1,1,256,128]{1,0,2,3} multiply(f32[1,1,256,128]{1,0,2,3} %copy.365, f32[1,1,256,128]{1,0,2,3} %broadcast.98), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_grad/Mul_1"}
  %add.12 = f32[1,1,256,128]{1,0,2,3} add(f32[1,1,256,128]{1,0,2,3} %param_0.7, f32[1,1,256,128]{1,0,2,3} %multiply.16), metadata={op_type="AddN" op_name="tower0/gradients/AddN_204"}
  ROOT %copy.364 = f32[1,1,256,128]{3,2,1,0} copy(f32[1,1,256,128]{1,0,2,3} %add.12), metadata={op_name="XLA_Retvals"}
}

%fused_computation.3 (param_0.10: f32[1,128,1,1], param_1.12: f32[1,128,1,1], param_2.11: f32[128], param_3.7: f32[128]) -> f32[128] {
  %param_3.7 = f32[128]{0} parameter(3)
  %bitcast.234 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.7), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.11 = f32[128]{0} parameter(2)
  %negate.53 = f32[128]{0} negate(f32[128]{0} %param_2.11), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.233 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.53), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.12 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.18 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.233, f32[1,128,1,1]{3,2,1,0} %param_1.12), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.13 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.234, f32[1,128,1,1]{3,2,1,0} %multiply.18), metadata={op_type="AddN" op_name="tower0/gradients/AddN_203"}
  %param_0.10 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.17 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.13, f32[1,128,1,1]{3,2,1,0} %param_0.10), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.232 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.17), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3075 (x.3076: f32[], y.3077: f32[]) -> f32[] {
  %x.3076 = f32[] parameter(0)
  %y.3077 = f32[] parameter(1)
  ROOT %add.3078 = f32[] add(f32[] %x.3076, f32[] %y.3077)
}

%tower0_gradients_tower0_group1_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3085 (x.3086: f32[], y.3087: f32[]) -> f32[] {
  %x.3086 = f32[] parameter(0)
  %y.3087 = f32[] parameter(1)
  ROOT %add.3088 = f32[] add(f32[] %x.3086, f32[] %y.3087)
}

%fused_computation.4 (param_0.673: f32[1,128,100,100], param_1.886: f32[1,128,100,100], param_2.781: f32[1,128,100,100]) -> (f32[128], f32[128], f32[1,128,100,100]) {
  %param_2.781 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %constant_142 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.245.clone.1 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_142), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.1.clone.1 = pred[1,128,100,100]{3,2,1,0} compare(f32[1,128,100,100]{3,2,1,0} %param_2.781, f32[1,128,100,100]{3,2,1,0} %broadcast.245.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv2/Relu_grad/ReluGrad"}
  %param_1.886 = f32[1,128,100,100]{3,2,1,0} parameter(1)
  %select.1.clone.1 = f32[1,128,100,100]{3,2,1,0} select(pred[1,128,100,100]{3,2,1,0} %compare.1.clone.1, f32[1,128,100,100]{3,2,1,0} %param_1.886, f32[1,128,100,100]{3,2,1,0} %broadcast.245.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/conv2/Relu_grad/ReluGrad"}
  %param_0.673 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %multiply.19 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %select.1.clone.1, f32[1,128,100,100]{3,2,1,0} %param_0.673), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.14 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %multiply.19, f32[] %constant_142), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3075, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.82 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %select.1.clone.1, f32[] %constant_142), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3085, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.236 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) tuple(f32[128]{0} %reduce.14, f32[128]{0} %reduce.82, f32[1,128,100,100]{3,2,1,0} %select.1.clone.1)
}

%fused_computation.5 (param_0.15: f32[3,3,128,128], param_1.693: f32[3,3,128,128]) -> f32[3,3,128,128] {
  %param_0.15 = f32[3,3,128,128]{1,0,2,3} parameter(0)
  %param_1.693 = f32[3,3,128,128]{3,2,1,0} parameter(1)
  %copy.453 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %param_1.693), metadata={op_name="XLA_Args"}
  %constant_219 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.212 = f32[3,3,128,128]{1,0,2,3} broadcast(f32[] %constant_219), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul"}
  %multiply.20 = f32[3,3,128,128]{1,0,2,3} multiply(f32[3,3,128,128]{1,0,2,3} %copy.453, f32[3,3,128,128]{1,0,2,3} %broadcast.212), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_1_grad/Mul_1"}
  %add.14 = f32[3,3,128,128]{1,0,2,3} add(f32[3,3,128,128]{1,0,2,3} %param_0.15, f32[3,3,128,128]{1,0,2,3} %multiply.20), metadata={op_type="AddN" op_name="tower0/gradients/AddN_202"}
  ROOT %copy.366 = f32[3,3,128,128]{3,2,1,0} copy(f32[3,3,128,128]{1,0,2,3} %add.14), metadata={op_name="XLA_Retvals"}
}

%fused_computation.8 (param_0.24: f32[1,512,1,1], param_1.807: f32[1,512,1,1], param_2.599: f32[512], param_3.334: f32[512], param_4.124: f32[1,512,1,1], param_5.125: f32[1,512,1,1], param_6.109: f32[512]) -> (f32[512], f32[512]) {
  %param_2.599 = f32[512]{0} parameter(2)
  %bitcast.238 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_2.599), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_3.334 = f32[512]{0} parameter(3)
  %negate.117 = f32[512]{0} negate(f32[512]{0} %param_3.334), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/sub_grad/Neg"}
  %bitcast.427 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.117), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/sub_grad/Neg"}
  %param_1.807 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.25 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.427, f32[1,512,1,1]{3,2,1,0} %param_1.807), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.16 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.238, f32[1,512,1,1]{3,2,1,0} %multiply.25), metadata={op_type="AddN" op_name="tower0/gradients/AddN_201"}
  %param_0.24 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.24 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.16, f32[1,512,1,1]{3,2,1,0} %param_0.24), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.237 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.24), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/Reshape_grad/Reshape"}
  %param_6.109 = f32[512]{0} parameter(6)
  %bitcast.236.clone.1 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_6.109), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_5.125 = f32[1,512,1,1]{3,2,1,0} parameter(5)
  %multiply.22.clone.1 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.427, f32[1,512,1,1]{3,2,1,0} %param_5.125), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.15.clone.1 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.236.clone.1, f32[1,512,1,1]{3,2,1,0} %multiply.22.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_199"}
  %param_4.124 = f32[1,512,1,1]{3,2,1,0} parameter(4)
  %multiply.21.clone.1 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.15.clone.1, f32[1,512,1,1]{3,2,1,0} %param_4.124), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.235.clone.1 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.21.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/Reshape_grad/Reshape"}
  ROOT %tuple.186 = (f32[512]{0}, f32[512]{0}) tuple(f32[512]{0} %bitcast.237, f32[512]{0} %bitcast.235.clone.1)
}

%tower0_gradients_tower0_group1_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3155 (x.3156: f32[], y.3157: f32[]) -> f32[] {
  %x.3156 = f32[] parameter(0)
  %y.3157 = f32[] parameter(1)
  ROOT %add.3158 = f32[] add(f32[] %x.3156, f32[] %y.3157)
}

%tower0_gradients_tower0_group1_block0_convshortcut_bn_batchnorm_add_1_grad_Sum_1-reduction.3165 (x.3166: f32[], y.3167: f32[]) -> f32[] {
  %x.3166 = f32[] parameter(0)
  %y.3167 = f32[] parameter(1)
  ROOT %add.3168 = f32[] add(f32[] %x.3166, f32[] %y.3167)
}

%tower0_gradients_tower0_group1_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3102 (x.3103: f32[], y.3104: f32[]) -> f32[] {
  %x.3103 = f32[] parameter(0)
  %y.3104 = f32[] parameter(1)
  ROOT %add.3105 = f32[] add(f32[] %x.3103, f32[] %y.3104)
}

%fused_computation.10 (param_0.672: f32[1,512,100,100], param_1.887: f32[1,512,100,100], param_2.782: f32[1,512,100,100], param_3.522: f32[1,512,100,100], param_4.275: f32[1,512,100,100]) -> (f32[512], f32[512], f32[512], f32[1,512,100,100]) {
  %param_4.275 = f32[1,512,100,100]{3,2,1,0} parameter(4)
  %constant_144 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.246.clone.1 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[] %constant_144), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %compare.2.clone.1 = pred[1,512,100,100]{3,2,1,0} compare(f32[1,512,100,100]{3,2,1,0} %param_4.275, f32[1,512,100,100]{3,2,1,0} %broadcast.246.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/output_grad/ReluGrad"}
  %param_2.782 = f32[1,512,100,100]{3,2,1,0} parameter(2)
  %param_3.522 = f32[1,512,100,100]{3,2,1,0} parameter(3)
  %add.130.clone.1 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %param_2.782, f32[1,512,100,100]{3,2,1,0} %param_3.522), metadata={op_type="AddN" op_name="tower0/gradients/AddN_195"}
  %select.2.clone.1 = f32[1,512,100,100]{3,2,1,0} select(pred[1,512,100,100]{3,2,1,0} %compare.2.clone.1, f32[1,512,100,100]{3,2,1,0} %add.130.clone.1, f32[1,512,100,100]{3,2,1,0} %broadcast.246.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block0/output_grad/ReluGrad"}
  %param_0.672 = f32[1,512,100,100]{3,2,1,0} parameter(0)
  %multiply.26 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %select.2.clone.1, f32[1,512,100,100]{3,2,1,0} %param_0.672), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.16 = f32[512]{0} reduce(f32[1,512,100,100]{3,2,1,0} %multiply.26, f32[] %constant_144), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3155, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.69 = f32[512]{0} reduce(f32[1,512,100,100]{3,2,1,0} %select.2.clone.1, f32[] %constant_144), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_convshortcut_bn_batchnorm_add_1_grad_Sum_1-reduction.3165, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/add_1_grad/Sum_1"}
  %param_1.887 = f32[1,512,100,100]{3,2,1,0} parameter(1)
  %multiply.23.clone.1 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %select.2.clone.1, f32[1,512,100,100]{3,2,1,0} %param_1.887), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.15.clone.1 = f32[512]{0} reduce(f32[1,512,100,100]{3,2,1,0} %multiply.23.clone.1, f32[] %constant_144), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3102, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  ROOT %tuple.235 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) tuple(f32[512]{0} %reduce.16, f32[512]{0} %reduce.69, f32[512]{0} %reduce.15.clone.1, f32[1,512,100,100]{3,2,1,0} %select.2.clone.1)
}

%fused_computation.11 (param_0.31: f32[1,1,128,512], param_1.694: f32[1,1,128,512]) -> f32[1,1,128,512] {
  %param_0.31 = f32[1,1,128,512]{1,0,2,3} parameter(0)
  %param_1.694 = f32[1,1,128,512]{3,2,1,0} parameter(1)
  %copy.368 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %param_1.694), metadata={op_name="XLA_Args"}
  %constant_220 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.213 = f32[1,1,128,512]{1,0,2,3} broadcast(f32[] %constant_220), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul"}
  %multiply.27 = f32[1,1,128,512]{1,0,2,3} multiply(f32[1,1,128,512]{1,0,2,3} %copy.368, f32[1,1,128,512]{1,0,2,3} %broadcast.213), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul_1"}
  %add.17 = f32[1,1,128,512]{1,0,2,3} add(f32[1,1,128,512]{1,0,2,3} %param_0.31, f32[1,1,128,512]{1,0,2,3} %multiply.27), metadata={op_type="AddN" op_name="tower0/gradients/AddN_198"}
  ROOT %copy.367 = f32[1,1,128,512]{3,2,1,0} copy(f32[1,1,128,512]{1,0,2,3} %add.17), metadata={op_name="XLA_Retvals"}
}

%fused_computation.12 (param_0.33: f32[1,1,256,512], param_1.36: f32[1,1,256,512]) -> f32[1,1,256,512] {
  %param_0.33 = f32[1,1,256,512]{1,0,2,3} parameter(0)
  %param_1.36 = f32[1,1,256,512]{3,2,1,0} parameter(1)
  %copy.370 = f32[1,1,256,512]{1,0,2,3} copy(f32[1,1,256,512]{3,2,1,0} %param_1.36), metadata={op_name="XLA_Args"}
  %constant_145 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.99 = f32[1,1,256,512]{1,0,2,3} broadcast(f32[] %constant_145), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_3_grad/Mul"}
  %multiply.28 = f32[1,1,256,512]{1,0,2,3} multiply(f32[1,1,256,512]{1,0,2,3} %copy.370, f32[1,1,256,512]{1,0,2,3} %broadcast.99), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_3_grad/Mul_1"}
  %add.18 = f32[1,1,256,512]{1,0,2,3} add(f32[1,1,256,512]{1,0,2,3} %param_0.33, f32[1,1,256,512]{1,0,2,3} %multiply.28), metadata={op_type="AddN" op_name="tower0/gradients/AddN_200"}
  ROOT %copy.369 = f32[1,1,256,512]{3,2,1,0} copy(f32[1,1,256,512]{1,0,2,3} %add.18), metadata={op_name="XLA_Retvals"}
}

%fused_computation.13 (param_0.36: f32[1,128,1,1], param_1.39: f32[1,128,1,1], param_2.30: f32[128], param_3.15: f32[128]) -> f32[128] {
  %param_3.15 = f32[128]{0} parameter(3)
  %bitcast.242 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.15), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.30 = f32[128]{0} parameter(2)
  %negate.55 = f32[128]{0} negate(f32[128]{0} %param_2.30), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.241 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.55), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.39 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.30 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.241, f32[1,128,1,1]{3,2,1,0} %param_1.39), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.19 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.242, f32[1,128,1,1]{3,2,1,0} %multiply.30), metadata={op_type="AddN" op_name="tower0/gradients/AddN_197"}
  %param_0.36 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.29 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.19, f32[1,128,1,1]{3,2,1,0} %param_0.36), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.240 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.29), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3182 (x.3183: f32[], y.3184: f32[]) -> f32[] {
  %x.3183 = f32[] parameter(0)
  %y.3184 = f32[] parameter(1)
  ROOT %add.3185 = f32[] add(f32[] %x.3183, f32[] %y.3184)
}

%tower0_gradients_tower0_group1_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3192 (x.3193: f32[], y.3194: f32[]) -> f32[] {
  %x.3193 = f32[] parameter(0)
  %y.3194 = f32[] parameter(1)
  ROOT %add.3195 = f32[] add(f32[] %x.3193, f32[] %y.3194)
}

%fused_computation.14 (param_0.671: f32[1,128,100,100], param_1.888: f32[1,128,100,100], param_2.783: f32[1,128,100,100]) -> (f32[128], f32[128], f32[1,128,100,100]) {
  %param_2.783 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %constant_146 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.247.clone.1 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_146), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.3.clone.1 = pred[1,128,100,100]{3,2,1,0} compare(f32[1,128,100,100]{3,2,1,0} %param_2.783, f32[1,128,100,100]{3,2,1,0} %broadcast.247.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/conv1/Relu_grad/ReluGrad"}
  %param_1.888 = f32[1,128,100,100]{3,2,1,0} parameter(1)
  %select.3.clone.1 = f32[1,128,100,100]{3,2,1,0} select(pred[1,128,100,100]{3,2,1,0} %compare.3.clone.1, f32[1,128,100,100]{3,2,1,0} %param_1.888, f32[1,128,100,100]{3,2,1,0} %broadcast.247.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/conv1/Relu_grad/ReluGrad"}
  %param_0.671 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %multiply.31 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %select.3.clone.1, f32[1,128,100,100]{3,2,1,0} %param_0.671), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.17 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %multiply.31, f32[] %constant_146), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3182, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.83 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %select.3.clone.1, f32[] %constant_146), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3192, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.234 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) tuple(f32[128]{0} %reduce.17, f32[128]{0} %reduce.83, f32[1,128,100,100]{3,2,1,0} %select.3.clone.1)
}

%fused_computation.15 (param_0.41: f32[1,1,512,128], param_1.695: f32[1,1,512,128]) -> f32[1,1,512,128] {
  %param_0.41 = f32[1,1,512,128]{1,0,2,3} parameter(0)
  %param_1.695 = f32[1,1,512,128]{3,2,1,0} parameter(1)
  %copy.372 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %param_1.695), metadata={op_name="XLA_Args"}
  %constant_221 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.214 = f32[1,1,512,128]{1,0,2,3} broadcast(f32[] %constant_221), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %multiply.32 = f32[1,1,512,128]{1,0,2,3} multiply(f32[1,1,512,128]{1,0,2,3} %copy.372, f32[1,1,512,128]{1,0,2,3} %broadcast.214), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_4_grad/Mul_1"}
  %add.20 = f32[1,1,512,128]{1,0,2,3} add(f32[1,1,512,128]{1,0,2,3} %param_0.41, f32[1,1,512,128]{1,0,2,3} %multiply.32), metadata={op_type="AddN" op_name="tower0/gradients/AddN_196"}
  ROOT %copy.371 = f32[1,1,512,128]{3,2,1,0} copy(f32[1,1,512,128]{1,0,2,3} %add.20), metadata={op_name="XLA_Retvals"}
}

%fused_computation.16 (param_0.44: f32[1,128,1,1], param_1.48: f32[1,128,1,1], param_2.37: f32[128], param_3.19: f32[128]) -> f32[128] {
  %param_3.19 = f32[128]{0} parameter(3)
  %bitcast.245 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.19), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.37 = f32[128]{0} parameter(2)
  %negate.56 = f32[128]{0} negate(f32[128]{0} %param_2.37), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.244 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.56), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.48 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.34 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.244, f32[1,128,1,1]{3,2,1,0} %param_1.48), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.21 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.245, f32[1,128,1,1]{3,2,1,0} %multiply.34), metadata={op_type="AddN" op_name="tower0/gradients/AddN_194"}
  %param_0.44 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.33 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.21, f32[1,128,1,1]{3,2,1,0} %param_0.44), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.243 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.33), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3209 (x.3210: f32[], y.3211: f32[]) -> f32[] {
  %x.3210 = f32[] parameter(0)
  %y.3211 = f32[] parameter(1)
  ROOT %add.3212 = f32[] add(f32[] %x.3210, f32[] %y.3211)
}

%tower0_gradients_tower0_group1_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3219 (x.3220: f32[], y.3221: f32[]) -> f32[] {
  %x.3220 = f32[] parameter(0)
  %y.3221 = f32[] parameter(1)
  ROOT %add.3222 = f32[] add(f32[] %x.3220, f32[] %y.3221)
}

%fused_computation.17 (param_0.670: f32[1,128,100,100], param_1.889: f32[1,128,100,100], param_2.784: f32[1,128,100,100]) -> (f32[128], f32[128], f32[1,128,100,100]) {
  %param_2.784 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %constant_147 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.248.clone.1 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_147), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.4.clone.1 = pred[1,128,100,100]{3,2,1,0} compare(f32[1,128,100,100]{3,2,1,0} %param_2.784, f32[1,128,100,100]{3,2,1,0} %broadcast.248.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/conv2/Relu_grad/ReluGrad"}
  %param_1.889 = f32[1,128,100,100]{3,2,1,0} parameter(1)
  %select.4.clone.1 = f32[1,128,100,100]{3,2,1,0} select(pred[1,128,100,100]{3,2,1,0} %compare.4.clone.1, f32[1,128,100,100]{3,2,1,0} %param_1.889, f32[1,128,100,100]{3,2,1,0} %broadcast.248.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/conv2/Relu_grad/ReluGrad"}
  %param_0.670 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %multiply.35 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %select.4.clone.1, f32[1,128,100,100]{3,2,1,0} %param_0.670), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.18 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %multiply.35, f32[] %constant_147), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3209, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.84 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %select.4.clone.1, f32[] %constant_147), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3219, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.233 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) tuple(f32[128]{0} %reduce.18, f32[128]{0} %reduce.84, f32[1,128,100,100]{3,2,1,0} %select.4.clone.1)
}

%fused_computation.18 (param_0.49: f32[3,3,128,128], param_1.696: f32[3,3,128,128]) -> f32[3,3,128,128] {
  %param_0.49 = f32[3,3,128,128]{1,0,2,3} parameter(0)
  %param_1.696 = f32[3,3,128,128]{3,2,1,0} parameter(1)
  %copy.454 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %param_1.696), metadata={op_name="XLA_Args"}
  %constant_222 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.215 = f32[3,3,128,128]{1,0,2,3} broadcast(f32[] %constant_222), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul"}
  %multiply.36 = f32[3,3,128,128]{1,0,2,3} multiply(f32[3,3,128,128]{1,0,2,3} %copy.454, f32[3,3,128,128]{1,0,2,3} %broadcast.215), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_5_grad/Mul_1"}
  %add.22 = f32[3,3,128,128]{1,0,2,3} add(f32[3,3,128,128]{1,0,2,3} %param_0.49, f32[3,3,128,128]{1,0,2,3} %multiply.36), metadata={op_type="AddN" op_name="tower0/gradients/AddN_193"}
  ROOT %copy.373 = f32[3,3,128,128]{3,2,1,0} copy(f32[3,3,128,128]{1,0,2,3} %add.22), metadata={op_name="XLA_Retvals"}
}

%fused_computation.19 (param_0.52: f32[1,512,1,1], param_1.56: f32[1,512,1,1], param_2.43: f32[512], param_3.23: f32[512]) -> f32[512] {
  %param_3.23 = f32[512]{0} parameter(3)
  %bitcast.248 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.23), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.43 = f32[512]{0} parameter(2)
  %negate.57 = f32[512]{0} negate(f32[512]{0} %param_2.43), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.247 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.57), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.56 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.38 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.247, f32[1,512,1,1]{3,2,1,0} %param_1.56), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.23 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.248, f32[1,512,1,1]{3,2,1,0} %multiply.38), metadata={op_type="AddN" op_name="tower0/gradients/AddN_192"}
  %param_0.52 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.37 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.23, f32[1,512,1,1]{3,2,1,0} %param_0.52), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.246 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.37), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3236 (x.3237: f32[], y.3238: f32[]) -> f32[] {
  %x.3237 = f32[] parameter(0)
  %y.3238 = f32[] parameter(1)
  ROOT %add.3239 = f32[] add(f32[] %x.3237, f32[] %y.3238)
}

%tower0_gradients_tower0_group1_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3246 (x.3247: f32[], y.3248: f32[]) -> f32[] {
  %x.3247 = f32[] parameter(0)
  %y.3248 = f32[] parameter(1)
  ROOT %add.3249 = f32[] add(f32[] %x.3247, f32[] %y.3248)
}

%fused_computation.20 (param_0.669: f32[1,512,100,100], param_1.890: f32[1,512,100,100], param_2.785: f32[1,512,100,100], param_3.523: f32[1,512,100,100]) -> (f32[512], f32[512], f32[1,512,100,100]) {
  %param_3.523 = f32[1,512,100,100]{3,2,1,0} parameter(3)
  %constant_148 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.249.clone.1 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[] %constant_148), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %compare.5.clone.1 = pred[1,512,100,100]{3,2,1,0} compare(f32[1,512,100,100]{3,2,1,0} %param_3.523, f32[1,512,100,100]{3,2,1,0} %broadcast.249.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/output_grad/ReluGrad"}
  %param_1.890 = f32[1,512,100,100]{3,2,1,0} parameter(1)
  %param_2.785 = f32[1,512,100,100]{3,2,1,0} parameter(2)
  %add.131.clone.1 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %param_1.890, f32[1,512,100,100]{3,2,1,0} %param_2.785), metadata={op_type="AddN" op_name="tower0/gradients/AddN_188"}
  %select.5.clone.1 = f32[1,512,100,100]{3,2,1,0} select(pred[1,512,100,100]{3,2,1,0} %compare.5.clone.1, f32[1,512,100,100]{3,2,1,0} %add.131.clone.1, f32[1,512,100,100]{3,2,1,0} %broadcast.249.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block1/output_grad/ReluGrad"}
  %param_0.669 = f32[1,512,100,100]{3,2,1,0} parameter(0)
  %multiply.39 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %select.5.clone.1, f32[1,512,100,100]{3,2,1,0} %param_0.669), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.19 = f32[512]{0} reduce(f32[1,512,100,100]{3,2,1,0} %multiply.39, f32[] %constant_148), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3236, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.68 = f32[512]{0} reduce(f32[1,512,100,100]{3,2,1,0} %select.5.clone.1, f32[] %constant_148), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3246, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.232 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) tuple(f32[512]{0} %reduce.19, f32[512]{0} %reduce.68, f32[1,512,100,100]{3,2,1,0} %select.5.clone.1)
}

%fused_computation.21 (param_0.57: f32[1,1,128,512], param_1.697: f32[1,1,128,512]) -> f32[1,1,128,512] {
  %param_0.57 = f32[1,1,128,512]{1,0,2,3} parameter(0)
  %param_1.697 = f32[1,1,128,512]{3,2,1,0} parameter(1)
  %copy.375 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %param_1.697), metadata={op_name="XLA_Args"}
  %constant_223 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.216 = f32[1,1,128,512]{1,0,2,3} broadcast(f32[] %constant_223), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul"}
  %multiply.40 = f32[1,1,128,512]{1,0,2,3} multiply(f32[1,1,128,512]{1,0,2,3} %copy.375, f32[1,1,128,512]{1,0,2,3} %broadcast.216), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_6_grad/Mul_1"}
  %add.24 = f32[1,1,128,512]{1,0,2,3} add(f32[1,1,128,512]{1,0,2,3} %param_0.57, f32[1,1,128,512]{1,0,2,3} %multiply.40), metadata={op_type="AddN" op_name="tower0/gradients/AddN_191"}
  ROOT %copy.374 = f32[1,1,128,512]{3,2,1,0} copy(f32[1,1,128,512]{1,0,2,3} %add.24), metadata={op_name="XLA_Retvals"}
}

%fused_computation.22 (param_0.60: f32[1,128,1,1], param_1.65: f32[1,128,1,1], param_2.50: f32[128], param_3.27: f32[128]) -> f32[128] {
  %param_3.27 = f32[128]{0} parameter(3)
  %bitcast.251 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.27), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.50 = f32[128]{0} parameter(2)
  %negate.58 = f32[128]{0} negate(f32[128]{0} %param_2.50), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.250 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.58), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.65 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.42 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.250, f32[1,128,1,1]{3,2,1,0} %param_1.65), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.25 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.251, f32[1,128,1,1]{3,2,1,0} %multiply.42), metadata={op_type="AddN" op_name="tower0/gradients/AddN_190"}
  %param_0.60 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.41 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.25, f32[1,128,1,1]{3,2,1,0} %param_0.60), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.249 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.41), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3263 (x.3264: f32[], y.3265: f32[]) -> f32[] {
  %x.3264 = f32[] parameter(0)
  %y.3265 = f32[] parameter(1)
  ROOT %add.3266 = f32[] add(f32[] %x.3264, f32[] %y.3265)
}

%tower0_gradients_tower0_group1_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3273 (x.3274: f32[], y.3275: f32[]) -> f32[] {
  %x.3274 = f32[] parameter(0)
  %y.3275 = f32[] parameter(1)
  ROOT %add.3276 = f32[] add(f32[] %x.3274, f32[] %y.3275)
}

%fused_computation.23 (param_0.668: f32[1,128,100,100], param_1.891: f32[1,128,100,100], param_2.786: f32[1,128,100,100]) -> (f32[128], f32[128], f32[1,128,100,100]) {
  %param_2.786 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %constant_149 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.250.clone.1 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_149), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.6.clone.1 = pred[1,128,100,100]{3,2,1,0} compare(f32[1,128,100,100]{3,2,1,0} %param_2.786, f32[1,128,100,100]{3,2,1,0} %broadcast.250.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/conv1/Relu_grad/ReluGrad"}
  %param_1.891 = f32[1,128,100,100]{3,2,1,0} parameter(1)
  %select.6.clone.1 = f32[1,128,100,100]{3,2,1,0} select(pred[1,128,100,100]{3,2,1,0} %compare.6.clone.1, f32[1,128,100,100]{3,2,1,0} %param_1.891, f32[1,128,100,100]{3,2,1,0} %broadcast.250.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/conv1/Relu_grad/ReluGrad"}
  %param_0.668 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %multiply.43 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %select.6.clone.1, f32[1,128,100,100]{3,2,1,0} %param_0.668), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.20 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %multiply.43, f32[] %constant_149), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3263, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.86 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %select.6.clone.1, f32[] %constant_149), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3273, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.231 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) tuple(f32[128]{0} %reduce.20, f32[128]{0} %reduce.86, f32[1,128,100,100]{3,2,1,0} %select.6.clone.1)
}

%fused_computation.24 (param_0.65: f32[1,1,512,128], param_1.698: f32[1,1,512,128]) -> f32[1,1,512,128] {
  %param_0.65 = f32[1,1,512,128]{1,0,2,3} parameter(0)
  %param_1.698 = f32[1,1,512,128]{3,2,1,0} parameter(1)
  %copy.377 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %param_1.698), metadata={op_name="XLA_Args"}
  %constant_224 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.217 = f32[1,1,512,128]{1,0,2,3} broadcast(f32[] %constant_224), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %multiply.44 = f32[1,1,512,128]{1,0,2,3} multiply(f32[1,1,512,128]{1,0,2,3} %copy.377, f32[1,1,512,128]{1,0,2,3} %broadcast.217), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_7_grad/Mul_1"}
  %add.26 = f32[1,1,512,128]{1,0,2,3} add(f32[1,1,512,128]{1,0,2,3} %param_0.65, f32[1,1,512,128]{1,0,2,3} %multiply.44), metadata={op_type="AddN" op_name="tower0/gradients/AddN_189"}
  ROOT %copy.376 = f32[1,1,512,128]{3,2,1,0} copy(f32[1,1,512,128]{1,0,2,3} %add.26), metadata={op_name="XLA_Retvals"}
}

%fused_computation.25 (param_0.68: f32[1,128,1,1], param_1.74: f32[1,128,1,1], param_2.57: f32[128], param_3.31: f32[128]) -> f32[128] {
  %param_3.31 = f32[128]{0} parameter(3)
  %bitcast.254 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.31), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.57 = f32[128]{0} parameter(2)
  %negate.59 = f32[128]{0} negate(f32[128]{0} %param_2.57), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.253 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.59), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.74 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.46 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.253, f32[1,128,1,1]{3,2,1,0} %param_1.74), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.27 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.254, f32[1,128,1,1]{3,2,1,0} %multiply.46), metadata={op_type="AddN" op_name="tower0/gradients/AddN_187"}
  %param_0.68 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.45 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.27, f32[1,128,1,1]{3,2,1,0} %param_0.68), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.252 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.45), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3290 (x.3291: f32[], y.3292: f32[]) -> f32[] {
  %x.3291 = f32[] parameter(0)
  %y.3292 = f32[] parameter(1)
  ROOT %add.3293 = f32[] add(f32[] %x.3291, f32[] %y.3292)
}

%tower0_gradients_tower0_group1_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3300 (x.3301: f32[], y.3302: f32[]) -> f32[] {
  %x.3301 = f32[] parameter(0)
  %y.3302 = f32[] parameter(1)
  ROOT %add.3303 = f32[] add(f32[] %x.3301, f32[] %y.3302)
}

%fused_computation.26 (param_0.667: f32[1,128,100,100], param_1.892: f32[1,128,100,100], param_2.787: f32[1,128,100,100]) -> (f32[128], f32[128], f32[1,128,100,100]) {
  %param_2.787 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %constant_150 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.251.clone.1 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_150), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.7.clone.1 = pred[1,128,100,100]{3,2,1,0} compare(f32[1,128,100,100]{3,2,1,0} %param_2.787, f32[1,128,100,100]{3,2,1,0} %broadcast.251.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/conv2/Relu_grad/ReluGrad"}
  %param_1.892 = f32[1,128,100,100]{3,2,1,0} parameter(1)
  %select.7.clone.1 = f32[1,128,100,100]{3,2,1,0} select(pred[1,128,100,100]{3,2,1,0} %compare.7.clone.1, f32[1,128,100,100]{3,2,1,0} %param_1.892, f32[1,128,100,100]{3,2,1,0} %broadcast.251.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/conv2/Relu_grad/ReluGrad"}
  %param_0.667 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %multiply.47 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %select.7.clone.1, f32[1,128,100,100]{3,2,1,0} %param_0.667), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.21 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %multiply.47, f32[] %constant_150), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3290, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.85 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %select.7.clone.1, f32[] %constant_150), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3300, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.230 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) tuple(f32[128]{0} %reduce.21, f32[128]{0} %reduce.85, f32[1,128,100,100]{3,2,1,0} %select.7.clone.1)
}

%fused_computation.27 (param_0.73: f32[3,3,128,128], param_1.699: f32[3,3,128,128]) -> f32[3,3,128,128] {
  %param_0.73 = f32[3,3,128,128]{1,0,2,3} parameter(0)
  %param_1.699 = f32[3,3,128,128]{3,2,1,0} parameter(1)
  %copy.455 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %param_1.699), metadata={op_name="XLA_Args"}
  %constant_225 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.218 = f32[3,3,128,128]{1,0,2,3} broadcast(f32[] %constant_225), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul"}
  %multiply.48 = f32[3,3,128,128]{1,0,2,3} multiply(f32[3,3,128,128]{1,0,2,3} %copy.455, f32[3,3,128,128]{1,0,2,3} %broadcast.218), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_8_grad/Mul_1"}
  %add.28 = f32[3,3,128,128]{1,0,2,3} add(f32[3,3,128,128]{1,0,2,3} %param_0.73, f32[3,3,128,128]{1,0,2,3} %multiply.48), metadata={op_type="AddN" op_name="tower0/gradients/AddN_186"}
  ROOT %copy.378 = f32[3,3,128,128]{3,2,1,0} copy(f32[3,3,128,128]{1,0,2,3} %add.28), metadata={op_name="XLA_Retvals"}
}

%fused_computation.28 (param_0.76: f32[1,512,1,1], param_1.82: f32[1,512,1,1], param_2.63: f32[512], param_3.35: f32[512]) -> f32[512] {
  %param_3.35 = f32[512]{0} parameter(3)
  %bitcast.257 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.35), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.63 = f32[512]{0} parameter(2)
  %negate.60 = f32[512]{0} negate(f32[512]{0} %param_2.63), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.256 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.60), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.82 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.50 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.256, f32[1,512,1,1]{3,2,1,0} %param_1.82), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.29 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.257, f32[1,512,1,1]{3,2,1,0} %multiply.50), metadata={op_type="AddN" op_name="tower0/gradients/AddN_185"}
  %param_0.76 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.49 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.29, f32[1,512,1,1]{3,2,1,0} %param_0.76), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.255 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.49), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3317 (x.3318: f32[], y.3319: f32[]) -> f32[] {
  %x.3318 = f32[] parameter(0)
  %y.3319 = f32[] parameter(1)
  ROOT %add.3320 = f32[] add(f32[] %x.3318, f32[] %y.3319)
}

%tower0_gradients_tower0_group1_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3327 (x.3328: f32[], y.3329: f32[]) -> f32[] {
  %x.3328 = f32[] parameter(0)
  %y.3329 = f32[] parameter(1)
  ROOT %add.3330 = f32[] add(f32[] %x.3328, f32[] %y.3329)
}

%fused_computation.29 (param_0.666: f32[1,512,100,100], param_1.893: f32[1,512,100,100], param_2.788: f32[1,512,100,100], param_3.524: f32[1,512,100,100]) -> (f32[512], f32[512], f32[1,512,100,100]) {
  %param_3.524 = f32[1,512,100,100]{3,2,1,0} parameter(3)
  %constant_151 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.252.clone.1 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[] %constant_151), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %compare.8.clone.1 = pred[1,512,100,100]{3,2,1,0} compare(f32[1,512,100,100]{3,2,1,0} %param_3.524, f32[1,512,100,100]{3,2,1,0} %broadcast.252.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/output_grad/ReluGrad"}
  %param_1.893 = f32[1,512,100,100]{3,2,1,0} parameter(1)
  %param_2.788 = f32[1,512,100,100]{3,2,1,0} parameter(2)
  %add.132.clone.1 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %param_1.893, f32[1,512,100,100]{3,2,1,0} %param_2.788), metadata={op_type="AddN" op_name="tower0/gradients/AddN_181"}
  %select.8.clone.1 = f32[1,512,100,100]{3,2,1,0} select(pred[1,512,100,100]{3,2,1,0} %compare.8.clone.1, f32[1,512,100,100]{3,2,1,0} %add.132.clone.1, f32[1,512,100,100]{3,2,1,0} %broadcast.252.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block2/output_grad/ReluGrad"}
  %param_0.666 = f32[1,512,100,100]{3,2,1,0} parameter(0)
  %multiply.51 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %select.8.clone.1, f32[1,512,100,100]{3,2,1,0} %param_0.666), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.22 = f32[512]{0} reduce(f32[1,512,100,100]{3,2,1,0} %multiply.51, f32[] %constant_151), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3317, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.72 = f32[512]{0} reduce(f32[1,512,100,100]{3,2,1,0} %select.8.clone.1, f32[] %constant_151), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3327, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.229 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) tuple(f32[512]{0} %reduce.22, f32[512]{0} %reduce.72, f32[1,512,100,100]{3,2,1,0} %select.8.clone.1)
}

%fused_computation.30 (param_0.81: f32[1,1,128,512], param_1.700: f32[1,1,128,512]) -> f32[1,1,128,512] {
  %param_0.81 = f32[1,1,128,512]{1,0,2,3} parameter(0)
  %param_1.700 = f32[1,1,128,512]{3,2,1,0} parameter(1)
  %copy.380 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %param_1.700), metadata={op_name="XLA_Args"}
  %constant_226 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.219 = f32[1,1,128,512]{1,0,2,3} broadcast(f32[] %constant_226), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul"}
  %multiply.52 = f32[1,1,128,512]{1,0,2,3} multiply(f32[1,1,128,512]{1,0,2,3} %copy.380, f32[1,1,128,512]{1,0,2,3} %broadcast.219), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_9_grad/Mul_1"}
  %add.30 = f32[1,1,128,512]{1,0,2,3} add(f32[1,1,128,512]{1,0,2,3} %param_0.81, f32[1,1,128,512]{1,0,2,3} %multiply.52), metadata={op_type="AddN" op_name="tower0/gradients/AddN_184"}
  ROOT %copy.379 = f32[1,1,128,512]{3,2,1,0} copy(f32[1,1,128,512]{1,0,2,3} %add.30), metadata={op_name="XLA_Retvals"}
}

%fused_computation.31 (param_0.84: f32[1,128,1,1], param_1.91: f32[1,128,1,1], param_2.70: f32[128], param_3.39: f32[128]) -> f32[128] {
  %param_3.39 = f32[128]{0} parameter(3)
  %bitcast.260 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.39), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.70 = f32[128]{0} parameter(2)
  %negate.61 = f32[128]{0} negate(f32[128]{0} %param_2.70), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.259 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.61), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.91 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.54 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.259, f32[1,128,1,1]{3,2,1,0} %param_1.91), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.31 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.260, f32[1,128,1,1]{3,2,1,0} %multiply.54), metadata={op_type="AddN" op_name="tower0/gradients/AddN_183"}
  %param_0.84 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.53 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.31, f32[1,128,1,1]{3,2,1,0} %param_0.84), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.258 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.53), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block3_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3344 (x.3345: f32[], y.3346: f32[]) -> f32[] {
  %x.3345 = f32[] parameter(0)
  %y.3346 = f32[] parameter(1)
  ROOT %add.3347 = f32[] add(f32[] %x.3345, f32[] %y.3346)
}

%tower0_gradients_tower0_group1_block3_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3354 (x.3355: f32[], y.3356: f32[]) -> f32[] {
  %x.3355 = f32[] parameter(0)
  %y.3356 = f32[] parameter(1)
  ROOT %add.3357 = f32[] add(f32[] %x.3355, f32[] %y.3356)
}

%fused_computation.32 (param_0.665: f32[1,128,100,100], param_1.894: f32[1,128,100,100], param_2.789: f32[1,128,100,100]) -> (f32[128], f32[128], f32[1,128,100,100]) {
  %param_2.789 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %constant_152 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.253.clone.1 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_152), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.9.clone.1 = pred[1,128,100,100]{3,2,1,0} compare(f32[1,128,100,100]{3,2,1,0} %param_2.789, f32[1,128,100,100]{3,2,1,0} %broadcast.253.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv1/Relu_grad/ReluGrad"}
  %param_1.894 = f32[1,128,100,100]{3,2,1,0} parameter(1)
  %select.9.clone.1 = f32[1,128,100,100]{3,2,1,0} select(pred[1,128,100,100]{3,2,1,0} %compare.9.clone.1, f32[1,128,100,100]{3,2,1,0} %param_1.894, f32[1,128,100,100]{3,2,1,0} %broadcast.253.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv1/Relu_grad/ReluGrad"}
  %param_0.665 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %multiply.55 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %select.9.clone.1, f32[1,128,100,100]{3,2,1,0} %param_0.665), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.23 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %multiply.55, f32[] %constant_152), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3344, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.87 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %select.9.clone.1, f32[] %constant_152), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3354, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.228 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) tuple(f32[128]{0} %reduce.23, f32[128]{0} %reduce.87, f32[1,128,100,100]{3,2,1,0} %select.9.clone.1)
}

%fused_computation.33 (param_0.89: f32[1,1,512,128], param_1.701: f32[1,1,512,128]) -> f32[1,1,512,128] {
  %param_0.89 = f32[1,1,512,128]{1,0,2,3} parameter(0)
  %param_1.701 = f32[1,1,512,128]{3,2,1,0} parameter(1)
  %copy.382 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %param_1.701), metadata={op_name="XLA_Args"}
  %constant_227 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.220 = f32[1,1,512,128]{1,0,2,3} broadcast(f32[] %constant_227), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %multiply.56 = f32[1,1,512,128]{1,0,2,3} multiply(f32[1,1,512,128]{1,0,2,3} %copy.382, f32[1,1,512,128]{1,0,2,3} %broadcast.220), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul_1"}
  %add.32 = f32[1,1,512,128]{1,0,2,3} add(f32[1,1,512,128]{1,0,2,3} %param_0.89, f32[1,1,512,128]{1,0,2,3} %multiply.56), metadata={op_type="AddN" op_name="tower0/gradients/AddN_182"}
  ROOT %copy.381 = f32[1,1,512,128]{3,2,1,0} copy(f32[1,1,512,128]{1,0,2,3} %add.32), metadata={op_name="XLA_Retvals"}
}

%fused_computation.34 (param_0.92: f32[1,128,1,1], param_1.100: f32[1,128,1,1], param_2.77: f32[128], param_3.43: f32[128]) -> f32[128] {
  %param_3.43 = f32[128]{0} parameter(3)
  %bitcast.263 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.43), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.77 = f32[128]{0} parameter(2)
  %negate.62 = f32[128]{0} negate(f32[128]{0} %param_2.77), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.262 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %negate.62), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.100 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %multiply.58 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.262, f32[1,128,1,1]{3,2,1,0} %param_1.100), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.33 = f32[1,128,1,1]{3,2,1,0} add(f32[1,128,1,1]{3,2,1,0} %bitcast.263, f32[1,128,1,1]{3,2,1,0} %multiply.58), metadata={op_type="AddN" op_name="tower0/gradients/AddN_180"}
  %param_0.92 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %multiply.57 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %add.33, f32[1,128,1,1]{3,2,1,0} %param_0.92), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.261 = f32[128]{0} bitcast(f32[1,128,1,1]{3,2,1,0} %multiply.57), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block3_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3371 (x.3372: f32[], y.3373: f32[]) -> f32[] {
  %x.3372 = f32[] parameter(0)
  %y.3373 = f32[] parameter(1)
  ROOT %add.3374 = f32[] add(f32[] %x.3372, f32[] %y.3373)
}

%tower0_gradients_tower0_group1_block3_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3381 (x.3382: f32[], y.3383: f32[]) -> f32[] {
  %x.3382 = f32[] parameter(0)
  %y.3383 = f32[] parameter(1)
  ROOT %add.3384 = f32[] add(f32[] %x.3382, f32[] %y.3383)
}

%fused_computation.35 (param_0.664: f32[1,128,100,100], param_1.895: f32[1,128,100,100], param_2.790: f32[1,128,100,100]) -> (f32[128], f32[128], f32[1,128,100,100]) {
  %param_2.790 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %constant_153 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.254.clone.1 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_153), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %compare.10.clone.1 = pred[1,128,100,100]{3,2,1,0} compare(f32[1,128,100,100]{3,2,1,0} %param_2.790, f32[1,128,100,100]{3,2,1,0} %broadcast.254.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %param_1.895 = f32[1,128,100,100]{3,2,1,0} parameter(1)
  %select.10.clone.1 = f32[1,128,100,100]{3,2,1,0} select(pred[1,128,100,100]{3,2,1,0} %compare.10.clone.1, f32[1,128,100,100]{3,2,1,0} %param_1.895, f32[1,128,100,100]{3,2,1,0} %broadcast.254.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/conv2/Relu_grad/ReluGrad"}
  %param_0.664 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %multiply.59 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %select.10.clone.1, f32[1,128,100,100]{3,2,1,0} %param_0.664), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.24 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %multiply.59, f32[] %constant_153), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3371, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.88 = f32[128]{0} reduce(f32[1,128,100,100]{3,2,1,0} %select.10.clone.1, f32[] %constant_153), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3381, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.227 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) tuple(f32[128]{0} %reduce.24, f32[128]{0} %reduce.88, f32[1,128,100,100]{3,2,1,0} %select.10.clone.1)
}

%fused_computation.36 (param_0.97: f32[3,3,128,128], param_1.702: f32[3,3,128,128]) -> f32[3,3,128,128] {
  %param_0.97 = f32[3,3,128,128]{1,0,2,3} parameter(0)
  %param_1.702 = f32[3,3,128,128]{3,2,1,0} parameter(1)
  %copy.456 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %param_1.702), metadata={op_name="XLA_Args"}
  %constant_228 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.221 = f32[3,3,128,128]{1,0,2,3} broadcast(f32[] %constant_228), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul"}
  %multiply.60 = f32[3,3,128,128]{1,0,2,3} multiply(f32[3,3,128,128]{1,0,2,3} %copy.456, f32[3,3,128,128]{1,0,2,3} %broadcast.221), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_11_grad/Mul_1"}
  %add.34 = f32[3,3,128,128]{1,0,2,3} add(f32[3,3,128,128]{1,0,2,3} %param_0.97, f32[3,3,128,128]{1,0,2,3} %multiply.60), metadata={op_type="AddN" op_name="tower0/gradients/AddN_179"}
  ROOT %copy.383 = f32[3,3,128,128]{3,2,1,0} copy(f32[3,3,128,128]{1,0,2,3} %add.34), metadata={op_name="XLA_Retvals"}
}

%fused_computation.37 (param_0.100: f32[1,512,1,1], param_1.108: f32[1,512,1,1], param_2.83: f32[512], param_3.47: f32[512]) -> f32[512] {
  %param_3.47 = f32[512]{0} parameter(3)
  %bitcast.266 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.47), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.83 = f32[512]{0} parameter(2)
  %negate.63 = f32[512]{0} negate(f32[512]{0} %param_2.83), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.265 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.63), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.108 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.62 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.265, f32[1,512,1,1]{3,2,1,0} %param_1.108), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.35 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.266, f32[1,512,1,1]{3,2,1,0} %multiply.62), metadata={op_type="AddN" op_name="tower0/gradients/AddN_178"}
  %param_0.100 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.61 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.35, f32[1,512,1,1]{3,2,1,0} %param_0.100), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.264 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.61), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group1_block3_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3398 (x.3399: f32[], y.3400: f32[]) -> f32[] {
  %x.3399 = f32[] parameter(0)
  %y.3400 = f32[] parameter(1)
  ROOT %add.3401 = f32[] add(f32[] %x.3399, f32[] %y.3400)
}

%tower0_gradients_tower0_group1_block3_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3408 (x.3409: f32[], y.3410: f32[]) -> f32[] {
  %x.3409 = f32[] parameter(0)
  %y.3410 = f32[] parameter(1)
  ROOT %add.3411 = f32[] add(f32[] %x.3409, f32[] %y.3410)
}

%fused_computation.38 (param_0.663: f32[1,512,100,100], param_1.896: f32[1,512,100,100], param_2.791: f32[1,512,100,100], param_3.525: f32[1,512,100,100], param_4.276: f32[1,512,100,100]) -> (f32[512], f32[512], f32[1,512,100,100]) {
  %param_4.276 = f32[1,512,100,100]{3,2,1,0} parameter(4)
  %constant_154 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.255.clone.1 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[] %constant_154), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %compare.11.clone.1 = pred[1,512,100,100]{3,2,1,0} compare(f32[1,512,100,100]{3,2,1,0} %param_4.276, f32[1,512,100,100]{3,2,1,0} %broadcast.255.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %param_2.791 = f32[1,512,100,100]{3,2,1,0} parameter(2)
  %param_3.525 = f32[1,512,100,100]{3,2,1,0} parameter(3)
  %add.134.clone.1 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %param_2.791, f32[1,512,100,100]{3,2,1,0} %param_3.525), metadata={op_type="AddN" op_name="tower0/gradients/AddN_174"}
  %param_1.896 = f32[1,512,100,100]{3,2,1,0} parameter(1)
  %add.133.clone.1 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %add.134.clone.1, f32[1,512,100,100]{3,2,1,0} %param_1.896), metadata={op_type="AddN" op_name="tower0/gradients/AddN_174"}
  %select.11.clone.1 = f32[1,512,100,100]{3,2,1,0} select(pred[1,512,100,100]{3,2,1,0} %compare.11.clone.1, f32[1,512,100,100]{3,2,1,0} %add.133.clone.1, f32[1,512,100,100]{3,2,1,0} %broadcast.255.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group1/block3/output_grad/ReluGrad"}
  %param_0.663 = f32[1,512,100,100]{3,2,1,0} parameter(0)
  %multiply.63 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %select.11.clone.1, f32[1,512,100,100]{3,2,1,0} %param_0.663), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.25 = f32[512]{0} reduce(f32[1,512,100,100]{3,2,1,0} %multiply.63, f32[] %constant_154), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3398, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.71 = f32[512]{0} reduce(f32[1,512,100,100]{3,2,1,0} %select.11.clone.1, f32[] %constant_154), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group1_block3_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3408, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.226 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) tuple(f32[512]{0} %reduce.25, f32[512]{0} %reduce.71, f32[1,512,100,100]{3,2,1,0} %select.11.clone.1)
}

%fused_computation.39 (param_0.105: f32[1,1,128,512], param_1.703: f32[1,1,128,512]) -> f32[1,1,128,512] {
  %param_0.105 = f32[1,1,128,512]{1,0,2,3} parameter(0)
  %param_1.703 = f32[1,1,128,512]{3,2,1,0} parameter(1)
  %copy.385 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %param_1.703), metadata={op_name="XLA_Args"}
  %constant_229 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.222 = f32[1,1,128,512]{1,0,2,3} broadcast(f32[] %constant_229), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_2_grad/Mul"}
  %multiply.64 = f32[1,1,128,512]{1,0,2,3} multiply(f32[1,1,128,512]{1,0,2,3} %copy.385, f32[1,1,128,512]{1,0,2,3} %broadcast.222), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_12_grad/Mul_1"}
  %add.36 = f32[1,1,128,512]{1,0,2,3} add(f32[1,1,128,512]{1,0,2,3} %param_0.105, f32[1,1,128,512]{1,0,2,3} %multiply.64), metadata={op_type="AddN" op_name="tower0/gradients/AddN_177"}
  ROOT %copy.384 = f32[1,1,128,512]{3,2,1,0} copy(f32[1,1,128,512]{1,0,2,3} %add.36), metadata={op_name="XLA_Retvals"}
}

%fused_computation.40 (param_0.108: f32[1,256,1,1], param_1.117: f32[1,256,1,1], param_2.90: f32[256], param_3.51: f32[256]) -> f32[256] {
  %param_3.51 = f32[256]{0} parameter(3)
  %bitcast.269 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.51), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.90 = f32[256]{0} parameter(2)
  %negate.64 = f32[256]{0} negate(f32[256]{0} %param_2.90), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.268 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.64), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.117 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.66 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.268, f32[1,256,1,1]{3,2,1,0} %param_1.117), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.37 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.269, f32[1,256,1,1]{3,2,1,0} %multiply.66), metadata={op_type="AddN" op_name="tower0/gradients/AddN_176"}
  %param_0.108 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.65 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.37, f32[1,256,1,1]{3,2,1,0} %param_0.108), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.267 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.65), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.2562 (x.2563: f32[], y.2564: f32[]) -> f32[] {
  %x.2563 = f32[] parameter(0)
  %y.2564 = f32[] parameter(1)
  ROOT %add.2565 = f32[] add(f32[] %x.2563, f32[] %y.2564)
}

%tower0_gradients_tower0_group2_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.2572 (x.2573: f32[], y.2574: f32[]) -> f32[] {
  %x.2573 = f32[] parameter(0)
  %y.2574 = f32[] parameter(1)
  ROOT %add.2575 = f32[] add(f32[] %x.2573, f32[] %y.2574)
}

%fused_computation.41 (param_0.662: f32[1,256,100,100], param_1.897: f32[1,256,101,101], param_2.792: f32[1,256,100,100]) -> (f32[256], f32[256], f32[1,256,100,100]) {
  %param_2.792 = f32[1,256,100,100]{3,2,1,0} parameter(2)
  %constant_155 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.256.clone.1 = f32[1,256,100,100]{3,2,1,0} broadcast(f32[] %constant_155), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/Relu_grad/ReluGrad"}
  %compare.12.clone.1 = pred[1,256,100,100]{3,2,1,0} compare(f32[1,256,100,100]{3,2,1,0} %param_2.792, f32[1,256,100,100]{3,2,1,0} %broadcast.256.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/conv1/Relu_grad/ReluGrad"}
  %param_1.897 = f32[1,256,101,101]{3,2,1,0} parameter(1)
  %slice.1.clone.1 = f32[1,256,100,100]{3,2,1,0} slice(f32[1,256,101,101]{3,2,1,0} %param_1.897), slice={[0:1], [0:256], [1:101], [1:101]}, metadata={op_type="Slice" op_name="tower0/gradients/tower0/group2/block0/Pad_grad/Slice_1"}
  %select.12.clone.1 = f32[1,256,100,100]{3,2,1,0} select(pred[1,256,100,100]{3,2,1,0} %compare.12.clone.1, f32[1,256,100,100]{3,2,1,0} %slice.1.clone.1, f32[1,256,100,100]{3,2,1,0} %broadcast.256.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/conv1/Relu_grad/ReluGrad"}
  %param_0.662 = f32[1,256,100,100]{3,2,1,0} parameter(0)
  %multiply.67 = f32[1,256,100,100]{3,2,1,0} multiply(f32[1,256,100,100]{3,2,1,0} %select.12.clone.1, f32[1,256,100,100]{3,2,1,0} %param_0.662), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.26 = f32[256]{0} reduce(f32[1,256,100,100]{3,2,1,0} %multiply.67, f32[] %constant_155), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.2562, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.77 = f32[256]{0} reduce(f32[1,256,100,100]{3,2,1,0} %select.12.clone.1, f32[] %constant_155), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.2572, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.225 = (f32[256]{0}, f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) tuple(f32[256]{0} %reduce.26, f32[256]{0} %reduce.77, f32[1,256,100,100]{3,2,1,0} %select.12.clone.1)
}

%fused_computation.42 (param_0.113: f32[1,1,512,256], param_1.704: f32[1,1,512,256]) -> f32[1,1,512,256] {
  %param_0.113 = f32[1,1,512,256]{1,0,2,3} parameter(0)
  %param_1.704 = f32[1,1,512,256]{3,2,1,0} parameter(1)
  %copy.387 = f32[1,1,512,256]{1,0,2,3} copy(f32[1,1,512,256]{3,2,1,0} %param_1.704), metadata={op_name="XLA_Args"}
  %constant_230 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.223 = f32[1,1,512,256]{1,0,2,3} broadcast(f32[] %constant_230), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_13_grad/Mul"}
  %multiply.68 = f32[1,1,512,256]{1,0,2,3} multiply(f32[1,1,512,256]{1,0,2,3} %copy.387, f32[1,1,512,256]{1,0,2,3} %broadcast.223), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_13_grad/Mul_1"}
  %add.38 = f32[1,1,512,256]{1,0,2,3} add(f32[1,1,512,256]{1,0,2,3} %param_0.113, f32[1,1,512,256]{1,0,2,3} %multiply.68), metadata={op_type="AddN" op_name="tower0/gradients/AddN_175"}
  ROOT %copy.386 = f32[1,1,512,256]{3,2,1,0} copy(f32[1,1,512,256]{1,0,2,3} %add.38), metadata={op_name="XLA_Retvals"}
}

%fused_computation.43 (param_0.116: f32[1,256,1,1], param_1.126: f32[1,256,1,1], param_2.97: f32[256], param_3.55: f32[256]) -> f32[256] {
  %param_3.55 = f32[256]{0} parameter(3)
  %bitcast.272 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.55), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.97 = f32[256]{0} parameter(2)
  %negate.65 = f32[256]{0} negate(f32[256]{0} %param_2.97), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.271 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.65), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.126 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.70 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.271, f32[1,256,1,1]{3,2,1,0} %param_1.126), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.39 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.272, f32[1,256,1,1]{3,2,1,0} %multiply.70), metadata={op_type="AddN" op_name="tower0/gradients/AddN_173"}
  %param_0.116 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.69 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.39, f32[1,256,1,1]{3,2,1,0} %param_0.116), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.270 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.69), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.2589 (x.2590: f32[], y.2591: f32[]) -> f32[] {
  %x.2590 = f32[] parameter(0)
  %y.2591 = f32[] parameter(1)
  ROOT %add.2592 = f32[] add(f32[] %x.2590, f32[] %y.2591)
}

%tower0_gradients_tower0_group2_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.2599 (x.2600: f32[], y.2601: f32[]) -> f32[] {
  %x.2600 = f32[] parameter(0)
  %y.2601 = f32[] parameter(1)
  ROOT %add.2602 = f32[] add(f32[] %x.2600, f32[] %y.2601)
}

%fused_computation.44 (param_0.661: f32[1,256,50,50], param_1.898: f32[1,256,50,50], param_2.793: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.793 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_156 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.257.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_156), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.13.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.793, f32[1,256,50,50]{3,2,1,0} %broadcast.257.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/conv2/Relu_grad/ReluGrad"}
  %param_1.898 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.13.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.13.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.898, f32[1,256,50,50]{3,2,1,0} %broadcast.257.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/conv2/Relu_grad/ReluGrad"}
  %param_0.661 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.71 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.13.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.661), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.27 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.71, f32[] %constant_156), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.2589, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.95 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.13.clone.1, f32[] %constant_156), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.2599, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.224 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.27, f32[256]{0} %reduce.95, f32[1,256,50,50]{3,2,1,0} %select.13.clone.1)
}

%fused_computation.48 (param_0.130: f32[1,1024,1,1], param_1.805: f32[1,1024,1,1], param_2.597: f32[1024], param_3.330: f32[1024], param_4.121: f32[1,1024,1,1], param_5.122: f32[1,1024,1,1], param_6.105: f32[1024]) -> (f32[1024], f32[1024]) {
  %param_2.597 = f32[1024]{0} parameter(2)
  %bitcast.276 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_2.597), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_3.330 = f32[1024]{0} parameter(3)
  %negate.113 = f32[1024]{0} negate(f32[1024]{0} %param_3.330), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/sub_grad/Neg"}
  %bitcast.423 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.113), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/sub_grad/Neg"}
  %param_1.805 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.77 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.423, f32[1,1024,1,1]{3,2,1,0} %param_1.805), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.42 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.276, f32[1,1024,1,1]{3,2,1,0} %multiply.77), metadata={op_type="AddN" op_name="tower0/gradients/AddN_171"}
  %param_0.130 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.76 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.42, f32[1,1024,1,1]{3,2,1,0} %param_0.130), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.275 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.76), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/Reshape_grad/Reshape"}
  %param_6.105 = f32[1024]{0} parameter(6)
  %bitcast.274.clone.1 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_6.105), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_5.122 = f32[1,1024,1,1]{3,2,1,0} parameter(5)
  %multiply.74.clone.1 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.423, f32[1,1024,1,1]{3,2,1,0} %param_5.122), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.41.clone.1 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.274.clone.1, f32[1,1024,1,1]{3,2,1,0} %multiply.74.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_169"}
  %param_4.121 = f32[1,1024,1,1]{3,2,1,0} parameter(4)
  %multiply.73.clone.1 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.41.clone.1, f32[1,1024,1,1]{3,2,1,0} %param_4.121), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.273.clone.1 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.73.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/Reshape_grad/Reshape"}
  ROOT %tuple.185 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %bitcast.275, f32[1024]{0} %bitcast.273.clone.1)
}

%tower0_gradients_tower0_group2_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3425 (x.3426: f32[], y.3427: f32[]) -> f32[] {
  %x.3426 = f32[] parameter(0)
  %y.3427 = f32[] parameter(1)
  ROOT %add.3428 = f32[] add(f32[] %x.3426, f32[] %y.3427)
}

%tower0_gradients_tower0_group2_block0_convshortcut_bn_batchnorm_add_1_grad_Sum_1-reduction.3435 (x.3436: f32[], y.3437: f32[]) -> f32[] {
  %x.3436 = f32[] parameter(0)
  %y.3437 = f32[] parameter(1)
  ROOT %add.3438 = f32[] add(f32[] %x.3436, f32[] %y.3437)
}

%tower0_gradients_tower0_group2_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.2616 (x.2617: f32[], y.2618: f32[]) -> f32[] {
  %x.2617 = f32[] parameter(0)
  %y.2618 = f32[] parameter(1)
  ROOT %add.2619 = f32[] add(f32[] %x.2617, f32[] %y.2618)
}

%fused_computation.50 (param_0.660: f32[1,1024,50,50], param_1.899: f32[1,1024,50,50], param_2.794: f32[1,1024,50,50], param_3.526: f32[1,1024,50,50], param_4.277: f32[1,1024,50,50]) -> (f32[1024], f32[1024], f32[1024], f32[1,1024,50,50]) {
  %param_4.277 = f32[1,1024,50,50]{3,2,1,0} parameter(4)
  %constant_158 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.258.clone.1 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_158), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.14.clone.1 = pred[1,1024,50,50]{3,2,1,0} compare(f32[1,1024,50,50]{3,2,1,0} %param_4.277, f32[1,1024,50,50]{3,2,1,0} %broadcast.258.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/output_grad/ReluGrad"}
  %param_2.794 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %param_3.526 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %add.135.clone.1 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_2.794, f32[1,1024,50,50]{3,2,1,0} %param_3.526), metadata={op_type="AddN" op_name="tower0/gradients/AddN_165"}
  %select.14.clone.1 = f32[1,1024,50,50]{3,2,1,0} select(pred[1,1024,50,50]{3,2,1,0} %compare.14.clone.1, f32[1,1024,50,50]{3,2,1,0} %add.135.clone.1, f32[1,1024,50,50]{3,2,1,0} %broadcast.258.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block0/output_grad/ReluGrad"}
  %param_0.660 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %multiply.78 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %select.14.clone.1, f32[1,1024,50,50]{3,2,1,0} %param_0.660), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.29 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %multiply.78, f32[] %constant_158), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3425, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.79 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %select.14.clone.1, f32[] %constant_158), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_convshortcut_bn_batchnorm_add_1_grad_Sum_1-reduction.3435, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/add_1_grad/Sum_1"}
  %param_1.899 = f32[1,1024,50,50]{3,2,1,0} parameter(1)
  %multiply.75.clone.1 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %select.14.clone.1, f32[1,1024,50,50]{3,2,1,0} %param_1.899), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.28.clone.1 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %multiply.75.clone.1, f32[] %constant_158), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.2616, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  ROOT %tuple.223 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) tuple(f32[1024]{0} %reduce.29, f32[1024]{0} %reduce.79, f32[1024]{0} %reduce.28.clone.1, f32[1,1024,50,50]{3,2,1,0} %select.14.clone.1)
}

%fused_computation.51 (param_0.137: f32[1,1,256,1024], param_1.706: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.137 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.706 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.390 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.706), metadata={op_name="XLA_Args"}
  %constant_231 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.224 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_231), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.79 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.390, f32[1,1,256,1024]{1,0,2,3} %broadcast.224), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_15_grad/Mul_1"}
  %add.43 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.137, f32[1,1,256,1024]{1,0,2,3} %multiply.79), metadata={op_type="AddN" op_name="tower0/gradients/AddN_168"}
  ROOT %copy.389 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.43), metadata={op_name="XLA_Retvals"}
}

%fused_computation.52 (param_0.139: f32[1,1,512,1024], param_1.707: f32[1,1,512,1024]) -> f32[1,1,512,1024] {
  %param_0.139 = f32[1,1,512,1024]{1,0,2,3} parameter(0)
  %param_1.707 = f32[1,1,512,1024]{3,2,1,0} parameter(1)
  %copy.459 = f32[1,1,512,1024]{1,0,2,3} copy(f32[1,1,512,1024]{3,2,1,0} %param_1.707), metadata={op_name="XLA_Args"}
  %constant_159 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.100 = f32[1,1,512,1024]{1,0,2,3} broadcast(f32[] %constant_159), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_16_grad/Mul"}
  %multiply.80 = f32[1,1,512,1024]{1,0,2,3} multiply(f32[1,1,512,1024]{1,0,2,3} %copy.459, f32[1,1,512,1024]{1,0,2,3} %broadcast.100), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_16_grad/Mul_1"}
  %add.44 = f32[1,1,512,1024]{1,0,2,3} add(f32[1,1,512,1024]{1,0,2,3} %param_0.139, f32[1,1,512,1024]{1,0,2,3} %multiply.80), metadata={op_type="AddN" op_name="tower0/gradients/AddN_170"}
  ROOT %copy.391 = f32[1,1,512,1024]{3,2,1,0} copy(f32[1,1,512,1024]{1,0,2,3} %add.44), metadata={op_name="XLA_Retvals"}
}

%fused_computation.53 (param_0.142: f32[1,256,1,1], param_1.151: f32[1,256,1,1], param_2.115: f32[256], param_3.63: f32[256]) -> f32[256] {
  %param_3.63 = f32[256]{0} parameter(3)
  %bitcast.280 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.63), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.115 = f32[256]{0} parameter(2)
  %negate.67 = f32[256]{0} negate(f32[256]{0} %param_2.115), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.279 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.67), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.151 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.82 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.279, f32[1,256,1,1]{3,2,1,0} %param_1.151), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.45 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.280, f32[1,256,1,1]{3,2,1,0} %multiply.82), metadata={op_type="AddN" op_name="tower0/gradients/AddN_167"}
  %param_0.142 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.81 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.45, f32[1,256,1,1]{3,2,1,0} %param_0.142), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.278 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.81), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3452 (x.3453: f32[], y.3454: f32[]) -> f32[] {
  %x.3453 = f32[] parameter(0)
  %y.3454 = f32[] parameter(1)
  ROOT %add.3455 = f32[] add(f32[] %x.3453, f32[] %y.3454)
}

%tower0_gradients_tower0_group2_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3462 (x.3463: f32[], y.3464: f32[]) -> f32[] {
  %x.3463 = f32[] parameter(0)
  %y.3464 = f32[] parameter(1)
  ROOT %add.3465 = f32[] add(f32[] %x.3463, f32[] %y.3464)
}

%fused_computation.54 (param_0.659: f32[1,256,50,50], param_1.900: f32[1,256,50,50], param_2.795: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.795 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_160 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.259.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_160), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.15.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.795, f32[1,256,50,50]{3,2,1,0} %broadcast.259.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/conv1/Relu_grad/ReluGrad"}
  %param_1.900 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.15.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.15.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.900, f32[1,256,50,50]{3,2,1,0} %broadcast.259.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/conv1/Relu_grad/ReluGrad"}
  %param_0.659 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.83 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.15.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.659), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.30 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.83, f32[] %constant_160), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3452, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.99 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.15.clone.1, f32[] %constant_160), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3462, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.222 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.30, f32[256]{0} %reduce.99, f32[1,256,50,50]{3,2,1,0} %select.15.clone.1)
}

%fused_computation.55 (param_0.147: f32[1,1,1024,256], param_1.708: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.147 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.708 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.393 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.708), metadata={op_name="XLA_Args"}
  %constant_232 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.225 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_232), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.84 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.393, f32[1,1,1024,256]{1,0,2,3} %broadcast.225), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_17_grad/Mul_1"}
  %add.46 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.147, f32[1,1,1024,256]{1,0,2,3} %multiply.84), metadata={op_type="AddN" op_name="tower0/gradients/AddN_166"}
  ROOT %copy.392 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.46), metadata={op_name="XLA_Retvals"}
}

%fused_computation.56 (param_0.150: f32[1,256,1,1], param_1.160: f32[1,256,1,1], param_2.122: f32[256], param_3.67: f32[256]) -> f32[256] {
  %param_3.67 = f32[256]{0} parameter(3)
  %bitcast.283 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.67), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.122 = f32[256]{0} parameter(2)
  %negate.68 = f32[256]{0} negate(f32[256]{0} %param_2.122), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.282 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.68), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.160 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.86 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.282, f32[1,256,1,1]{3,2,1,0} %param_1.160), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.47 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.283, f32[1,256,1,1]{3,2,1,0} %multiply.86), metadata={op_type="AddN" op_name="tower0/gradients/AddN_164"}
  %param_0.150 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.85 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.47, f32[1,256,1,1]{3,2,1,0} %param_0.150), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.281 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.85), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3479 (x.3480: f32[], y.3481: f32[]) -> f32[] {
  %x.3480 = f32[] parameter(0)
  %y.3481 = f32[] parameter(1)
  ROOT %add.3482 = f32[] add(f32[] %x.3480, f32[] %y.3481)
}

%tower0_gradients_tower0_group2_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3489 (x.3490: f32[], y.3491: f32[]) -> f32[] {
  %x.3490 = f32[] parameter(0)
  %y.3491 = f32[] parameter(1)
  ROOT %add.3492 = f32[] add(f32[] %x.3490, f32[] %y.3491)
}

%fused_computation.57 (param_0.658: f32[1,256,50,50], param_1.901: f32[1,256,50,50], param_2.796: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.796 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_161 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.260.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_161), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.16.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.796, f32[1,256,50,50]{3,2,1,0} %broadcast.260.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/conv2/Relu_grad/ReluGrad"}
  %param_1.901 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.16.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.16.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.901, f32[1,256,50,50]{3,2,1,0} %broadcast.260.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/conv2/Relu_grad/ReluGrad"}
  %param_0.658 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.87 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.16.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.658), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.31 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.87, f32[] %constant_161), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3479, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.98 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.16.clone.1, f32[] %constant_161), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3489, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.221 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.31, f32[256]{0} %reduce.98, f32[1,256,50,50]{3,2,1,0} %select.16.clone.1)
}

%fused_computation.59 (param_0.158: f32[1,1024,1,1], param_1.168: f32[1,1024,1,1], param_2.128: f32[1024], param_3.71: f32[1024]) -> f32[1024] {
  %param_3.71 = f32[1024]{0} parameter(3)
  %bitcast.286 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.71), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.128 = f32[1024]{0} parameter(2)
  %negate.69 = f32[1024]{0} negate(f32[1024]{0} %param_2.128), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.285 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.69), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.168 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.90 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.285, f32[1,1024,1,1]{3,2,1,0} %param_1.168), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.49 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.286, f32[1,1024,1,1]{3,2,1,0} %multiply.90), metadata={op_type="AddN" op_name="tower0/gradients/AddN_162"}
  %param_0.158 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.89 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.49, f32[1,1024,1,1]{3,2,1,0} %param_0.158), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.284 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.89), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3506 (x.3507: f32[], y.3508: f32[]) -> f32[] {
  %x.3507 = f32[] parameter(0)
  %y.3508 = f32[] parameter(1)
  ROOT %add.3509 = f32[] add(f32[] %x.3507, f32[] %y.3508)
}

%tower0_gradients_tower0_group2_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3516 (x.3517: f32[], y.3518: f32[]) -> f32[] {
  %x.3517 = f32[] parameter(0)
  %y.3518 = f32[] parameter(1)
  ROOT %add.3519 = f32[] add(f32[] %x.3517, f32[] %y.3518)
}

%fused_computation.60 (param_0.657: f32[1,1024,50,50], param_1.902: f32[1,1024,50,50], param_2.797: f32[1,1024,50,50], param_3.527: f32[1,1024,50,50]) -> (f32[1024], f32[1024], f32[1,1024,50,50]) {
  %param_3.527 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %constant_162 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.261.clone.1 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_162), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.17.clone.1 = pred[1,1024,50,50]{3,2,1,0} compare(f32[1,1024,50,50]{3,2,1,0} %param_3.527, f32[1,1024,50,50]{3,2,1,0} %broadcast.261.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/output_grad/ReluGrad"}
  %param_1.902 = f32[1,1024,50,50]{3,2,1,0} parameter(1)
  %param_2.797 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %add.136.clone.1 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_1.902, f32[1,1024,50,50]{3,2,1,0} %param_2.797), metadata={op_type="AddN" op_name="tower0/gradients/AddN_158"}
  %select.17.clone.1 = f32[1,1024,50,50]{3,2,1,0} select(pred[1,1024,50,50]{3,2,1,0} %compare.17.clone.1, f32[1,1024,50,50]{3,2,1,0} %add.136.clone.1, f32[1,1024,50,50]{3,2,1,0} %broadcast.261.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block1/output_grad/ReluGrad"}
  %param_0.657 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %multiply.91 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %select.17.clone.1, f32[1,1024,50,50]{3,2,1,0} %param_0.657), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.32 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %multiply.91, f32[] %constant_162), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3506, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.78 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %select.17.clone.1, f32[] %constant_162), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3516, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.220 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) tuple(f32[1024]{0} %reduce.32, f32[1024]{0} %reduce.78, f32[1,1024,50,50]{3,2,1,0} %select.17.clone.1)
}

%fused_computation.61 (param_0.163: f32[1,1,256,1024], param_1.710: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.163 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.710 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.396 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.710), metadata={op_name="XLA_Args"}
  %constant_233 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.226 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_233), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.92 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.396, f32[1,1,256,1024]{1,0,2,3} %broadcast.226), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_19_grad/Mul_1"}
  %add.50 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.163, f32[1,1,256,1024]{1,0,2,3} %multiply.92), metadata={op_type="AddN" op_name="tower0/gradients/AddN_161"}
  ROOT %copy.395 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.50), metadata={op_name="XLA_Retvals"}
}

%fused_computation.62 (param_0.166: f32[1,256,1,1], param_1.177: f32[1,256,1,1], param_2.135: f32[256], param_3.75: f32[256]) -> f32[256] {
  %param_3.75 = f32[256]{0} parameter(3)
  %bitcast.289 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.75), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.135 = f32[256]{0} parameter(2)
  %negate.70 = f32[256]{0} negate(f32[256]{0} %param_2.135), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.288 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.70), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.177 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.94 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.288, f32[1,256,1,1]{3,2,1,0} %param_1.177), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.51 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.289, f32[1,256,1,1]{3,2,1,0} %multiply.94), metadata={op_type="AddN" op_name="tower0/gradients/AddN_160"}
  %param_0.166 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.93 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.51, f32[1,256,1,1]{3,2,1,0} %param_0.166), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.287 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.93), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3533 (x.3534: f32[], y.3535: f32[]) -> f32[] {
  %x.3534 = f32[] parameter(0)
  %y.3535 = f32[] parameter(1)
  ROOT %add.3536 = f32[] add(f32[] %x.3534, f32[] %y.3535)
}

%tower0_gradients_tower0_group2_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3543 (x.3544: f32[], y.3545: f32[]) -> f32[] {
  %x.3544 = f32[] parameter(0)
  %y.3545 = f32[] parameter(1)
  ROOT %add.3546 = f32[] add(f32[] %x.3544, f32[] %y.3545)
}

%fused_computation.63 (param_0.656: f32[1,256,50,50], param_1.903: f32[1,256,50,50], param_2.798: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.798 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_163 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.262.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_163), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.18.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.798, f32[1,256,50,50]{3,2,1,0} %broadcast.262.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/conv1/Relu_grad/ReluGrad"}
  %param_1.903 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.18.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.18.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.903, f32[1,256,50,50]{3,2,1,0} %broadcast.262.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/conv1/Relu_grad/ReluGrad"}
  %param_0.656 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.95 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.18.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.656), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.33 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.95, f32[] %constant_163), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3533, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.92 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.18.clone.1, f32[] %constant_163), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3543, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.219 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.33, f32[256]{0} %reduce.92, f32[1,256,50,50]{3,2,1,0} %select.18.clone.1)
}

%fused_computation.64 (param_0.171: f32[1,1,1024,256], param_1.711: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.171 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.711 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.398 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.711), metadata={op_name="XLA_Args"}
  %constant_234 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.227 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_234), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.96 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.398, f32[1,1,1024,256]{1,0,2,3} %broadcast.227), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_20_grad/Mul_1"}
  %add.52 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.171, f32[1,1,1024,256]{1,0,2,3} %multiply.96), metadata={op_type="AddN" op_name="tower0/gradients/AddN_159"}
  ROOT %copy.397 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.52), metadata={op_name="XLA_Retvals"}
}

%fused_computation.65 (param_0.174: f32[1,256,1,1], param_1.186: f32[1,256,1,1], param_2.142: f32[256], param_3.79: f32[256]) -> f32[256] {
  %param_3.79 = f32[256]{0} parameter(3)
  %bitcast.292 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.79), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.142 = f32[256]{0} parameter(2)
  %negate.71 = f32[256]{0} negate(f32[256]{0} %param_2.142), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.291 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.71), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.186 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.98 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.291, f32[1,256,1,1]{3,2,1,0} %param_1.186), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.53 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.292, f32[1,256,1,1]{3,2,1,0} %multiply.98), metadata={op_type="AddN" op_name="tower0/gradients/AddN_157"}
  %param_0.174 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.97 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.53, f32[1,256,1,1]{3,2,1,0} %param_0.174), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.290 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.97), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3560 (x.3561: f32[], y.3562: f32[]) -> f32[] {
  %x.3561 = f32[] parameter(0)
  %y.3562 = f32[] parameter(1)
  ROOT %add.3563 = f32[] add(f32[] %x.3561, f32[] %y.3562)
}

%tower0_gradients_tower0_group2_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3570 (x.3571: f32[], y.3572: f32[]) -> f32[] {
  %x.3571 = f32[] parameter(0)
  %y.3572 = f32[] parameter(1)
  ROOT %add.3573 = f32[] add(f32[] %x.3571, f32[] %y.3572)
}

%fused_computation.66 (param_0.655: f32[1,256,50,50], param_1.904: f32[1,256,50,50], param_2.799: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.799 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_164 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.263.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_164), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.19.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.799, f32[1,256,50,50]{3,2,1,0} %broadcast.263.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/conv2/Relu_grad/ReluGrad"}
  %param_1.904 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.19.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.19.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.904, f32[1,256,50,50]{3,2,1,0} %broadcast.263.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/conv2/Relu_grad/ReluGrad"}
  %param_0.655 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.99 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.19.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.655), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.34 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.99, f32[] %constant_164), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3560, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.91 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.19.clone.1, f32[] %constant_164), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3570, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.218 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.34, f32[256]{0} %reduce.91, f32[1,256,50,50]{3,2,1,0} %select.19.clone.1)
}

%fused_computation.68 (param_0.182: f32[1,1024,1,1], param_1.194: f32[1,1024,1,1], param_2.148: f32[1024], param_3.83: f32[1024]) -> f32[1024] {
  %param_3.83 = f32[1024]{0} parameter(3)
  %bitcast.295 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.83), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.148 = f32[1024]{0} parameter(2)
  %negate.72 = f32[1024]{0} negate(f32[1024]{0} %param_2.148), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.294 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.72), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.194 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.102 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.294, f32[1,1024,1,1]{3,2,1,0} %param_1.194), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.55 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.295, f32[1,1024,1,1]{3,2,1,0} %multiply.102), metadata={op_type="AddN" op_name="tower0/gradients/AddN_155"}
  %param_0.182 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.101 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.55, f32[1,1024,1,1]{3,2,1,0} %param_0.182), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.293 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.101), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3587 (x.3588: f32[], y.3589: f32[]) -> f32[] {
  %x.3588 = f32[] parameter(0)
  %y.3589 = f32[] parameter(1)
  ROOT %add.3590 = f32[] add(f32[] %x.3588, f32[] %y.3589)
}

%tower0_gradients_tower0_group2_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3597 (x.3598: f32[], y.3599: f32[]) -> f32[] {
  %x.3598 = f32[] parameter(0)
  %y.3599 = f32[] parameter(1)
  ROOT %add.3600 = f32[] add(f32[] %x.3598, f32[] %y.3599)
}

%fused_computation.69 (param_0.654: f32[1,1024,50,50], param_1.905: f32[1,1024,50,50], param_2.800: f32[1,1024,50,50], param_3.528: f32[1,1024,50,50]) -> (f32[1024], f32[1024], f32[1,1024,50,50]) {
  %param_3.528 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %constant_165 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.264.clone.1 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_165), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.20.clone.1 = pred[1,1024,50,50]{3,2,1,0} compare(f32[1,1024,50,50]{3,2,1,0} %param_3.528, f32[1,1024,50,50]{3,2,1,0} %broadcast.264.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/output_grad/ReluGrad"}
  %param_1.905 = f32[1,1024,50,50]{3,2,1,0} parameter(1)
  %param_2.800 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %add.137.clone.1 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_1.905, f32[1,1024,50,50]{3,2,1,0} %param_2.800), metadata={op_type="AddN" op_name="tower0/gradients/AddN_151"}
  %select.20.clone.1 = f32[1,1024,50,50]{3,2,1,0} select(pred[1,1024,50,50]{3,2,1,0} %compare.20.clone.1, f32[1,1024,50,50]{3,2,1,0} %add.137.clone.1, f32[1,1024,50,50]{3,2,1,0} %broadcast.264.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block2/output_grad/ReluGrad"}
  %param_0.654 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %multiply.103 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %select.20.clone.1, f32[1,1024,50,50]{3,2,1,0} %param_0.654), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.35 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %multiply.103, f32[] %constant_165), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3587, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.73 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %select.20.clone.1, f32[] %constant_165), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3597, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.217 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) tuple(f32[1024]{0} %reduce.35, f32[1024]{0} %reduce.73, f32[1,1024,50,50]{3,2,1,0} %select.20.clone.1)
}

%fused_computation.70 (param_0.187: f32[1,1,256,1024], param_1.713: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.187 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.713 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.401 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.713), metadata={op_name="XLA_Args"}
  %constant_235 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.228 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_235), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.104 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.401, f32[1,1,256,1024]{1,0,2,3} %broadcast.228), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_22_grad/Mul_1"}
  %add.56 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.187, f32[1,1,256,1024]{1,0,2,3} %multiply.104), metadata={op_type="AddN" op_name="tower0/gradients/AddN_154"}
  ROOT %copy.400 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.56), metadata={op_name="XLA_Retvals"}
}

%fused_computation.71 (param_0.190: f32[1,256,1,1], param_1.203: f32[1,256,1,1], param_2.155: f32[256], param_3.87: f32[256]) -> f32[256] {
  %param_3.87 = f32[256]{0} parameter(3)
  %bitcast.298 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.87), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.155 = f32[256]{0} parameter(2)
  %negate.73 = f32[256]{0} negate(f32[256]{0} %param_2.155), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.297 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.73), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.203 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.106 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.297, f32[1,256,1,1]{3,2,1,0} %param_1.203), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.57 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.298, f32[1,256,1,1]{3,2,1,0} %multiply.106), metadata={op_type="AddN" op_name="tower0/gradients/AddN_153"}
  %param_0.190 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.105 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.57, f32[1,256,1,1]{3,2,1,0} %param_0.190), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.296 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.105), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block3_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3614 (x.3615: f32[], y.3616: f32[]) -> f32[] {
  %x.3615 = f32[] parameter(0)
  %y.3616 = f32[] parameter(1)
  ROOT %add.3617 = f32[] add(f32[] %x.3615, f32[] %y.3616)
}

%tower0_gradients_tower0_group2_block3_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3624 (x.3625: f32[], y.3626: f32[]) -> f32[] {
  %x.3625 = f32[] parameter(0)
  %y.3626 = f32[] parameter(1)
  ROOT %add.3627 = f32[] add(f32[] %x.3625, f32[] %y.3626)
}

%fused_computation.72 (param_0.653: f32[1,256,50,50], param_1.906: f32[1,256,50,50], param_2.801: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.801 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_166 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.265.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_166), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.21.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.801, f32[1,256,50,50]{3,2,1,0} %broadcast.265.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/conv1/Relu_grad/ReluGrad"}
  %param_1.906 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.21.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.21.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.906, f32[1,256,50,50]{3,2,1,0} %broadcast.265.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/conv1/Relu_grad/ReluGrad"}
  %param_0.653 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.107 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.21.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.653), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.36 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.107, f32[] %constant_166), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3614, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.93 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.21.clone.1, f32[] %constant_166), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3624, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.216 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.36, f32[256]{0} %reduce.93, f32[1,256,50,50]{3,2,1,0} %select.21.clone.1)
}

%fused_computation.73 (param_0.195: f32[1,1,1024,256], param_1.714: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.195 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.714 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.403 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.714), metadata={op_name="XLA_Args"}
  %constant_236 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.229 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_236), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.108 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.403, f32[1,1,1024,256]{1,0,2,3} %broadcast.229), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_23_grad/Mul_1"}
  %add.58 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.195, f32[1,1,1024,256]{1,0,2,3} %multiply.108), metadata={op_type="AddN" op_name="tower0/gradients/AddN_152"}
  ROOT %copy.402 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.58), metadata={op_name="XLA_Retvals"}
}

%fused_computation.74 (param_0.198: f32[1,256,1,1], param_1.212: f32[1,256,1,1], param_2.162: f32[256], param_3.91: f32[256]) -> f32[256] {
  %param_3.91 = f32[256]{0} parameter(3)
  %bitcast.301 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.91), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.162 = f32[256]{0} parameter(2)
  %negate.74 = f32[256]{0} negate(f32[256]{0} %param_2.162), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.300 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.74), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.212 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.110 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.300, f32[1,256,1,1]{3,2,1,0} %param_1.212), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.59 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.301, f32[1,256,1,1]{3,2,1,0} %multiply.110), metadata={op_type="AddN" op_name="tower0/gradients/AddN_150"}
  %param_0.198 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.109 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.59, f32[1,256,1,1]{3,2,1,0} %param_0.198), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.299 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.109), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block3_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3641 (x.3642: f32[], y.3643: f32[]) -> f32[] {
  %x.3642 = f32[] parameter(0)
  %y.3643 = f32[] parameter(1)
  ROOT %add.3644 = f32[] add(f32[] %x.3642, f32[] %y.3643)
}

%tower0_gradients_tower0_group2_block3_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3651 (x.3652: f32[], y.3653: f32[]) -> f32[] {
  %x.3652 = f32[] parameter(0)
  %y.3653 = f32[] parameter(1)
  ROOT %add.3654 = f32[] add(f32[] %x.3652, f32[] %y.3653)
}

%fused_computation.75 (param_0.652: f32[1,256,50,50], param_1.907: f32[1,256,50,50], param_2.802: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.802 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_167 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.266.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_167), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.22.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.802, f32[1,256,50,50]{3,2,1,0} %broadcast.266.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/conv2/Relu_grad/ReluGrad"}
  %param_1.907 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.22.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.22.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.907, f32[1,256,50,50]{3,2,1,0} %broadcast.266.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/conv2/Relu_grad/ReluGrad"}
  %param_0.652 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.111 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.22.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.652), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.37 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.111, f32[] %constant_167), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3641, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.94 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.22.clone.1, f32[] %constant_167), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3651, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.215 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.37, f32[256]{0} %reduce.94, f32[1,256,50,50]{3,2,1,0} %select.22.clone.1)
}

%fused_computation.77 (param_0.206: f32[1,1024,1,1], param_1.220: f32[1,1024,1,1], param_2.168: f32[1024], param_3.95: f32[1024]) -> f32[1024] {
  %param_3.95 = f32[1024]{0} parameter(3)
  %bitcast.304 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.95), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.168 = f32[1024]{0} parameter(2)
  %negate.75 = f32[1024]{0} negate(f32[1024]{0} %param_2.168), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.303 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.75), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.220 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.114 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.303, f32[1,1024,1,1]{3,2,1,0} %param_1.220), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.61 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.304, f32[1,1024,1,1]{3,2,1,0} %multiply.114), metadata={op_type="AddN" op_name="tower0/gradients/AddN_148"}
  %param_0.206 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.113 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.61, f32[1,1024,1,1]{3,2,1,0} %param_0.206), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.302 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.113), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block3_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3668 (x.3669: f32[], y.3670: f32[]) -> f32[] {
  %x.3669 = f32[] parameter(0)
  %y.3670 = f32[] parameter(1)
  ROOT %add.3671 = f32[] add(f32[] %x.3669, f32[] %y.3670)
}

%tower0_gradients_tower0_group2_block3_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3678 (x.3679: f32[], y.3680: f32[]) -> f32[] {
  %x.3679 = f32[] parameter(0)
  %y.3680 = f32[] parameter(1)
  ROOT %add.3681 = f32[] add(f32[] %x.3679, f32[] %y.3680)
}

%fused_computation.78 (param_0.651: f32[1,1024,50,50], param_1.908: f32[1,1024,50,50], param_2.803: f32[1,1024,50,50], param_3.529: f32[1,1024,50,50]) -> (f32[1024], f32[1024], f32[1,1024,50,50]) {
  %param_3.529 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %constant_168 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.267.clone.1 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_168), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.23.clone.1 = pred[1,1024,50,50]{3,2,1,0} compare(f32[1,1024,50,50]{3,2,1,0} %param_3.529, f32[1,1024,50,50]{3,2,1,0} %broadcast.267.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/output_grad/ReluGrad"}
  %param_1.908 = f32[1,1024,50,50]{3,2,1,0} parameter(1)
  %param_2.803 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %add.138.clone.1 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_1.908, f32[1,1024,50,50]{3,2,1,0} %param_2.803), metadata={op_type="AddN" op_name="tower0/gradients/AddN_144"}
  %select.23.clone.1 = f32[1,1024,50,50]{3,2,1,0} select(pred[1,1024,50,50]{3,2,1,0} %compare.23.clone.1, f32[1,1024,50,50]{3,2,1,0} %add.138.clone.1, f32[1,1024,50,50]{3,2,1,0} %broadcast.267.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block3/output_grad/ReluGrad"}
  %param_0.651 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %multiply.115 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %select.23.clone.1, f32[1,1024,50,50]{3,2,1,0} %param_0.651), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.38 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %multiply.115, f32[] %constant_168), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3668, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.76 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %select.23.clone.1, f32[] %constant_168), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block3_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3678, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.214 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) tuple(f32[1024]{0} %reduce.38, f32[1024]{0} %reduce.76, f32[1,1024,50,50]{3,2,1,0} %select.23.clone.1)
}

%fused_computation.79 (param_0.211: f32[1,1,256,1024], param_1.716: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.211 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.716 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.406 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.716), metadata={op_name="XLA_Args"}
  %constant_237 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.230 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_237), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.116 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.406, f32[1,1,256,1024]{1,0,2,3} %broadcast.230), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_25_grad/Mul_1"}
  %add.62 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.211, f32[1,1,256,1024]{1,0,2,3} %multiply.116), metadata={op_type="AddN" op_name="tower0/gradients/AddN_147"}
  ROOT %copy.405 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.62), metadata={op_name="XLA_Retvals"}
}

%fused_computation.80 (param_0.214: f32[1,256,1,1], param_1.229: f32[1,256,1,1], param_2.175: f32[256], param_3.99: f32[256]) -> f32[256] {
  %param_3.99 = f32[256]{0} parameter(3)
  %bitcast.307 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.99), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.175 = f32[256]{0} parameter(2)
  %negate.76 = f32[256]{0} negate(f32[256]{0} %param_2.175), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.306 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.76), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.229 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.118 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.306, f32[1,256,1,1]{3,2,1,0} %param_1.229), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.63 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.307, f32[1,256,1,1]{3,2,1,0} %multiply.118), metadata={op_type="AddN" op_name="tower0/gradients/AddN_146"}
  %param_0.214 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.117 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.63, f32[1,256,1,1]{3,2,1,0} %param_0.214), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.305 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.117), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block4_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3695 (x.3696: f32[], y.3697: f32[]) -> f32[] {
  %x.3696 = f32[] parameter(0)
  %y.3697 = f32[] parameter(1)
  ROOT %add.3698 = f32[] add(f32[] %x.3696, f32[] %y.3697)
}

%tower0_gradients_tower0_group2_block4_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3705 (x.3706: f32[], y.3707: f32[]) -> f32[] {
  %x.3706 = f32[] parameter(0)
  %y.3707 = f32[] parameter(1)
  ROOT %add.3708 = f32[] add(f32[] %x.3706, f32[] %y.3707)
}

%fused_computation.81 (param_0.650: f32[1,256,50,50], param_1.909: f32[1,256,50,50], param_2.804: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.804 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_169 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.268.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_169), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.24.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.804, f32[1,256,50,50]{3,2,1,0} %broadcast.268.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/conv1/Relu_grad/ReluGrad"}
  %param_1.909 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.24.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.24.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.909, f32[1,256,50,50]{3,2,1,0} %broadcast.268.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/conv1/Relu_grad/ReluGrad"}
  %param_0.650 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.119 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.24.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.650), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.39 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.119, f32[] %constant_169), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3695, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.96 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.24.clone.1, f32[] %constant_169), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3705, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.213 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.39, f32[256]{0} %reduce.96, f32[1,256,50,50]{3,2,1,0} %select.24.clone.1)
}

%fused_computation.82 (param_0.219: f32[1,1,1024,256], param_1.717: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.219 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.717 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.408 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.717), metadata={op_name="XLA_Args"}
  %constant_238 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.231 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_238), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.120 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.408, f32[1,1,1024,256]{1,0,2,3} %broadcast.231), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_26_grad/Mul_1"}
  %add.64 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.219, f32[1,1,1024,256]{1,0,2,3} %multiply.120), metadata={op_type="AddN" op_name="tower0/gradients/AddN_145"}
  ROOT %copy.407 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.64), metadata={op_name="XLA_Retvals"}
}

%fused_computation.83 (param_0.222: f32[1,256,1,1], param_1.238: f32[1,256,1,1], param_2.182: f32[256], param_3.103: f32[256]) -> f32[256] {
  %param_3.103 = f32[256]{0} parameter(3)
  %bitcast.310 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.103), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.182 = f32[256]{0} parameter(2)
  %negate.77 = f32[256]{0} negate(f32[256]{0} %param_2.182), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.309 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.77), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.238 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.122 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.309, f32[1,256,1,1]{3,2,1,0} %param_1.238), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.65 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.310, f32[1,256,1,1]{3,2,1,0} %multiply.122), metadata={op_type="AddN" op_name="tower0/gradients/AddN_143"}
  %param_0.222 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.121 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.65, f32[1,256,1,1]{3,2,1,0} %param_0.222), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.308 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.121), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block4_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3722 (x.3723: f32[], y.3724: f32[]) -> f32[] {
  %x.3723 = f32[] parameter(0)
  %y.3724 = f32[] parameter(1)
  ROOT %add.3725 = f32[] add(f32[] %x.3723, f32[] %y.3724)
}

%tower0_gradients_tower0_group2_block4_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3732 (x.3733: f32[], y.3734: f32[]) -> f32[] {
  %x.3733 = f32[] parameter(0)
  %y.3734 = f32[] parameter(1)
  ROOT %add.3735 = f32[] add(f32[] %x.3733, f32[] %y.3734)
}

%fused_computation.84 (param_0.649: f32[1,256,50,50], param_1.910: f32[1,256,50,50], param_2.805: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.805 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_170 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.269.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_170), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.25.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.805, f32[1,256,50,50]{3,2,1,0} %broadcast.269.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/conv2/Relu_grad/ReluGrad"}
  %param_1.910 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.25.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.25.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.910, f32[1,256,50,50]{3,2,1,0} %broadcast.269.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/conv2/Relu_grad/ReluGrad"}
  %param_0.649 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.123 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.25.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.649), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.40 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.123, f32[] %constant_170), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3722, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.97 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.25.clone.1, f32[] %constant_170), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3732, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.212 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.40, f32[256]{0} %reduce.97, f32[1,256,50,50]{3,2,1,0} %select.25.clone.1)
}

%fused_computation.85 (param_0.227: f32[3,3,256,256], param_1.718: f32[3,3,256,256], param_2.526: f32[3,3,256,256], param_3.346: f32[3,3,256,256], param_4.115: f32[3,3,256,256], param_5.116: f32[3,3,256,256], param_6.97: f32[3,3,256,256], param_7.114: f32[3,3,256,256], param_8.86: f32[3,3,256,256], param_9.41: f32[3,3,256,256], param_10.27: f32[3,3,256,256], param_11.15: f32[3,3,256,256], param_12.11: f32[3,3,256,256], param_13.12: f32[3,3,256,256], param_14.12: f32[3,3,256,256], param_15.13: f32[3,3,256,256], param_16.13: f32[3,3,256,256], param_17.12: f32[3,3,256,256], param_18.12: f32[3,3,256,256], param_19.13: f32[3,3,256,256], param_20.12: f32[3,3,256,256], param_21.11: f32[3,3,256,256], param_22.8: f32[3,3,256,256], param_23.5: f32[3,3,256,256], param_24.5: f32[3,3,256,256], param_25.5: f32[3,3,256,256], param_26.3: f32[3,3,256,256]) -> (f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256]) {
  %param_0.227 = f32[3,3,256,256]{1,0,2,3} parameter(0)
  %param_2.526 = f32[3,3,256,256]{3,2,1,0} parameter(2)
  %copy.467 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_2.526), metadata={op_name="XLA_Args"}
  %param_1.718 = f32[3,3,256,256]{3,2,1,0} parameter(1)
  %copy.466 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_1.718), metadata={op_name="XLA_Args"}
  %multiply.124 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.467, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_27_grad/Mul_1"}
  %add.66 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_0.227, f32[3,3,256,256]{1,0,2,3} %multiply.124), metadata={op_type="AddN" op_name="tower0/gradients/AddN_142"}
  %copy.409 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.66), metadata={op_name="XLA_Retvals"}
  %param_3.346 = f32[3,3,256,256]{1,0,2,3} parameter(3)
  %param_4.115 = f32[3,3,256,256]{3,2,1,0} parameter(4)
  %copy.458.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_4.115), metadata={op_name="XLA_Args"}
  %multiply.72.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.458.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_14_grad/Mul_1"}
  %add.40.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_3.346, f32[3,3,256,256]{1,0,2,3} %multiply.72.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_172"}
  %copy.388.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.40.clone.1), metadata={op_name="XLA_Retvals"}
  %param_25.5 = f32[3,3,256,256]{1,0,2,3} parameter(25)
  %param_26.3 = f32[3,3,256,256]{3,2,1,0} parameter(26)
  %copy.479.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_26.3), metadata={op_name="XLA_Args"}
  %multiply.187.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.479.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_47_grad/Mul_1"}
  %add.101.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_25.5, f32[3,3,256,256]{1,0,2,3} %multiply.187.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_100"}
  %copy.443.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.101.clone.1), metadata={op_name="XLA_Retvals"}
  %param_23.5 = f32[3,3,256,256]{1,0,2,3} parameter(23)
  %param_24.5 = f32[3,3,256,256]{3,2,1,0} parameter(24)
  %copy.465.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_24.5), metadata={op_name="XLA_Args"}
  %multiply.112.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.465.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_24_grad/Mul_1"}
  %add.60.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_23.5, f32[3,3,256,256]{1,0,2,3} %multiply.112.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_149"}
  %copy.404.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.60.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_21.11 = f32[3,3,256,256]{1,0,2,3} parameter(21)
  %param_22.8 = f32[3,3,256,256]{3,2,1,0} parameter(22)
  %copy.477.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_22.8), metadata={op_name="XLA_Args"}
  %multiply.186.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.477.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_48_grad/Mul_1"}
  %add.100.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_21.11, f32[3,3,256,256]{1,0,2,3} %multiply.186.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_101"}
  %copy.442.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.100.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_19.13 = f32[3,3,256,256]{1,0,2,3} parameter(19)
  %param_20.12 = f32[3,3,256,256]{1,0,2,3} parameter(20)
  %add.107.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_19.13, f32[3,3,256,256]{1,0,2,3} %param_20.12), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %param_18.12 = f32[3,3,256,256]{1,0,2,3} parameter(18)
  %add.106.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %add.107.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %param_18.12), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %param_17.12 = f32[3,3,256,256]{1,0,2,3} parameter(17)
  %add.105.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %add.106.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %param_17.12), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %param_16.13 = f32[3,3,256,256]{1,0,2,3} parameter(16)
  %add.104.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %add.105.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %param_16.13), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %param_15.13 = f32[3,3,256,256]{3,2,1,0} parameter(15)
  %copy.483.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_15.13), metadata={op_name="XLA_Args"}
  %multiply.189.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.483.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_50_grad/Mul_1"}
  %add.103.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %add.104.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %multiply.189.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_97"}
  %copy.445.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.103.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_13.12 = f32[3,3,256,256]{1,0,2,3} parameter(13)
  %param_14.12 = f32[3,3,256,256]{3,2,1,0} parameter(14)
  %copy.461.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_14.12), metadata={op_name="XLA_Args"}
  %multiply.88.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.461.clone.1.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_18_grad/Mul_1"}
  %add.48.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_13.12, f32[3,3,256,256]{1,0,2,3} %multiply.88.clone.1.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_163"}
  %copy.394.clone.1.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.48.clone.1.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_11.15 = f32[3,3,256,256]{1,0,2,3} parameter(11)
  %param_12.11 = f32[3,3,256,256]{3,2,1,0} parameter(12)
  %copy.475.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_12.11), metadata={op_name="XLA_Args"}
  %multiply.185.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.475.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_49_grad/Mul_1"}
  %add.99.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_11.15, f32[3,3,256,256]{1,0,2,3} %multiply.185.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_102"}
  %copy.441.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.99.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_9.41 = f32[3,3,256,256]{1,0,2,3} parameter(9)
  %param_10.27 = f32[3,3,256,256]{3,2,1,0} parameter(10)
  %copy.481.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_10.27), metadata={op_name="XLA_Args"}
  %multiply.188.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.481.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_46_grad/Mul_1"}
  %add.102.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_9.41, f32[3,3,256,256]{1,0,2,3} %multiply.188.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_99"}
  %copy.444.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.102.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_7.114 = f32[3,3,256,256]{1,0,2,3} parameter(7)
  %param_8.86 = f32[3,3,256,256]{3,2,1,0} parameter(8)
  %copy.469.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_8.86), metadata={op_name="XLA_Args"}
  %multiply.136.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.469.clone.1.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_30_grad/Mul_1"}
  %add.72.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_7.114, f32[3,3,256,256]{1,0,2,3} %multiply.136.clone.1.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_135"}
  %copy.414.clone.1.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.72.clone.1.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  %param_5.116 = f32[3,3,256,256]{1,0,2,3} parameter(5)
  %param_6.97 = f32[3,3,256,256]{3,2,1,0} parameter(6)
  %copy.463.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %param_6.97), metadata={op_name="XLA_Args"}
  %multiply.100.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} multiply(f32[3,3,256,256]{1,0,2,3} %copy.463.clone.1.clone.1.clone.1, f32[3,3,256,256]{1,0,2,3} %copy.466), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_21_grad/Mul_1"}
  %add.54.clone.1.clone.1.clone.1 = f32[3,3,256,256]{1,0,2,3} add(f32[3,3,256,256]{1,0,2,3} %param_5.116, f32[3,3,256,256]{1,0,2,3} %multiply.100.clone.1.clone.1.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_156"}
  %copy.399.clone.1.clone.1.clone.1 = f32[3,3,256,256]{3,2,1,0} copy(f32[3,3,256,256]{1,0,2,3} %add.54.clone.1.clone.1.clone.1), metadata={op_name="XLA_Retvals"}
  ROOT %tuple.178 = (f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) tuple(f32[3,3,256,256]{3,2,1,0} %copy.409, f32[3,3,256,256]{3,2,1,0} %copy.388.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.443.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.404.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.442.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.445.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.394.clone.1.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.441.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.444.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.414.clone.1.clone.1.clone.1, f32[3,3,256,256]{3,2,1,0} %copy.399.clone.1.clone.1.clone.1)
}

%fused_computation.86 (param_0.230: f32[1,1024,1,1], param_1.246: f32[1,1024,1,1], param_2.188: f32[1024], param_3.107: f32[1024]) -> f32[1024] {
  %param_3.107 = f32[1024]{0} parameter(3)
  %bitcast.313 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.107), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.188 = f32[1024]{0} parameter(2)
  %negate.78 = f32[1024]{0} negate(f32[1024]{0} %param_2.188), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.312 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.78), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.246 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.126 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.312, f32[1,1024,1,1]{3,2,1,0} %param_1.246), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.67 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.313, f32[1,1024,1,1]{3,2,1,0} %multiply.126), metadata={op_type="AddN" op_name="tower0/gradients/AddN_141"}
  %param_0.230 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.125 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.67, f32[1,1024,1,1]{3,2,1,0} %param_0.230), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.311 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.125), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block4_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3749 (x.3750: f32[], y.3751: f32[]) -> f32[] {
  %x.3750 = f32[] parameter(0)
  %y.3751 = f32[] parameter(1)
  ROOT %add.3752 = f32[] add(f32[] %x.3750, f32[] %y.3751)
}

%tower0_gradients_tower0_group2_block4_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3759 (x.3760: f32[], y.3761: f32[]) -> f32[] {
  %x.3760 = f32[] parameter(0)
  %y.3761 = f32[] parameter(1)
  ROOT %add.3762 = f32[] add(f32[] %x.3760, f32[] %y.3761)
}

%fused_computation.87 (param_0.648: f32[1,1024,50,50], param_1.911: f32[1,1024,50,50], param_2.806: f32[1,1024,50,50], param_3.530: f32[1,1024,50,50]) -> (f32[1024], f32[1024], f32[1,1024,50,50]) {
  %param_3.530 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %constant_171 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.270.clone.1 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_171), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.26.clone.1 = pred[1,1024,50,50]{3,2,1,0} compare(f32[1,1024,50,50]{3,2,1,0} %param_3.530, f32[1,1024,50,50]{3,2,1,0} %broadcast.270.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/output_grad/ReluGrad"}
  %param_1.911 = f32[1,1024,50,50]{3,2,1,0} parameter(1)
  %param_2.806 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %add.139.clone.1 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_1.911, f32[1,1024,50,50]{3,2,1,0} %param_2.806), metadata={op_type="AddN" op_name="tower0/gradients/AddN_137"}
  %select.26.clone.1 = f32[1,1024,50,50]{3,2,1,0} select(pred[1,1024,50,50]{3,2,1,0} %compare.26.clone.1, f32[1,1024,50,50]{3,2,1,0} %add.139.clone.1, f32[1,1024,50,50]{3,2,1,0} %broadcast.270.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block4/output_grad/ReluGrad"}
  %param_0.648 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %multiply.127 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %select.26.clone.1, f32[1,1024,50,50]{3,2,1,0} %param_0.648), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.41 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %multiply.127, f32[] %constant_171), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3749, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.75 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %select.26.clone.1, f32[] %constant_171), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block4_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3759, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.211 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) tuple(f32[1024]{0} %reduce.41, f32[1024]{0} %reduce.75, f32[1,1024,50,50]{3,2,1,0} %select.26.clone.1)
}

%fused_computation.88 (param_0.235: f32[1,1,256,1024], param_1.719: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.235 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.719 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.411 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.719), metadata={op_name="XLA_Args"}
  %constant_239 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.232 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_239), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.128 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.411, f32[1,1,256,1024]{1,0,2,3} %broadcast.232), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_28_grad/Mul_1"}
  %add.68 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.235, f32[1,1,256,1024]{1,0,2,3} %multiply.128), metadata={op_type="AddN" op_name="tower0/gradients/AddN_140"}
  ROOT %copy.410 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.68), metadata={op_name="XLA_Retvals"}
}

%fused_computation.89 (param_0.238: f32[1,256,1,1], param_1.255: f32[1,256,1,1], param_2.195: f32[256], param_3.111: f32[256]) -> f32[256] {
  %param_3.111 = f32[256]{0} parameter(3)
  %bitcast.316 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.111), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.195 = f32[256]{0} parameter(2)
  %negate.79 = f32[256]{0} negate(f32[256]{0} %param_2.195), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.315 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.79), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.255 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.130 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.315, f32[1,256,1,1]{3,2,1,0} %param_1.255), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.69 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.316, f32[1,256,1,1]{3,2,1,0} %multiply.130), metadata={op_type="AddN" op_name="tower0/gradients/AddN_139"}
  %param_0.238 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.129 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.69, f32[1,256,1,1]{3,2,1,0} %param_0.238), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.314 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.129), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block5_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3776 (x.3777: f32[], y.3778: f32[]) -> f32[] {
  %x.3777 = f32[] parameter(0)
  %y.3778 = f32[] parameter(1)
  ROOT %add.3779 = f32[] add(f32[] %x.3777, f32[] %y.3778)
}

%tower0_gradients_tower0_group2_block5_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3786 (x.3787: f32[], y.3788: f32[]) -> f32[] {
  %x.3787 = f32[] parameter(0)
  %y.3788 = f32[] parameter(1)
  ROOT %add.3789 = f32[] add(f32[] %x.3787, f32[] %y.3788)
}

%fused_computation.90 (param_0.647: f32[1,256,50,50], param_1.912: f32[1,256,50,50], param_2.807: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.807 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_172 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.271.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_172), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.27.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.807, f32[1,256,50,50]{3,2,1,0} %broadcast.271.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/conv1/Relu_grad/ReluGrad"}
  %param_1.912 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.27.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.27.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.912, f32[1,256,50,50]{3,2,1,0} %broadcast.271.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/conv1/Relu_grad/ReluGrad"}
  %param_0.647 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.131 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.27.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.647), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.42 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.131, f32[] %constant_172), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3776, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.101 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.27.clone.1, f32[] %constant_172), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3786, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.210 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.42, f32[256]{0} %reduce.101, f32[1,256,50,50]{3,2,1,0} %select.27.clone.1)
}

%fused_computation.91 (param_0.243: f32[1,1,1024,256], param_1.720: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.243 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.720 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.413 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.720), metadata={op_name="XLA_Args"}
  %constant_240 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.233 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_240), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.132 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.413, f32[1,1,1024,256]{1,0,2,3} %broadcast.233), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul_1"}
  %add.70 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.243, f32[1,1,1024,256]{1,0,2,3} %multiply.132), metadata={op_type="AddN" op_name="tower0/gradients/AddN_138"}
  ROOT %copy.412 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.70), metadata={op_name="XLA_Retvals"}
}

%fused_computation.92 (param_0.246: f32[1,256,1,1], param_1.264: f32[1,256,1,1], param_2.202: f32[256], param_3.115: f32[256]) -> f32[256] {
  %param_3.115 = f32[256]{0} parameter(3)
  %bitcast.319 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.115), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.202 = f32[256]{0} parameter(2)
  %negate.80 = f32[256]{0} negate(f32[256]{0} %param_2.202), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.318 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %negate.80), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.264 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %multiply.134 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.318, f32[1,256,1,1]{3,2,1,0} %param_1.264), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.71 = f32[1,256,1,1]{3,2,1,0} add(f32[1,256,1,1]{3,2,1,0} %bitcast.319, f32[1,256,1,1]{3,2,1,0} %multiply.134), metadata={op_type="AddN" op_name="tower0/gradients/AddN_136"}
  %param_0.246 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %multiply.133 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %add.71, f32[1,256,1,1]{3,2,1,0} %param_0.246), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.317 = f32[256]{0} bitcast(f32[1,256,1,1]{3,2,1,0} %multiply.133), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block5_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3803 (x.3804: f32[], y.3805: f32[]) -> f32[] {
  %x.3804 = f32[] parameter(0)
  %y.3805 = f32[] parameter(1)
  ROOT %add.3806 = f32[] add(f32[] %x.3804, f32[] %y.3805)
}

%tower0_gradients_tower0_group2_block5_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3813 (x.3814: f32[], y.3815: f32[]) -> f32[] {
  %x.3814 = f32[] parameter(0)
  %y.3815 = f32[] parameter(1)
  ROOT %add.3816 = f32[] add(f32[] %x.3814, f32[] %y.3815)
}

%fused_computation.93 (param_0.646: f32[1,256,50,50], param_1.913: f32[1,256,50,50], param_2.808: f32[1,256,50,50]) -> (f32[256], f32[256], f32[1,256,50,50]) {
  %param_2.808 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_173 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.272.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_173), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.28.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.808, f32[1,256,50,50]{3,2,1,0} %broadcast.272.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/conv2/Relu_grad/ReluGrad"}
  %param_1.913 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %select.28.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.28.clone.1, f32[1,256,50,50]{3,2,1,0} %param_1.913, f32[1,256,50,50]{3,2,1,0} %broadcast.272.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/conv2/Relu_grad/ReluGrad"}
  %param_0.646 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %multiply.135 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %select.28.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.646), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.43 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %multiply.135, f32[] %constant_173), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3803, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.100 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.28.clone.1, f32[] %constant_173), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3813, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.209 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.43, f32[256]{0} %reduce.100, f32[1,256,50,50]{3,2,1,0} %select.28.clone.1)
}

%fused_computation.95 (param_0.254: f32[1,1024,1,1], param_1.272: f32[1,1024,1,1], param_2.208: f32[1024], param_3.119: f32[1024]) -> f32[1024] {
  %param_3.119 = f32[1024]{0} parameter(3)
  %bitcast.322 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_3.119), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.208 = f32[1024]{0} parameter(2)
  %negate.81 = f32[1024]{0} negate(f32[1024]{0} %param_2.208), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.321 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %negate.81), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.272 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %multiply.138 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.321, f32[1,1024,1,1]{3,2,1,0} %param_1.272), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.73 = f32[1,1024,1,1]{3,2,1,0} add(f32[1,1024,1,1]{3,2,1,0} %bitcast.322, f32[1,1024,1,1]{3,2,1,0} %multiply.138), metadata={op_type="AddN" op_name="tower0/gradients/AddN_134"}
  %param_0.254 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %multiply.137 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %add.73, f32[1,1024,1,1]{3,2,1,0} %param_0.254), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.320 = f32[1024]{0} bitcast(f32[1,1024,1,1]{3,2,1,0} %multiply.137), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group2_block5_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3830 (x.3831: f32[], y.3832: f32[]) -> f32[] {
  %x.3831 = f32[] parameter(0)
  %y.3832 = f32[] parameter(1)
  ROOT %add.3833 = f32[] add(f32[] %x.3831, f32[] %y.3832)
}

%tower0_gradients_tower0_group2_block5_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3840 (x.3841: f32[], y.3842: f32[]) -> f32[] {
  %x.3841 = f32[] parameter(0)
  %y.3842 = f32[] parameter(1)
  ROOT %add.3843 = f32[] add(f32[] %x.3841, f32[] %y.3842)
}

%fused_computation.96 (param_0.645: f32[1,1024,50,50], param_1.914: f32[1,1024,50,50], param_2.809: f32[1,1024,50,50], param_3.531: f32[1,1024,50,50], param_4.278: f32[1,1024,50,50]) -> (f32[1024], f32[1024], f32[1,1024,50,50]) {
  %param_4.278 = f32[1,1024,50,50]{3,2,1,0} parameter(4)
  %constant_174 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.273.clone.1 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_174), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %compare.29.clone.1 = pred[1,1024,50,50]{3,2,1,0} compare(f32[1,1024,50,50]{3,2,1,0} %param_4.278, f32[1,1024,50,50]{3,2,1,0} %broadcast.273.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %param_2.809 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %param_3.531 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %add.141.clone.1 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_2.809, f32[1,1024,50,50]{3,2,1,0} %param_3.531), metadata={op_type="AddN" op_name="tower0/gradients/AddN_130"}
  %param_1.914 = f32[1,1024,50,50]{3,2,1,0} parameter(1)
  %add.140.clone.1 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %add.141.clone.1, f32[1,1024,50,50]{3,2,1,0} %param_1.914), metadata={op_type="AddN" op_name="tower0/gradients/AddN_130"}
  %select.29.clone.1 = f32[1,1024,50,50]{3,2,1,0} select(pred[1,1024,50,50]{3,2,1,0} %compare.29.clone.1, f32[1,1024,50,50]{3,2,1,0} %add.140.clone.1, f32[1,1024,50,50]{3,2,1,0} %broadcast.273.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group2/block5/output_grad/ReluGrad"}
  %param_0.645 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %multiply.139 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %select.29.clone.1, f32[1,1024,50,50]{3,2,1,0} %param_0.645), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.44 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %multiply.139, f32[] %constant_174), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3830, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.74 = f32[1024]{0} reduce(f32[1,1024,50,50]{3,2,1,0} %select.29.clone.1, f32[] %constant_174), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group2_block5_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3840, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.208 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) tuple(f32[1024]{0} %reduce.44, f32[1024]{0} %reduce.74, f32[1,1024,50,50]{3,2,1,0} %select.29.clone.1)
}

%fused_computation.97 (param_0.259: f32[1,1,256,1024], param_1.722: f32[1,1,256,1024]) -> f32[1,1,256,1024] {
  %param_0.259 = f32[1,1,256,1024]{1,0,2,3} parameter(0)
  %param_1.722 = f32[1,1,256,1024]{3,2,1,0} parameter(1)
  %copy.416 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %param_1.722), metadata={op_name="XLA_Args"}
  %constant_241 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.234 = f32[1,1,256,1024]{1,0,2,3} broadcast(f32[] %constant_241), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul"}
  %multiply.140 = f32[1,1,256,1024]{1,0,2,3} multiply(f32[1,1,256,1024]{1,0,2,3} %copy.416, f32[1,1,256,1024]{1,0,2,3} %broadcast.234), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_31_grad/Mul_1"}
  %add.74 = f32[1,1,256,1024]{1,0,2,3} add(f32[1,1,256,1024]{1,0,2,3} %param_0.259, f32[1,1,256,1024]{1,0,2,3} %multiply.140), metadata={op_type="AddN" op_name="tower0/gradients/AddN_133"}
  ROOT %copy.415 = f32[1,1,256,1024]{3,2,1,0} copy(f32[1,1,256,1024]{1,0,2,3} %add.74), metadata={op_name="XLA_Retvals"}
}

%fused_computation.98 (param_0.262: f32[1,512,1,1], param_1.281: f32[1,512,1,1], param_2.215: f32[512], param_3.123: f32[512]) -> f32[512] {
  %param_3.123 = f32[512]{0} parameter(3)
  %bitcast.325 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.123), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.215 = f32[512]{0} parameter(2)
  %negate.82 = f32[512]{0} negate(f32[512]{0} %param_2.215), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.324 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.82), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.281 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.142 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.324, f32[1,512,1,1]{3,2,1,0} %param_1.281), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.75 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.325, f32[1,512,1,1]{3,2,1,0} %multiply.142), metadata={op_type="AddN" op_name="tower0/gradients/AddN_132"}
  %param_0.262 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.141 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.75, f32[1,512,1,1]{3,2,1,0} %param_0.262), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.323 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.141), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.1886 (x.1887: f32[], y.1888: f32[]) -> f32[] {
  %x.1887 = f32[] parameter(0)
  %y.1888 = f32[] parameter(1)
  ROOT %add.1889 = f32[] add(f32[] %x.1887, f32[] %y.1888)
}

%tower0_gradients_tower0_group3_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.1896 (x.1897: f32[], y.1898: f32[]) -> f32[] {
  %x.1897 = f32[] parameter(0)
  %y.1898 = f32[] parameter(1)
  ROOT %add.1899 = f32[] add(f32[] %x.1897, f32[] %y.1898)
}

%fused_computation.99 (param_0.644: f32[1,512,50,50], param_1.915: f32[1,512,51,51], param_2.810: f32[1,512,50,50]) -> (f32[512], f32[512], f32[1,512,50,50]) {
  %param_2.810 = f32[1,512,50,50]{3,2,1,0} parameter(2)
  %constant_175 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.142.clone.1 = f32[1,512,50,50]{3,2,1,0} broadcast(f32[] %constant_175), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv1/Relu_grad/ReluGrad"}
  %compare.30.clone.1 = pred[1,512,50,50]{3,2,1,0} compare(f32[1,512,50,50]{3,2,1,0} %param_2.810, f32[1,512,50,50]{3,2,1,0} %broadcast.142.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv1/Relu_grad/ReluGrad"}
  %param_1.915 = f32[1,512,51,51]{3,2,1,0} parameter(1)
  %slice.2.clone.1 = f32[1,512,50,50]{3,2,1,0} slice(f32[1,512,51,51]{3,2,1,0} %param_1.915), slice={[0:1], [0:512], [1:51], [1:51]}, metadata={op_type="Slice" op_name="tower0/gradients/tower0/group3/block0/Pad_grad/Slice_1"}
  %select.30.clone.1 = f32[1,512,50,50]{3,2,1,0} select(pred[1,512,50,50]{3,2,1,0} %compare.30.clone.1, f32[1,512,50,50]{3,2,1,0} %slice.2.clone.1, f32[1,512,50,50]{3,2,1,0} %broadcast.142.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv1/Relu_grad/ReluGrad"}
  %param_0.644 = f32[1,512,50,50]{3,2,1,0} parameter(0)
  %multiply.143 = f32[1,512,50,50]{3,2,1,0} multiply(f32[1,512,50,50]{3,2,1,0} %select.30.clone.1, f32[1,512,50,50]{3,2,1,0} %param_0.644), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.45 = f32[512]{0} reduce(f32[1,512,50,50]{3,2,1,0} %multiply.143, f32[] %constant_175), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.1886, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.81 = f32[512]{0} reduce(f32[1,512,50,50]{3,2,1,0} %select.30.clone.1, f32[] %constant_175), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.1896, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.207 = (f32[512]{0}, f32[512]{0}, f32[1,512,50,50]{3,2,1,0}) tuple(f32[512]{0} %reduce.45, f32[512]{0} %reduce.81, f32[1,512,50,50]{3,2,1,0} %select.30.clone.1)
}

%fused_computation.100 (param_0.267: f32[1,1,1024,512], param_1.288: f32[1,1,1024,512]) -> f32[1,1,1024,512] {
  %param_0.267 = f32[1,1,1024,512]{1,0,2,3} parameter(0)
  %param_1.288 = f32[1,1,1024,512]{3,2,1,0} parameter(1)
  %copy.418 = f32[1,1,1024,512]{1,0,2,3} copy(f32[1,1,1024,512]{3,2,1,0} %param_1.288), metadata={op_name="XLA_Args"}
  %constant_176 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.101 = f32[1,1,1024,512]{1,0,2,3} broadcast(f32[] %constant_176), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_32_grad/Mul"}
  %multiply.144 = f32[1,1,1024,512]{1,0,2,3} multiply(f32[1,1,1024,512]{1,0,2,3} %copy.418, f32[1,1,1024,512]{1,0,2,3} %broadcast.101), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_32_grad/Mul_1"}
  %add.76 = f32[1,1,1024,512]{1,0,2,3} add(f32[1,1,1024,512]{1,0,2,3} %param_0.267, f32[1,1,1024,512]{1,0,2,3} %multiply.144), metadata={op_type="AddN" op_name="tower0/gradients/AddN_131"}
  ROOT %copy.417 = f32[1,1,1024,512]{3,2,1,0} copy(f32[1,1,1024,512]{1,0,2,3} %add.76), metadata={op_name="XLA_Retvals"}
}

%fused_computation.101 (param_0.270: f32[1,512,1,1], param_1.291: f32[1,512,1,1], param_2.223: f32[512], param_3.127: f32[512]) -> f32[512] {
  %param_3.127 = f32[512]{0} parameter(3)
  %bitcast.328 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.127), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.223 = f32[512]{0} parameter(2)
  %negate.83 = f32[512]{0} negate(f32[512]{0} %param_2.223), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.327 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.83), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.291 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.146 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.327, f32[1,512,1,1]{3,2,1,0} %param_1.291), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.77 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.328, f32[1,512,1,1]{3,2,1,0} %multiply.146), metadata={op_type="AddN" op_name="tower0/gradients/AddN_129"}
  %param_0.270 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.145 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.77, f32[1,512,1,1]{3,2,1,0} %param_0.270), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.326 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.145), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.1913 (x.1914: f32[], y.1915: f32[]) -> f32[] {
  %x.1914 = f32[] parameter(0)
  %y.1915 = f32[] parameter(1)
  ROOT %add.1916 = f32[] add(f32[] %x.1914, f32[] %y.1915)
}

%tower0_gradients_tower0_group3_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.1923 (x.1924: f32[], y.1925: f32[]) -> f32[] {
  %x.1924 = f32[] parameter(0)
  %y.1925 = f32[] parameter(1)
  ROOT %add.1926 = f32[] add(f32[] %x.1924, f32[] %y.1925)
}

%fused_computation.102 (param_0.643: f32[1,512,25,25], param_1.916: f32[1,512,25,25], param_2.811: f32[1,512,25,25]) -> (f32[512], f32[512], f32[1,512,25,25]) {
  %param_2.811 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %constant_177 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.274.clone.1 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_177), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.31.clone.1 = pred[1,512,25,25]{3,2,1,0} compare(f32[1,512,25,25]{3,2,1,0} %param_2.811, f32[1,512,25,25]{3,2,1,0} %broadcast.274.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv2/Relu_grad/ReluGrad"}
  %param_1.916 = f32[1,512,25,25]{3,2,1,0} parameter(1)
  %select.31.clone.1 = f32[1,512,25,25]{3,2,1,0} select(pred[1,512,25,25]{3,2,1,0} %compare.31.clone.1, f32[1,512,25,25]{3,2,1,0} %param_1.916, f32[1,512,25,25]{3,2,1,0} %broadcast.274.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/conv2/Relu_grad/ReluGrad"}
  %param_0.643 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %multiply.147 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %select.31.clone.1, f32[1,512,25,25]{3,2,1,0} %param_0.643), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.46 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %multiply.147, f32[] %constant_177), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.1913, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.102 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %select.31.clone.1, f32[] %constant_177), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.1923, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.206 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) tuple(f32[512]{0} %reduce.46, f32[512]{0} %reduce.102, f32[1,512,25,25]{3,2,1,0} %select.31.clone.1)
}

%fused_computation.103 (param_0.275: f32[3,3,512,512], param_1.723: f32[3,3,512,512]) -> f32[3,3,512,512] {
  %param_0.275 = f32[3,3,512,512]{1,0,2,3} parameter(0)
  %param_1.723 = f32[3,3,512,512]{3,2,1,0} parameter(1)
  %copy.470 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %param_1.723), metadata={op_name="XLA_Args"}
  %constant_242 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.235 = f32[3,3,512,512]{1,0,2,3} broadcast(f32[] %constant_242), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_33_grad/Mul"}
  %multiply.148 = f32[3,3,512,512]{1,0,2,3} multiply(f32[3,3,512,512]{1,0,2,3} %copy.470, f32[3,3,512,512]{1,0,2,3} %broadcast.235), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_33_grad/Mul_1"}
  %add.78 = f32[3,3,512,512]{1,0,2,3} add(f32[3,3,512,512]{1,0,2,3} %param_0.275, f32[3,3,512,512]{1,0,2,3} %multiply.148), metadata={op_type="AddN" op_name="tower0/gradients/AddN_128"}
  ROOT %copy.419 = f32[3,3,512,512]{3,2,1,0} copy(f32[3,3,512,512]{1,0,2,3} %add.78), metadata={op_name="XLA_Retvals"}
}

%fused_computation.106 (param_0.284: f32[1,2048,1,1], param_1.803: f32[1,2048,1,1], param_2.595: f32[2048], param_3.326: f32[2048], param_4.118: f32[1,2048,1,1], param_5.119: f32[1,2048,1,1], param_6.101: f32[2048]) -> (f32[2048], f32[2048]) {
  %param_2.595 = f32[2048]{0} parameter(2)
  %bitcast.332 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_2.595), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_3.326 = f32[2048]{0} parameter(3)
  %negate.109 = f32[2048]{0} negate(f32[2048]{0} %param_3.326), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.419 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %negate.109), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.803 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %multiply.153 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.419, f32[1,2048,1,1]{3,2,1,0} %param_1.803), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.80 = f32[1,2048,1,1]{3,2,1,0} add(f32[1,2048,1,1]{3,2,1,0} %bitcast.332, f32[1,2048,1,1]{3,2,1,0} %multiply.153), metadata={op_type="AddN" op_name="tower0/gradients/AddN_125"}
  %param_0.284 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  %multiply.152 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %add.80, f32[1,2048,1,1]{3,2,1,0} %param_0.284), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.331 = f32[2048]{0} bitcast(f32[1,2048,1,1]{3,2,1,0} %multiply.152), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/Reshape_grad/Reshape"}
  %param_6.101 = f32[2048]{0} parameter(6)
  %bitcast.330.clone.1 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_6.101), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_5.119 = f32[1,2048,1,1]{3,2,1,0} parameter(5)
  %multiply.150.clone.1 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.419, f32[1,2048,1,1]{3,2,1,0} %param_5.119), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.79.clone.1 = f32[1,2048,1,1]{3,2,1,0} add(f32[1,2048,1,1]{3,2,1,0} %bitcast.330.clone.1, f32[1,2048,1,1]{3,2,1,0} %multiply.150.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_127"}
  %param_4.118 = f32[1,2048,1,1]{3,2,1,0} parameter(4)
  %multiply.149.clone.1 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %add.79.clone.1, f32[1,2048,1,1]{3,2,1,0} %param_4.118), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_grad/Mul_1"}
  %bitcast.329.clone.1 = f32[2048]{0} bitcast(f32[1,2048,1,1]{3,2,1,0} %multiply.149.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/Reshape_grad/Reshape"}
  ROOT %tuple.184 = (f32[2048]{0}, f32[2048]{0}) tuple(f32[2048]{0} %bitcast.331, f32[2048]{0} %bitcast.329.clone.1)
}

%tower0_gradients_tower0_group3_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.1940 (x.1941: f32[], y.1942: f32[]) -> f32[] {
  %x.1941 = f32[] parameter(0)
  %y.1942 = f32[] parameter(1)
  ROOT %add.1943 = f32[] add(f32[] %x.1941, f32[] %y.1942)
}

%tower0_gradients_tower0_group3_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3857 (x.3858: f32[], y.3859: f32[]) -> f32[] {
  %x.3858 = f32[] parameter(0)
  %y.3859 = f32[] parameter(1)
  ROOT %add.3860 = f32[] add(f32[] %x.3858, f32[] %y.3859)
}

%tower0_gradients_tower0_group3_block0_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.1950 (x.1951: f32[], y.1952: f32[]) -> f32[] {
  %x.1951 = f32[] parameter(0)
  %y.1952 = f32[] parameter(1)
  ROOT %add.1953 = f32[] add(f32[] %x.1951, f32[] %y.1952)
}

%fused_computation.108 (param_0.642: f32[1,2048,25,25], param_1.852: f32[1,2048,25,25], param_2.812: f32[1,2048,25,25], param_3.532: f32[1,2048,25,25], param_4.279: f32[1,2048,25,25]) -> (f32[2048], f32[2048], f32[2048], f32[1,2048,25,25]) {
  %param_4.279 = f32[1,2048,25,25]{3,2,1,0} parameter(4)
  %constant_179 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.275.clone.1 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[] %constant_179), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %compare.32.clone.1 = pred[1,2048,25,25]{3,2,1,0} compare(f32[1,2048,25,25]{3,2,1,0} %param_4.279, f32[1,2048,25,25]{3,2,1,0} %broadcast.275.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/output_grad/ReluGrad"}
  %param_2.812 = f32[1,2048,25,25]{3,2,1,0} parameter(2)
  %param_3.532 = f32[1,2048,25,25]{3,2,1,0} parameter(3)
  %add.142.clone.1 = f32[1,2048,25,25]{3,2,1,0} add(f32[1,2048,25,25]{3,2,1,0} %param_2.812, f32[1,2048,25,25]{3,2,1,0} %param_3.532), metadata={op_type="AddN" op_name="tower0/gradients/AddN_121"}
  %select.32.clone.1 = f32[1,2048,25,25]{3,2,1,0} select(pred[1,2048,25,25]{3,2,1,0} %compare.32.clone.1, f32[1,2048,25,25]{3,2,1,0} %add.142.clone.1, f32[1,2048,25,25]{3,2,1,0} %broadcast.275.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block0/output_grad/ReluGrad"}
  %param_0.642 = f32[1,2048,25,25]{3,2,1,0} parameter(0)
  %multiply.154 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %select.32.clone.1, f32[1,2048,25,25]{3,2,1,0} %param_0.642), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.48 = f32[2048]{0} reduce(f32[1,2048,25,25]{3,2,1,0} %multiply.154, f32[] %constant_179), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.1940, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %param_1.852 = f32[1,2048,25,25]{3,2,1,0} parameter(1)
  %multiply.151.clone.1 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %select.32.clone.1, f32[1,2048,25,25]{3,2,1,0} %param_1.852), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.47.clone.1 = f32[2048]{0} reduce(f32[1,2048,25,25]{3,2,1,0} %multiply.151.clone.1, f32[] %constant_179), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_convshortcut_bn_batchnorm_mul_1_grad_Sum_1-reduction.3857, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.89 = f32[2048]{0} reduce(f32[1,2048,25,25]{3,2,1,0} %select.32.clone.1, f32[] %constant_179), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block0_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.1950, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.205 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) tuple(f32[2048]{0} %reduce.48, f32[2048]{0} %reduce.47.clone.1, f32[2048]{0} %reduce.89, f32[1,2048,25,25]{3,2,1,0} %select.32.clone.1)
}

%fused_computation.109 (param_0.291: f32[1,1,1024,2048], param_1.724: f32[1,1,1024,2048]) -> f32[1,1,1024,2048] {
  %param_0.291 = f32[1,1,1024,2048]{1,0,2,3} parameter(0)
  %param_1.724 = f32[1,1,1024,2048]{3,2,1,0} parameter(1)
  %copy.471 = f32[1,1,1024,2048]{1,0,2,3} copy(f32[1,1,1024,2048]{3,2,1,0} %param_1.724), metadata={op_name="XLA_Args"}
  %constant_180 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.102 = f32[1,1,1024,2048]{1,0,2,3} broadcast(f32[] %constant_180), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_35_grad/Mul"}
  %multiply.155 = f32[1,1,1024,2048]{1,0,2,3} multiply(f32[1,1,1024,2048]{1,0,2,3} %copy.471, f32[1,1,1024,2048]{1,0,2,3} %broadcast.102), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_35_grad/Mul_1"}
  %add.81 = f32[1,1,1024,2048]{1,0,2,3} add(f32[1,1,1024,2048]{1,0,2,3} %param_0.291, f32[1,1,1024,2048]{1,0,2,3} %multiply.155), metadata={op_type="AddN" op_name="tower0/gradients/AddN_126"}
  ROOT %copy.420 = f32[1,1,1024,2048]{3,2,1,0} copy(f32[1,1,1024,2048]{1,0,2,3} %add.81), metadata={op_name="XLA_Retvals"}
}

%fused_computation.110 (param_0.293: f32[1,1,512,2048], param_1.725: f32[1,1,512,2048]) -> f32[1,1,512,2048] {
  %param_0.293 = f32[1,1,512,2048]{1,0,2,3} parameter(0)
  %param_1.725 = f32[1,1,512,2048]{3,2,1,0} parameter(1)
  %copy.422 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %param_1.725), metadata={op_name="XLA_Args"}
  %constant_243 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.236 = f32[1,1,512,2048]{1,0,2,3} broadcast(f32[] %constant_243), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_41_grad/Mul"}
  %multiply.156 = f32[1,1,512,2048]{1,0,2,3} multiply(f32[1,1,512,2048]{1,0,2,3} %copy.422, f32[1,1,512,2048]{1,0,2,3} %broadcast.236), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_34_grad/Mul_1"}
  %add.82 = f32[1,1,512,2048]{1,0,2,3} add(f32[1,1,512,2048]{1,0,2,3} %param_0.293, f32[1,1,512,2048]{1,0,2,3} %multiply.156), metadata={op_type="AddN" op_name="tower0/gradients/AddN_124"}
  ROOT %copy.421 = f32[1,1,512,2048]{3,2,1,0} copy(f32[1,1,512,2048]{1,0,2,3} %add.82), metadata={op_name="XLA_Retvals"}
}

%fused_computation.111 (param_0.296: f32[1,512,1,1], param_1.316: f32[1,512,1,1], param_2.241: f32[512], param_3.135: f32[512]) -> f32[512] {
  %param_3.135 = f32[512]{0} parameter(3)
  %bitcast.336 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.135), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.241 = f32[512]{0} parameter(2)
  %negate.85 = f32[512]{0} negate(f32[512]{0} %param_2.241), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.335 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.85), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.316 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.158 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.335, f32[1,512,1,1]{3,2,1,0} %param_1.316), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.83 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.336, f32[1,512,1,1]{3,2,1,0} %multiply.158), metadata={op_type="AddN" op_name="tower0/gradients/AddN_123"}
  %param_0.296 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.157 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.83, f32[1,512,1,1]{3,2,1,0} %param_0.296), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.334 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.157), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3884 (x.3885: f32[], y.3886: f32[]) -> f32[] {
  %x.3885 = f32[] parameter(0)
  %y.3886 = f32[] parameter(1)
  ROOT %add.3887 = f32[] add(f32[] %x.3885, f32[] %y.3886)
}

%tower0_gradients_tower0_group3_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3894 (x.3895: f32[], y.3896: f32[]) -> f32[] {
  %x.3895 = f32[] parameter(0)
  %y.3896 = f32[] parameter(1)
  ROOT %add.3897 = f32[] add(f32[] %x.3895, f32[] %y.3896)
}

%fused_computation.112 (param_0.641: f32[1,512,25,25], param_1.917: f32[1,512,25,25], param_2.813: f32[1,512,25,25]) -> (f32[512], f32[512], f32[1,512,25,25]) {
  %param_2.813 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %constant_181 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.276.clone.1 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_181), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.33.clone.1 = pred[1,512,25,25]{3,2,1,0} compare(f32[1,512,25,25]{3,2,1,0} %param_2.813, f32[1,512,25,25]{3,2,1,0} %broadcast.276.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/conv1/Relu_grad/ReluGrad"}
  %param_1.917 = f32[1,512,25,25]{3,2,1,0} parameter(1)
  %select.33.clone.1 = f32[1,512,25,25]{3,2,1,0} select(pred[1,512,25,25]{3,2,1,0} %compare.33.clone.1, f32[1,512,25,25]{3,2,1,0} %param_1.917, f32[1,512,25,25]{3,2,1,0} %broadcast.276.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/conv1/Relu_grad/ReluGrad"}
  %param_0.641 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %multiply.159 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %select.33.clone.1, f32[1,512,25,25]{3,2,1,0} %param_0.641), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.49 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %multiply.159, f32[] %constant_181), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3884, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.104 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %select.33.clone.1, f32[] %constant_181), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3894, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.204 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) tuple(f32[512]{0} %reduce.49, f32[512]{0} %reduce.104, f32[1,512,25,25]{3,2,1,0} %select.33.clone.1)
}

%fused_computation.113 (param_0.301: f32[1,1,2048,512], param_1.726: f32[1,1,2048,512]) -> f32[1,1,2048,512] {
  %param_0.301 = f32[1,1,2048,512]{1,0,2,3} parameter(0)
  %param_1.726 = f32[1,1,2048,512]{3,2,1,0} parameter(1)
  %copy.424 = f32[1,1,2048,512]{1,0,2,3} copy(f32[1,1,2048,512]{3,2,1,0} %param_1.726), metadata={op_name="XLA_Args"}
  %constant_244 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.237 = f32[1,1,2048,512]{1,0,2,3} broadcast(f32[] %constant_244), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_39_grad/Mul"}
  %multiply.160 = f32[1,1,2048,512]{1,0,2,3} multiply(f32[1,1,2048,512]{1,0,2,3} %copy.424, f32[1,1,2048,512]{1,0,2,3} %broadcast.237), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_36_grad/Mul_1"}
  %add.84 = f32[1,1,2048,512]{1,0,2,3} add(f32[1,1,2048,512]{1,0,2,3} %param_0.301, f32[1,1,2048,512]{1,0,2,3} %multiply.160), metadata={op_type="AddN" op_name="tower0/gradients/AddN_122"}
  ROOT %copy.423 = f32[1,1,2048,512]{3,2,1,0} copy(f32[1,1,2048,512]{1,0,2,3} %add.84), metadata={op_name="XLA_Retvals"}
}

%fused_computation.114 (param_0.304: f32[1,512,1,1], param_1.325: f32[1,512,1,1], param_2.248: f32[512], param_3.139: f32[512]) -> f32[512] {
  %param_3.139 = f32[512]{0} parameter(3)
  %bitcast.339 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.139), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.248 = f32[512]{0} parameter(2)
  %negate.86 = f32[512]{0} negate(f32[512]{0} %param_2.248), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.338 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.86), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.325 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.162 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.338, f32[1,512,1,1]{3,2,1,0} %param_1.325), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.85 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.339, f32[1,512,1,1]{3,2,1,0} %multiply.162), metadata={op_type="AddN" op_name="tower0/gradients/AddN_120"}
  %param_0.304 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.161 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.85, f32[1,512,1,1]{3,2,1,0} %param_0.304), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.337 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.161), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3911 (x.3912: f32[], y.3913: f32[]) -> f32[] {
  %x.3912 = f32[] parameter(0)
  %y.3913 = f32[] parameter(1)
  ROOT %add.3914 = f32[] add(f32[] %x.3912, f32[] %y.3913)
}

%tower0_gradients_tower0_group3_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3921 (x.3922: f32[], y.3923: f32[]) -> f32[] {
  %x.3922 = f32[] parameter(0)
  %y.3923 = f32[] parameter(1)
  ROOT %add.3924 = f32[] add(f32[] %x.3922, f32[] %y.3923)
}

%fused_computation.115 (param_0.640: f32[1,512,25,25], param_1.918: f32[1,512,25,25], param_2.814: f32[1,512,25,25]) -> (f32[512], f32[512], f32[1,512,25,25]) {
  %param_2.814 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %constant_182 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.277.clone.1 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_182), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.34.clone.1 = pred[1,512,25,25]{3,2,1,0} compare(f32[1,512,25,25]{3,2,1,0} %param_2.814, f32[1,512,25,25]{3,2,1,0} %broadcast.277.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/conv2/Relu_grad/ReluGrad"}
  %param_1.918 = f32[1,512,25,25]{3,2,1,0} parameter(1)
  %select.34.clone.1 = f32[1,512,25,25]{3,2,1,0} select(pred[1,512,25,25]{3,2,1,0} %compare.34.clone.1, f32[1,512,25,25]{3,2,1,0} %param_1.918, f32[1,512,25,25]{3,2,1,0} %broadcast.277.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/conv2/Relu_grad/ReluGrad"}
  %param_0.640 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %multiply.163 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %select.34.clone.1, f32[1,512,25,25]{3,2,1,0} %param_0.640), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.50 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %multiply.163, f32[] %constant_182), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3911, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.103 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %select.34.clone.1, f32[] %constant_182), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.3921, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.203 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) tuple(f32[512]{0} %reduce.50, f32[512]{0} %reduce.103, f32[1,512,25,25]{3,2,1,0} %select.34.clone.1)
}

%fused_computation.116 (param_0.309: f32[3,3,512,512], param_1.727: f32[3,3,512,512]) -> f32[3,3,512,512] {
  %param_0.309 = f32[3,3,512,512]{1,0,2,3} parameter(0)
  %param_1.727 = f32[3,3,512,512]{3,2,1,0} parameter(1)
  %copy.472 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %param_1.727), metadata={op_name="XLA_Args"}
  %constant_245 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.238 = f32[3,3,512,512]{1,0,2,3} broadcast(f32[] %constant_245), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_33_grad/Mul"}
  %multiply.164 = f32[3,3,512,512]{1,0,2,3} multiply(f32[3,3,512,512]{1,0,2,3} %copy.472, f32[3,3,512,512]{1,0,2,3} %broadcast.238), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_37_grad/Mul_1"}
  %add.86 = f32[3,3,512,512]{1,0,2,3} add(f32[3,3,512,512]{1,0,2,3} %param_0.309, f32[3,3,512,512]{1,0,2,3} %multiply.164), metadata={op_type="AddN" op_name="tower0/gradients/AddN_119"}
  ROOT %copy.425 = f32[3,3,512,512]{3,2,1,0} copy(f32[3,3,512,512]{1,0,2,3} %add.86), metadata={op_name="XLA_Retvals"}
}

%fused_computation.117 (param_0.312: f32[1,2048,1,1], param_1.333: f32[1,2048,1,1], param_2.254: f32[2048], param_3.143: f32[2048]) -> f32[2048] {
  %param_3.143 = f32[2048]{0} parameter(3)
  %bitcast.342 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_3.143), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.254 = f32[2048]{0} parameter(2)
  %negate.87 = f32[2048]{0} negate(f32[2048]{0} %param_2.254), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.341 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %negate.87), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.333 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %multiply.166 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.341, f32[1,2048,1,1]{3,2,1,0} %param_1.333), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.87 = f32[1,2048,1,1]{3,2,1,0} add(f32[1,2048,1,1]{3,2,1,0} %bitcast.342, f32[1,2048,1,1]{3,2,1,0} %multiply.166), metadata={op_type="AddN" op_name="tower0/gradients/AddN_118"}
  %param_0.312 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  %multiply.165 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %add.87, f32[1,2048,1,1]{3,2,1,0} %param_0.312), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.340 = f32[2048]{0} bitcast(f32[1,2048,1,1]{3,2,1,0} %multiply.165), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3938 (x.3939: f32[], y.3940: f32[]) -> f32[] {
  %x.3939 = f32[] parameter(0)
  %y.3940 = f32[] parameter(1)
  ROOT %add.3941 = f32[] add(f32[] %x.3939, f32[] %y.3940)
}

%tower0_gradients_tower0_group3_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3948 (x.3949: f32[], y.3950: f32[]) -> f32[] {
  %x.3949 = f32[] parameter(0)
  %y.3950 = f32[] parameter(1)
  ROOT %add.3951 = f32[] add(f32[] %x.3949, f32[] %y.3950)
}

%fused_computation.118 (param_0.639: f32[1,2048,25,25], param_1.919: f32[1,2048,25,25], param_2.815: f32[1,2048,25,25], param_3.533: f32[1,2048,25,25]) -> (f32[2048], f32[2048], f32[1,2048,25,25]) {
  %param_3.533 = f32[1,2048,25,25]{3,2,1,0} parameter(3)
  %constant_183 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.278.clone.1 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[] %constant_183), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %compare.35.clone.1 = pred[1,2048,25,25]{3,2,1,0} compare(f32[1,2048,25,25]{3,2,1,0} %param_3.533, f32[1,2048,25,25]{3,2,1,0} %broadcast.278.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/output_grad/ReluGrad"}
  %param_1.919 = f32[1,2048,25,25]{3,2,1,0} parameter(1)
  %param_2.815 = f32[1,2048,25,25]{3,2,1,0} parameter(2)
  %add.143.clone.1 = f32[1,2048,25,25]{3,2,1,0} add(f32[1,2048,25,25]{3,2,1,0} %param_1.919, f32[1,2048,25,25]{3,2,1,0} %param_2.815), metadata={op_type="AddN" op_name="tower0/gradients/AddN_114"}
  %select.35.clone.1 = f32[1,2048,25,25]{3,2,1,0} select(pred[1,2048,25,25]{3,2,1,0} %compare.35.clone.1, f32[1,2048,25,25]{3,2,1,0} %add.143.clone.1, f32[1,2048,25,25]{3,2,1,0} %broadcast.278.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block1/output_grad/ReluGrad"}
  %param_0.639 = f32[1,2048,25,25]{3,2,1,0} parameter(0)
  %multiply.167 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %select.35.clone.1, f32[1,2048,25,25]{3,2,1,0} %param_0.639), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.51 = f32[2048]{0} reduce(f32[1,2048,25,25]{3,2,1,0} %multiply.167, f32[] %constant_183), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.3938, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.90 = f32[2048]{0} reduce(f32[1,2048,25,25]{3,2,1,0} %select.35.clone.1, f32[] %constant_183), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block1_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.3948, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.202 = (f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) tuple(f32[2048]{0} %reduce.51, f32[2048]{0} %reduce.90, f32[1,2048,25,25]{3,2,1,0} %select.35.clone.1)
}

%fused_computation.119 (param_0.317: f32[1,1,512,2048], param_1.728: f32[1,1,512,2048]) -> f32[1,1,512,2048] {
  %param_0.317 = f32[1,1,512,2048]{1,0,2,3} parameter(0)
  %param_1.728 = f32[1,1,512,2048]{3,2,1,0} parameter(1)
  %copy.427 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %param_1.728), metadata={op_name="XLA_Args"}
  %constant_246 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.239 = f32[1,1,512,2048]{1,0,2,3} broadcast(f32[] %constant_246), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_41_grad/Mul"}
  %multiply.168 = f32[1,1,512,2048]{1,0,2,3} multiply(f32[1,1,512,2048]{1,0,2,3} %copy.427, f32[1,1,512,2048]{1,0,2,3} %broadcast.239), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_38_grad/Mul_1"}
  %add.88 = f32[1,1,512,2048]{1,0,2,3} add(f32[1,1,512,2048]{1,0,2,3} %param_0.317, f32[1,1,512,2048]{1,0,2,3} %multiply.168), metadata={op_type="AddN" op_name="tower0/gradients/AddN_117"}
  ROOT %copy.426 = f32[1,1,512,2048]{3,2,1,0} copy(f32[1,1,512,2048]{1,0,2,3} %add.88), metadata={op_name="XLA_Retvals"}
}

%fused_computation.120 (param_0.320: f32[1,512,1,1], param_1.342: f32[1,512,1,1], param_2.261: f32[512], param_3.147: f32[512]) -> f32[512] {
  %param_3.147 = f32[512]{0} parameter(3)
  %bitcast.345 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.147), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.261 = f32[512]{0} parameter(2)
  %negate.88 = f32[512]{0} negate(f32[512]{0} %param_2.261), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %bitcast.344 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.88), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/sub_grad/Neg"}
  %param_1.342 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.170 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.344, f32[1,512,1,1]{3,2,1,0} %param_1.342), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.89 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.345, f32[1,512,1,1]{3,2,1,0} %multiply.170), metadata={op_type="AddN" op_name="tower0/gradients/AddN_116"}
  %param_0.320 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.169 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.89, f32[1,512,1,1]{3,2,1,0} %param_0.320), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.343 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.169), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3965 (x.3966: f32[], y.3967: f32[]) -> f32[] {
  %x.3966 = f32[] parameter(0)
  %y.3967 = f32[] parameter(1)
  ROOT %add.3968 = f32[] add(f32[] %x.3966, f32[] %y.3967)
}

%tower0_gradients_tower0_group3_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3975 (x.3976: f32[], y.3977: f32[]) -> f32[] {
  %x.3976 = f32[] parameter(0)
  %y.3977 = f32[] parameter(1)
  ROOT %add.3978 = f32[] add(f32[] %x.3976, f32[] %y.3977)
}

%fused_computation.121 (param_0.638: f32[1,512,25,25], param_1.920: f32[1,512,25,25], param_2.816: f32[1,512,25,25]) -> (f32[512], f32[512], f32[1,512,25,25]) {
  %param_2.816 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %constant_184 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.279.clone.1 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_184), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.36.clone.1 = pred[1,512,25,25]{3,2,1,0} compare(f32[1,512,25,25]{3,2,1,0} %param_2.816, f32[1,512,25,25]{3,2,1,0} %broadcast.279.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv1/Relu_grad/ReluGrad"}
  %param_1.920 = f32[1,512,25,25]{3,2,1,0} parameter(1)
  %select.36.clone.1 = f32[1,512,25,25]{3,2,1,0} select(pred[1,512,25,25]{3,2,1,0} %compare.36.clone.1, f32[1,512,25,25]{3,2,1,0} %param_1.920, f32[1,512,25,25]{3,2,1,0} %broadcast.279.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv1/Relu_grad/ReluGrad"}
  %param_0.638 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %multiply.171 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %select.36.clone.1, f32[1,512,25,25]{3,2,1,0} %param_0.638), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.52 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %multiply.171, f32[] %constant_184), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv1_bn_batchnorm_mul_1_grad_Sum_1-reduction.3965, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.106 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %select.36.clone.1, f32[] %constant_184), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv1_bn_batchnorm_add_1_grad_Sum_1-reduction.3975, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.201 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) tuple(f32[512]{0} %reduce.52, f32[512]{0} %reduce.106, f32[1,512,25,25]{3,2,1,0} %select.36.clone.1)
}

%fused_computation.122 (param_0.325: f32[1,1,2048,512], param_1.729: f32[1,1,2048,512]) -> f32[1,1,2048,512] {
  %param_0.325 = f32[1,1,2048,512]{1,0,2,3} parameter(0)
  %param_1.729 = f32[1,1,2048,512]{3,2,1,0} parameter(1)
  %copy.429 = f32[1,1,2048,512]{1,0,2,3} copy(f32[1,1,2048,512]{3,2,1,0} %param_1.729), metadata={op_name="XLA_Args"}
  %constant_247 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.240 = f32[1,1,2048,512]{1,0,2,3} broadcast(f32[] %constant_247), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_39_grad/Mul"}
  %multiply.172 = f32[1,1,2048,512]{1,0,2,3} multiply(f32[1,1,2048,512]{1,0,2,3} %copy.429, f32[1,1,2048,512]{1,0,2,3} %broadcast.240), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_39_grad/Mul_1"}
  %add.90 = f32[1,1,2048,512]{1,0,2,3} add(f32[1,1,2048,512]{1,0,2,3} %param_0.325, f32[1,1,2048,512]{1,0,2,3} %multiply.172), metadata={op_type="AddN" op_name="tower0/gradients/AddN_115"}
  ROOT %copy.428 = f32[1,1,2048,512]{3,2,1,0} copy(f32[1,1,2048,512]{1,0,2,3} %add.90), metadata={op_name="XLA_Retvals"}
}

%fused_computation.123 (param_0.328: f32[1,512,1,1], param_1.351: f32[1,512,1,1], param_2.268: f32[512], param_3.151: f32[512]) -> f32[512] {
  %param_3.151 = f32[512]{0} parameter(3)
  %bitcast.348 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.151), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.268 = f32[512]{0} parameter(2)
  %negate.89 = f32[512]{0} negate(f32[512]{0} %param_2.268), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %bitcast.347 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %negate.89), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/sub_grad/Neg"}
  %param_1.351 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %multiply.174 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.347, f32[1,512,1,1]{3,2,1,0} %param_1.351), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.91 = f32[1,512,1,1]{3,2,1,0} add(f32[1,512,1,1]{3,2,1,0} %bitcast.348, f32[1,512,1,1]{3,2,1,0} %multiply.174), metadata={op_type="AddN" op_name="tower0/gradients/AddN_113"}
  %param_0.328 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %multiply.173 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %add.91, f32[1,512,1,1]{3,2,1,0} %param_0.328), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.346 = f32[512]{0} bitcast(f32[1,512,1,1]{3,2,1,0} %multiply.173), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3992 (x.3993: f32[], y.3994: f32[]) -> f32[] {
  %x.3993 = f32[] parameter(0)
  %y.3994 = f32[] parameter(1)
  ROOT %add.3995 = f32[] add(f32[] %x.3993, f32[] %y.3994)
}

%tower0_gradients_tower0_group3_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.4002 (x.4003: f32[], y.4004: f32[]) -> f32[] {
  %x.4003 = f32[] parameter(0)
  %y.4004 = f32[] parameter(1)
  ROOT %add.4005 = f32[] add(f32[] %x.4003, f32[] %y.4004)
}

%fused_computation.124 (param_0.637: f32[1,512,25,25], param_1.921: f32[1,512,25,25], param_2.817: f32[1,512,25,25]) -> (f32[512], f32[512], f32[1,512,25,25]) {
  %param_2.817 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %constant_185 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.280.clone.1 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_185), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %compare.37.clone.1 = pred[1,512,25,25]{3,2,1,0} compare(f32[1,512,25,25]{3,2,1,0} %param_2.817, f32[1,512,25,25]{3,2,1,0} %broadcast.280.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %param_1.921 = f32[1,512,25,25]{3,2,1,0} parameter(1)
  %select.37.clone.1 = f32[1,512,25,25]{3,2,1,0} select(pred[1,512,25,25]{3,2,1,0} %compare.37.clone.1, f32[1,512,25,25]{3,2,1,0} %param_1.921, f32[1,512,25,25]{3,2,1,0} %broadcast.280.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/conv2/Relu_grad/ReluGrad"}
  %param_0.637 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %multiply.175 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %select.37.clone.1, f32[1,512,25,25]{3,2,1,0} %param_0.637), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.53 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %multiply.175, f32[] %constant_185), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv2_bn_batchnorm_mul_1_grad_Sum_1-reduction.3992, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.105 = f32[512]{0} reduce(f32[1,512,25,25]{3,2,1,0} %select.37.clone.1, f32[] %constant_185), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv2_bn_batchnorm_add_1_grad_Sum_1-reduction.4002, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.200 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) tuple(f32[512]{0} %reduce.53, f32[512]{0} %reduce.105, f32[1,512,25,25]{3,2,1,0} %select.37.clone.1)
}

%fused_computation.125 (param_0.333: f32[3,3,512,512], param_1.730: f32[3,3,512,512]) -> f32[3,3,512,512] {
  %param_0.333 = f32[3,3,512,512]{1,0,2,3} parameter(0)
  %param_1.730 = f32[3,3,512,512]{3,2,1,0} parameter(1)
  %copy.473 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %param_1.730), metadata={op_name="XLA_Args"}
  %constant_248 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.241 = f32[3,3,512,512]{1,0,2,3} broadcast(f32[] %constant_248), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_33_grad/Mul"}
  %multiply.176 = f32[3,3,512,512]{1,0,2,3} multiply(f32[3,3,512,512]{1,0,2,3} %copy.473, f32[3,3,512,512]{1,0,2,3} %broadcast.241), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_40_grad/Mul_1"}
  %add.92 = f32[3,3,512,512]{1,0,2,3} add(f32[3,3,512,512]{1,0,2,3} %param_0.333, f32[3,3,512,512]{1,0,2,3} %multiply.176), metadata={op_type="AddN" op_name="tower0/gradients/AddN_112"}
  ROOT %copy.430 = f32[3,3,512,512]{3,2,1,0} copy(f32[3,3,512,512]{1,0,2,3} %add.92), metadata={op_name="XLA_Retvals"}
}

%fused_computation.126 (param_0.336: f32[1,2048,1,1], param_1.359: f32[1,2048,1,1], param_2.274: f32[2048], param_3.155: f32[2048]) -> f32[2048] {
  %param_3.155 = f32[2048]{0} parameter(3)
  %bitcast.351 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_3.155), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Reshape_1"}
  %param_2.274 = f32[2048]{0} parameter(2)
  %negate.90 = f32[2048]{0} negate(f32[2048]{0} %param_2.274), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %bitcast.350 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %negate.90), metadata={op_type="Neg" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/sub_grad/Neg"}
  %param_1.359 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %multiply.178 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.350, f32[1,2048,1,1]{3,2,1,0} %param_1.359), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_2_grad/Mul_1"}
  %add.93 = f32[1,2048,1,1]{3,2,1,0} add(f32[1,2048,1,1]{3,2,1,0} %bitcast.351, f32[1,2048,1,1]{3,2,1,0} %multiply.178), metadata={op_type="AddN" op_name="tower0/gradients/AddN_111"}
  %param_0.336 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  %multiply.177 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %add.93, f32[1,2048,1,1]{3,2,1,0} %param_0.336), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_grad/Mul_1"}
  ROOT %bitcast.349 = f32[2048]{0} bitcast(f32[1,2048,1,1]{3,2,1,0} %multiply.177), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/Reshape_grad/Reshape"}
}

%tower0_gradients_tower0_group3_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.4019 (x.4020: f32[], y.4021: f32[]) -> f32[] {
  %x.4020 = f32[] parameter(0)
  %y.4021 = f32[] parameter(1)
  ROOT %add.4022 = f32[] add(f32[] %x.4020, f32[] %y.4021)
}

%tower0_gradients_tower0_group3_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.4029 (x.4030: f32[], y.4031: f32[]) -> f32[] {
  %x.4030 = f32[] parameter(0)
  %y.4031 = f32[] parameter(1)
  ROOT %add.4032 = f32[] add(f32[] %x.4030, f32[] %y.4031)
}

%fused_computation.127 (param_0.636: f32[1,2048,25,25], param_1.922: f32[1,2048,25,25], param_2.818: f32[1,2048,25,25]) -> (f32[2048], f32[2048], f32[1,2048,25,25]) {
  %param_2.818 = f32[1,2048,25,25]{3,2,1,0} parameter(2)
  %constant_186 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.281.clone.1 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[] %constant_186), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %compare.38.clone.1 = pred[1,2048,25,25]{3,2,1,0} compare(f32[1,2048,25,25]{3,2,1,0} %param_2.818, f32[1,2048,25,25]{3,2,1,0} %broadcast.281.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %param_1.922 = f32[1,2048,25,25]{3,2,1,0} parameter(1)
  %select.38.clone.1 = f32[1,2048,25,25]{3,2,1,0} select(pred[1,2048,25,25]{3,2,1,0} %compare.38.clone.1, f32[1,2048,25,25]{3,2,1,0} %param_1.922, f32[1,2048,25,25]{3,2,1,0} %broadcast.281.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/group3/block2/output_grad/ReluGrad"}
  %param_0.636 = f32[1,2048,25,25]{3,2,1,0} parameter(0)
  %multiply.179 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %select.38.clone.1, f32[1,2048,25,25]{3,2,1,0} %param_0.636), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul_1"}
  %reduce.54 = f32[2048]{0} reduce(f32[1,2048,25,25]{3,2,1,0} %multiply.179, f32[] %constant_186), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv3_bn_batchnorm_mul_1_grad_Sum_1-reduction.4019, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %reduce.80 = f32[2048]{0} reduce(f32[1,2048,25,25]{3,2,1,0} %select.38.clone.1, f32[] %constant_186), dimensions={0,2,3}, to_apply=%tower0_gradients_tower0_group3_block2_conv3_bn_batchnorm_add_1_grad_Sum_1-reduction.4029, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/add_1_grad/Sum_1"}
  ROOT %tuple.199 = (f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) tuple(f32[2048]{0} %reduce.54, f32[2048]{0} %reduce.80, f32[1,2048,25,25]{3,2,1,0} %select.38.clone.1)
}

%fused_computation.128 (param_0.341: f32[1,1,512,2048], param_1.731: f32[1,1,512,2048]) -> f32[1,1,512,2048] {
  %param_0.341 = f32[1,1,512,2048]{1,0,2,3} parameter(0)
  %param_1.731 = f32[1,1,512,2048]{3,2,1,0} parameter(1)
  %copy.432 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %param_1.731), metadata={op_name="XLA_Args"}
  %constant_249 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.242 = f32[1,1,512,2048]{1,0,2,3} broadcast(f32[] %constant_249), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_41_grad/Mul"}
  %multiply.180 = f32[1,1,512,2048]{1,0,2,3} multiply(f32[1,1,512,2048]{1,0,2,3} %copy.432, f32[1,1,512,2048]{1,0,2,3} %broadcast.242), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_41_grad/Mul_1"}
  %add.94 = f32[1,1,512,2048]{1,0,2,3} add(f32[1,1,512,2048]{1,0,2,3} %param_0.341, f32[1,1,512,2048]{1,0,2,3} %multiply.180), metadata={op_type="AddN" op_name="tower0/gradients/AddN_110"}
  ROOT %copy.431 = f32[1,1,512,2048]{3,2,1,0} copy(f32[1,1,512,2048]{1,0,2,3} %add.94), metadata={op_name="XLA_Retvals"}
}

%fused_computation.129 (param_0.343: f32[1,1,2048,256], param_1.369: f32[1,1,2048,256]) -> f32[1,1,2048,256] {
  %param_0.343 = f32[1,1,2048,256]{1,0,2,3} parameter(0)
  %param_1.369 = f32[1,1,2048,256]{3,2,1,0} parameter(1)
  %copy.434 = f32[1,1,2048,256]{1,0,2,3} copy(f32[1,1,2048,256]{3,2,1,0} %param_1.369), metadata={op_name="XLA_Args"}
  %constant_187 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.103 = f32[1,1,2048,256]{1,0,2,3} broadcast(f32[] %constant_187), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_45_grad/Mul"}
  %multiply.181 = f32[1,1,2048,256]{1,0,2,3} multiply(f32[1,1,2048,256]{1,0,2,3} %copy.434, f32[1,1,2048,256]{1,0,2,3} %broadcast.103), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_45_grad/Mul_1"}
  %add.95 = f32[1,1,2048,256]{1,0,2,3} add(f32[1,1,2048,256]{1,0,2,3} %param_0.343, f32[1,1,2048,256]{1,0,2,3} %multiply.181), metadata={op_type="AddN" op_name="tower0/gradients/AddN_109"}
  ROOT %copy.433 = f32[1,1,2048,256]{3,2,1,0} copy(f32[1,1,2048,256]{1,0,2,3} %add.95), metadata={op_name="XLA_Retvals"}
}

%fused_computation.130 (param_0.345: f32[1,1,1024,256], param_1.732: f32[1,1,1024,256]) -> f32[1,1,1024,256] {
  %param_0.345 = f32[1,1,1024,256]{1,0,2,3} parameter(0)
  %param_1.732 = f32[1,1,1024,256]{3,2,1,0} parameter(1)
  %copy.436 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %param_1.732), metadata={op_name="XLA_Args"}
  %constant_250 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.243 = f32[1,1,1024,256]{1,0,2,3} broadcast(f32[] %constant_250), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_29_grad/Mul"}
  %multiply.182 = f32[1,1,1024,256]{1,0,2,3} multiply(f32[1,1,1024,256]{1,0,2,3} %copy.436, f32[1,1,1024,256]{1,0,2,3} %broadcast.243), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_44_grad/Mul_1"}
  %add.96 = f32[1,1,1024,256]{1,0,2,3} add(f32[1,1,1024,256]{1,0,2,3} %param_0.345, f32[1,1,1024,256]{1,0,2,3} %multiply.182), metadata={op_type="AddN" op_name="tower0/gradients/AddN_107"}
  ROOT %copy.435 = f32[1,1,1024,256]{3,2,1,0} copy(f32[1,1,1024,256]{1,0,2,3} %add.96), metadata={op_name="XLA_Retvals"}
}

%fused_computation.131 (param_0.347: f32[1,1,512,256], param_1.733: f32[1,1,512,256]) -> f32[1,1,512,256] {
  %param_0.347 = f32[1,1,512,256]{1,0,2,3} parameter(0)
  %param_1.733 = f32[1,1,512,256]{3,2,1,0} parameter(1)
  %copy.438 = f32[1,1,512,256]{1,0,2,3} copy(f32[1,1,512,256]{3,2,1,0} %param_1.733), metadata={op_name="XLA_Args"}
  %constant_251 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.244 = f32[1,1,512,256]{1,0,2,3} broadcast(f32[] %constant_251), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_13_grad/Mul"}
  %multiply.183 = f32[1,1,512,256]{1,0,2,3} multiply(f32[1,1,512,256]{1,0,2,3} %copy.438, f32[1,1,512,256]{1,0,2,3} %broadcast.244), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_43_grad/Mul_1"}
  %add.97 = f32[1,1,512,256]{1,0,2,3} add(f32[1,1,512,256]{1,0,2,3} %param_0.347, f32[1,1,512,256]{1,0,2,3} %multiply.183), metadata={op_type="AddN" op_name="tower0/gradients/AddN_105"}
  ROOT %copy.437 = f32[1,1,512,256]{3,2,1,0} copy(f32[1,1,512,256]{1,0,2,3} %add.97), metadata={op_name="XLA_Retvals"}
}

%fused_computation.132 (param_0.349: f32[1,1,256,256], param_1.379: f32[1,1,256,256]) -> f32[1,1,256,256] {
  %param_0.349 = f32[1,1,256,256]{1,0,2,3} parameter(0)
  %param_1.379 = f32[1,1,256,256]{3,2,1,0} parameter(1)
  %copy.440 = f32[1,1,256,256]{1,0,2,3} copy(f32[1,1,256,256]{3,2,1,0} %param_1.379), metadata={op_name="XLA_Args"}
  %constant_188 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.104 = f32[1,1,256,256]{1,0,2,3} broadcast(f32[] %constant_188), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_42_grad/Mul"}
  %multiply.184 = f32[1,1,256,256]{1,0,2,3} multiply(f32[1,1,256,256]{1,0,2,3} %copy.440, f32[1,1,256,256]{1,0,2,3} %broadcast.104), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_42_grad/Mul_1"}
  %add.98 = f32[1,1,256,256]{1,0,2,3} add(f32[1,1,256,256]{1,0,2,3} %param_0.349, f32[1,1,256,256]{1,0,2,3} %multiply.184), metadata={op_type="AddN" op_name="tower0/gradients/AddN_103"}
  ROOT %copy.439 = f32[1,1,256,256]{3,2,1,0} copy(f32[1,1,256,256]{1,0,2,3} %add.98), metadata={op_name="XLA_Retvals"}
}

%fused_computation.138 (param_0.362: f32[256], param_1.393: f32[256], param_2.297: f32[256], param_3.160: f32[256], param_4.3: f32[256]) -> f32[256] {
  %param_3.160 = f32[256]{0} parameter(3)
  %param_4.3 = f32[256]{0} parameter(4)
  %add.111 = f32[256]{0} add(f32[256]{0} %param_3.160, f32[256]{0} %param_4.3), metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
  %param_2.297 = f32[256]{0} parameter(2)
  %add.110 = f32[256]{0} add(f32[256]{0} %add.111, f32[256]{0} %param_2.297), metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
  %param_1.393 = f32[256]{0} parameter(1)
  %add.109 = f32[256]{0} add(f32[256]{0} %add.110, f32[256]{0} %param_1.393), metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
  %param_0.362 = f32[256]{0} parameter(0)
  ROOT %add.108 = f32[256]{0} add(f32[256]{0} %add.109, f32[256]{0} %param_0.362), metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
}

%fused_computation.139 (param_0.366: f32[1,1,256,12], param_1.398: f32[1,1,256,12], param_2.303: f32[1,1,256,12], param_3.164: f32[1,1,256,12], param_4.7: f32[1,1,256,12], param_5.4: f32[1,1,256,12]) -> f32[1,1,256,12] {
  %param_4.7 = f32[1,1,256,12]{1,0,2,3} parameter(4)
  %param_5.4 = f32[1,1,256,12]{1,0,2,3} parameter(5)
  %add.116 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %param_4.7, f32[1,1,256,12]{1,0,2,3} %param_5.4), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  %param_3.164 = f32[1,1,256,12]{1,0,2,3} parameter(3)
  %add.115 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %add.116, f32[1,1,256,12]{1,0,2,3} %param_3.164), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  %param_2.303 = f32[1,1,256,12]{1,0,2,3} parameter(2)
  %add.114 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %add.115, f32[1,1,256,12]{1,0,2,3} %param_2.303), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  %param_1.398 = f32[1,1,256,12]{1,0,2,3} parameter(1)
  %add.113 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %add.114, f32[1,1,256,12]{1,0,2,3} %param_1.398), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  %param_0.366 = f32[1,1,256,12]{3,2,1,0} parameter(0)
  %copy.447 = f32[1,1,256,12]{1,0,2,3} copy(f32[1,1,256,12]{3,2,1,0} %param_0.366), metadata={op_name="XLA_Args"}
  %constant_189 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.105 = f32[1,1,256,12]{1,0,2,3} broadcast(f32[] %constant_189), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_52_grad/Mul"}
  %multiply.190 = f32[1,1,256,12]{1,0,2,3} multiply(f32[1,1,256,12]{1,0,2,3} %copy.447, f32[1,1,256,12]{1,0,2,3} %broadcast.105), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_52_grad/Mul_1"}
  %add.112 = f32[1,1,256,12]{1,0,2,3} add(f32[1,1,256,12]{1,0,2,3} %add.113, f32[1,1,256,12]{1,0,2,3} %multiply.190), metadata={op_type="AddN" op_name="tower0/gradients/AddN_92"}
  ROOT %copy.446 = f32[1,1,256,12]{3,2,1,0} copy(f32[1,1,256,12]{1,0,2,3} %add.112), metadata={op_name="XLA_Retvals"}
}

%fused_computation.140 (param_0.368: f32[12], param_1.401: f32[12], param_2.306: f32[12], param_3.166: f32[12], param_4.8: f32[12]) -> f32[12] {
  %param_3.166 = f32[12]{0} parameter(3)
  %param_4.8 = f32[12]{0} parameter(4)
  %add.120 = f32[12]{0} add(f32[12]{0} %param_3.166, f32[12]{0} %param_4.8), metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
  %param_2.306 = f32[12]{0} parameter(2)
  %add.119 = f32[12]{0} add(f32[12]{0} %add.120, f32[12]{0} %param_2.306), metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
  %param_1.401 = f32[12]{0} parameter(1)
  %add.118 = f32[12]{0} add(f32[12]{0} %add.119, f32[12]{0} %param_1.401), metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
  %param_0.368 = f32[12]{0} parameter(0)
  ROOT %add.117 = f32[12]{0} add(f32[12]{0} %add.118, f32[12]{0} %param_0.368), metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
}

%fused_computation.141 (param_0.371: f32[1,1,256,3], param_1.406: f32[1,1,256,3], param_2.311: f32[1,1,256,3], param_3.169: f32[1,1,256,3], param_4.10: f32[1,1,256,3], param_5.5: f32[1,1,256,3]) -> f32[1,1,256,3] {
  %param_4.10 = f32[1,1,256,3]{1,0,2,3} parameter(4)
  %param_5.5 = f32[1,1,256,3]{1,0,2,3} parameter(5)
  %add.125 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %param_4.10, f32[1,1,256,3]{1,0,2,3} %param_5.5), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  %param_3.169 = f32[1,1,256,3]{1,0,2,3} parameter(3)
  %add.124 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %add.125, f32[1,1,256,3]{1,0,2,3} %param_3.169), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  %param_2.311 = f32[1,1,256,3]{1,0,2,3} parameter(2)
  %add.123 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %add.124, f32[1,1,256,3]{1,0,2,3} %param_2.311), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  %param_1.406 = f32[1,1,256,3]{1,0,2,3} parameter(1)
  %add.122 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %add.123, f32[1,1,256,3]{1,0,2,3} %param_1.406), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  %param_0.371 = f32[1,1,256,3]{3,2,1,0} parameter(0)
  %copy.449 = f32[1,1,256,3]{1,0,2,3} copy(f32[1,1,256,3]{3,2,1,0} %param_0.371), metadata={op_name="XLA_Args"}
  %constant_190 = f32[] constant(0.0001), metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_10_grad/Mul"}
  %broadcast.106 = f32[1,1,256,3]{1,0,2,3} broadcast(f32[] %constant_190), dimensions={}, metadata={op_type="Const" op_name="tower0/gradients/tower0/wd_cost_internals/Square_51_grad/Mul"}
  %multiply.191 = f32[1,1,256,3]{1,0,2,3} multiply(f32[1,1,256,3]{1,0,2,3} %copy.449, f32[1,1,256,3]{1,0,2,3} %broadcast.106), metadata={op_type="Mul" op_name="tower0/gradients/tower0/wd_cost_internals/Square_51_grad/Mul_1"}
  %add.121 = f32[1,1,256,3]{1,0,2,3} add(f32[1,1,256,3]{1,0,2,3} %add.122, f32[1,1,256,3]{1,0,2,3} %multiply.191), metadata={op_type="AddN" op_name="tower0/gradients/AddN_85"}
  ROOT %copy.448 = f32[1,1,256,3]{3,2,1,0} copy(f32[1,1,256,3]{1,0,2,3} %add.121), metadata={op_name="XLA_Retvals"}
}

%fused_computation.142 (param_0.373: f32[3], param_1.409: f32[3], param_2.314: f32[3], param_3.171: f32[3], param_4.11: f32[3]) -> f32[3] {
  %param_3.171 = f32[3]{0} parameter(3)
  %param_4.11 = f32[3]{0} parameter(4)
  %add.129 = f32[3]{0} add(f32[3]{0} %param_3.171, f32[3]{0} %param_4.11), metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
  %param_2.314 = f32[3]{0} parameter(2)
  %add.128 = f32[3]{0} add(f32[3]{0} %add.129, f32[3]{0} %param_2.314), metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
  %param_1.409 = f32[3]{0} parameter(1)
  %add.127 = f32[3]{0} add(f32[3]{0} %add.128, f32[3]{0} %param_1.409), metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
  %param_0.373 = f32[3]{0} parameter(0)
  ROOT %add.126 = f32[3]{0} add(f32[3]{0} %add.127, f32[3]{0} %param_0.373), metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
}

%fused_computation.144 (param_0.375: f32[1,128,200,200], param_1.415: f32[1,128,1,1]) -> f32[1,128,200,200] {
  %param_0.375 = f32[1,128,200,200]{3,2,1,0} parameter(0)
  %param_1.415 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.353 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.415), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.108 = f32[1,128,200,200]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.353), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.193 = f32[1,128,200,200]{3,2,1,0} multiply(f32[1,128,200,200]{3,2,1,0} %param_0.375, f32[1,128,200,200]{3,2,1,0} %broadcast.108), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.146 (param_0.379: f32[1,128,100,100], param_1.422: f32[1,128,1,1]) -> f32[1,128,100,100] {
  %param_0.379 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %param_1.422 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.354 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.422), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.110 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.354), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.194 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_0.379, f32[1,128,100,100]{3,2,1,0} %broadcast.110), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.148 (param_0.382: f32[1,512,100,100], param_1.427: f32[1,512,1,1], param_2.602: f32[1,512,1,1]) -> (f32[1,512,100,100], f32[1,512,100,100]) {
  %param_0.382 = f32[1,512,100,100]{3,2,1,0} parameter(0)
  %param_1.427 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.355 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.427), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.111 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.355), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.195 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_0.382, f32[1,512,100,100]{3,2,1,0} %broadcast.111), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %param_2.602 = f32[1,512,1,1]{3,2,1,0} parameter(2)
  %bitcast.352.clone.1 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_2.602), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.107.clone.1 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.352.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.192.clone.1 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_0.382, f32[1,512,100,100]{3,2,1,0} %broadcast.107.clone.1), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %tuple.129 = (f32[1,512,100,100]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}) tuple(f32[1,512,100,100]{3,2,1,0} %multiply.195, f32[1,512,100,100]{3,2,1,0} %multiply.192.clone.1)
}

%fused_computation.150 (param_0.385: f32[1,128,100,100], param_1.433: f32[1,128,1,1]) -> f32[1,128,100,100] {
  %param_0.385 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %param_1.433 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.356 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.433), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.112 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.356), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.196 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_0.385, f32[1,128,100,100]{3,2,1,0} %broadcast.112), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.152 (param_0.388: f32[1,128,100,100], param_1.438: f32[1,128,1,1]) -> f32[1,128,100,100] {
  %param_0.388 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %param_1.438 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.357 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.438), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.113 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.357), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.197 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_0.388, f32[1,128,100,100]{3,2,1,0} %broadcast.113), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.154 (param_0.391: f32[1,512,100,100], param_1.443: f32[1,512,1,1]) -> f32[1,512,100,100] {
  %param_0.391 = f32[1,512,100,100]{3,2,1,0} parameter(0)
  %param_1.443 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.358 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.443), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.114 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.358), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.198 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_0.391, f32[1,512,100,100]{3,2,1,0} %broadcast.114), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.156 (param_0.394: f32[1,128,100,100], param_1.449: f32[1,128,1,1]) -> f32[1,128,100,100] {
  %param_0.394 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %param_1.449 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.359 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.449), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.115 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.359), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.199 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_0.394, f32[1,128,100,100]{3,2,1,0} %broadcast.115), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.158 (param_0.397: f32[1,128,100,100], param_1.454: f32[1,128,1,1]) -> f32[1,128,100,100] {
  %param_0.397 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %param_1.454 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.360 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.454), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.116 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.360), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.200 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_0.397, f32[1,128,100,100]{3,2,1,0} %broadcast.116), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.160 (param_0.400: f32[1,512,100,100], param_1.459: f32[1,512,1,1]) -> f32[1,512,100,100] {
  %param_0.400 = f32[1,512,100,100]{3,2,1,0} parameter(0)
  %param_1.459 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.361 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.459), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.117 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.361), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.201 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_0.400, f32[1,512,100,100]{3,2,1,0} %broadcast.117), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.162 (param_0.403: f32[1,128,100,100], param_1.465: f32[1,128,1,1]) -> f32[1,128,100,100] {
  %param_0.403 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %param_1.465 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.362 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.465), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.118 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.362), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.202 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_0.403, f32[1,128,100,100]{3,2,1,0} %broadcast.118), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.164 (param_0.406: f32[1,128,100,100], param_1.470: f32[1,128,1,1]) -> f32[1,128,100,100] {
  %param_0.406 = f32[1,128,100,100]{3,2,1,0} parameter(0)
  %param_1.470 = f32[1,128,1,1]{3,2,1,0} parameter(1)
  %bitcast.363 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_1.470), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.119 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.363), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.203 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_0.406, f32[1,128,100,100]{3,2,1,0} %broadcast.119), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.166 (param_0.409: f32[1,512,100,100], param_1.475: f32[1,512,1,1]) -> f32[1,512,100,100] {
  %param_0.409 = f32[1,512,100,100]{3,2,1,0} parameter(0)
  %param_1.475 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.364 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.475), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.120 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.364), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.204 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_0.409, f32[1,512,100,100]{3,2,1,0} %broadcast.120), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.169 (param_0.413: f32[1,256,100,100], param_1.484: f32[1,256,1,1]) -> f32[1,256,100,100] {
  %param_0.413 = f32[1,256,100,100]{3,2,1,0} parameter(0)
  %param_1.484 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.366 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.484), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.122 = f32[1,256,100,100]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.366), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.206 = f32[1,256,100,100]{3,2,1,0} multiply(f32[1,256,100,100]{3,2,1,0} %param_0.413, f32[1,256,100,100]{3,2,1,0} %broadcast.122), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.171 (param_0.416: f32[1,256,50,50], param_1.490: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.416 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.490 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.367 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.490), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.123 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.367), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.207 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.416, f32[1,256,50,50]{3,2,1,0} %broadcast.123), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.173 (param_0.419: f32[1,1024,50,50], param_1.495: f32[1,1024,1,1], param_2.611: f32[1,1024,1,1]) -> (f32[1,1024,50,50], f32[1,1024,50,50]) {
  %param_0.419 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %param_1.495 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.368 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.495), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.124 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.368), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.208 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_0.419, f32[1,1024,50,50]{3,2,1,0} %broadcast.124), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %param_2.611 = f32[1,1024,1,1]{3,2,1,0} parameter(2)
  %bitcast.365.clone.1 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_2.611), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.121.clone.1 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.365.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.205.clone.1 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_0.419, f32[1,1024,50,50]{3,2,1,0} %broadcast.121.clone.1), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %tuple.137 = (f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}) tuple(f32[1,1024,50,50]{3,2,1,0} %multiply.208, f32[1,1024,50,50]{3,2,1,0} %multiply.205.clone.1)
}

%fused_computation.175 (param_0.422: f32[1,256,50,50], param_1.501: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.422 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.501 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.369 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.501), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.125 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.369), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.209 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.422, f32[1,256,50,50]{3,2,1,0} %broadcast.125), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.177 (param_0.425: f32[1,256,50,50], param_1.506: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.425 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.506 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.370 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.506), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.126 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.370), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.210 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.425, f32[1,256,50,50]{3,2,1,0} %broadcast.126), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.179 (param_0.428: f32[1,1024,50,50], param_1.511: f32[1,1024,1,1]) -> f32[1,1024,50,50] {
  %param_0.428 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %param_1.511 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.371 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.511), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.127 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.371), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.211 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_0.428, f32[1,1024,50,50]{3,2,1,0} %broadcast.127), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.181 (param_0.431: f32[1,256,50,50], param_1.517: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.431 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.517 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.372 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.517), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.128 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.372), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.212 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.431, f32[1,256,50,50]{3,2,1,0} %broadcast.128), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.183 (param_0.434: f32[1,256,50,50], param_1.522: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.434 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.522 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.373 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.522), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.129 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.373), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.213 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.434, f32[1,256,50,50]{3,2,1,0} %broadcast.129), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.185 (param_0.437: f32[1,1024,50,50], param_1.527: f32[1,1024,1,1]) -> f32[1,1024,50,50] {
  %param_0.437 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %param_1.527 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.374 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.527), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.130 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.374), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.214 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_0.437, f32[1,1024,50,50]{3,2,1,0} %broadcast.130), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.187 (param_0.440: f32[1,256,50,50], param_1.533: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.440 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.533 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.375 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.533), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.131 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.375), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.215 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.440, f32[1,256,50,50]{3,2,1,0} %broadcast.131), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.189 (param_0.443: f32[1,256,50,50], param_1.538: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.443 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.538 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.376 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.538), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.132 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.376), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.216 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.443, f32[1,256,50,50]{3,2,1,0} %broadcast.132), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.191 (param_0.446: f32[1,1024,50,50], param_1.543: f32[1,1024,1,1]) -> f32[1,1024,50,50] {
  %param_0.446 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %param_1.543 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.377 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.543), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.133 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.377), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.217 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_0.446, f32[1,1024,50,50]{3,2,1,0} %broadcast.133), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.193 (param_0.449: f32[1,256,50,50], param_1.549: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.449 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.549 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.378 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.549), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.134 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.378), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.218 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.449, f32[1,256,50,50]{3,2,1,0} %broadcast.134), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.195 (param_0.452: f32[1,256,50,50], param_1.554: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.452 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.554 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.379 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.554), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.135 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.379), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.219 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.452, f32[1,256,50,50]{3,2,1,0} %broadcast.135), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.197 (param_0.455: f32[1,1024,50,50], param_1.559: f32[1,1024,1,1]) -> f32[1,1024,50,50] {
  %param_0.455 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %param_1.559 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.380 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.559), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.136 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.380), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.220 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_0.455, f32[1,1024,50,50]{3,2,1,0} %broadcast.136), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.199 (param_0.458: f32[1,256,50,50], param_1.565: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.458 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.565 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.381 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.565), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.137 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.381), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.221 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.458, f32[1,256,50,50]{3,2,1,0} %broadcast.137), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.201 (param_0.461: f32[1,256,50,50], param_1.570: f32[1,256,1,1]) -> f32[1,256,50,50] {
  %param_0.461 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.570 = f32[1,256,1,1]{3,2,1,0} parameter(1)
  %bitcast.382 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_1.570), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.138 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.382), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.222 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_0.461, f32[1,256,50,50]{3,2,1,0} %broadcast.138), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.203 (param_0.464: f32[1,1024,50,50], param_1.575: f32[1,1024,1,1]) -> f32[1,1024,50,50] {
  %param_0.464 = f32[1,1024,50,50]{3,2,1,0} parameter(0)
  %param_1.575 = f32[1,1024,1,1]{3,2,1,0} parameter(1)
  %bitcast.383 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_1.575), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.139 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.383), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.223 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_0.464, f32[1,1024,50,50]{3,2,1,0} %broadcast.139), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.206 (param_0.468: f32[1,512,50,50], param_1.584: f32[1,512,1,1]) -> f32[1,512,50,50] {
  %param_0.468 = f32[1,512,50,50]{3,2,1,0} parameter(0)
  %param_1.584 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.385 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.584), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.141 = f32[1,512,50,50]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.385), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.225 = f32[1,512,50,50]{3,2,1,0} multiply(f32[1,512,50,50]{3,2,1,0} %param_0.468, f32[1,512,50,50]{3,2,1,0} %broadcast.141), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.208 (param_0.472: f32[1,512,25,25], param_1.591: f32[1,512,1,1]) -> f32[1,512,25,25] {
  %param_0.472 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %param_1.591 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.386 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.591), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.143 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.386), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.226 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_0.472, f32[1,512,25,25]{3,2,1,0} %broadcast.143), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.210 (param_0.475: f32[1,2048,25,25], param_1.596: f32[1,2048,1,1], param_2.631: f32[1,2048,1,1]) -> (f32[1,2048,25,25], f32[1,2048,25,25]) {
  %param_0.475 = f32[1,2048,25,25]{3,2,1,0} parameter(0)
  %param_1.596 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %bitcast.387 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_1.596), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.144 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.387), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.227 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %param_0.475, f32[1,2048,25,25]{3,2,1,0} %broadcast.144), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %param_2.631 = f32[1,2048,1,1]{3,2,1,0} parameter(2)
  %bitcast.384.clone.1 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_2.631), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.140.clone.1 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.384.clone.1), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  %multiply.224.clone.1 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %param_0.475, f32[1,2048,25,25]{3,2,1,0} %broadcast.140.clone.1), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/convshortcut/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %tuple.156 = (f32[1,2048,25,25]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}) tuple(f32[1,2048,25,25]{3,2,1,0} %multiply.227, f32[1,2048,25,25]{3,2,1,0} %multiply.224.clone.1)
}

%fused_computation.212 (param_0.478: f32[1,512,25,25], param_1.602: f32[1,512,1,1]) -> f32[1,512,25,25] {
  %param_0.478 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %param_1.602 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.388 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.602), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.145 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.388), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.228 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_0.478, f32[1,512,25,25]{3,2,1,0} %broadcast.145), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.214 (param_0.481: f32[1,512,25,25], param_1.607: f32[1,512,1,1]) -> f32[1,512,25,25] {
  %param_0.481 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %param_1.607 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.389 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.607), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.146 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.389), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.229 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_0.481, f32[1,512,25,25]{3,2,1,0} %broadcast.146), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.216 (param_0.484: f32[1,2048,25,25], param_1.612: f32[1,2048,1,1]) -> f32[1,2048,25,25] {
  %param_0.484 = f32[1,2048,25,25]{3,2,1,0} parameter(0)
  %param_1.612 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %bitcast.390 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_1.612), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.147 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.390), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.230 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %param_0.484, f32[1,2048,25,25]{3,2,1,0} %broadcast.147), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.218 (param_0.487: f32[1,512,25,25], param_1.618: f32[1,512,1,1]) -> f32[1,512,25,25] {
  %param_0.487 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %param_1.618 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.391 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.618), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.148 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.391), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.231 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_0.487, f32[1,512,25,25]{3,2,1,0} %broadcast.148), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.220 (param_0.490: f32[1,512,25,25], param_1.623: f32[1,512,1,1]) -> f32[1,512,25,25] {
  %param_0.490 = f32[1,512,25,25]{3,2,1,0} parameter(0)
  %param_1.623 = f32[1,512,1,1]{3,2,1,0} parameter(1)
  %bitcast.392 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_1.623), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.149 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.392), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.232 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_0.490, f32[1,512,25,25]{3,2,1,0} %broadcast.149), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
}

%fused_computation.222 (param_0.493: f32[1,2048,25,25], param_1.628: f32[1,2048,1,1]) -> f32[1,2048,25,25] {
  %param_0.493 = f32[1,2048,25,25]{3,2,1,0} parameter(0)
  %param_1.628 = f32[1,2048,1,1]{3,2,1,0} parameter(1)
  %bitcast.393 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_1.628), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %broadcast.150 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.393), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  ROOT %multiply.233 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %param_0.493, f32[1,2048,25,25]{3,2,1,0} %broadcast.150), metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
}

%scatter-combiner.1223 (p0.1224: f32[], p1.1225: f32[]) -> f32[] {
  %p0.1224 = f32[] parameter(0)
  %p1.1225 = f32[] parameter(1)
  ROOT %add.1226 = f32[] add(f32[] %p0.1224, f32[] %p1.1225)
}

%fused_computation.227 (param_0.503: s64[2], param_1.644: f32[2], param_2.425: pred[2], param_3.213: f32[2], param_4.23: f32[], param_5.12: pred[]) -> f32[507] {
  %constant_196 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.157 = f32[507]{0} broadcast(f32[] %constant_196), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level6/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %param_0.503 = s64[2]{0} parameter(0)
  %param_2.425 = pred[2]{0} parameter(2)
  %constant_195 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %broadcast.156 = f32[2]{0} broadcast(f32[] %constant_195), dimensions={}, metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Log1p_grad/Reciprocal"}
  %param_3.213 = f32[2]{0} parameter(3)
  %add.152 = f32[2]{0} add(f32[2]{0} %broadcast.156, f32[2]{0} %param_3.213), metadata={op_type="Add" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Log1p_grad/add"}
  %divide.0 = f32[2]{0} divide(f32[2]{0} %broadcast.156, f32[2]{0} %add.152), metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Log1p_grad/Reciprocal"}
  %param_5.12 = pred[] parameter(5)
  %select.43 = f32[] select(pred[] %param_5.12, f32[] %constant_196, f32[] %constant_195), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level6/label_loss_grad/Select_1"}
  %param_4.23 = f32[] parameter(4)
  %multiply.238 = f32[] multiply(f32[] %select.43, f32[] %param_4.23), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level6/mul_grad/Mul"}
  %broadcast.155 = f32[2]{0} broadcast(f32[] %multiply.238), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss_grad/Reshape_1"}
  %multiply.237 = f32[2]{0} multiply(f32[2]{0} %divide.0, f32[2]{0} %broadcast.155), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Log1p_grad/mul"}
  %multiply.236 = f32[2]{0} multiply(f32[2]{0} %multiply.237, f32[2]{0} %param_3.213), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Exp_grad/mul"}
  %broadcast.154 = f32[2]{0} broadcast(f32[] %constant_196), dimensions={}, metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Select_1_grad/zeros_like"}
  %select.42 = f32[2]{0} select(pred[2]{0} %param_2.425, f32[2]{0} %multiply.236, f32[2]{0} %broadcast.154), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Select_1_grad/Select"}
  %negate.92 = f32[2]{0} negate(f32[2]{0} %select.42), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Neg_grad/Neg"}
  %select.41 = f32[2]{0} select(pred[2]{0} %param_2.425, f32[2]{0} %broadcast.154, f32[2]{0} %multiply.236), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Select_1_grad/Select_1"}
  %add.151 = f32[2]{0} add(f32[2]{0} %negate.92, f32[2]{0} %select.41), metadata={op_type="AddN" op_name="tower0/gradients/AddN_14"}
  %select.40 = f32[2]{0} select(pred[2]{0} %param_2.425, f32[2]{0} %broadcast.155, f32[2]{0} %broadcast.154), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/Select_grad/Select"}
  %add.150 = f32[2]{0} add(f32[2]{0} %add.151, f32[2]{0} %select.40), metadata={op_type="AddN" op_name="tower0/gradients/AddN_14"}
  %negate.91 = f32[] negate(f32[] %multiply.238), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/sub_grad/Neg"}
  %broadcast.153 = f32[2]{0} broadcast(f32[] %negate.91), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/sub_grad/Reshape_1"}
  %param_1.644 = f32[2]{0} parameter(1)
  %multiply.235 = f32[2]{0} multiply(f32[2]{0} %broadcast.153, f32[2]{0} %param_1.644), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level6/logistic_loss/mul_grad/Mul"}
  %add.149 = f32[2]{0} add(f32[2]{0} %add.150, f32[2]{0} %multiply.235), metadata={op_type="AddN" op_name="tower0/gradients/AddN_14"}
  ROOT %scatter.0 = f32[507]{0} scatter(f32[507]{0} %broadcast.157, s64[2]{0} %param_0.503, f32[2]{0} %add.149), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1223, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level6/boolean_mask_1/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.1198 (p0.1199: f32[], p1.1200: f32[]) -> f32[] {
  %p0.1199 = f32[] parameter(0)
  %p1.1200 = f32[] parameter(1)
  ROOT %add.1201 = f32[] add(f32[] %p0.1199, f32[] %p1.1200)
}

%fused_computation.228 (param_0.505: s64[2], param_1.646: f32[2,4], param_2.431: f32[], param_3.222: f32[2,4], param_4.32: f32[], param_5.25: f32[], param_6.18: pred[], param_7.20: f32[2,4]) -> f32[507,4] {
  %constant_198 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.163 = f32[507,4]{1,0} broadcast(f32[] %constant_198), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level6/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %param_0.505 = s64[2]{0} parameter(0)
  %param_7.20 = f32[2,4]{1,0} parameter(7)
  %param_2.431 = f32[] parameter(2)
  %broadcast.162 = f32[2,4]{1,0} broadcast(f32[] %param_2.431), dimensions={}, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Minimum_grad/LessEqual"}
  %compare.41 = pred[2,4]{1,0} compare(f32[2,4]{1,0} %param_7.20, f32[2,4]{1,0} %broadcast.162), direction=LE, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Minimum_grad/LessEqual"}
  %param_4.32 = f32[] parameter(4)
  %param_5.25 = f32[] parameter(5)
  %param_6.18 = pred[] parameter(6)
  %constant_197 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %select.46 = f32[] select(pred[] %param_6.18, f32[] %constant_198, f32[] %constant_197), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level6/box_loss_grad/Select_1"}
  %multiply.243 = f32[] multiply(f32[] %param_5.25, f32[] %select.46), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level6/truediv_grad/RealDiv"}
  %multiply.242 = f32[] multiply(f32[] %param_4.32, f32[] %multiply.243), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Mul_1_grad/Mul_1"}
  %broadcast.161 = f32[2,4]{1,0} broadcast(f32[] %multiply.242), dimensions={}
  %param_3.222 = f32[2,4]{1,0} parameter(3)
  %multiply.241 = f32[2,4]{1,0} multiply(f32[2,4]{1,0} %broadcast.161, f32[2,4]{1,0} %param_3.222), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Mul_grad/Mul_1"}
  %add.155 = f32[2,4]{1,0} add(f32[2,4]{1,0} %multiply.241, f32[2,4]{1,0} %multiply.241), metadata={op_type="AddN" op_name="tower0/gradients/AddN_6"}
  %multiply.240 = f32[] multiply(f32[] %multiply.243, f32[] %param_2.431), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Mul_2_grad/Mul_1"}
  %negate.93 = f32[] negate(f32[] %multiply.240), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Sub_1_grad/Neg"}
  %broadcast.160 = f32[2,4]{1,0} broadcast(f32[] %negate.93), dimensions={}
  %add.154 = f32[2,4]{1,0} add(f32[2,4]{1,0} %add.155, f32[2,4]{1,0} %broadcast.160), metadata={op_type="AddN" op_name="tower0/gradients/AddN_6"}
  %broadcast.159 = f32[2,4]{1,0} broadcast(f32[] %constant_198), dimensions={}, metadata={op_type="Fill" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Minimum_grad/zeros"}
  %select.45 = f32[2,4]{1,0} select(pred[2,4]{1,0} %compare.41, f32[2,4]{1,0} %add.154, f32[2,4]{1,0} %broadcast.159), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Minimum_grad/Select"}
  %broadcast.158 = f32[2,4]{1,0} broadcast(f32[] %multiply.240), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Mul_2_grad/Reshape_1"}
  %add.153 = f32[2,4]{1,0} add(f32[2,4]{1,0} %select.45, f32[2,4]{1,0} %broadcast.158), metadata={op_type="AddN" op_name="tower0/gradients/AddN_19"}
  %param_1.646 = f32[2,4]{1,0} parameter(1)
  %compare.40 = pred[2,4]{1,0} compare(f32[2,4]{1,0} %param_1.646, f32[2,4]{1,0} %param_1.646), direction=NE, metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Abs_grad/Sign"}
  %sign.0 = f32[2,4]{1,0} sign(f32[2,4]{1,0} %param_1.646), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Abs_grad/Sign"}
  %select.44 = f32[2,4]{1,0} select(pred[2,4]{1,0} %compare.40, f32[2,4]{1,0} %broadcast.159, f32[2,4]{1,0} %sign.0), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Abs_grad/Sign"}
  %multiply.239 = f32[2,4]{1,0} multiply(f32[2,4]{1,0} %add.153, f32[2,4]{1,0} %select.44), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level6/huber_loss/Abs_grad/mul"}
  ROOT %scatter.1 = f32[507,4]{1,0} scatter(f32[507,4]{1,0} %broadcast.163, s64[2]{0} %param_0.505, f32[2,4]{1,0} %multiply.239), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1198, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level6/boolean_mask_3/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.1136 (p0.1137: f32[], p1.1138: f32[]) -> f32[] {
  %p0.1137 = f32[] parameter(0)
  %p1.1138 = f32[] parameter(1)
  ROOT %add.1139 = f32[] add(f32[] %p0.1137, f32[] %p1.1138)
}

%fused_computation.230 (param_0.510: s64[34], param_1.652: f32[34], param_2.439: pred[34], param_3.232: f32[34], param_4.40: f32[], param_5.32: pred[]) -> f32[1875] {
  %constant_201 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.169 = f32[1875]{0} broadcast(f32[] %constant_201), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %param_0.510 = s64[34]{0} parameter(0)
  %param_2.439 = pred[34]{0} parameter(2)
  %constant_200 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %broadcast.168 = f32[34]{0} broadcast(f32[] %constant_200), dimensions={}, metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Log1p_grad/Reciprocal"}
  %param_3.232 = f32[34]{0} parameter(3)
  %add.160 = f32[34]{0} add(f32[34]{0} %broadcast.168, f32[34]{0} %param_3.232), metadata={op_type="Add" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Log1p_grad/add"}
  %divide.1 = f32[34]{0} divide(f32[34]{0} %broadcast.168, f32[34]{0} %add.160), metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Log1p_grad/Reciprocal"}
  %param_5.32 = pred[] parameter(5)
  %select.51 = f32[] select(pred[] %param_5.32, f32[] %constant_201, f32[] %constant_200), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/label_loss_grad/Select_1"}
  %param_4.40 = f32[] parameter(4)
  %multiply.247 = f32[] multiply(f32[] %select.51, f32[] %param_4.40), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/mul_grad/Mul"}
  %broadcast.167 = f32[34]{0} broadcast(f32[] %multiply.247), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss_grad/Reshape_1"}
  %multiply.246 = f32[34]{0} multiply(f32[34]{0} %divide.1, f32[34]{0} %broadcast.167), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Log1p_grad/mul"}
  %multiply.245 = f32[34]{0} multiply(f32[34]{0} %multiply.246, f32[34]{0} %param_3.232), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Exp_grad/mul"}
  %broadcast.166 = f32[34]{0} broadcast(f32[] %constant_201), dimensions={}, metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Select_1_grad/zeros_like"}
  %select.50 = f32[34]{0} select(pred[34]{0} %param_2.439, f32[34]{0} %multiply.245, f32[34]{0} %broadcast.166), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Select_1_grad/Select"}
  %negate.95 = f32[34]{0} negate(f32[34]{0} %select.50), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Neg_grad/Neg"}
  %select.49 = f32[34]{0} select(pred[34]{0} %param_2.439, f32[34]{0} %broadcast.166, f32[34]{0} %multiply.245), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Select_1_grad/Select_1"}
  %add.159 = f32[34]{0} add(f32[34]{0} %negate.95, f32[34]{0} %select.49), metadata={op_type="AddN" op_name="tower0/gradients/AddN_13"}
  %select.48 = f32[34]{0} select(pred[34]{0} %param_2.439, f32[34]{0} %broadcast.167, f32[34]{0} %broadcast.166), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/Select_grad/Select"}
  %add.158 = f32[34]{0} add(f32[34]{0} %add.159, f32[34]{0} %select.48), metadata={op_type="AddN" op_name="tower0/gradients/AddN_13"}
  %negate.94 = f32[] negate(f32[] %multiply.247), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/sub_grad/Neg"}
  %broadcast.165 = f32[34]{0} broadcast(f32[] %negate.94), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/sub_grad/Reshape_1"}
  %param_1.652 = f32[34]{0} parameter(1)
  %multiply.244 = f32[34]{0} multiply(f32[34]{0} %broadcast.165, f32[34]{0} %param_1.652), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/logistic_loss/mul_grad/Mul"}
  %add.157 = f32[34]{0} add(f32[34]{0} %add.158, f32[34]{0} %multiply.244), metadata={op_type="AddN" op_name="tower0/gradients/AddN_13"}
  ROOT %scatter.2 = f32[1875]{0} scatter(f32[1875]{0} %broadcast.169, s64[34]{0} %param_0.510, f32[34]{0} %add.157), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1136, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_1/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.1111 (p0.1112: f32[], p1.1113: f32[]) -> f32[] {
  %p0.1112 = f32[] parameter(0)
  %p1.1113 = f32[] parameter(1)
  ROOT %add.1114 = f32[] add(f32[] %p0.1112, f32[] %p1.1113)
}

%fused_computation.231 (param_0.512: s64[33], param_1.654: f32[33,4], param_2.445: f32[], param_3.241: f32[33,4], param_4.49: f32[], param_5.45: f32[], param_6.35: pred[], param_7.41: f32[33,4]) -> f32[1875,4] {
  %constant_203 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.175 = f32[1875,4]{1,0} broadcast(f32[] %constant_203), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %param_0.512 = s64[33]{0} parameter(0)
  %param_7.41 = f32[33,4]{1,0} parameter(7)
  %param_2.445 = f32[] parameter(2)
  %broadcast.174 = f32[33,4]{1,0} broadcast(f32[] %param_2.445), dimensions={}, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Minimum_grad/LessEqual"}
  %compare.44 = pred[33,4]{1,0} compare(f32[33,4]{1,0} %param_7.41, f32[33,4]{1,0} %broadcast.174), direction=LE, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Minimum_grad/LessEqual"}
  %param_4.49 = f32[] parameter(4)
  %param_5.45 = f32[] parameter(5)
  %param_6.35 = pred[] parameter(6)
  %constant_202 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %select.54 = f32[] select(pred[] %param_6.35, f32[] %constant_203, f32[] %constant_202), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/box_loss_grad/Select_1"}
  %multiply.252 = f32[] multiply(f32[] %param_5.45, f32[] %select.54), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/truediv_grad/RealDiv"}
  %multiply.251 = f32[] multiply(f32[] %param_4.49, f32[] %multiply.252), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Mul_1_grad/Mul_1"}
  %broadcast.173 = f32[33,4]{1,0} broadcast(f32[] %multiply.251), dimensions={}
  %param_3.241 = f32[33,4]{1,0} parameter(3)
  %multiply.250 = f32[33,4]{1,0} multiply(f32[33,4]{1,0} %broadcast.173, f32[33,4]{1,0} %param_3.241), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Mul_grad/Mul_1"}
  %add.163 = f32[33,4]{1,0} add(f32[33,4]{1,0} %multiply.250, f32[33,4]{1,0} %multiply.250), metadata={op_type="AddN" op_name="tower0/gradients/AddN_5"}
  %multiply.249 = f32[] multiply(f32[] %multiply.252, f32[] %param_2.445), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Mul_2_grad/Mul_1"}
  %negate.96 = f32[] negate(f32[] %multiply.249), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Sub_1_grad/Neg"}
  %broadcast.172 = f32[33,4]{1,0} broadcast(f32[] %negate.96), dimensions={}
  %add.162 = f32[33,4]{1,0} add(f32[33,4]{1,0} %add.163, f32[33,4]{1,0} %broadcast.172), metadata={op_type="AddN" op_name="tower0/gradients/AddN_5"}
  %broadcast.171 = f32[33,4]{1,0} broadcast(f32[] %constant_203), dimensions={}, metadata={op_type="Fill" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Minimum_grad/zeros"}
  %select.53 = f32[33,4]{1,0} select(pred[33,4]{1,0} %compare.44, f32[33,4]{1,0} %add.162, f32[33,4]{1,0} %broadcast.171), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Minimum_grad/Select"}
  %broadcast.170 = f32[33,4]{1,0} broadcast(f32[] %multiply.249), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Mul_2_grad/Reshape_1"}
  %add.161 = f32[33,4]{1,0} add(f32[33,4]{1,0} %select.53, f32[33,4]{1,0} %broadcast.170), metadata={op_type="AddN" op_name="tower0/gradients/AddN_18"}
  %param_1.654 = f32[33,4]{1,0} parameter(1)
  %compare.43 = pred[33,4]{1,0} compare(f32[33,4]{1,0} %param_1.654, f32[33,4]{1,0} %param_1.654), direction=NE, metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Abs_grad/Sign"}
  %sign.1 = f32[33,4]{1,0} sign(f32[33,4]{1,0} %param_1.654), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Abs_grad/Sign"}
  %select.52 = f32[33,4]{1,0} select(pred[33,4]{1,0} %compare.43, f32[33,4]{1,0} %broadcast.171, f32[33,4]{1,0} %sign.1), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Abs_grad/Sign"}
  %multiply.248 = f32[33,4]{1,0} multiply(f32[33,4]{1,0} %add.161, f32[33,4]{1,0} %select.52), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level5/huber_loss/Abs_grad/mul"}
  ROOT %scatter.3 = f32[1875,4]{1,0} scatter(f32[1875,4]{1,0} %broadcast.175, s64[33]{0} %param_0.512, f32[33,4]{1,0} %multiply.248), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1111, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_3/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.1049 (p0.1050: f32[], p1.1051: f32[]) -> f32[] {
  %p0.1050 = f32[] parameter(0)
  %p1.1051 = f32[] parameter(1)
  ROOT %add.1052 = f32[] add(f32[] %p0.1050, f32[] %p1.1051)
}

%fused_computation.235 (param_0.519: s64[32], param_1.666: f32[32], param_2.457: pred[32], param_3.255: f32[32], param_4.57: f32[], param_5.52: pred[]) -> f32[7500] {
  %constant_206 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.181 = f32[7500]{0} broadcast(f32[] %constant_206), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %param_0.519 = s64[32]{0} parameter(0)
  %param_2.457 = pred[32]{0} parameter(2)
  %constant_205 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %broadcast.180 = f32[32]{0} broadcast(f32[] %constant_205), dimensions={}, metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Log1p_grad/Reciprocal"}
  %param_3.255 = f32[32]{0} parameter(3)
  %add.171 = f32[32]{0} add(f32[32]{0} %broadcast.180, f32[32]{0} %param_3.255), metadata={op_type="Add" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Log1p_grad/add"}
  %divide.2 = f32[32]{0} divide(f32[32]{0} %broadcast.180, f32[32]{0} %add.171), metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Log1p_grad/Reciprocal"}
  %param_5.52 = pred[] parameter(5)
  %select.59 = f32[] select(pred[] %param_5.52, f32[] %constant_206, f32[] %constant_205), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/label_loss_grad/Select_1"}
  %param_4.57 = f32[] parameter(4)
  %multiply.257 = f32[] multiply(f32[] %select.59, f32[] %param_4.57), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/mul_grad/Mul"}
  %broadcast.179 = f32[32]{0} broadcast(f32[] %multiply.257), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss_grad/Reshape_1"}
  %multiply.256 = f32[32]{0} multiply(f32[32]{0} %divide.2, f32[32]{0} %broadcast.179), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Log1p_grad/mul"}
  %multiply.255 = f32[32]{0} multiply(f32[32]{0} %multiply.256, f32[32]{0} %param_3.255), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Exp_grad/mul"}
  %broadcast.178 = f32[32]{0} broadcast(f32[] %constant_206), dimensions={}, metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Select_1_grad/zeros_like"}
  %select.58 = f32[32]{0} select(pred[32]{0} %param_2.457, f32[32]{0} %multiply.255, f32[32]{0} %broadcast.178), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Select_1_grad/Select"}
  %negate.98 = f32[32]{0} negate(f32[32]{0} %select.58), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Neg_grad/Neg"}
  %select.57 = f32[32]{0} select(pred[32]{0} %param_2.457, f32[32]{0} %broadcast.178, f32[32]{0} %multiply.255), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Select_1_grad/Select_1"}
  %add.170 = f32[32]{0} add(f32[32]{0} %negate.98, f32[32]{0} %select.57), metadata={op_type="AddN" op_name="tower0/gradients/AddN_12"}
  %select.56 = f32[32]{0} select(pred[32]{0} %param_2.457, f32[32]{0} %broadcast.179, f32[32]{0} %broadcast.178), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/Select_grad/Select"}
  %add.169 = f32[32]{0} add(f32[32]{0} %add.170, f32[32]{0} %select.56), metadata={op_type="AddN" op_name="tower0/gradients/AddN_12"}
  %negate.97 = f32[] negate(f32[] %multiply.257), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/sub_grad/Neg"}
  %broadcast.177 = f32[32]{0} broadcast(f32[] %negate.97), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/sub_grad/Reshape_1"}
  %param_1.666 = f32[32]{0} parameter(1)
  %multiply.254 = f32[32]{0} multiply(f32[32]{0} %broadcast.177, f32[32]{0} %param_1.666), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/logistic_loss/mul_grad/Mul"}
  %add.168 = f32[32]{0} add(f32[32]{0} %add.169, f32[32]{0} %multiply.254), metadata={op_type="AddN" op_name="tower0/gradients/AddN_12"}
  ROOT %scatter.4 = f32[7500]{0} scatter(f32[7500]{0} %broadcast.181, s64[32]{0} %param_0.519, f32[32]{0} %add.168), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1049, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_1/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.1024 (p0.1025: f32[], p1.1026: f32[]) -> f32[] {
  %p0.1025 = f32[] parameter(0)
  %p1.1026 = f32[] parameter(1)
  ROOT %add.1027 = f32[] add(f32[] %p0.1025, f32[] %p1.1026)
}

%fused_computation.236 (param_0.521: s64[22], param_1.668: f32[22,4], param_2.463: f32[], param_3.264: f32[22,4], param_4.66: f32[], param_5.65: f32[], param_6.52: pred[], param_7.62: f32[22,4]) -> f32[7500,4] {
  %constant_208 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.187 = f32[7500,4]{1,0} broadcast(f32[] %constant_208), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %param_0.521 = s64[22]{0} parameter(0)
  %param_7.62 = f32[22,4]{1,0} parameter(7)
  %param_2.463 = f32[] parameter(2)
  %broadcast.186 = f32[22,4]{1,0} broadcast(f32[] %param_2.463), dimensions={}, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Minimum_grad/LessEqual"}
  %compare.47 = pred[22,4]{1,0} compare(f32[22,4]{1,0} %param_7.62, f32[22,4]{1,0} %broadcast.186), direction=LE, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Minimum_grad/LessEqual"}
  %param_4.66 = f32[] parameter(4)
  %param_5.65 = f32[] parameter(5)
  %param_6.52 = pred[] parameter(6)
  %constant_207 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %select.62 = f32[] select(pred[] %param_6.52, f32[] %constant_208, f32[] %constant_207), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/box_loss_grad/Select_1"}
  %multiply.262 = f32[] multiply(f32[] %param_5.65, f32[] %select.62), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/truediv_grad/RealDiv"}
  %multiply.261 = f32[] multiply(f32[] %param_4.66, f32[] %multiply.262), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Mul_1_grad/Mul_1"}
  %broadcast.185 = f32[22,4]{1,0} broadcast(f32[] %multiply.261), dimensions={}
  %param_3.264 = f32[22,4]{1,0} parameter(3)
  %multiply.260 = f32[22,4]{1,0} multiply(f32[22,4]{1,0} %broadcast.185, f32[22,4]{1,0} %param_3.264), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Mul_grad/Mul_1"}
  %add.174 = f32[22,4]{1,0} add(f32[22,4]{1,0} %multiply.260, f32[22,4]{1,0} %multiply.260), metadata={op_type="AddN" op_name="tower0/gradients/AddN_4"}
  %multiply.259 = f32[] multiply(f32[] %multiply.262, f32[] %param_2.463), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Mul_2_grad/Mul_1"}
  %negate.99 = f32[] negate(f32[] %multiply.259), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Sub_1_grad/Neg"}
  %broadcast.184 = f32[22,4]{1,0} broadcast(f32[] %negate.99), dimensions={}
  %add.173 = f32[22,4]{1,0} add(f32[22,4]{1,0} %add.174, f32[22,4]{1,0} %broadcast.184), metadata={op_type="AddN" op_name="tower0/gradients/AddN_4"}
  %broadcast.183 = f32[22,4]{1,0} broadcast(f32[] %constant_208), dimensions={}, metadata={op_type="Fill" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Minimum_grad/zeros"}
  %select.61 = f32[22,4]{1,0} select(pred[22,4]{1,0} %compare.47, f32[22,4]{1,0} %add.173, f32[22,4]{1,0} %broadcast.183), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Minimum_grad/Select"}
  %broadcast.182 = f32[22,4]{1,0} broadcast(f32[] %multiply.259), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Mul_2_grad/Reshape_1"}
  %add.172 = f32[22,4]{1,0} add(f32[22,4]{1,0} %select.61, f32[22,4]{1,0} %broadcast.182), metadata={op_type="AddN" op_name="tower0/gradients/AddN_17"}
  %param_1.668 = f32[22,4]{1,0} parameter(1)
  %compare.46 = pred[22,4]{1,0} compare(f32[22,4]{1,0} %param_1.668, f32[22,4]{1,0} %param_1.668), direction=NE, metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Abs_grad/Sign"}
  %sign.2 = f32[22,4]{1,0} sign(f32[22,4]{1,0} %param_1.668), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Abs_grad/Sign"}
  %select.60 = f32[22,4]{1,0} select(pred[22,4]{1,0} %compare.46, f32[22,4]{1,0} %broadcast.183, f32[22,4]{1,0} %sign.2), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Abs_grad/Sign"}
  %multiply.258 = f32[22,4]{1,0} multiply(f32[22,4]{1,0} %add.172, f32[22,4]{1,0} %select.60), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level4/huber_loss/Abs_grad/mul"}
  ROOT %scatter.5 = f32[7500,4]{1,0} scatter(f32[7500,4]{1,0} %broadcast.187, s64[22]{0} %param_0.521, f32[22,4]{1,0} %multiply.258), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.1024, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_3/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.962 (p0.963: f32[], p1.964: f32[]) -> f32[] {
  %p0.963 = f32[] parameter(0)
  %p1.964 = f32[] parameter(1)
  ROOT %add.965 = f32[] add(f32[] %p0.963, f32[] %p1.964)
}

%fused_computation.240 (param_0.528: s64[48], param_1.680: f32[48], param_2.475: pred[48], param_3.278: f32[48], param_4.74: f32[], param_5.72: pred[]) -> f32[30000] {
  %constant_211 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.193 = f32[30000]{0} broadcast(f32[] %constant_211), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level3/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %param_0.528 = s64[48]{0} parameter(0)
  %param_2.475 = pred[48]{0} parameter(2)
  %constant_210 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %broadcast.192 = f32[48]{0} broadcast(f32[] %constant_210), dimensions={}, metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Log1p_grad/Reciprocal"}
  %param_3.278 = f32[48]{0} parameter(3)
  %add.182 = f32[48]{0} add(f32[48]{0} %broadcast.192, f32[48]{0} %param_3.278), metadata={op_type="Add" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Log1p_grad/add"}
  %divide.3 = f32[48]{0} divide(f32[48]{0} %broadcast.192, f32[48]{0} %add.182), metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Log1p_grad/Reciprocal"}
  %param_5.72 = pred[] parameter(5)
  %select.67 = f32[] select(pred[] %param_5.72, f32[] %constant_211, f32[] %constant_210), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/label_loss_grad/Select_1"}
  %param_4.74 = f32[] parameter(4)
  %multiply.267 = f32[] multiply(f32[] %select.67, f32[] %param_4.74), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/mul_grad/Mul"}
  %broadcast.191 = f32[48]{0} broadcast(f32[] %multiply.267), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss_grad/Reshape_1"}
  %multiply.266 = f32[48]{0} multiply(f32[48]{0} %divide.3, f32[48]{0} %broadcast.191), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Log1p_grad/mul"}
  %multiply.265 = f32[48]{0} multiply(f32[48]{0} %multiply.266, f32[48]{0} %param_3.278), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Exp_grad/mul"}
  %broadcast.190 = f32[48]{0} broadcast(f32[] %constant_211), dimensions={}, metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Select_1_grad/zeros_like"}
  %select.66 = f32[48]{0} select(pred[48]{0} %param_2.475, f32[48]{0} %multiply.265, f32[48]{0} %broadcast.190), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Select_1_grad/Select"}
  %negate.101 = f32[48]{0} negate(f32[48]{0} %select.66), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Neg_grad/Neg"}
  %select.65 = f32[48]{0} select(pred[48]{0} %param_2.475, f32[48]{0} %broadcast.190, f32[48]{0} %multiply.265), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Select_1_grad/Select_1"}
  %add.181 = f32[48]{0} add(f32[48]{0} %negate.101, f32[48]{0} %select.65), metadata={op_type="AddN" op_name="tower0/gradients/AddN_11"}
  %select.64 = f32[48]{0} select(pred[48]{0} %param_2.475, f32[48]{0} %broadcast.191, f32[48]{0} %broadcast.190), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/Select_grad/Select"}
  %add.180 = f32[48]{0} add(f32[48]{0} %add.181, f32[48]{0} %select.64), metadata={op_type="AddN" op_name="tower0/gradients/AddN_11"}
  %negate.100 = f32[] negate(f32[] %multiply.267), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/sub_grad/Neg"}
  %broadcast.189 = f32[48]{0} broadcast(f32[] %negate.100), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/sub_grad/Reshape_1"}
  %param_1.680 = f32[48]{0} parameter(1)
  %multiply.264 = f32[48]{0} multiply(f32[48]{0} %broadcast.189, f32[48]{0} %param_1.680), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/logistic_loss/mul_grad/Mul"}
  %add.179 = f32[48]{0} add(f32[48]{0} %add.180, f32[48]{0} %multiply.264), metadata={op_type="AddN" op_name="tower0/gradients/AddN_11"}
  ROOT %scatter.6 = f32[30000]{0} scatter(f32[30000]{0} %broadcast.193, s64[48]{0} %param_0.528, f32[48]{0} %add.179), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.962, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level3/boolean_mask_1/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.937 (p0.938: f32[], p1.939: f32[]) -> f32[] {
  %p0.938 = f32[] parameter(0)
  %p1.939 = f32[] parameter(1)
  ROOT %add.940 = f32[] add(f32[] %p0.938, f32[] %p1.939)
}

%fused_computation.241 (param_0.530: s64[11], param_1.682: f32[11,4], param_2.481: f32[], param_3.287: f32[11,4], param_4.83: f32[], param_5.85: f32[], param_6.69: pred[], param_7.83: f32[11,4]) -> f32[30000,4] {
  %constant_213 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.199 = f32[30000,4]{1,0} broadcast(f32[] %constant_213), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level3/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %param_0.530 = s64[11]{0} parameter(0)
  %param_7.83 = f32[11,4]{1,0} parameter(7)
  %param_2.481 = f32[] parameter(2)
  %broadcast.198 = f32[11,4]{1,0} broadcast(f32[] %param_2.481), dimensions={}, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Minimum_grad/LessEqual"}
  %compare.50 = pred[11,4]{1,0} compare(f32[11,4]{1,0} %param_7.83, f32[11,4]{1,0} %broadcast.198), direction=LE, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Minimum_grad/LessEqual"}
  %param_4.83 = f32[] parameter(4)
  %param_5.85 = f32[] parameter(5)
  %param_6.69 = pred[] parameter(6)
  %constant_212 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %select.70 = f32[] select(pred[] %param_6.69, f32[] %constant_213, f32[] %constant_212), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/box_loss_grad/Select_1"}
  %multiply.272 = f32[] multiply(f32[] %param_5.85, f32[] %select.70), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/truediv_grad/RealDiv"}
  %multiply.271 = f32[] multiply(f32[] %param_4.83, f32[] %multiply.272), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Mul_1_grad/Mul_1"}
  %broadcast.197 = f32[11,4]{1,0} broadcast(f32[] %multiply.271), dimensions={}
  %param_3.287 = f32[11,4]{1,0} parameter(3)
  %multiply.270 = f32[11,4]{1,0} multiply(f32[11,4]{1,0} %broadcast.197, f32[11,4]{1,0} %param_3.287), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Mul_grad/Mul_1"}
  %add.185 = f32[11,4]{1,0} add(f32[11,4]{1,0} %multiply.270, f32[11,4]{1,0} %multiply.270), metadata={op_type="AddN" op_name="tower0/gradients/AddN_3"}
  %multiply.269 = f32[] multiply(f32[] %multiply.272, f32[] %param_2.481), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Mul_2_grad/Mul_1"}
  %negate.102 = f32[] negate(f32[] %multiply.269), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Sub_1_grad/Neg"}
  %broadcast.196 = f32[11,4]{1,0} broadcast(f32[] %negate.102), dimensions={}
  %add.184 = f32[11,4]{1,0} add(f32[11,4]{1,0} %add.185, f32[11,4]{1,0} %broadcast.196), metadata={op_type="AddN" op_name="tower0/gradients/AddN_3"}
  %broadcast.195 = f32[11,4]{1,0} broadcast(f32[] %constant_213), dimensions={}, metadata={op_type="Fill" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Minimum_grad/zeros"}
  %select.69 = f32[11,4]{1,0} select(pred[11,4]{1,0} %compare.50, f32[11,4]{1,0} %add.184, f32[11,4]{1,0} %broadcast.195), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Minimum_grad/Select"}
  %broadcast.194 = f32[11,4]{1,0} broadcast(f32[] %multiply.269), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Mul_2_grad/Reshape_1"}
  %add.183 = f32[11,4]{1,0} add(f32[11,4]{1,0} %select.69, f32[11,4]{1,0} %broadcast.194), metadata={op_type="AddN" op_name="tower0/gradients/AddN_16"}
  %param_1.682 = f32[11,4]{1,0} parameter(1)
  %compare.49 = pred[11,4]{1,0} compare(f32[11,4]{1,0} %param_1.682, f32[11,4]{1,0} %param_1.682), direction=NE, metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Abs_grad/Sign"}
  %sign.3 = f32[11,4]{1,0} sign(f32[11,4]{1,0} %param_1.682), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Abs_grad/Sign"}
  %select.68 = f32[11,4]{1,0} select(pred[11,4]{1,0} %compare.49, f32[11,4]{1,0} %broadcast.195, f32[11,4]{1,0} %sign.3), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Abs_grad/Sign"}
  %multiply.268 = f32[11,4]{1,0} multiply(f32[11,4]{1,0} %add.183, f32[11,4]{1,0} %select.68), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level3/huber_loss/Abs_grad/mul"}
  ROOT %scatter.7 = f32[30000,4]{1,0} scatter(f32[30000,4]{1,0} %broadcast.199, s64[11]{0} %param_0.530, f32[11,4]{1,0} %multiply.268), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.937, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level3/boolean_mask_3/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.875 (p0.876: f32[], p1.877: f32[]) -> f32[] {
  %p0.876 = f32[] parameter(0)
  %p1.877 = f32[] parameter(1)
  ROOT %add.878 = f32[] add(f32[] %p0.876, f32[] %p1.877)
}

%fused_computation.244 (param_0.537: s64[140], param_1.690: f32[140], param_2.490: pred[140], param_3.297: f32[140], param_4.91: f32[], param_5.92: pred[]) -> f32[120000] {
  %constant_216 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.205 = f32[120000]{0} broadcast(f32[] %constant_216), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level2/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %param_0.537 = s64[140]{0} parameter(0)
  %param_2.490 = pred[140]{0} parameter(2)
  %constant_215 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %broadcast.204 = f32[140]{0} broadcast(f32[] %constant_215), dimensions={}, metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Log1p_grad/Reciprocal"}
  %param_3.297 = f32[140]{0} parameter(3)
  %add.192 = f32[140]{0} add(f32[140]{0} %broadcast.204, f32[140]{0} %param_3.297), metadata={op_type="Add" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Log1p_grad/add"}
  %divide.4 = f32[140]{0} divide(f32[140]{0} %broadcast.204, f32[140]{0} %add.192), metadata={op_type="Reciprocal" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Log1p_grad/Reciprocal"}
  %param_5.92 = pred[] parameter(5)
  %select.75 = f32[] select(pred[] %param_5.92, f32[] %constant_216, f32[] %constant_215), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %param_4.91 = f32[] parameter(4)
  %multiply.276 = f32[] multiply(f32[] %select.75, f32[] %param_4.91), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/mul_grad/Mul"}
  %broadcast.203 = f32[140]{0} broadcast(f32[] %multiply.276), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss_grad/Reshape_1"}
  %multiply.275 = f32[140]{0} multiply(f32[140]{0} %divide.4, f32[140]{0} %broadcast.203), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Log1p_grad/mul"}
  %multiply.274 = f32[140]{0} multiply(f32[140]{0} %multiply.275, f32[140]{0} %param_3.297), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Exp_grad/mul"}
  %broadcast.202 = f32[140]{0} broadcast(f32[] %constant_216), dimensions={}, metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_1_grad/zeros_like"}
  %select.74 = f32[140]{0} select(pred[140]{0} %param_2.490, f32[140]{0} %multiply.274, f32[140]{0} %broadcast.202), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_1_grad/Select"}
  %negate.104 = f32[140]{0} negate(f32[140]{0} %select.74), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Neg_grad/Neg"}
  %select.73 = f32[140]{0} select(pred[140]{0} %param_2.490, f32[140]{0} %broadcast.202, f32[140]{0} %multiply.274), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_1_grad/Select_1"}
  %add.191 = f32[140]{0} add(f32[140]{0} %negate.104, f32[140]{0} %select.73), metadata={op_type="AddN" op_name="tower0/gradients/AddN_10"}
  %select.72 = f32[140]{0} select(pred[140]{0} %param_2.490, f32[140]{0} %broadcast.203, f32[140]{0} %broadcast.202), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/Select"}
  %add.190 = f32[140]{0} add(f32[140]{0} %add.191, f32[140]{0} %select.72), metadata={op_type="AddN" op_name="tower0/gradients/AddN_10"}
  %negate.103 = f32[] negate(f32[] %multiply.276), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/sub_grad/Neg"}
  %broadcast.201 = f32[140]{0} broadcast(f32[] %negate.103), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/sub_grad/Reshape_1"}
  %param_1.690 = f32[140]{0} parameter(1)
  %multiply.273 = f32[140]{0} multiply(f32[140]{0} %broadcast.201, f32[140]{0} %param_1.690), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/mul_grad/Mul"}
  %add.189 = f32[140]{0} add(f32[140]{0} %add.190, f32[140]{0} %multiply.273), metadata={op_type="AddN" op_name="tower0/gradients/AddN_10"}
  ROOT %scatter.8 = f32[120000]{0} scatter(f32[120000]{0} %broadcast.205, s64[140]{0} %param_0.537, f32[140]{0} %add.189), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.875, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level2/boolean_mask_1/Reshape_grad/Reshape/tensor"}
}

%scatter-combiner.836 (p0.837: f32[], p1.838: f32[]) -> f32[] {
  %p0.837 = f32[] parameter(0)
  %p1.838 = f32[] parameter(1)
  ROOT %add.839 = f32[] add(f32[] %p0.837, f32[] %p1.838)
}

%fused_computation.245 (param_0.539: s64[4], param_1.692: f32[4,4], param_2.496: f32[], param_3.306: f32[4,4], param_4.100: f32[], param_5.105: f32[], param_6.86: pred[], param_7.104: f32[4,4]) -> f32[120000,4] {
  %constant_218 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.211 = f32[120000,4]{1,0} broadcast(f32[] %constant_218), dimensions={}, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level2/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %param_0.539 = s64[4]{0} parameter(0)
  %param_7.104 = f32[4,4]{1,0} parameter(7)
  %param_2.496 = f32[] parameter(2)
  %broadcast.210 = f32[4,4]{1,0} broadcast(f32[] %param_2.496), dimensions={}, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Minimum_grad/LessEqual"}
  %compare.53 = pred[4,4]{1,0} compare(f32[4,4]{1,0} %param_7.104, f32[4,4]{1,0} %broadcast.210), direction=LE, metadata={op_type="LessEqual" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Minimum_grad/LessEqual"}
  %param_4.100 = f32[] parameter(4)
  %param_6.86 = pred[] parameter(6)
  %constant_217 = f32[] constant(1), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/label_loss_grad/Select_1"}
  %select.78 = f32[] select(pred[] %param_6.86, f32[] %constant_218, f32[] %constant_217), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/box_loss_grad/Select_1"}
  %param_5.105 = f32[] parameter(5)
  %multiply.281 = f32[] multiply(f32[] %select.78, f32[] %param_5.105), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/truediv_grad/RealDiv"}
  %multiply.280 = f32[] multiply(f32[] %param_4.100, f32[] %multiply.281), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Mul_1_grad/Mul_1"}
  %broadcast.209 = f32[4,4]{1,0} broadcast(f32[] %multiply.280), dimensions={}
  %param_3.306 = f32[4,4]{1,0} parameter(3)
  %multiply.279 = f32[4,4]{1,0} multiply(f32[4,4]{1,0} %broadcast.209, f32[4,4]{1,0} %param_3.306), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Mul_grad/Mul_1"}
  %add.195 = f32[4,4]{1,0} add(f32[4,4]{1,0} %multiply.279, f32[4,4]{1,0} %multiply.279), metadata={op_type="AddN" op_name="tower0/gradients/AddN_2"}
  %multiply.278 = f32[] multiply(f32[] %multiply.281, f32[] %param_2.496), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Mul_2_grad/Mul_1"}
  %negate.105 = f32[] negate(f32[] %multiply.278), metadata={op_type="Neg" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Sub_1_grad/Neg"}
  %broadcast.208 = f32[4,4]{1,0} broadcast(f32[] %negate.105), dimensions={}
  %add.194 = f32[4,4]{1,0} add(f32[4,4]{1,0} %add.195, f32[4,4]{1,0} %broadcast.208), metadata={op_type="AddN" op_name="tower0/gradients/AddN_2"}
  %broadcast.207 = f32[4,4]{1,0} broadcast(f32[] %constant_218), dimensions={}, metadata={op_type="Fill" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Minimum_grad/zeros"}
  %select.77 = f32[4,4]{1,0} select(pred[4,4]{1,0} %compare.53, f32[4,4]{1,0} %add.194, f32[4,4]{1,0} %broadcast.207), metadata={op_type="Select" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Minimum_grad/Select"}
  %broadcast.206 = f32[4,4]{1,0} broadcast(f32[] %multiply.278), dimensions={}, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Mul_2_grad/Reshape_1"}
  %add.193 = f32[4,4]{1,0} add(f32[4,4]{1,0} %select.77, f32[4,4]{1,0} %broadcast.206), metadata={op_type="AddN" op_name="tower0/gradients/AddN_15"}
  %param_1.692 = f32[4,4]{1,0} parameter(1)
  %compare.52 = pred[4,4]{1,0} compare(f32[4,4]{1,0} %param_1.692, f32[4,4]{1,0} %param_1.692), direction=NE, metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Abs_grad/Sign"}
  %sign.4 = f32[4,4]{1,0} sign(f32[4,4]{1,0} %param_1.692), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Abs_grad/Sign"}
  %select.76 = f32[4,4]{1,0} select(pred[4,4]{1,0} %compare.52, f32[4,4]{1,0} %broadcast.207, f32[4,4]{1,0} %sign.4), metadata={op_type="Sign" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Abs_grad/Sign"}
  %multiply.277 = f32[4,4]{1,0} multiply(f32[4,4]{1,0} %add.193, f32[4,4]{1,0} %select.76), metadata={op_type="Mul" op_name="tower0/gradients/tower0/rpn_losses/level2/huber_loss/Abs_grad/mul"}
  ROOT %scatter.9 = f32[120000,4]{1,0} scatter(f32[120000,4]{1,0} %broadcast.211, s64[4]{0} %param_0.539, f32[4,4]{1,0} %multiply.277), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.836, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level2/boolean_mask_3/Reshape_grad/Reshape/tensor"}
}

%add_float_.1303 (x.1304: f32[], y.1305: f32[]) -> f32[] {
  %x.1304 = f32[] parameter(0)
  %y.1305 = f32[] parameter(1)
  ROOT %add.1306 = f32[] add(f32[] %x.1304, f32[] %y.1305)
}

%fused_computation.246 (param_0.543: f32[30000,4]) -> f32[12] {
  %param_0.543 = f32[30000,4]{1,0} parameter(0)
  %reshape.285 = f32[1,100,100,12]{2,1,3,0} reshape(f32[30000,4]{1,0} %param_0.543), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_1/Reshape_grad/Reshape"}
  %constant_252 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.58 = f32[12]{0} reduce(f32[1,100,100,12]{2,1,3,0} %reshape.285, f32[] %constant_252), dimensions={0,1,2}, to_apply=%add_float_.1303, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/box/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1232 (x.1233: f32[], y.1234: f32[]) -> f32[] {
  %x.1233 = f32[] parameter(0)
  %y.1234 = f32[] parameter(1)
  ROOT %add.1235 = f32[] add(f32[] %x.1233, f32[] %y.1234)
}

%fused_computation.247 (param_0.546: f32[120000,4]) -> f32[12] {
  %param_0.546 = f32[120000,4]{1,0} parameter(0)
  %reshape.286 = f32[1,200,200,12]{2,1,3,0} reshape(f32[120000,4]{1,0} %param_0.546), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn/Reshape_grad/Reshape"}
  %constant_253 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.59 = f32[12]{0} reduce(f32[1,200,200,12]{2,1,3,0} %reshape.286, f32[] %constant_253), dimensions={0,1,2}, to_apply=%add_float_.1232, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/box/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1377 (x.1378: f32[], y.1379: f32[]) -> f32[] {
  %x.1378 = f32[] parameter(0)
  %y.1379 = f32[] parameter(1)
  ROOT %add.1380 = f32[] add(f32[] %x.1378, f32[] %y.1379)
}

%fused_computation.248 (param_0.549: f32[7500,4]) -> f32[12] {
  %param_0.549 = f32[7500,4]{1,0} parameter(0)
  %reshape.287 = f32[1,50,50,12]{2,1,3,0} reshape(f32[7500,4]{1,0} %param_0.549), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_2/Reshape_grad/Reshape"}
  %constant_254 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.60 = f32[12]{0} reduce(f32[1,50,50,12]{2,1,3,0} %reshape.287, f32[] %constant_254), dimensions={0,1,2}, to_apply=%add_float_.1377, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/box/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1451 (x.1452: f32[], y.1453: f32[]) -> f32[] {
  %x.1452 = f32[] parameter(0)
  %y.1453 = f32[] parameter(1)
  ROOT %add.1454 = f32[] add(f32[] %x.1452, f32[] %y.1453)
}

%fused_computation.249 (param_0.552: f32[1875,4]) -> f32[12] {
  %param_0.552 = f32[1875,4]{1,0} parameter(0)
  %reshape.288 = f32[1,25,25,12]{2,1,3,0} reshape(f32[1875,4]{1,0} %param_0.552), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_3/Reshape_grad/Reshape"}
  %constant_255 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.61 = f32[12]{0} reduce(f32[1,25,25,12]{2,1,3,0} %reshape.288, f32[] %constant_255), dimensions={0,1,2}, to_apply=%add_float_.1451, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/box/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1491 (x.1492: f32[], y.1493: f32[]) -> f32[] {
  %x.1492 = f32[] parameter(0)
  %y.1493 = f32[] parameter(1)
  ROOT %add.1494 = f32[] add(f32[] %x.1492, f32[] %y.1493)
}

%fused_computation.250 (param_0.555: f32[507,4]) -> f32[12] {
  %param_0.555 = f32[507,4]{1,0} parameter(0)
  %reshape.289 = f32[1,13,13,12]{2,1,3,0} reshape(f32[507,4]{1,0} %param_0.555), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_4/Reshape_grad/Reshape"}
  %constant_256 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.62 = f32[12]{0} reduce(f32[1,13,13,12]{2,1,3,0} %reshape.289, f32[] %constant_256), dimensions={0,1,2}, to_apply=%add_float_.1491, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/box/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1315 (x.1316: f32[], y.1317: f32[]) -> f32[] {
  %x.1316 = f32[] parameter(0)
  %y.1317 = f32[] parameter(1)
  ROOT %add.1318 = f32[] add(f32[] %x.1316, f32[] %y.1317)
}

%fused_computation.251 (param_0.558: f32[30000]) -> f32[3] {
  %param_0.558 = f32[30000]{0} parameter(0)
  %reshape.290 = f32[1,100,100,3]{2,1,3,0} reshape(f32[30000]{0} %param_0.558), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_1/Squeeze_grad/Reshape"}
  %constant_257 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.63 = f32[3]{0} reduce(f32[1,100,100,3]{2,1,3,0} %reshape.290, f32[] %constant_257), dimensions={0,1,2}, to_apply=%add_float_.1315, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/class/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1244 (x.1245: f32[], y.1246: f32[]) -> f32[] {
  %x.1245 = f32[] parameter(0)
  %y.1246 = f32[] parameter(1)
  ROOT %add.1247 = f32[] add(f32[] %x.1245, f32[] %y.1246)
}

%fused_computation.252 (param_0.561: f32[120000]) -> f32[3] {
  %param_0.561 = f32[120000]{0} parameter(0)
  %reshape.291 = f32[1,200,200,3]{2,1,3,0} reshape(f32[120000]{0} %param_0.561), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn/Squeeze_grad/Reshape"}
  %constant_258 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.64 = f32[3]{0} reduce(f32[1,200,200,3]{2,1,3,0} %reshape.291, f32[] %constant_258), dimensions={0,1,2}, to_apply=%add_float_.1244, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/class/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1389 (x.1390: f32[], y.1391: f32[]) -> f32[] {
  %x.1390 = f32[] parameter(0)
  %y.1391 = f32[] parameter(1)
  ROOT %add.1392 = f32[] add(f32[] %x.1390, f32[] %y.1391)
}

%fused_computation.253 (param_0.564: f32[7500]) -> f32[3] {
  %param_0.564 = f32[7500]{0} parameter(0)
  %reshape.292 = f32[1,50,50,3]{2,1,3,0} reshape(f32[7500]{0} %param_0.564), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_2/Squeeze_grad/Reshape"}
  %constant_259 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.65 = f32[3]{0} reduce(f32[1,50,50,3]{2,1,3,0} %reshape.292, f32[] %constant_259), dimensions={0,1,2}, to_apply=%add_float_.1389, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/class/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1463 (x.1464: f32[], y.1465: f32[]) -> f32[] {
  %x.1464 = f32[] parameter(0)
  %y.1465 = f32[] parameter(1)
  ROOT %add.1466 = f32[] add(f32[] %x.1464, f32[] %y.1465)
}

%fused_computation.254 (param_0.567: f32[1875]) -> f32[3] {
  %param_0.567 = f32[1875]{0} parameter(0)
  %reshape.293 = f32[1,25,25,3]{2,1,3,0} reshape(f32[1875]{0} %param_0.567), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_3/Squeeze_grad/Reshape"}
  %constant_260 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.66 = f32[3]{0} reduce(f32[1,25,25,3]{2,1,3,0} %reshape.293, f32[] %constant_260), dimensions={0,1,2}, to_apply=%add_float_.1463, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/class/BiasAdd_grad/BiasAddGrad"}
}

%add_float_.1512 (x.1513: f32[], y.1514: f32[]) -> f32[] {
  %x.1513 = f32[] parameter(0)
  %y.1514 = f32[] parameter(1)
  ROOT %add.1515 = f32[] add(f32[] %x.1513, f32[] %y.1514)
}

%fused_computation.255 (param_0.570: f32[507]) -> f32[3] {
  %param_0.570 = f32[507]{0} parameter(0)
  %reshape.294 = f32[1,13,13,3]{2,1,3,0} reshape(f32[507]{0} %param_0.570), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_4/Squeeze_grad/Reshape"}
  %constant_261 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  ROOT %reduce.67 = f32[3]{0} reduce(f32[1,13,13,3]{2,1,3,0} %reshape.294, f32[] %constant_261), dimensions={0,1,2}, to_apply=%add_float_.1512, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/class/BiasAdd_grad/BiasAddGrad"}
}

%fused_computation.256 (param_0.585: f32[507]) -> f32[1,3,13,13] {
  %param_0.585 = f32[507]{0} parameter(0)
  %reshape.295 = f32[1,13,13,3]{2,1,3,0} reshape(f32[507]{0} %param_0.585), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_4/Squeeze_grad/Reshape"}
  ROOT %bitcast.404 = f32[1,3,13,13]{3,2,1,0} bitcast(f32[1,13,13,3]{2,1,3,0} %reshape.295), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_4/transpose_grad/transpose"}
}

%fused_computation.257 (param_0.587: f32[507,4]) -> f32[1,12,13,13] {
  %param_0.587 = f32[507,4]{1,0} parameter(0)
  %reshape.296 = f32[1,13,13,12]{2,1,3,0} reshape(f32[507,4]{1,0} %param_0.587), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_4/Reshape_grad/Reshape"}
  ROOT %bitcast.405 = f32[1,12,13,13]{3,2,1,0} bitcast(f32[1,13,13,12]{2,1,3,0} %reshape.296), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_4/transpose_1_grad/transpose"}
}

%fused_computation.258 (param_0.589: f32[1875]) -> f32[1,3,25,25] {
  %param_0.589 = f32[1875]{0} parameter(0)
  %reshape.297 = f32[1,25,25,3]{2,1,3,0} reshape(f32[1875]{0} %param_0.589), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_3/Squeeze_grad/Reshape"}
  ROOT %bitcast.406 = f32[1,3,25,25]{3,2,1,0} bitcast(f32[1,25,25,3]{2,1,3,0} %reshape.297), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_3/transpose_grad/transpose"}
}

%fused_computation.259 (param_0.591: f32[1875,4]) -> f32[1,12,25,25] {
  %param_0.591 = f32[1875,4]{1,0} parameter(0)
  %reshape.298 = f32[1,25,25,12]{2,1,3,0} reshape(f32[1875,4]{1,0} %param_0.591), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_3/Reshape_grad/Reshape"}
  ROOT %bitcast.407 = f32[1,12,25,25]{3,2,1,0} bitcast(f32[1,25,25,12]{2,1,3,0} %reshape.298), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_3/transpose_1_grad/transpose"}
}

%fused_computation.260 (param_0.594: f32[7500]) -> f32[1,3,50,50] {
  %param_0.594 = f32[7500]{0} parameter(0)
  %reshape.299 = f32[1,50,50,3]{2,1,3,0} reshape(f32[7500]{0} %param_0.594), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_2/Squeeze_grad/Reshape"}
  ROOT %bitcast.409 = f32[1,3,50,50]{3,2,1,0} bitcast(f32[1,50,50,3]{2,1,3,0} %reshape.299), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_2/transpose_grad/transpose"}
}

%fused_computation.261 (param_0.596: f32[7500,4]) -> f32[1,12,50,50] {
  %param_0.596 = f32[7500,4]{1,0} parameter(0)
  %reshape.300 = f32[1,50,50,12]{2,1,3,0} reshape(f32[7500,4]{1,0} %param_0.596), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_2/Reshape_grad/Reshape"}
  ROOT %bitcast.410 = f32[1,12,50,50]{3,2,1,0} bitcast(f32[1,50,50,12]{2,1,3,0} %reshape.300), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_2/transpose_1_grad/transpose"}
}

%fused_computation.262 (param_0.599: f32[30000]) -> f32[1,3,100,100] {
  %param_0.599 = f32[30000]{0} parameter(0)
  %reshape.301 = f32[1,100,100,3]{2,1,3,0} reshape(f32[30000]{0} %param_0.599), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_1/Squeeze_grad/Reshape"}
  ROOT %bitcast.412 = f32[1,3,100,100]{3,2,1,0} bitcast(f32[1,100,100,3]{2,1,3,0} %reshape.301), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_1/transpose_grad/transpose"}
}

%fused_computation.263 (param_0.601: f32[30000,4]) -> f32[1,12,100,100] {
  %param_0.601 = f32[30000,4]{1,0} parameter(0)
  %reshape.302 = f32[1,100,100,12]{2,1,3,0} reshape(f32[30000,4]{1,0} %param_0.601), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn_1/Reshape_grad/Reshape"}
  ROOT %bitcast.413 = f32[1,12,100,100]{3,2,1,0} bitcast(f32[1,100,100,12]{2,1,3,0} %reshape.302), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_1/transpose_1_grad/transpose"}
}

%fused_computation.264 (param_0.603: f32[120000]) -> f32[1,3,200,200] {
  %param_0.603 = f32[120000]{0} parameter(0)
  %reshape.303 = f32[1,200,200,3]{2,1,3,0} reshape(f32[120000]{0} %param_0.603), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn/Squeeze_grad/Reshape"}
  ROOT %bitcast.414 = f32[1,3,200,200]{3,2,1,0} bitcast(f32[1,200,200,3]{2,1,3,0} %reshape.303), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn/transpose_grad/transpose"}
}

%fused_computation.265 (param_0.605: f32[120000,4]) -> f32[1,12,200,200] {
  %param_0.605 = f32[120000,4]{1,0} parameter(0)
  %reshape.304 = f32[1,200,200,12]{2,1,3,0} reshape(f32[120000,4]{1,0} %param_0.605), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/rpn/Reshape_grad/Reshape"}
  ROOT %bitcast.415 = f32[1,12,200,200]{3,2,1,0} bitcast(f32[1,200,200,12]{2,1,3,0} %reshape.304), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn/transpose_1_grad/transpose"}
}

%add_float_.1537 (x.1538: f32[], y.1539: f32[]) -> f32[] {
  %x.1538 = f32[] parameter(0)
  %y.1539 = f32[] parameter(1)
  ROOT %add.1540 = f32[] add(f32[] %x.1538, f32[] %y.1539)
}

%fused_computation.266 (param_0.686: f32[1,256,13,13], param_1.934: f32[1,256,13,13], param_2.830: f32[1,256,13,13]) -> (f32[256], f32[1,256,13,13]) {
  %param_2.830 = f32[1,256,13,13]{3,2,1,0} parameter(2)
  %constant_194_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.152.clone.1 = f32[1,256,13,13]{3,2,1,0} broadcast(f32[] %constant_194_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/Relu_grad/ReluGrad"}
  %compare.39.clone.1 = pred[1,256,13,13]{3,2,1,0} compare(f32[1,256,13,13]{3,2,1,0} %param_2.830, f32[1,256,13,13]{3,2,1,0} %broadcast.152.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/Relu_grad/ReluGrad"}
  %param_0.686 = f32[1,256,13,13]{3,2,1,0} parameter(0)
  %param_1.934 = f32[1,256,13,13]{3,2,1,0} parameter(1)
  %add.148.clone.1 = f32[1,256,13,13]{3,2,1,0} add(f32[1,256,13,13]{3,2,1,0} %param_0.686, f32[1,256,13,13]{3,2,1,0} %param_1.934), metadata={op_type="AddN" op_name="tower0/gradients/AddN_91"}
  %select.39.clone.1 = f32[1,256,13,13]{3,2,1,0} select(pred[1,256,13,13]{3,2,1,0} %compare.39.clone.1, f32[1,256,13,13]{3,2,1,0} %add.148.clone.1, f32[1,256,13,13]{3,2,1,0} %broadcast.152.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/Relu_grad/ReluGrad"}
  %reduce.107 = f32[256]{0} reduce(f32[1,256,13,13]{3,2,1,0} %select.39.clone.1, f32[] %constant_194_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1537, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.187 = (f32[256]{0}, f32[1,256,13,13]{3,2,1,0}) tuple(f32[256]{0} %reduce.107, f32[1,256,13,13]{3,2,1,0} %select.39.clone.1)
}

%add_float_.1479 (x.1480: f32[], y.1481: f32[]) -> f32[] {
  %x.1480 = f32[] parameter(0)
  %y.1481 = f32[] parameter(1)
  ROOT %add.1482 = f32[] add(f32[] %x.1480, f32[] %y.1481)
}

%fused_computation.267 (param_0.685: f32[1,256,25,25], param_1.933: f32[1,256,25,25], param_2.829: f32[1,256,25,25]) -> (f32[256], f32[1,256,25,25]) {
  %param_2.829 = f32[1,256,25,25]{3,2,1,0} parameter(2)
  %constant_199_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.164.clone.1 = f32[1,256,25,25]{3,2,1,0} broadcast(f32[] %constant_199_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/Relu_grad/ReluGrad"}
  %compare.42.clone.1 = pred[1,256,25,25]{3,2,1,0} compare(f32[1,256,25,25]{3,2,1,0} %param_2.829, f32[1,256,25,25]{3,2,1,0} %broadcast.164.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/Relu_grad/ReluGrad"}
  %param_0.685 = f32[1,256,25,25]{3,2,1,0} parameter(0)
  %param_1.933 = f32[1,256,25,25]{3,2,1,0} parameter(1)
  %add.156.clone.1 = f32[1,256,25,25]{3,2,1,0} add(f32[1,256,25,25]{3,2,1,0} %param_0.685, f32[1,256,25,25]{3,2,1,0} %param_1.933), metadata={op_type="AddN" op_name="tower0/gradients/AddN_90"}
  %select.47.clone.1 = f32[1,256,25,25]{3,2,1,0} select(pred[1,256,25,25]{3,2,1,0} %compare.42.clone.1, f32[1,256,25,25]{3,2,1,0} %add.156.clone.1, f32[1,256,25,25]{3,2,1,0} %broadcast.164.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/Relu_grad/ReluGrad"}
  %reduce.108 = f32[256]{0} reduce(f32[1,256,25,25]{3,2,1,0} %select.47.clone.1, f32[] %constant_199_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1479, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.188 = (f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) tuple(f32[256]{0} %reduce.108, f32[1,256,25,25]{3,2,1,0} %select.47.clone.1)
}

%add_float_.1405 (x.1406: f32[], y.1407: f32[]) -> f32[] {
  %x.1406 = f32[] parameter(0)
  %y.1407 = f32[] parameter(1)
  ROOT %add.1408 = f32[] add(f32[] %x.1406, f32[] %y.1407)
}

%fused_computation.268 (param_0.684: f32[1,256,50,50], param_1.932: f32[1,256,50,50], param_2.828: f32[1,256,50,50]) -> (f32[256], f32[1,256,50,50]) {
  %param_2.828 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %constant_299_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.282.clone.1 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_299_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %compare.45.clone.1 = pred[1,256,50,50]{3,2,1,0} compare(f32[1,256,50,50]{3,2,1,0} %param_2.828, f32[1,256,50,50]{3,2,1,0} %broadcast.282.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %param_0.684 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.932 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %add.167.clone.1 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %param_0.684, f32[1,256,50,50]{3,2,1,0} %param_1.932), metadata={op_type="AddN" op_name="tower0/gradients/AddN_89"}
  %select.55.clone.1 = f32[1,256,50,50]{3,2,1,0} select(pred[1,256,50,50]{3,2,1,0} %compare.45.clone.1, f32[1,256,50,50]{3,2,1,0} %add.167.clone.1, f32[1,256,50,50]{3,2,1,0} %broadcast.282.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/Relu_grad/ReluGrad"}
  %reduce.109 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %select.55.clone.1, f32[] %constant_299_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1405, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.189 = (f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.109, f32[1,256,50,50]{3,2,1,0} %select.55.clone.1)
}

%add_float_.1260 (x.1261: f32[], y.1262: f32[]) -> f32[] {
  %x.1261 = f32[] parameter(0)
  %y.1262 = f32[] parameter(1)
  ROOT %add.1263 = f32[] add(f32[] %x.1261, f32[] %y.1262)
}

%fused_computation.269 (param_0.683: f32[1,256,200,200], param_1.931: f32[1,256,200,200], param_2.827: f32[1,256,200,200]) -> (f32[256], f32[1,256,200,200]) {
  %param_2.827 = f32[1,256,200,200]{3,2,1,0} parameter(2)
  %constant_214_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.200.clone.1 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[] %constant_214_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn/conv0/Relu_grad/ReluGrad"}
  %compare.51.clone.1 = pred[1,256,200,200]{3,2,1,0} compare(f32[1,256,200,200]{3,2,1,0} %param_2.827, f32[1,256,200,200]{3,2,1,0} %broadcast.200.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn/conv0/Relu_grad/ReluGrad"}
  %param_0.683 = f32[1,256,200,200]{3,2,1,0} parameter(0)
  %param_1.931 = f32[1,256,200,200]{3,2,1,0} parameter(1)
  %add.188.clone.1 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %param_0.683, f32[1,256,200,200]{3,2,1,0} %param_1.931), metadata={op_type="AddN" op_name="tower0/gradients/AddN_87"}
  %select.71.clone.1 = f32[1,256,200,200]{3,2,1,0} select(pred[1,256,200,200]{3,2,1,0} %compare.51.clone.1, f32[1,256,200,200]{3,2,1,0} %add.188.clone.1, f32[1,256,200,200]{3,2,1,0} %broadcast.200.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn/conv0/Relu_grad/ReluGrad"}
  %reduce.110 = f32[256]{0} reduce(f32[1,256,200,200]{3,2,1,0} %select.71.clone.1, f32[] %constant_214_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1260, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.190 = (f32[256]{0}, f32[1,256,200,200]{3,2,1,0}) tuple(f32[256]{0} %reduce.110, f32[1,256,200,200]{3,2,1,0} %select.71.clone.1)
}

%add_float_.1331 (x.1332: f32[], y.1333: f32[]) -> f32[] {
  %x.1332 = f32[] parameter(0)
  %y.1333 = f32[] parameter(1)
  ROOT %add.1334 = f32[] add(f32[] %x.1332, f32[] %y.1333)
}

%fused_computation.270 (param_0.682: f32[1,256,100,100], param_1.930: f32[1,256,100,100], param_2.826: f32[1,256,100,100]) -> (f32[256], f32[1,256,100,100]) {
  %param_2.826 = f32[1,256,100,100]{3,2,1,0} parameter(2)
  %constant_300_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %broadcast.283.clone.1 = f32[1,256,100,100]{3,2,1,0} broadcast(f32[] %constant_300_clone_1), dimensions={}, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/Relu_grad/ReluGrad"}
  %compare.48.clone.1 = pred[1,256,100,100]{3,2,1,0} compare(f32[1,256,100,100]{3,2,1,0} %param_2.826, f32[1,256,100,100]{3,2,1,0} %broadcast.283.clone.1), direction=GT, metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/Relu_grad/ReluGrad"}
  %param_0.682 = f32[1,256,100,100]{3,2,1,0} parameter(0)
  %param_1.930 = f32[1,256,100,100]{3,2,1,0} parameter(1)
  %add.178.clone.1 = f32[1,256,100,100]{3,2,1,0} add(f32[1,256,100,100]{3,2,1,0} %param_0.682, f32[1,256,100,100]{3,2,1,0} %param_1.930), metadata={op_type="AddN" op_name="tower0/gradients/AddN_88"}
  %select.63.clone.1 = f32[1,256,100,100]{3,2,1,0} select(pred[1,256,100,100]{3,2,1,0} %compare.48.clone.1, f32[1,256,100,100]{3,2,1,0} %add.178.clone.1, f32[1,256,100,100]{3,2,1,0} %broadcast.283.clone.1), metadata={op_type="ReluGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/Relu_grad/ReluGrad"}
  %reduce.111 = f32[256]{0} reduce(f32[1,256,100,100]{3,2,1,0} %select.63.clone.1, f32[] %constant_300_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1331, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.191 = (f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) tuple(f32[256]{0} %reduce.111, f32[1,256,100,100]{3,2,1,0} %select.63.clone.1)
}

%add_float_.1273 (x.1274: f32[], y.1275: f32[]) -> f32[] {
  %x.1274 = f32[] parameter(0)
  %y.1275 = f32[] parameter(1)
  ROOT %add.1276 = f32[] add(f32[] %x.1274, f32[] %y.1275)
}

%fused_computation.271 (param_0.678: f32[1,256,200,200], param_1.926: f32[1,256,200,200], param_2.822: f32[1,256,200,200]) -> (f32[256], f32[1,256,200,200]) {
  %param_1.926 = f32[1,256,200,200]{3,2,1,0} parameter(1)
  %param_2.822 = f32[1,256,200,200]{3,2,1,0} parameter(2)
  %add.187.clone.1 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %param_1.926, f32[1,256,200,200]{3,2,1,0} %param_2.822), metadata={op_type="AddN" op_name="tower0/gradients/AddN_94"}
  %param_0.678 = f32[1,256,200,200]{3,2,1,0} parameter(0)
  %add.186.clone.1 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %add.187.clone.1, f32[1,256,200,200]{3,2,1,0} %param_0.678), metadata={op_type="AddN" op_name="tower0/gradients/AddN_94"}
  %constant_343 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.112 = f32[256]{0} reduce(f32[1,256,200,200]{3,2,1,0} %add.186.clone.1, f32[] %constant_343), dimensions={0,2,3}, to_apply=%add_float_.1273, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p2/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.192 = (f32[256]{0}, f32[1,256,200,200]{3,2,1,0}) tuple(f32[256]{0} %reduce.112, f32[1,256,200,200]{3,2,1,0} %add.186.clone.1)
}

%add_float_.1344 (x.1345: f32[], y.1346: f32[]) -> f32[] {
  %x.1345 = f32[] parameter(0)
  %y.1346 = f32[] parameter(1)
  ROOT %add.1347 = f32[] add(f32[] %x.1345, f32[] %y.1346)
}

%fused_computation.272 (param_0.679: f32[1,256,100,100], param_1.927: f32[1,256,100,100], param_2.823: f32[1,256,100,100]) -> (f32[256], f32[1,256,100,100]) {
  %param_1.927 = f32[1,256,100,100]{3,2,1,0} parameter(1)
  %param_2.823 = f32[1,256,100,100]{3,2,1,0} parameter(2)
  %add.177.clone.1 = f32[1,256,100,100]{3,2,1,0} add(f32[1,256,100,100]{3,2,1,0} %param_1.927, f32[1,256,100,100]{3,2,1,0} %param_2.823), metadata={op_type="AddN" op_name="tower0/gradients/AddN_95"}
  %param_0.679 = f32[1,256,100,100]{3,2,1,0} parameter(0)
  %add.176.clone.1 = f32[1,256,100,100]{3,2,1,0} add(f32[1,256,100,100]{3,2,1,0} %add.177.clone.1, f32[1,256,100,100]{3,2,1,0} %param_0.679), metadata={op_type="AddN" op_name="tower0/gradients/AddN_95"}
  %constant_344 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.113 = f32[256]{0} reduce(f32[1,256,100,100]{3,2,1,0} %add.176.clone.1, f32[] %constant_344), dimensions={0,2,3}, to_apply=%add_float_.1344, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p3/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.193 = (f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) tuple(f32[256]{0} %reduce.113, f32[1,256,100,100]{3,2,1,0} %add.176.clone.1)
}

%add_float_.1418 (x.1419: f32[], y.1420: f32[]) -> f32[] {
  %x.1419 = f32[] parameter(0)
  %y.1420 = f32[] parameter(1)
  ROOT %add.1421 = f32[] add(f32[] %x.1419, f32[] %y.1420)
}

%fused_computation.273 (param_0.680: f32[1,256,50,50], param_1.928: f32[1,256,50,50], param_2.824: f32[1,256,50,50]) -> (f32[256], f32[1,256,50,50]) {
  %param_1.928 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %param_2.824 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %add.166.clone.1 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %param_1.928, f32[1,256,50,50]{3,2,1,0} %param_2.824), metadata={op_type="AddN" op_name="tower0/gradients/AddN_96"}
  %param_0.680 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %add.165.clone.1 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %add.166.clone.1, f32[1,256,50,50]{3,2,1,0} %param_0.680), metadata={op_type="AddN" op_name="tower0/gradients/AddN_96"}
  %constant_345 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.114 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %add.165.clone.1, f32[] %constant_345), dimensions={0,2,3}, to_apply=%add_float_.1418, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p4/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.194 = (f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.114, f32[1,256,50,50]{3,2,1,0} %add.165.clone.1)
}

%add_float_.1576 (x.1577: f32[], y.1578: f32[]) -> f32[] {
  %x.1577 = f32[] parameter(0)
  %y.1578 = f32[] parameter(1)
  ROOT %add.1579 = f32[] add(f32[] %x.1577, f32[] %y.1578)
}

%fused_computation.274 (param_0.681: f32[1,256,25,25], param_1.929: f32[1,256,25,25], param_2.825: f32[1,256,25,25], param_3.534: f32[1,256,25,25]) -> (f32[256], f32[1,256,25,25]) {
  %param_2.825 = f32[1,256,25,25]{3,2,1,0} parameter(2)
  %param_3.534 = f32[1,256,25,25]{3,2,1,0} parameter(3)
  %add.147.clone.1 = f32[1,256,25,25]{3,2,1,0} add(f32[1,256,25,25]{3,2,1,0} %param_2.825, f32[1,256,25,25]{3,2,1,0} %param_3.534), metadata={op_type="AddN" op_name="tower0/gradients/AddN_98"}
  %param_1.929 = f32[1,256,25,25]{3,2,1,0} parameter(1)
  %add.146.clone.1 = f32[1,256,25,25]{3,2,1,0} add(f32[1,256,25,25]{3,2,1,0} %add.147.clone.1, f32[1,256,25,25]{3,2,1,0} %param_1.929), metadata={op_type="AddN" op_name="tower0/gradients/AddN_98"}
  %param_0.681 = f32[1,256,25,25]{3,2,1,0} parameter(0)
  %add.145.clone.1 = f32[1,256,25,25]{3,2,1,0} add(f32[1,256,25,25]{3,2,1,0} %add.146.clone.1, f32[1,256,25,25]{3,2,1,0} %param_0.681), metadata={op_type="AddN" op_name="tower0/gradients/AddN_98"}
  %constant_346 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.115 = f32[256]{0} reduce(f32[1,256,25,25]{3,2,1,0} %add.145.clone.1, f32[] %constant_346), dimensions={0,2,3}, to_apply=%add_float_.1576, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p5/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.195 = (f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) tuple(f32[256]{0} %reduce.115, f32[1,256,25,25]{3,2,1,0} %add.145.clone.1)
}

%add_float_.1358 (x.1359: f32[], y.1360: f32[]) -> f32[] {
  %x.1359 = f32[] parameter(0)
  %y.1360 = f32[] parameter(1)
  ROOT %add.1361 = f32[] add(f32[] %x.1359, f32[] %y.1360)
}

%scalar_add_computation (scalar_lhs: f32[], scalar_rhs: f32[]) -> f32[] {
  %scalar_lhs = f32[] parameter(0)
  %scalar_rhs = f32[] parameter(1)
  ROOT %add.10 = f32[] add(f32[] %scalar_lhs, f32[] %scalar_rhs)
}

%fused_computation.275 (param_0.677: f32[1,256,100,100], param_1.925: f32[1,4], param_2.821: f32[1,256,200,200]) -> (f32[256], f32[1,256,100,100]) {
  %param_0.677 = f32[1,256,100,100]{3,2,1,0} parameter(0)
  %param_2.821 = f32[1,256,200,200]{3,2,1,0} parameter(2)
  %bitcast.402.clone.1 = f32[1,256,100,100,2,2]{5,3,4,2,1,0} bitcast(f32[1,256,200,200]{3,2,1,0} %param_2.821), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat3/transpose_grad/transpose"}
  %copy.452.clone.1 = f32[1,256,100,100,2,2]{5,4,3,2,1,0} copy(f32[1,256,100,100,2,2]{5,3,4,2,1,0} %bitcast.402.clone.1), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat3/transpose_grad/transpose"}
  %bitcast.401.clone.1 = f32[2560000,4]{1,0} bitcast(f32[1,256,100,100,2,2]{5,4,3,2,1,0} %copy.452.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat3/Tensordot_grad/Reshape"}
  %param_1.925 = f32[1,4]{1,0} parameter(1)
  %bitcast.411.clone.1 = f32[4]{0} bitcast(f32[1,4]{1,0} %param_1.925)
  %broadcast.188.clone.1 = f32[2560000,4]{1,0} broadcast(f32[4]{0} %bitcast.411.clone.1), dimensions={1}
  %multiply.263.clone.1 = f32[2560000,4]{1,0} multiply(f32[2560000,4]{1,0} %bitcast.401.clone.1, f32[2560000,4]{1,0} %broadcast.188.clone.1)
  %constant_209_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.57.clone.1 = f32[2560000]{0} reduce(f32[2560000,4]{1,0} %multiply.263.clone.1, f32[] %constant_209_clone_1), dimensions={1}, to_apply=%scalar_add_computation
  %bitcast.400.clone.1 = f32[1,256,100,100]{3,2,1,0} bitcast(f32[2560000]{0} %reduce.57.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat3/ExpandDims_grad/Reshape"}
  %add.175.clone.1 = f32[1,256,100,100]{3,2,1,0} add(f32[1,256,100,100]{3,2,1,0} %param_0.677, f32[1,256,100,100]{3,2,1,0} %bitcast.400.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_104"}
  %reduce.116 = f32[256]{0} reduce(f32[1,256,100,100]{3,2,1,0} %add.175.clone.1, f32[] %constant_209_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1358, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c3/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.196 = (f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) tuple(f32[256]{0} %reduce.116, f32[1,256,100,100]{3,2,1,0} %add.175.clone.1)
}

%add_float_.1432 (x.1433: f32[], y.1434: f32[]) -> f32[] {
  %x.1433 = f32[] parameter(0)
  %y.1434 = f32[] parameter(1)
  ROOT %add.1435 = f32[] add(f32[] %x.1433, f32[] %y.1434)
}

%fused_computation.276 (param_0.676: f32[1,256,50,50], param_1.924: f32[1,4], param_2.820: f32[1,256,100,100]) -> (f32[256], f32[1,256,50,50]) {
  %param_0.676 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_2.820 = f32[1,256,100,100]{3,2,1,0} parameter(2)
  %bitcast.399.clone.1 = f32[1,256,50,50,2,2]{5,3,4,2,1,0} bitcast(f32[1,256,100,100]{3,2,1,0} %param_2.820), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat4/transpose_grad/transpose"}
  %copy.451.clone.1 = f32[1,256,50,50,2,2]{5,4,3,2,1,0} copy(f32[1,256,50,50,2,2]{5,3,4,2,1,0} %bitcast.399.clone.1), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat4/transpose_grad/transpose"}
  %bitcast.398.clone.1 = f32[640000,4]{1,0} bitcast(f32[1,256,50,50,2,2]{5,4,3,2,1,0} %copy.451.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat4/Tensordot_grad/Reshape"}
  %param_1.924 = f32[1,4]{1,0} parameter(1)
  %bitcast.408.clone.1 = f32[4]{0} bitcast(f32[1,4]{1,0} %param_1.924)
  %broadcast.176.clone.1 = f32[640000,4]{1,0} broadcast(f32[4]{0} %bitcast.408.clone.1), dimensions={1}
  %multiply.253.clone.1 = f32[640000,4]{1,0} multiply(f32[640000,4]{1,0} %bitcast.398.clone.1, f32[640000,4]{1,0} %broadcast.176.clone.1)
  %constant_204_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.56.clone.1 = f32[640000]{0} reduce(f32[640000,4]{1,0} %multiply.253.clone.1, f32[] %constant_204_clone_1), dimensions={1}, to_apply=%scalar_add_computation
  %bitcast.397.clone.1 = f32[1,256,50,50]{3,2,1,0} bitcast(f32[640000]{0} %reduce.56.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat4/ExpandDims_grad/Reshape"}
  %add.164.clone.1 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %param_0.676, f32[1,256,50,50]{3,2,1,0} %bitcast.397.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_106"}
  %reduce.117 = f32[256]{0} reduce(f32[1,256,50,50]{3,2,1,0} %add.164.clone.1, f32[] %constant_204_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1432, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c4/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.197 = (f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) tuple(f32[256]{0} %reduce.117, f32[1,256,50,50]{3,2,1,0} %add.164.clone.1)
}

%add_float_.1589 (x.1590: f32[], y.1591: f32[]) -> f32[] {
  %x.1590 = f32[] parameter(0)
  %y.1591 = f32[] parameter(1)
  ROOT %add.1592 = f32[] add(f32[] %x.1590, f32[] %y.1591)
}

%fused_computation.277 (param_0.675: f32[1,256,25,25], param_1.923: f32[1,4], param_2.819: f32[1,256,50,50]) -> (f32[256], f32[1,256,25,25]) {
  %param_0.675 = f32[1,256,25,25]{3,2,1,0} parameter(0)
  %param_2.819 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %bitcast.396.clone.1 = f32[1,256,25,25,2,2]{5,3,4,2,1,0} bitcast(f32[1,256,50,50]{3,2,1,0} %param_2.819), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat5/transpose_grad/transpose"}
  %copy.450.clone.1 = f32[1,256,25,25,2,2]{5,4,3,2,1,0} copy(f32[1,256,25,25,2,2]{5,3,4,2,1,0} %bitcast.396.clone.1), metadata={op_type="Transpose" op_name="tower0/gradients/tower0/fpn/upsample_lat5/transpose_grad/transpose"}
  %bitcast.395.clone.1 = f32[160000,4]{1,0} bitcast(f32[1,256,25,25,2,2]{5,4,3,2,1,0} %copy.450.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat5/Tensordot_grad/Reshape"}
  %param_1.923 = f32[1,4]{1,0} parameter(1)
  %bitcast.403.clone.1 = f32[4]{0} bitcast(f32[1,4]{1,0} %param_1.923)
  %broadcast.151.clone.1 = f32[160000,4]{1,0} broadcast(f32[4]{0} %bitcast.403.clone.1), dimensions={1}
  %multiply.234.clone.1 = f32[160000,4]{1,0} multiply(f32[160000,4]{1,0} %bitcast.395.clone.1, f32[160000,4]{1,0} %broadcast.151.clone.1)
  %constant_193_clone_1 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %reduce.55.clone.1 = f32[160000]{0} reduce(f32[160000,4]{1,0} %multiply.234.clone.1, f32[] %constant_193_clone_1), dimensions={1}, to_apply=%scalar_add_computation
  %bitcast.394.clone.1 = f32[1,256,25,25]{3,2,1,0} bitcast(f32[160000]{0} %reduce.55.clone.1), metadata={op_type="Reshape" op_name="tower0/gradients/tower0/fpn/upsample_lat5/ExpandDims_grad/Reshape"}
  %add.144.clone.1 = f32[1,256,25,25]{3,2,1,0} add(f32[1,256,25,25]{3,2,1,0} %param_0.675, f32[1,256,25,25]{3,2,1,0} %bitcast.394.clone.1), metadata={op_type="AddN" op_name="tower0/gradients/AddN_108"}
  %reduce.118 = f32[256]{0} reduce(f32[1,256,25,25]{3,2,1,0} %add.144.clone.1, f32[] %constant_193_clone_1), dimensions={0,2,3}, to_apply=%add_float_.1589, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c5/BiasAdd_grad/BiasAddGrad"}
  ROOT %tuple.198 = (f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) tuple(f32[256]{0} %reduce.118, f32[1,256,25,25]{3,2,1,0} %add.144.clone.1)
}

ENTRY %cluster_16__XlaCompiledKernel_true__XlaNumConstantArgs_723__XlaNumResourceArgs_0_.4191 (arg0.1: f32[1,1,1024,512], arg1.2: f32[1,1,256,1024], arg2.3: f32[3,3,256,256], arg3.4: f32[3,3,256,256], arg4.5: f32[1,1,1024,256], arg5.6: f32[1,1,256,1024], arg6.7: f32[3,3,256,256], arg7.8: f32[1,1,1024,256], arg8.9: f32[1,1,256,1024], arg9.10: f32[3,3,256,256], arg10.11: f32[1,1,1024,256], arg11.12: f32[1,1,256,1024], arg12.13: f32[3,3,256,256], arg13.14: f32[1,1,1024,256], arg14.15: f32[1,1,256,1024], arg15.16: f32[3,3,256,256], arg16.17: f32[1,1,1024,256], arg17.18: f32[1,1,512,1024], arg18.19: f32[1,1,128,512], arg19.20: f32[3,3,128,128], arg20.21: f32[1,1,512,128], arg21.22: f32[1,1,128,512], arg22.23: f32[3,3,128,128], arg23.24: f32[1,1,512,128], arg24.25: f32[1,1,128,512], arg25.26: f32[3,3,128,128], arg26.27: f32[1,1,512,128], arg27.28: f32[1,1,256,512], arg28.29: f32[1,1,128,512], arg29.30: f32[3,3,128,128], arg30.31: f32[1,1,256,128], arg31.32: f32[1,1,256,1024], arg32.33: f32[3,3,256,256], arg33.34: f32[1,1,512,256], arg34.35: f32[3,3,512,512], arg35.36: f32[1,1,512,2048], arg36.37: f32[1,1,1024,2048], arg37.38: f32[1,1,2048,512], arg38.39: f32[3,3,512,512], arg39.40: f32[1,1,512,2048], arg40.41: f32[1,1,2048,512], arg41.42: f32[3,3,512,512], arg42.43: f32[1,1,512,2048], arg43.44: f32[1,1,2048,256], arg44.45: f32[1,1,1024,256], arg45.46: f32[1,1,512,256], arg46.47: f32[1,1,256,256], arg47.48: f32[3,3,256,256], arg48.49: f32[3,3,256,256], arg49.50: f32[1,1,256,3], arg50.51: f32[1,1,256,12], arg51.52: f32[3,3,256,256], arg52.53: f32[3,3,256,256], arg53.54: f32[3,3,256,256], arg54.55: pred[], arg55.56: pred[], arg56.57: pred[], arg57.58: pred[], arg58.59: f32[], arg59.60: f32[], arg60.61: f32[34], arg61.62: f32[2], arg62.63: f32[34], arg63.64: f32[2], arg64.65: pred[], arg65.66: pred[], arg66.67: f32[33,4], arg67.68: f32[2,4], arg68.69: f32[33,4], arg69.70: f32[], arg70.71: f32[2,4], arg71.72: f32[32], arg72.73: f32[34], arg73.74: f32[32], arg74.75: f32[2], arg75.76: pred[34], arg76.77: pred[2], arg77.78: f32[34], arg78.79: f32[2], arg79.80: pred[], arg80.81: pred[], arg81.82: f32[22,4], arg82.83: s64[34], arg83.84: f32[], arg84.85: f32[22,4], arg85.86: s64[2], arg86.87: f32[1,256,25,25], arg87.88: f32[33,4], arg88.89: f32[48], arg89.90: f32[32], arg90.91: f32[1,256,13,13], arg91.92: f32[2,4], arg92.93: f32[48], arg93.94: pred[32], arg94.95: f32[32], arg95.96: pred[], arg96.97: pred[], arg97.98: f32[11,4], arg98.99: s64[32], arg99.100: f32[11,4], arg100.101: f32[1,256,50,50], arg101.102: f32[22,4], arg102.103: s64[33], arg103.104: f32[140], arg104.105: f32[48], arg105.106: s64[2], arg106.107: f32[140], arg107.108: pred[48], arg108.109: f32[48], arg109.110: f32[1,256,25,25], arg110.111: f32[1,256,13,13], arg111.112: f32[4,4], arg112.113: s64[48], arg113.114: f32[4,4], arg114.115: f32[1,256,100,100], arg115.116: f32[11,4], arg116.117: s64[22], arg117.118: f32[140], arg118.119: pred[140], arg119.120: f32[140], arg120.121: f32[1,256,50,50], arg121.122: s64[140], arg122.123: f32[1,256,200,200], arg123.124: f32[4,4], arg124.125: s64[11], arg125.126: f32[1,256,100,100], arg126.127: s64[4], arg127.128: f32[1,256,200,200], arg128.129: f32[1,256,200,200], arg129.130: f32[1,256,200,200], arg130.131: f32[1,256,100,100], arg131.132: f32[1,256,100,100], arg132.133: f32[1,256,50,50], arg133.134: f32[1,256,50,50], arg134.135: f32[1,256,25,25], arg135.136: f32[1,256,25,25], arg136.137: f32[1,256,200,200], arg137.138: f32[1,256,100,100], arg138.139: f32[1,256,50,50], arg139.140: f32[1,256,25,25], arg140.141: f32[1,256,200,200], arg141.142: f32[1,4], arg142.143: f32[1,512,100,100], arg143.144: f32[1,1024,50,50], arg144.145: f32[1,2048,25,25], arg145.146: f32[1,2048,1,1], arg146.147: f32[1,2048,25,25], arg147.148: f32[1,2048,1,1], arg148.149: f32[1,512,25,25], arg149.150: f32[1,2048,1,1], arg150.151: f32[1,512,1,1], arg151.152: f32[1,512,25,25], arg152.153: f32[1,512,1,1], arg153.154: f32[1,512,25,25], arg154.155: f32[1,512,1,1], arg155.156: f32[1,512,1,1], arg156.157: f32[1,512,25,25], arg157.158: f32[1,512,1,1], arg158.159: f32[1,2048,25,25], arg159.160: f32[1,512,1,1], arg160.161: f32[1,2048,1,1], arg161.162: f32[1,2048,25,25], arg162.163: f32[1,2048,1,1], arg163.164: f32[1,512,25,25], arg164.165: f32[1,2048,1,1], arg165.166: f32[1,512,1,1], arg166.167: f32[1,512,25,25], arg167.168: f32[1,512,1,1], arg168.169: f32[1,512,25,25], arg169.170: f32[1,512,1,1], arg170.171: f32[1,512,1,1], arg171.172: f32[1,512,25,25], arg172.173: f32[1,512,1,1], arg173.174: f32[1,2048,25,25], arg174.175: f32[1,512,1,1], arg175.176: f32[1,2048,1,1], arg176.177: f32[1,2048,25,25], arg177.178: f32[1,2048,1,1], arg178.179: f32[1,2048,25,25], arg179.180: f32[1,2048,1,1], arg180.181: f32[1,2048,1,1], arg181.182: f32[1,512,25,25], arg182.183: f32[1,2048,1,1], arg183.184: f32[1,2048,1,1], arg184.185: f32[1,512,1,1], arg185.186: f32[1,512,25,25], arg186.187: f32[1,512,1,1], arg187.188: f32[1,512,51,51], arg188.189: f32[1,512,1,1], arg189.190: f32[1,512,50,50], arg190.191: f32[1,512,50,50], arg191.192: f32[1,512,1,1], arg192.193: f32[1,512,1,1], arg193.194: f32[1,512,1,1], arg194.195: f32[1,1024,50,50], arg195.196: f32[1,1024,1,1], arg196.197: f32[1,1024,1,1], arg197.198: f32[1,256,50,50], arg198.199: f32[1,1024,1,1], arg199.200: f32[1,256,50,50], arg200.201: f32[1,256,1,1], arg201.202: f32[1,256,1,1], arg202.203: f32[1,256,50,50], arg203.204: f32[1,256,1,1], arg204.205: f32[1,256,1,1], arg205.206: f32[1,256,50,50], arg206.207: f32[1,256,1,1], arg207.208: f32[1,1024,50,50], arg208.209: f32[1,256,1,1], arg209.210: f32[1,1024,1,1], arg210.211: f32[1,1024,50,50], arg211.212: f32[1,1024,1,1], arg212.213: f32[1,256,50,50], arg213.214: f32[1,1024,1,1], arg214.215: f32[1,256,1,1], arg215.216: f32[1,256,50,50], arg216.217: f32[1,256,1,1], arg217.218: f32[1,256,50,50], arg218.219: f32[1,256,1,1], arg219.220: f32[1,256,1,1], arg220.221: f32[1,256,50,50], arg221.222: f32[1,256,1,1], arg222.223: f32[1,1024,50,50], arg223.224: f32[1,256,1,1], arg224.225: f32[1,1024,1,1], arg225.226: f32[1,1024,50,50], arg226.227: f32[1,1024,1,1], arg227.228: f32[1,256,50,50], arg228.229: f32[1,1024,1,1], arg229.230: f32[1,256,1,1], arg230.231: f32[1,256,50,50], arg231.232: f32[1,256,1,1], arg232.233: f32[1,256,50,50], arg233.234: f32[1,256,1,1], arg234.235: f32[1,256,1,1], arg235.236: f32[1,256,50,50], arg236.237: f32[1,256,1,1], arg237.238: f32[1,1024,50,50], arg238.239: f32[1,256,1,1], arg239.240: f32[1,1024,1,1], arg240.241: f32[1,1024,50,50], arg241.242: f32[1,1024,1,1], arg242.243: f32[1,256,50,50], arg243.244: f32[1,1024,1,1], arg244.245: f32[1,256,1,1], arg245.246: f32[1,256,50,50], arg246.247: f32[1,256,1,1], arg247.248: f32[1,256,50,50], arg248.249: f32[1,256,1,1], arg249.250: f32[1,256,1,1], arg250.251: f32[1,256,50,50], arg251.252: f32[1,256,1,1], arg252.253: f32[1,1024,50,50], arg253.254: f32[1,256,1,1], arg254.255: f32[1,1024,1,1], arg255.256: f32[1,1024,50,50], arg256.257: f32[1,1024,1,1], arg257.258: f32[1,256,50,50], arg258.259: f32[1,1024,1,1], arg259.260: f32[1,256,1,1], arg260.261: f32[1,256,50,50], arg261.262: f32[1,256,1,1], arg262.263: f32[1,256,50,50], arg263.264: f32[1,256,1,1], arg264.265: f32[1,256,1,1], arg265.266: f32[1,256,50,50], arg266.267: f32[1,256,1,1], arg267.268: f32[1,1024,50,50], arg268.269: f32[1,256,1,1], arg269.270: f32[1,1024,1,1], arg270.271: f32[1,1024,50,50], arg271.272: f32[1,1024,1,1], arg272.273: f32[1,1024,50,50], arg273.274: f32[1,1024,1,1], arg274.275: f32[1,1024,1,1], arg275.276: f32[1,256,50,50], arg276.277: f32[1,1024,1,1], arg277.278: f32[1,1024,1,1], arg278.279: f32[1,256,1,1], arg279.280: f32[1,256,50,50], arg280.281: f32[1,256,1,1], arg281.282: f32[1,256,101,101], arg282.283: f32[1,256,1,1], arg283.284: f32[1,256,100,100], arg284.285: f32[1,256,1,1], arg285.286: f32[1,256,100,100], arg286.287: f32[1,256,1,1], arg287.288: f32[1,256,1,1], arg288.289: f32[1,512,1,1], arg289.290: f32[1,512,100,100], arg290.291: f32[1,512,1,1], arg291.292: f32[1,128,100,100], arg292.293: f32[1,512,1,1], arg293.294: f32[1,128,1,1], arg294.295: f32[1,128,100,100], arg295.296: f32[1,128,1,1], arg296.297: f32[1,128,100,100], arg297.298: f32[1,128,1,1], arg298.299: f32[1,128,1,1], arg299.300: f32[1,128,100,100], arg300.301: f32[1,128,1,1], arg301.302: f32[1,512,100,100], arg302.303: f32[1,128,1,1], arg303.304: f32[1,512,1,1], arg304.305: f32[1,512,100,100], arg305.306: f32[1,512,1,1], arg306.307: f32[1,128,100,100], arg307.308: f32[1,512,1,1], arg308.309: f32[1,128,1,1], arg309.310: f32[1,128,100,100], arg310.311: f32[1,128,1,1], arg311.312: f32[1,128,100,100], arg312.313: f32[1,128,1,1], arg313.314: f32[1,128,1,1], arg314.315: f32[1,128,100,100], arg315.316: f32[1,128,1,1], arg316.317: f32[1,512,100,100], arg317.318: f32[1,128,1,1], arg318.319: f32[1,512,1,1], arg319.320: f32[1,512,100,100], arg320.321: f32[1,512,1,1], arg321.322: f32[1,128,100,100], arg322.323: f32[1,512,1,1], arg323.324: f32[1,128,1,1], arg324.325: f32[1,128,100,100], arg325.326: f32[1,128,1,1], arg326.327: f32[1,128,100,100], arg327.328: f32[1,128,1,1], arg328.329: f32[1,128,1,1], arg329.330: f32[1,128,100,100], arg330.331: f32[1,128,1,1], arg331.332: f32[1,512,100,100], arg332.333: f32[1,128,1,1], arg333.334: f32[1,512,1,1], arg334.335: f32[1,512,100,100], arg335.336: f32[1,512,100,100], arg336.337: f32[1,512,1,1], arg337.338: f32[1,512,1,1], arg338.339: f32[1,512,1,1], arg339.340: f32[1,128,100,100], arg340.341: f32[1,512,1,1], arg341.342: f32[1,512,1,1], arg342.343: f32[1,128,100,100], arg343.344: f32[1,128,1,1], arg344.345: f32[1,128,1,1], arg345.346: f32[1,128,201,201], arg346.347: f32[1,128,1,1], arg347.348: f32[1,128,200,200], arg348.349: f32[1,128,200,200], arg349.350: f32[1,128,1,1], arg350.351: f32[1,128,1,1], arg351.352: f32[1,128,1,1]) -> (f32[3], f32[1,1,256,3], f32[12], f32[1,1,256,12], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[256], f32[256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[3,3,256,256], f32[256], f32[1,1,256,256], f32[256], f32[1,1,512,256], f32[256], f32[1,1,1024,256], f32[256], f32[1,1,2048,256], f32[2048], f32[1,1,512,2048], f32[2048], f32[512], f32[3,3,512,512], f32[512], f32[512], f32[1,1,2048,512], f32[512], f32[2048], f32[1,1,512,2048], f32[2048], f32[512], f32[3,3,512,512], f32[512], f32[512], f32[1,1,2048,512], f32[512], f32[2048], f32[2048], f32[1,1,512,2048], f32[1,1,1024,2048], f32[2048], f32[2048], f32[512], f32[3,3,512,512], f32[512], f32[512], f32[1,1,1024,512], f32[512], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1,1,256,1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,1024,256], f32[256], f32[1024], f32[1024], f32[1,1,512,1024], f32[1,1,256,1024], f32[1024], f32[1024], f32[256], f32[3,3,256,256], f32[256], f32[256], f32[1,1,512,256], f32[256], f32[512], f32[1,1,128,512], f32[512], f32[128], f32[3,3,128,128], f32[128], f32[128], f32[1,1,512,128], f32[128], f32[512], f32[1,1,128,512], f32[512], f32[128], f32[3,3,128,128], f32[128], f32[128], f32[1,1,512,128], f32[128], f32[512], f32[1,1,128,512], f32[512], f32[128], f32[3,3,128,128], f32[128], f32[128], f32[1,1,512,128], f32[128], f32[512], f32[512], f32[1,1,256,512], f32[1,1,128,512], f32[512], f32[512], f32[128], f32[3,3,128,128], f32[128], f32[128], f32[1,1,256,128], f32[128]) {
  %arg60.61 = f32[34]{0} parameter(60), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg61.62 = f32[2]{0} parameter(61), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg62.63 = f32[34]{0} parameter(62), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg63.64 = f32[2]{0} parameter(63), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg71.72 = f32[32]{0} parameter(71), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg73.74 = f32[32]{0} parameter(73), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg88.89 = f32[48]{0} parameter(88), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg92.93 = f32[48]{0} parameter(92), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg103.104 = f32[140]{0} parameter(103), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg106.107 = f32[140]{0} parameter(106), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg85.86 = s64[2]{0} parameter(85), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg78.79 = f32[2]{0} parameter(78), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg76.77 = pred[2]{0} parameter(76), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg74.75 = f32[2]{0} parameter(74), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg58.59 = f32[] parameter(58), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg56.57 = pred[] parameter(56), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.227 = f32[507]{0} fusion(s64[2]{0} %arg85.86, f32[2]{0} %arg78.79, pred[2]{0} %arg76.77, f32[2]{0} %arg74.75, f32[] %arg58.59, pred[] %arg56.57), kind=kInput, calls=%fused_computation.227, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level6/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %fusion.255 = f32[3]{0} fusion(f32[507]{0} %fusion.227), kind=kInput, calls=%fused_computation.255, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/class/BiasAdd_grad/BiasAddGrad"}
  %arg82.83 = s64[34]{0} parameter(82), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg77.78 = f32[34]{0} parameter(77), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg75.76 = pred[34]{0} parameter(75), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg72.73 = f32[34]{0} parameter(72), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg54.55 = pred[] parameter(54), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.230 = f32[1875]{0} fusion(s64[34]{0} %arg82.83, f32[34]{0} %arg77.78, pred[34]{0} %arg75.76, f32[34]{0} %arg72.73, f32[] %arg58.59, pred[] %arg54.55), kind=kInput, calls=%fused_computation.230, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %fusion.254 = f32[3]{0} fusion(f32[1875]{0} %fusion.230), kind=kInput, calls=%fused_computation.254, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/class/BiasAdd_grad/BiasAddGrad"}
  %arg98.99 = s64[32]{0} parameter(98), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg94.95 = f32[32]{0} parameter(94), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg93.94 = pred[32]{0} parameter(93), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg89.90 = f32[32]{0} parameter(89), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg64.65 = pred[] parameter(64), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.235 = f32[7500]{0} fusion(s64[32]{0} %arg98.99, f32[32]{0} %arg94.95, pred[32]{0} %arg93.94, f32[32]{0} %arg89.90, f32[] %arg58.59, pred[] %arg64.65), kind=kInput, calls=%fused_computation.235, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %fusion.253 = f32[3]{0} fusion(f32[7500]{0} %fusion.235), kind=kInput, calls=%fused_computation.253, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/class/BiasAdd_grad/BiasAddGrad"}
  %arg121.122 = s64[140]{0} parameter(121), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg119.120 = f32[140]{0} parameter(119), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg118.119 = pred[140]{0} parameter(118), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg117.118 = f32[140]{0} parameter(117), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg95.96 = pred[] parameter(95), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.244 = f32[120000]{0} fusion(s64[140]{0} %arg121.122, f32[140]{0} %arg119.120, pred[140]{0} %arg118.119, f32[140]{0} %arg117.118, f32[] %arg58.59, pred[] %arg95.96), kind=kInput, calls=%fused_computation.244, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level2/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %fusion.252 = f32[3]{0} fusion(f32[120000]{0} %fusion.244), kind=kInput, calls=%fused_computation.252, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/class/BiasAdd_grad/BiasAddGrad"}
  %arg112.113 = s64[48]{0} parameter(112), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg108.109 = f32[48]{0} parameter(108), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg107.108 = pred[48]{0} parameter(107), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg104.105 = f32[48]{0} parameter(104), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg79.80 = pred[] parameter(79), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.240 = f32[30000]{0} fusion(s64[48]{0} %arg112.113, f32[48]{0} %arg108.109, pred[48]{0} %arg107.108, f32[48]{0} %arg104.105, f32[] %arg58.59, pred[] %arg79.80), kind=kInput, calls=%fused_computation.240, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level3/boolean_mask_1/Reshape_grad/Reshape/tensor"}
  %fusion.251 = f32[3]{0} fusion(f32[30000]{0} %fusion.240), kind=kInput, calls=%fused_computation.251, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/class/BiasAdd_grad/BiasAddGrad"}
  %fusion.142 = f32[3]{0} fusion(f32[3]{0} %fusion.255, f32[3]{0} %fusion.254, f32[3]{0} %fusion.253, f32[3]{0} %fusion.252, f32[3]{0} %fusion.251), kind=kLoop, calls=%fused_computation.142, metadata={op_type="AddN" op_name="tower0/gradients/AddN_84"}
  %arg49.50 = f32[1,1,256,3]{3,2,1,0} parameter(49), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg90.91 = f32[1,256,13,13]{3,2,1,0} parameter(90), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.256 = f32[1,3,13,13]{3,2,1,0} fusion(f32[507]{0} %fusion.227), kind=kLoop, calls=%fused_computation.256, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_4/transpose_grad/transpose"}
  %custom-call.164 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,13,13]{3,2,1,0} %arg90.91, f32[1,3,13,13]{3,2,1,0} %fusion.256), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_4/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.460 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.164), index=0
  %arg86.87 = f32[1,256,25,25]{3,2,1,0} parameter(86), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.258 = f32[1,3,25,25]{3,2,1,0} fusion(f32[1875]{0} %fusion.230), kind=kLoop, calls=%fused_computation.258, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_3/transpose_grad/transpose"}
  %custom-call.158 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %arg86.87, f32[1,3,25,25]{3,2,1,0} %fusion.258), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_3/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.454 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.158), index=0
  %arg100.101 = f32[1,256,50,50]{3,2,1,0} parameter(100), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.260 = f32[1,3,50,50]{3,2,1,0} fusion(f32[7500]{0} %fusion.235), kind=kLoop, calls=%fused_computation.260, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_2/transpose_grad/transpose"}
  %custom-call.148 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg100.101, f32[1,3,50,50]{3,2,1,0} %fusion.260), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_2/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.444 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.148), index=0
  %arg122.123 = f32[1,256,200,200]{3,2,1,0} parameter(122), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.264 = f32[1,3,200,200]{3,2,1,0} fusion(f32[120000]{0} %fusion.244), kind=kLoop, calls=%fused_computation.264, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn/transpose_grad/transpose"}
  %custom-call.129 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %arg122.123, f32[1,3,200,200]{3,2,1,0} %fusion.264), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.425 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.129), index=0
  %arg114.115 = f32[1,256,100,100]{3,2,1,0} parameter(114), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.262 = f32[1,3,100,100]{3,2,1,0} fusion(f32[30000]{0} %fusion.240), kind=kLoop, calls=%fused_computation.262, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_1/transpose_grad/transpose"}
  %custom-call.138 = (f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %arg114.115, f32[1,3,100,100]{3,2,1,0} %fusion.262), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_1/class/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.434 = f32[1,1,256,3]{1,0,2,3} get-tuple-element((f32[1,1,256,3]{1,0,2,3}, u8[0]{0}) %custom-call.138), index=0
  %fusion.141 = f32[1,1,256,3]{3,2,1,0} fusion(f32[1,1,256,3]{3,2,1,0} %arg49.50, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.460, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.454, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.444, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.425, f32[1,1,256,3]{1,0,2,3} %get-tuple-element.434), kind=kLoop, calls=%fused_computation.141, metadata={op_name="XLA_Retvals"}
  %arg105.106 = s64[2]{0} parameter(105), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg67.68 = f32[2,4]{1,0} parameter(67), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg69.70 = f32[] parameter(69), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg91.92 = f32[2,4]{1,0} parameter(91), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg83.84 = f32[] parameter(83), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg59.60 = f32[] parameter(59), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg57.58 = pred[] parameter(57), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg70.71 = f32[2,4]{1,0} parameter(70), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.228 = f32[507,4]{1,0} fusion(s64[2]{0} %arg105.106, f32[2,4]{1,0} %arg67.68, f32[] %arg69.70, f32[2,4]{1,0} %arg91.92, f32[] %arg83.84, f32[] %arg59.60, pred[] %arg57.58, f32[2,4]{1,0} %arg70.71), kind=kInput, calls=%fused_computation.228, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level6/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %fusion.250 = f32[12]{0} fusion(f32[507,4]{1,0} %fusion.228), kind=kInput, calls=%fused_computation.250, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/box/BiasAdd_grad/BiasAddGrad"}
  %arg102.103 = s64[33]{0} parameter(102), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg66.67 = f32[33,4]{1,0} parameter(66), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg87.88 = f32[33,4]{1,0} parameter(87), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg55.56 = pred[] parameter(55), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg68.69 = f32[33,4]{1,0} parameter(68), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.231 = f32[1875,4]{1,0} fusion(s64[33]{0} %arg102.103, f32[33,4]{1,0} %arg66.67, f32[] %arg69.70, f32[33,4]{1,0} %arg87.88, f32[] %arg83.84, f32[] %arg59.60, pred[] %arg55.56, f32[33,4]{1,0} %arg68.69), kind=kInput, calls=%fused_computation.231, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level5/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %fusion.249 = f32[12]{0} fusion(f32[1875,4]{1,0} %fusion.231), kind=kInput, calls=%fused_computation.249, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/box/BiasAdd_grad/BiasAddGrad"}
  %arg116.117 = s64[22]{0} parameter(116), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg81.82 = f32[22,4]{1,0} parameter(81), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg101.102 = f32[22,4]{1,0} parameter(101), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg65.66 = pred[] parameter(65), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg84.85 = f32[22,4]{1,0} parameter(84), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.236 = f32[7500,4]{1,0} fusion(s64[22]{0} %arg116.117, f32[22,4]{1,0} %arg81.82, f32[] %arg69.70, f32[22,4]{1,0} %arg101.102, f32[] %arg83.84, f32[] %arg59.60, pred[] %arg65.66, f32[22,4]{1,0} %arg84.85), kind=kInput, calls=%fused_computation.236, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level4/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %fusion.248 = f32[12]{0} fusion(f32[7500,4]{1,0} %fusion.236), kind=kInput, calls=%fused_computation.248, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/box/BiasAdd_grad/BiasAddGrad"}
  %arg126.127 = s64[4]{0} parameter(126), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg111.112 = f32[4,4]{1,0} parameter(111), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg123.124 = f32[4,4]{1,0} parameter(123), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg96.97 = pred[] parameter(96), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg113.114 = f32[4,4]{1,0} parameter(113), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.245 = f32[120000,4]{1,0} fusion(s64[4]{0} %arg126.127, f32[4,4]{1,0} %arg111.112, f32[] %arg69.70, f32[4,4]{1,0} %arg123.124, f32[] %arg83.84, f32[] %arg59.60, pred[] %arg96.97, f32[4,4]{1,0} %arg113.114), kind=kInput, calls=%fused_computation.245, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level2/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %fusion.247 = f32[12]{0} fusion(f32[120000,4]{1,0} %fusion.245), kind=kInput, calls=%fused_computation.247, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/box/BiasAdd_grad/BiasAddGrad"}
  %arg124.125 = s64[11]{0} parameter(124), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg97.98 = f32[11,4]{1,0} parameter(97), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg115.116 = f32[11,4]{1,0} parameter(115), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg80.81 = pred[] parameter(80), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg99.100 = f32[11,4]{1,0} parameter(99), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.241 = f32[30000,4]{1,0} fusion(s64[11]{0} %arg124.125, f32[11,4]{1,0} %arg97.98, f32[] %arg69.70, f32[11,4]{1,0} %arg115.116, f32[] %arg83.84, f32[] %arg59.60, pred[] %arg80.81, f32[11,4]{1,0} %arg99.100), kind=kInput, calls=%fused_computation.241, metadata={op_type="UnsortedSegmentSum" op_name="tower0/gradients/tower0/rpn_losses/level3/boolean_mask_3/Reshape_grad/Reshape/tensor"}
  %fusion.246 = f32[12]{0} fusion(f32[30000,4]{1,0} %fusion.241), kind=kInput, calls=%fused_computation.246, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/box/BiasAdd_grad/BiasAddGrad"}
  %fusion.140 = f32[12]{0} fusion(f32[12]{0} %fusion.250, f32[12]{0} %fusion.249, f32[12]{0} %fusion.248, f32[12]{0} %fusion.247, f32[12]{0} %fusion.246), kind=kLoop, calls=%fused_computation.140, metadata={op_type="AddN" op_name="tower0/gradients/AddN_86"}
  %arg50.51 = f32[1,1,256,12]{3,2,1,0} parameter(50), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.257 = f32[1,12,13,13]{3,2,1,0} fusion(f32[507,4]{1,0} %fusion.228), kind=kLoop, calls=%fused_computation.257, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_4/transpose_1_grad/transpose"}
  %custom-call.162 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,13,13]{3,2,1,0} %arg90.91, f32[1,12,13,13]{3,2,1,0} %fusion.257), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_4/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.458 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.162), index=0
  %fusion.259 = f32[1,12,25,25]{3,2,1,0} fusion(f32[1875,4]{1,0} %fusion.231), kind=kLoop, calls=%fused_computation.259, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_3/transpose_1_grad/transpose"}
  %custom-call.156 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %arg86.87, f32[1,12,25,25]{3,2,1,0} %fusion.259), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_3/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.452 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.156), index=0
  %fusion.261 = f32[1,12,50,50]{3,2,1,0} fusion(f32[7500,4]{1,0} %fusion.236), kind=kLoop, calls=%fused_computation.261, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_2/transpose_1_grad/transpose"}
  %custom-call.146 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg100.101, f32[1,12,50,50]{3,2,1,0} %fusion.261), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_2/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.442 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.146), index=0
  %fusion.265 = f32[1,12,200,200]{3,2,1,0} fusion(f32[120000,4]{1,0} %fusion.245), kind=kLoop, calls=%fused_computation.265, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn/transpose_1_grad/transpose"}
  %custom-call.127 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %arg122.123, f32[1,12,200,200]{3,2,1,0} %fusion.265), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.423 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.127), index=0
  %fusion.263 = f32[1,12,100,100]{3,2,1,0} fusion(f32[30000,4]{1,0} %fusion.241), kind=kLoop, calls=%fused_computation.263, metadata={op_type="Transpose" op_name="tower0/gradients/tower0/rpn_1/transpose_1_grad/transpose"}
  %custom-call.136 = (f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %arg114.115, f32[1,12,100,100]{3,2,1,0} %fusion.263), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_1/box/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.432 = f32[1,1,256,12]{1,0,2,3} get-tuple-element((f32[1,1,256,12]{1,0,2,3}, u8[0]{0}) %custom-call.136), index=0
  %fusion.139 = f32[1,1,256,12]{3,2,1,0} fusion(f32[1,1,256,12]{3,2,1,0} %arg50.51, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.458, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.452, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.442, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.423, f32[1,1,256,12]{1,0,2,3} %get-tuple-element.432), kind=kLoop, calls=%fused_computation.139, metadata={op_name="XLA_Retvals"}
  %bitcast.10 = f32[1,1,256,12]{1,0,3,2} bitcast(f32[1,1,256,12]{3,2,1,0} %arg50.51), metadata={op_name="XLA_Args"}
  %custom-call.163 = (f32[1,256,13,13]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,12,13,13]{3,2,1,0} %fusion.257, f32[1,1,256,12]{1,0,3,2} %bitcast.10), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_4/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.459 = f32[1,256,13,13]{3,2,1,0} get-tuple-element((f32[1,256,13,13]{3,2,1,0}, u8[0]{0}) %custom-call.163), index=0
  %bitcast.11 = f32[1,1,256,3]{1,0,3,2} bitcast(f32[1,1,256,3]{3,2,1,0} %arg49.50), metadata={op_name="XLA_Args"}
  %custom-call.165 = (f32[1,256,13,13]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,3,13,13]{3,2,1,0} %fusion.256, f32[1,1,256,3]{1,0,3,2} %bitcast.11), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_4/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.461 = f32[1,256,13,13]{3,2,1,0} get-tuple-element((f32[1,256,13,13]{3,2,1,0}, u8[0]{0}) %custom-call.165), index=0
  %fusion.266 = (f32[256]{0}, f32[1,256,13,13]{3,2,1,0}) fusion(f32[1,256,13,13]{3,2,1,0} %get-tuple-element.459, f32[1,256,13,13]{3,2,1,0} %get-tuple-element.461, f32[1,256,13,13]{3,2,1,0} %arg90.91), kind=kInput, calls=%fused_computation.266, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_4/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.668 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,13,13]{3,2,1,0}) %fusion.266), index=0
  %custom-call.157 = (f32[1,256,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,12,25,25]{3,2,1,0} %fusion.259, f32[1,1,256,12]{1,0,3,2} %bitcast.10), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_3/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.453 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[1,256,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.157), index=0
  %custom-call.159 = (f32[1,256,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,3,25,25]{3,2,1,0} %fusion.258, f32[1,1,256,3]{1,0,3,2} %bitcast.11), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_3/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.455 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[1,256,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.159), index=0
  %fusion.267 = (f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) fusion(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.453, f32[1,256,25,25]{3,2,1,0} %get-tuple-element.455, f32[1,256,25,25]{3,2,1,0} %arg86.87), kind=kInput, calls=%fused_computation.267, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_3/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.670 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) %fusion.267), index=0
  %custom-call.147 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,12,50,50]{3,2,1,0} %fusion.261, f32[1,1,256,12]{1,0,3,2} %bitcast.10), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_2/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.443 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.147), index=0
  %custom-call.149 = (f32[1,256,50,50]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,3,50,50]{3,2,1,0} %fusion.260, f32[1,1,256,3]{1,0,3,2} %bitcast.11), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_2/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.445 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[0]{0}) %custom-call.149), index=0
  %fusion.268 = (f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.443, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.445, f32[1,256,50,50]{3,2,1,0} %arg100.101), kind=kInput, calls=%fused_computation.268, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_2/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.672 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.268), index=0
  %custom-call.128 = (f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,12,200,200]{3,2,1,0} %fusion.265, f32[1,1,256,12]{1,0,3,2} %bitcast.10), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.424 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.128), index=0
  %custom-call.130 = (f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,3,200,200]{3,2,1,0} %fusion.264, f32[1,1,256,3]{1,0,3,2} %bitcast.11), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.426 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.130), index=0
  %fusion.269 = (f32[256]{0}, f32[1,256,200,200]{3,2,1,0}) fusion(f32[1,256,200,200]{3,2,1,0} %get-tuple-element.424, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.426, f32[1,256,200,200]{3,2,1,0} %arg122.123), kind=kInput, calls=%fused_computation.269, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.674 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,200,200]{3,2,1,0}) %fusion.269), index=0
  %custom-call.137 = (f32[1,256,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,12,100,100]{3,2,1,0} %fusion.263, f32[1,1,256,12]{1,0,3,2} %bitcast.10), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_1/box/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.433 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[1,256,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.137), index=0
  %custom-call.139 = (f32[1,256,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,3,100,100]{3,2,1,0} %fusion.262, f32[1,1,256,3]{1,0,3,2} %bitcast.11), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_1/class/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.435 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[1,256,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.139), index=0
  %fusion.270 = (f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) fusion(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.433, f32[1,256,100,100]{3,2,1,0} %get-tuple-element.435, f32[1,256,100,100]{3,2,1,0} %arg114.115), kind=kInput, calls=%fused_computation.270, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/rpn_1/conv0/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.676 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) %fusion.270), index=0
  %fusion.138 = f32[256]{0} fusion(f32[256]{0} %get-tuple-element.668, f32[256]{0} %get-tuple-element.670, f32[256]{0} %get-tuple-element.672, f32[256]{0} %get-tuple-element.674, f32[256]{0} %get-tuple-element.676), kind=kLoop, calls=%fused_computation.138, metadata={op_type="AddN" op_name="tower0/gradients/AddN_93"}
  %arg217.218 = f32[1,256,50,50]{3,2,1,0} parameter(217), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg215.216 = f32[1,256,50,50]{3,2,1,0} parameter(215), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg210.211 = f32[1,1024,50,50]{3,2,1,0} parameter(210), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg205.206 = f32[1,256,50,50]{3,2,1,0} parameter(205), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg199.200 = f32[1,256,50,50]{3,2,1,0} parameter(199), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg194.195 = f32[1,1024,50,50]{3,2,1,0} parameter(194), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg176.177 = f32[1,2048,25,25]{3,2,1,0} parameter(176), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg178.179 = f32[1,2048,25,25]{3,2,1,0} parameter(178), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg171.172 = f32[1,512,25,25]{3,2,1,0} parameter(171), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg166.167 = f32[1,512,25,25]{3,2,1,0} parameter(166), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg161.162 = f32[1,2048,25,25]{3,2,1,0} parameter(161), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg156.157 = f32[1,512,25,25]{3,2,1,0} parameter(156), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg151.152 = f32[1,512,25,25]{3,2,1,0} parameter(151), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg146.147 = f32[1,2048,25,25]{3,2,1,0} parameter(146), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.671 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) %fusion.267), index=1
  %arg47.48 = f32[3,3,256,256]{3,2,1,0} parameter(47), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.196 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg47.48), metadata={op_name="XLA_Args"}
  %custom-call.161 = (f32[1,256,25,25]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.671, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_3/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.457 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[1,256,25,25]{3,2,1,0}, u8[6554624]{0}) %custom-call.161), index=0
  %arg135.136 = f32[1,256,25,25]{3,2,1,0} parameter(135), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg109.110 = f32[1,256,25,25]{3,2,1,0} parameter(109), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.669 = f32[1,256,13,13]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,13,13]{3,2,1,0}) %fusion.266), index=1
  %custom-call.167 = (f32[1,256,13,13]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,13,13]{3,2,1,0} %get-tuple-element.669, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_4/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.463 = f32[1,256,13,13]{3,2,1,0} get-tuple-element((f32[1,256,13,13]{3,2,1,0}, u8[6554624]{0}) %custom-call.167), index=0
  %constant_794 = f32[] constant(0), metadata={op_type="ZerosLike" op_name="tower0/gradients/tower0/rpn_losses/level2/logistic_loss/Select_grad/zeros_like"}
  %select-and-scatter.1570 = f32[1,256,25,25]{3,2,1,0} select-and-scatter(f32[1,256,25,25]{3,2,1,0} %arg109.110, f32[1,256,13,13]{3,2,1,0} %get-tuple-element.463, f32[] %constant_794), window={size=1x1x1x1 stride=1x1x2x2}, select=%ge_F32.1562, scatter=%add_F32.1566, metadata={op_type="MaxPoolGrad" op_name="tower0/gradients/tower0/fpn/maxpool_p6/MaxPool_grad/MaxPoolGrad"}
  %arg134.135 = f32[1,256,25,25]{3,2,1,0} parameter(134), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.274 = (f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) fusion(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.457, f32[1,256,25,25]{3,2,1,0} %arg135.136, f32[1,256,25,25]{3,2,1,0} %select-and-scatter.1570, f32[1,256,25,25]{3,2,1,0} %arg134.135), kind=kInput, calls=%fused_computation.274, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p5/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.685 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) %fusion.274), index=1
  %arg51.52 = f32[3,3,256,256]{3,2,1,0} parameter(51), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.217 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg51.52), metadata={op_name="XLA_Args"}
  %custom-call.169 = (f32[1,256,25,25]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.685, f32[3,3,256,256]{1,0,2,3} %copy.217), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p5/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.465 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[1,256,25,25]{3,2,1,0}, u8[6554624]{0}) %custom-call.169), index=0
  %arg141.142 = f32[1,4]{1,0} parameter(141), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.673 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.268), index=1
  %custom-call.151 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.673, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_2/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.447 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.151), index=0
  %arg132.133 = f32[1,256,50,50]{3,2,1,0} parameter(132), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg133.134 = f32[1,256,50,50]{3,2,1,0} parameter(133), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.273 = (f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.447, f32[1,256,50,50]{3,2,1,0} %arg132.133, f32[1,256,50,50]{3,2,1,0} %arg133.134), kind=kInput, calls=%fused_computation.273, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p4/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.683 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.273), index=1
  %arg52.53 = f32[3,3,256,256]{3,2,1,0} parameter(52), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.214 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg52.53), metadata={op_name="XLA_Args"}
  %custom-call.153 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.683, f32[3,3,256,256]{1,0,2,3} %copy.214), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p4/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.449 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.153), index=0
  %get-tuple-element.677 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) %fusion.270), index=1
  %custom-call.141 = (f32[1,256,100,100]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.677, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn_1/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.437 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[1,256,100,100]{3,2,1,0}, u8[6554624]{0}) %custom-call.141), index=0
  %arg130.131 = f32[1,256,100,100]{3,2,1,0} parameter(130), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg131.132 = f32[1,256,100,100]{3,2,1,0} parameter(131), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.272 = (f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) fusion(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.437, f32[1,256,100,100]{3,2,1,0} %arg130.131, f32[1,256,100,100]{3,2,1,0} %arg131.132), kind=kInput, calls=%fused_computation.272, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p3/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.681 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) %fusion.272), index=1
  %arg53.54 = f32[3,3,256,256]{3,2,1,0} parameter(53), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.211 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg53.54), metadata={op_name="XLA_Args"}
  %custom-call.143 = (f32[1,256,100,100]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.681, f32[3,3,256,256]{1,0,2,3} %copy.211), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.439 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[1,256,100,100]{3,2,1,0}, u8[6554624]{0}) %custom-call.143), index=0
  %get-tuple-element.675 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,200,200]{3,2,1,0}) %fusion.269), index=1
  %custom-call.132 = (f32[1,256,200,200]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %get-tuple-element.675, f32[3,3,256,256]{1,0,2,3} %copy.196), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/rpn/conv0/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.428 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[6554624]{0}) %custom-call.132), index=0
  %arg128.129 = f32[1,256,200,200]{3,2,1,0} parameter(128), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg129.130 = f32[1,256,200,200]{3,2,1,0} parameter(129), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.271 = (f32[256]{0}, f32[1,256,200,200]{3,2,1,0}) fusion(f32[1,256,200,200]{3,2,1,0} %get-tuple-element.428, f32[1,256,200,200]{3,2,1,0} %arg128.129, f32[1,256,200,200]{3,2,1,0} %arg129.130), kind=kInput, calls=%fused_computation.271, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p2/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.679 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,200,200]{3,2,1,0}) %fusion.271), index=1
  %arg48.49 = f32[3,3,256,256]{3,2,1,0} parameter(48), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.209 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg48.49), metadata={op_name="XLA_Args"}
  %custom-call.134 = (f32[1,256,200,200]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %get-tuple-element.679, f32[3,3,256,256]{1,0,2,3} %copy.209), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.430 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[6554624]{0}) %custom-call.134), index=0
  %fusion.275 = (f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) fusion(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.439, f32[1,4]{1,0} %arg141.142, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.430), kind=kInput, calls=%fused_computation.275, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c3/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.687 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) %fusion.275), index=1
  %fusion.276 = (f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.449, f32[1,4]{1,0} %arg141.142, f32[1,256,100,100]{3,2,1,0} %get-tuple-element.687), kind=kInput, calls=%fused_computation.276, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c4/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.689 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.276), index=1
  %fusion.277 = (f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) fusion(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.465, f32[1,4]{1,0} %arg141.142, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.689), kind=kInput, calls=%fused_computation.277, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c5/BiasAdd_grad/BiasAddGrad"}
  %get-tuple-element.691 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) %fusion.277), index=1
  %arg43.44 = f32[1,1,2048,256]{3,2,1,0} parameter(43), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.33 = f32[1,1,2048,256]{1,0,3,2} bitcast(f32[1,1,2048,256]{3,2,1,0} %arg43.44), metadata={op_name="XLA_Args"}
  %custom-call.171 = (f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.691, f32[1,1,2048,256]{1,0,3,2} %bitcast.33), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c5/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.467 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) %custom-call.171), index=0
  %arg144.145 = f32[1,2048,25,25]{3,2,1,0} parameter(144), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.127 = (f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) fusion(f32[1,2048,25,25]{3,2,1,0} %arg146.147, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.467, f32[1,2048,25,25]{3,2,1,0} %arg144.145), kind=kInput, calls=%fused_computation.127, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.692 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.127), index=2
  %arg145.146 = f32[1,2048,1,1]{3,2,1,0} parameter(145), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.222 = f32[1,2048,25,25]{3,2,1,0} fusion(f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.692, f32[1,2048,1,1]{3,2,1,0} %arg145.146), kind=kLoop, calls=%fused_computation.222, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg42.43 = f32[1,1,512,2048]{3,2,1,0} parameter(42), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.38 = f32[1,1,512,2048]{1,0,3,2} bitcast(f32[1,1,512,2048]{3,2,1,0} %arg42.43), metadata={op_name="XLA_Args"}
  %custom-call.173 = (f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %fusion.222, f32[1,1,512,2048]{1,0,3,2} %bitcast.38), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block2/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.469 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.173), index=0
  %arg148.149 = f32[1,512,25,25]{3,2,1,0} parameter(148), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.124 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) fusion(f32[1,512,25,25]{3,2,1,0} %arg151.152, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.469, f32[1,512,25,25]{3,2,1,0} %arg148.149), kind=kInput, calls=%fused_computation.124, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.693 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.124), index=2
  %arg150.151 = f32[1,512,1,1]{3,2,1,0} parameter(150), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.220 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,25,25]{3,2,1,0} %get-tuple-element.693, f32[1,512,1,1]{3,2,1,0} %arg150.151), kind=kLoop, calls=%fused_computation.220, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg41.42 = f32[3,3,512,512]{3,2,1,0} parameter(41), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.224 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg41.42), metadata={op_name="XLA_Args"}
  %custom-call.175 = (f32[1,512,25,25]{3,2,1,0}, u8[26216448]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.220, f32[3,3,512,512]{1,0,2,3} %copy.224), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block2/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.471 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[26216448]{0}) %custom-call.175), index=0
  %arg153.154 = f32[1,512,25,25]{3,2,1,0} parameter(153), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.121 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) fusion(f32[1,512,25,25]{3,2,1,0} %arg156.157, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.471, f32[1,512,25,25]{3,2,1,0} %arg153.154), kind=kInput, calls=%fused_computation.121, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.694 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.121), index=2
  %arg155.156 = f32[1,512,1,1]{3,2,1,0} parameter(155), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.218 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,25,25]{3,2,1,0} %get-tuple-element.694, f32[1,512,1,1]{3,2,1,0} %arg155.156), kind=kLoop, calls=%fused_computation.218, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg40.41 = f32[1,1,2048,512]{3,2,1,0} parameter(40), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.47 = f32[1,1,2048,512]{1,0,3,2} bitcast(f32[1,1,2048,512]{3,2,1,0} %arg40.41), metadata={op_name="XLA_Args"}
  %custom-call.177 = (f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.218, f32[1,1,2048,512]{1,0,3,2} %bitcast.47), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block2/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.473 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) %custom-call.177), index=0
  %arg158.159 = f32[1,2048,25,25]{3,2,1,0} parameter(158), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.118 = (f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) fusion(f32[1,2048,25,25]{3,2,1,0} %arg161.162, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.473, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.692, f32[1,2048,25,25]{3,2,1,0} %arg158.159), kind=kInput, calls=%fused_computation.118, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.695 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.118), index=2
  %arg160.161 = f32[1,2048,1,1]{3,2,1,0} parameter(160), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.216 = f32[1,2048,25,25]{3,2,1,0} fusion(f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.695, f32[1,2048,1,1]{3,2,1,0} %arg160.161), kind=kLoop, calls=%fused_computation.216, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg39.40 = f32[1,1,512,2048]{3,2,1,0} parameter(39), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.52 = f32[1,1,512,2048]{1,0,3,2} bitcast(f32[1,1,512,2048]{3,2,1,0} %arg39.40), metadata={op_name="XLA_Args"}
  %custom-call.179 = (f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %fusion.216, f32[1,1,512,2048]{1,0,3,2} %bitcast.52), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block1/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.475 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.179), index=0
  %arg163.164 = f32[1,512,25,25]{3,2,1,0} parameter(163), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.115 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) fusion(f32[1,512,25,25]{3,2,1,0} %arg166.167, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.475, f32[1,512,25,25]{3,2,1,0} %arg163.164), kind=kInput, calls=%fused_computation.115, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.696 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.115), index=2
  %arg165.166 = f32[1,512,1,1]{3,2,1,0} parameter(165), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.214 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,25,25]{3,2,1,0} %get-tuple-element.696, f32[1,512,1,1]{3,2,1,0} %arg165.166), kind=kLoop, calls=%fused_computation.214, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg38.39 = f32[3,3,512,512]{3,2,1,0} parameter(38), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.230 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg38.39), metadata={op_name="XLA_Args"}
  %custom-call.181 = (f32[1,512,25,25]{3,2,1,0}, u8[26216448]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.214, f32[3,3,512,512]{1,0,2,3} %copy.230), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block1/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.477 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[26216448]{0}) %custom-call.181), index=0
  %arg168.169 = f32[1,512,25,25]{3,2,1,0} parameter(168), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.112 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) fusion(f32[1,512,25,25]{3,2,1,0} %arg171.172, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.477, f32[1,512,25,25]{3,2,1,0} %arg168.169), kind=kInput, calls=%fused_computation.112, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.697 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.112), index=2
  %arg170.171 = f32[1,512,1,1]{3,2,1,0} parameter(170), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.212 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,25,25]{3,2,1,0} %get-tuple-element.697, f32[1,512,1,1]{3,2,1,0} %arg170.171), kind=kLoop, calls=%fused_computation.212, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg37.38 = f32[1,1,2048,512]{3,2,1,0} parameter(37), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.61 = f32[1,1,2048,512]{1,0,3,2} bitcast(f32[1,1,2048,512]{3,2,1,0} %arg37.38), metadata={op_name="XLA_Args"}
  %custom-call.183 = (f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.212, f32[1,1,2048,512]{1,0,3,2} %bitcast.61), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block1/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.479 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) %custom-call.183), index=0
  %arg173.174 = f32[1,2048,25,25]{3,2,1,0} parameter(173), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.108 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) fusion(f32[1,2048,25,25]{3,2,1,0} %arg176.177, f32[1,2048,25,25]{3,2,1,0} %arg178.179, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.479, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.695, f32[1,2048,25,25]{3,2,1,0} %arg173.174), kind=kInput, calls=%fused_computation.108, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.698 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.108), index=3
  %arg175.176 = f32[1,2048,1,1]{3,2,1,0} parameter(175), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg177.178 = f32[1,2048,1,1]{3,2,1,0} parameter(177), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.210 = (f32[1,2048,25,25]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}) fusion(f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.698, f32[1,2048,1,1]{3,2,1,0} %arg175.176, f32[1,2048,1,1]{3,2,1,0} %arg177.178), kind=kLoop, calls=%fused_computation.210, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %get-tuple-element.602 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[1,2048,25,25]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.210), index=1
  %arg36.37 = f32[1,1,1024,2048]{3,2,1,0} parameter(36), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.241 = f32[1,1,1024,2048]{1,0,2,3} copy(f32[1,1,1024,2048]{3,2,1,0} %arg36.37), metadata={op_name="XLA_Args"}
  %custom-call.191 = (f32[1,1024,50,50]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.602, f32[1,1,1024,2048]{1,0,2,3} %copy.241), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block0/convshortcut/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.487 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[0]{0}) %custom-call.191), index=0
  %arg44.45 = f32[1,1,1024,256]{3,2,1,0} parameter(44), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.78 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg44.45), metadata={op_name="XLA_Args"}
  %custom-call.155 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.689, f32[1,1,1024,256]{1,0,3,2} %bitcast.78), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c4/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.451 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.155), index=0
  %arg190.191 = f32[1,512,50,50]{3,2,1,0} parameter(190), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg185.186 = f32[1,512,25,25]{3,2,1,0} parameter(185), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.601 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[1,2048,25,25]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.210), index=0
  %arg35.36 = f32[1,1,512,2048]{3,2,1,0} parameter(35), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.69 = f32[1,1,512,2048]{1,0,3,2} bitcast(f32[1,1,512,2048]{3,2,1,0} %arg35.36), metadata={op_name="XLA_Args"}
  %custom-call.185 = (f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.601, f32[1,1,512,2048]{1,0,3,2} %bitcast.69), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block0/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.481 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.185), index=0
  %arg181.182 = f32[1,512,25,25]{3,2,1,0} parameter(181), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.102 = (f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) fusion(f32[1,512,25,25]{3,2,1,0} %arg185.186, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.481, f32[1,512,25,25]{3,2,1,0} %arg181.182), kind=kInput, calls=%fused_computation.102, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.699 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.102), index=2
  %arg184.185 = f32[1,512,1,1]{3,2,1,0} parameter(184), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.208 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,25,25]{3,2,1,0} %get-tuple-element.699, f32[1,512,1,1]{3,2,1,0} %arg184.185), kind=kLoop, calls=%fused_computation.208, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg34.35 = f32[3,3,512,512]{3,2,1,0} parameter(34), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.237 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg34.35), metadata={op_name="XLA_Args"}
  %custom-call.187 = (f32[1,512,51,51]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.208, f32[3,3,512,512]{1,0,2,3} %copy.237), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block0/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.483 = f32[1,512,51,51]{3,2,1,0} get-tuple-element((f32[1,512,51,51]{3,2,1,0}, u8[0]{0}) %custom-call.187), index=0
  %arg189.190 = f32[1,512,50,50]{3,2,1,0} parameter(189), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.99 = (f32[512]{0}, f32[512]{0}, f32[1,512,50,50]{3,2,1,0}) fusion(f32[1,512,50,50]{3,2,1,0} %arg190.191, f32[1,512,51,51]{3,2,1,0} %get-tuple-element.483, f32[1,512,50,50]{3,2,1,0} %arg189.190), kind=kInput, calls=%fused_computation.99, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.700 = f32[1,512,50,50]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,50,50]{3,2,1,0}) %fusion.99), index=2
  %arg191.192 = f32[1,512,1,1]{3,2,1,0} parameter(191), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.206 = f32[1,512,50,50]{3,2,1,0} fusion(f32[1,512,50,50]{3,2,1,0} %get-tuple-element.700, f32[1,512,1,1]{3,2,1,0} %arg191.192), kind=kLoop, calls=%fused_computation.206, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg0.1 = f32[1,1,1024,512]{3,2,1,0} parameter(0), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.79 = f32[1,1,1024,512]{1,0,3,2} bitcast(f32[1,1,1024,512]{3,2,1,0} %arg0.1), metadata={op_name="XLA_Args"}
  %custom-call.189 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,512,50,50]{3,2,1,0} %fusion.206, f32[1,1,1024,512]{1,0,3,2} %bitcast.79), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group3/block0/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.485 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.189), index=0
  %arg143.144 = f32[1,1024,50,50]{3,2,1,0} parameter(143), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.96 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) fusion(f32[1,1024,50,50]{3,2,1,0} %arg194.195, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.487, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.451, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.485, f32[1,1024,50,50]{3,2,1,0} %arg143.144), kind=kInput, calls=%fused_computation.96, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.701 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.96), index=2
  %arg195.196 = f32[1,1024,1,1]{3,2,1,0} parameter(195), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.203 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.701, f32[1,1024,1,1]{3,2,1,0} %arg195.196), kind=kLoop, calls=%fused_computation.203, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg1.2 = f32[1,1,256,1024]{3,2,1,0} parameter(1), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.84 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg1.2), metadata={op_name="XLA_Args"}
  %custom-call.193 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.203, f32[1,1,256,1024]{1,0,3,2} %bitcast.84), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block5/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.489 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.193), index=0
  %arg197.198 = f32[1,256,50,50]{3,2,1,0} parameter(197), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.93 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg199.200, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.489, f32[1,256,50,50]{3,2,1,0} %arg197.198), kind=kInput, calls=%fused_computation.93, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.702 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.93), index=2
  %arg200.201 = f32[1,256,1,1]{3,2,1,0} parameter(200), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.201 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.702, f32[1,256,1,1]{3,2,1,0} %arg200.201), kind=kLoop, calls=%fused_computation.201, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg2.3 = f32[3,3,256,256]{3,2,1,0} parameter(2), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.246 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg2.3), metadata={op_name="XLA_Args"}
  %custom-call.195 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.201, f32[3,3,256,256]{1,0,2,3} %copy.246), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block5/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.491 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.195), index=0
  %arg202.203 = f32[1,256,50,50]{3,2,1,0} parameter(202), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.90 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg205.206, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.491, f32[1,256,50,50]{3,2,1,0} %arg202.203), kind=kInput, calls=%fused_computation.90, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.703 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.90), index=2
  %arg204.205 = f32[1,256,1,1]{3,2,1,0} parameter(204), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.199 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.703, f32[1,256,1,1]{3,2,1,0} %arg204.205), kind=kLoop, calls=%fused_computation.199, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg4.5 = f32[1,1,1024,256]{3,2,1,0} parameter(4), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.93 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg4.5), metadata={op_name="XLA_Args"}
  %custom-call.197 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.199, f32[1,1,1024,256]{1,0,3,2} %bitcast.93), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block5/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.493 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.197), index=0
  %arg207.208 = f32[1,1024,50,50]{3,2,1,0} parameter(207), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.87 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) fusion(f32[1,1024,50,50]{3,2,1,0} %arg210.211, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.493, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.701, f32[1,1024,50,50]{3,2,1,0} %arg207.208), kind=kInput, calls=%fused_computation.87, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.704 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.87), index=2
  %arg209.210 = f32[1,1024,1,1]{3,2,1,0} parameter(209), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.197 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.704, f32[1,1024,1,1]{3,2,1,0} %arg209.210), kind=kLoop, calls=%fused_computation.197, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg5.6 = f32[1,1,256,1024]{3,2,1,0} parameter(5), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.98 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg5.6), metadata={op_name="XLA_Args"}
  %custom-call.199 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.197, f32[1,1,256,1024]{1,0,3,2} %bitcast.98), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block4/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.495 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.199), index=0
  %arg212.213 = f32[1,256,50,50]{3,2,1,0} parameter(212), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.84 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg215.216, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.495, f32[1,256,50,50]{3,2,1,0} %arg212.213), kind=kInput, calls=%fused_computation.84, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.705 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.84), index=2
  %arg214.215 = f32[1,256,1,1]{3,2,1,0} parameter(214), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.195 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.705, f32[1,256,1,1]{3,2,1,0} %arg214.215), kind=kLoop, calls=%fused_computation.195, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.200 = (f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg217.218, f32[1,256,50,50]{3,2,1,0} %fusion.195), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block4/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.496 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) %custom-call.200), index=0
  %arg3.4 = f32[3,3,256,256]{3,2,1,0} parameter(3), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg6.7 = f32[3,3,256,256]{3,2,1,0} parameter(6), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg281.282 = f32[1,256,101,101]{3,2,1,0} parameter(281), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg279.280 = f32[1,256,50,50]{3,2,1,0} parameter(279), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg270.271 = f32[1,1024,50,50]{3,2,1,0} parameter(270), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg272.273 = f32[1,1024,50,50]{3,2,1,0} parameter(272), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg265.266 = f32[1,256,50,50]{3,2,1,0} parameter(265), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg260.261 = f32[1,256,50,50]{3,2,1,0} parameter(260), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg255.256 = f32[1,1024,50,50]{3,2,1,0} parameter(255), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg250.251 = f32[1,256,50,50]{3,2,1,0} parameter(250), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg245.246 = f32[1,256,50,50]{3,2,1,0} parameter(245), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg240.241 = f32[1,1024,50,50]{3,2,1,0} parameter(240), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg235.236 = f32[1,256,50,50]{3,2,1,0} parameter(235), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg230.231 = f32[1,256,50,50]{3,2,1,0} parameter(230), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg225.226 = f32[1,1024,50,50]{3,2,1,0} parameter(225), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg220.221 = f32[1,256,50,50]{3,2,1,0} parameter(220), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.253 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg6.7), metadata={op_name="XLA_Args"}
  %custom-call.201 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.195, f32[3,3,256,256]{1,0,2,3} %copy.253), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block4/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.497 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.201), index=0
  %fusion.81 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg220.221, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.497, f32[1,256,50,50]{3,2,1,0} %arg217.218), kind=kInput, calls=%fused_computation.81, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.706 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.81), index=2
  %arg219.220 = f32[1,256,1,1]{3,2,1,0} parameter(219), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.193 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.706, f32[1,256,1,1]{3,2,1,0} %arg219.220), kind=kLoop, calls=%fused_computation.193, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg7.8 = f32[1,1,1024,256]{3,2,1,0} parameter(7), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.107 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg7.8), metadata={op_name="XLA_Args"}
  %custom-call.203 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.193, f32[1,1,1024,256]{1,0,3,2} %bitcast.107), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block4/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.499 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.203), index=0
  %arg222.223 = f32[1,1024,50,50]{3,2,1,0} parameter(222), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.78 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) fusion(f32[1,1024,50,50]{3,2,1,0} %arg225.226, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.499, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.704, f32[1,1024,50,50]{3,2,1,0} %arg222.223), kind=kInput, calls=%fused_computation.78, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.707 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.78), index=2
  %arg224.225 = f32[1,1024,1,1]{3,2,1,0} parameter(224), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.191 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.707, f32[1,1024,1,1]{3,2,1,0} %arg224.225), kind=kLoop, calls=%fused_computation.191, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg8.9 = f32[1,1,256,1024]{3,2,1,0} parameter(8), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.112 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg8.9), metadata={op_name="XLA_Args"}
  %custom-call.205 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.191, f32[1,1,256,1024]{1,0,3,2} %bitcast.112), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block3/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.501 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.205), index=0
  %arg227.228 = f32[1,256,50,50]{3,2,1,0} parameter(227), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.75 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg230.231, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.501, f32[1,256,50,50]{3,2,1,0} %arg227.228), kind=kInput, calls=%fused_computation.75, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.708 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.75), index=2
  %arg229.230 = f32[1,256,1,1]{3,2,1,0} parameter(229), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.189 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.708, f32[1,256,1,1]{3,2,1,0} %arg229.230), kind=kLoop, calls=%fused_computation.189, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg9.10 = f32[3,3,256,256]{3,2,1,0} parameter(9), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.260 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg9.10), metadata={op_name="XLA_Args"}
  %custom-call.207 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.189, f32[3,3,256,256]{1,0,2,3} %copy.260), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block3/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.503 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.207), index=0
  %arg232.233 = f32[1,256,50,50]{3,2,1,0} parameter(232), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.72 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg235.236, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.503, f32[1,256,50,50]{3,2,1,0} %arg232.233), kind=kInput, calls=%fused_computation.72, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.709 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.72), index=2
  %arg234.235 = f32[1,256,1,1]{3,2,1,0} parameter(234), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.187 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.709, f32[1,256,1,1]{3,2,1,0} %arg234.235), kind=kLoop, calls=%fused_computation.187, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg10.11 = f32[1,1,1024,256]{3,2,1,0} parameter(10), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.121 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg10.11), metadata={op_name="XLA_Args"}
  %custom-call.209 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.187, f32[1,1,1024,256]{1,0,3,2} %bitcast.121), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block3/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.505 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.209), index=0
  %arg237.238 = f32[1,1024,50,50]{3,2,1,0} parameter(237), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.69 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) fusion(f32[1,1024,50,50]{3,2,1,0} %arg240.241, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.505, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.707, f32[1,1024,50,50]{3,2,1,0} %arg237.238), kind=kInput, calls=%fused_computation.69, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.710 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.69), index=2
  %arg239.240 = f32[1,1024,1,1]{3,2,1,0} parameter(239), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.185 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.710, f32[1,1024,1,1]{3,2,1,0} %arg239.240), kind=kLoop, calls=%fused_computation.185, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg11.12 = f32[1,1,256,1024]{3,2,1,0} parameter(11), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.126 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg11.12), metadata={op_name="XLA_Args"}
  %custom-call.211 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.185, f32[1,1,256,1024]{1,0,3,2} %bitcast.126), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block2/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.507 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.211), index=0
  %arg242.243 = f32[1,256,50,50]{3,2,1,0} parameter(242), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.66 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg245.246, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.507, f32[1,256,50,50]{3,2,1,0} %arg242.243), kind=kInput, calls=%fused_computation.66, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.711 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.66), index=2
  %arg244.245 = f32[1,256,1,1]{3,2,1,0} parameter(244), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.183 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.711, f32[1,256,1,1]{3,2,1,0} %arg244.245), kind=kLoop, calls=%fused_computation.183, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg12.13 = f32[3,3,256,256]{3,2,1,0} parameter(12), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.267 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg12.13), metadata={op_name="XLA_Args"}
  %custom-call.213 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.183, f32[3,3,256,256]{1,0,2,3} %copy.267), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block2/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.509 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.213), index=0
  %arg247.248 = f32[1,256,50,50]{3,2,1,0} parameter(247), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.63 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg250.251, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.509, f32[1,256,50,50]{3,2,1,0} %arg247.248), kind=kInput, calls=%fused_computation.63, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.712 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.63), index=2
  %arg249.250 = f32[1,256,1,1]{3,2,1,0} parameter(249), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.181 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.712, f32[1,256,1,1]{3,2,1,0} %arg249.250), kind=kLoop, calls=%fused_computation.181, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg13.14 = f32[1,1,1024,256]{3,2,1,0} parameter(13), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.135 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg13.14), metadata={op_name="XLA_Args"}
  %custom-call.215 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.181, f32[1,1,1024,256]{1,0,3,2} %bitcast.135), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block2/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.511 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.215), index=0
  %arg252.253 = f32[1,1024,50,50]{3,2,1,0} parameter(252), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.60 = (f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) fusion(f32[1,1024,50,50]{3,2,1,0} %arg255.256, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.511, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.710, f32[1,1024,50,50]{3,2,1,0} %arg252.253), kind=kInput, calls=%fused_computation.60, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.713 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.60), index=2
  %arg254.255 = f32[1,1024,1,1]{3,2,1,0} parameter(254), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.179 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.713, f32[1,1024,1,1]{3,2,1,0} %arg254.255), kind=kLoop, calls=%fused_computation.179, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %arg14.15 = f32[1,1,256,1024]{3,2,1,0} parameter(14), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.140 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg14.15), metadata={op_name="XLA_Args"}
  %custom-call.217 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.179, f32[1,1,256,1024]{1,0,3,2} %bitcast.140), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block1/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.513 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.217), index=0
  %arg257.258 = f32[1,256,50,50]{3,2,1,0} parameter(257), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.57 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg260.261, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.513, f32[1,256,50,50]{3,2,1,0} %arg257.258), kind=kInput, calls=%fused_computation.57, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.714 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.57), index=2
  %arg259.260 = f32[1,256,1,1]{3,2,1,0} parameter(259), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.177 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.714, f32[1,256,1,1]{3,2,1,0} %arg259.260), kind=kLoop, calls=%fused_computation.177, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %arg15.16 = f32[3,3,256,256]{3,2,1,0} parameter(15), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.274 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg15.16), metadata={op_name="XLA_Args"}
  %custom-call.219 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.177, f32[3,3,256,256]{1,0,2,3} %copy.274), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block1/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"convResultScale\":1}"
  %get-tuple-element.515 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.219), index=0
  %arg262.263 = f32[1,256,50,50]{3,2,1,0} parameter(262), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.54 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg265.266, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.515, f32[1,256,50,50]{3,2,1,0} %arg262.263), kind=kInput, calls=%fused_computation.54, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.715 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.54), index=2
  %arg264.265 = f32[1,256,1,1]{3,2,1,0} parameter(264), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.175 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.715, f32[1,256,1,1]{3,2,1,0} %arg264.265), kind=kLoop, calls=%fused_computation.175, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %arg16.17 = f32[1,1,1024,256]{3,2,1,0} parameter(16), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.149 = f32[1,1,1024,256]{1,0,3,2} bitcast(f32[1,1,1024,256]{3,2,1,0} %arg16.17), metadata={op_name="XLA_Args"}
  %custom-call.221 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.175, f32[1,1,1024,256]{1,0,3,2} %bitcast.149), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block1/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.517 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.221), index=0
  %arg267.268 = f32[1,1024,50,50]{3,2,1,0} parameter(267), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.50 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) fusion(f32[1,1024,50,50]{3,2,1,0} %arg270.271, f32[1,1024,50,50]{3,2,1,0} %arg272.273, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.517, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.713, f32[1,1024,50,50]{3,2,1,0} %arg267.268), kind=kInput, calls=%fused_computation.50, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.716 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.50), index=3
  %arg271.272 = f32[1,1024,1,1]{3,2,1,0} parameter(271), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg269.270 = f32[1,1024,1,1]{3,2,1,0} parameter(269), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.173 = (f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}) fusion(f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.716, f32[1,1024,1,1]{3,2,1,0} %arg271.272, f32[1,1024,1,1]{3,2,1,0} %arg269.270), kind=kLoop, calls=%fused_computation.173, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %get-tuple-element.565 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.173), index=0
  %arg31.32 = f32[1,1,256,1024]{3,2,1,0} parameter(31), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.157 = f32[1,1,256,1024]{1,0,3,2} bitcast(f32[1,1,256,1024]{3,2,1,0} %arg31.32), metadata={op_name="XLA_Args"}
  %custom-call.223 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.565, f32[1,1,256,1024]{1,0,3,2} %bitcast.157), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block0/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.519 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.223), index=0
  %arg275.276 = f32[1,256,50,50]{3,2,1,0} parameter(275), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.44 = (f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) fusion(f32[1,256,50,50]{3,2,1,0} %arg279.280, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.519, f32[1,256,50,50]{3,2,1,0} %arg275.276), kind=kInput, calls=%fused_computation.44, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.717 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.44), index=2
  %arg278.279 = f32[1,256,1,1]{3,2,1,0} parameter(278), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.171 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.717, f32[1,256,1,1]{3,2,1,0} %arg278.279), kind=kLoop, calls=%fused_computation.171, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.224 = (f32[3,3,256,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,101,101]{3,2,1,0} %arg281.282, f32[1,256,50,50]{3,2,1,0} %fusion.171), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block0/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.520 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[0]{0}) %custom-call.224), index=0
  %arg32.33 = f32[3,3,256,256]{3,2,1,0} parameter(32), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.212 = (f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg247.248, f32[1,256,50,50]{3,2,1,0} %fusion.183), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block2/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.508 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) %custom-call.212), index=0
  %custom-call.194 = (f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg202.203, f32[1,256,50,50]{3,2,1,0} %fusion.201), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block5/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.490 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) %custom-call.194), index=0
  %arg136.137 = f32[1,256,200,200]{3,2,1,0} parameter(136), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.133 = (f32[3,3,256,256]{1,0,2,3}, u8[193757184]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %arg136.137, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.679), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.429 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[193757184]{0}) %custom-call.133), index=0
  %arg139.140 = f32[1,256,25,25]{3,2,1,0} parameter(139), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.168 = (f32[3,3,256,256]{1,0,2,3}, u8[13049856]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %arg139.140, f32[1,256,25,25]{3,2,1,0} %get-tuple-element.685), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p5/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.464 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[13049856]{0}) %custom-call.168), index=0
  %custom-call.218 = (f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg262.263, f32[1,256,50,50]{3,2,1,0} %fusion.177), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block1/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.514 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) %custom-call.218), index=0
  %arg110.111 = f32[1,256,13,13]{3,2,1,0} parameter(110), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.166 = (f32[3,3,256,256]{1,0,2,3}, u8[10616832]{0}) custom-call(f32[1,256,13,13]{3,2,1,0} %arg110.111, f32[1,256,13,13]{3,2,1,0} %get-tuple-element.669), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_4/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.462 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[10616832]{0}) %custom-call.166), index=0
  %custom-call.160 = (f32[3,3,256,256]{1,0,2,3}, u8[13049856]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %arg109.110, f32[1,256,25,25]{3,2,1,0} %get-tuple-element.671), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_3/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.456 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[13049856]{0}) %custom-call.160), index=0
  %arg120.121 = f32[1,256,50,50]{3,2,1,0} parameter(120), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.150 = (f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg120.121, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.673), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_2/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.446 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) %custom-call.150), index=0
  %arg127.128 = f32[1,256,200,200]{3,2,1,0} parameter(127), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.131 = (f32[3,3,256,256]{1,0,2,3}, u8[193757184]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %arg127.128, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.675), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.427 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[193757184]{0}) %custom-call.131), index=0
  %arg125.126 = f32[1,256,100,100]{3,2,1,0} parameter(125), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.140 = (f32[3,3,256,256]{1,0,2,3}, u8[55517184]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %arg125.126, f32[1,256,100,100]{3,2,1,0} %get-tuple-element.677), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/rpn_1/conv0/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.436 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[55517184]{0}) %custom-call.140), index=0
  %arg138.139 = f32[1,256,50,50]{3,2,1,0} parameter(138), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.152 = (f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg138.139, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.683), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p4/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.448 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) %custom-call.152), index=0
  %custom-call.206 = (f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg232.233, f32[1,256,50,50]{3,2,1,0} %fusion.189), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block3/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"convResultScale\":1}"
  %get-tuple-element.502 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[21897216]{0}) %custom-call.206), index=0
  %arg137.138 = f32[1,256,100,100]{3,2,1,0} parameter(137), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.142 = (f32[3,3,256,256]{1,0,2,3}, u8[55517184]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %arg137.138, f32[1,256,100,100]{3,2,1,0} %get-tuple-element.681), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/posthoc_3x3_p3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.438 = f32[3,3,256,256]{1,0,2,3} get-tuple-element((f32[3,3,256,256]{1,0,2,3}, u8[55517184]{0}) %custom-call.142), index=0
  %fusion.85 = (f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) fusion(f32[3,3,256,256]{1,0,2,3} %get-tuple-element.496, f32[3,3,256,256]{3,2,1,0} %arg3.4, f32[3,3,256,256]{3,2,1,0} %arg6.7, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.520, f32[3,3,256,256]{3,2,1,0} %arg32.33, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.508, f32[3,3,256,256]{3,2,1,0} %arg12.13, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.490, f32[3,3,256,256]{3,2,1,0} %arg2.3, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.429, f32[3,3,256,256]{3,2,1,0} %arg48.49, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.464, f32[3,3,256,256]{3,2,1,0} %arg51.52, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.514, f32[3,3,256,256]{3,2,1,0} %arg15.16, f32[3,3,256,256]{3,2,1,0} %arg47.48, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.462, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.456, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.446, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.427, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.436, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.448, f32[3,3,256,256]{3,2,1,0} %arg52.53, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.502, f32[3,3,256,256]{3,2,1,0} %arg9.10, f32[3,3,256,256]{1,0,2,3} %get-tuple-element.438, f32[3,3,256,256]{3,2,1,0} %arg53.54), kind=kLoop, calls=%fused_computation.85, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.646 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=5
  %get-tuple-element.678 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,200,200]{3,2,1,0}) %fusion.271), index=0
  %get-tuple-element.680 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) %fusion.272), index=0
  %get-tuple-element.682 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.273), index=0
  %get-tuple-element.684 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) %fusion.274), index=0
  %get-tuple-element.649 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=8
  %get-tuple-element.643 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=2
  %get-tuple-element.645 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=4
  %get-tuple-element.648 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=7
  %reduce.1290 = f32[256]{0} reduce(f32[1,256,200,200]{3,2,1,0} %get-tuple-element.430, f32[] %constant_794), dimensions={0,2,3}, to_apply=%add_float_.1286, metadata={op_type="BiasAddGrad" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c2/BiasAdd_grad/BiasAddGrad"}
  %arg140.141 = f32[1,256,200,200]{3,2,1,0} parameter(140), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.135 = (f32[1,1,256,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %arg140.141, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.430), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.431 = f32[1,1,256,256]{1,0,2,3} get-tuple-element((f32[1,1,256,256]{1,0,2,3}, u8[0]{0}) %custom-call.135), index=0
  %arg46.47 = f32[1,1,256,256]{3,2,1,0} parameter(46), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.132 = f32[1,1,256,256]{3,2,1,0} fusion(f32[1,1,256,256]{1,0,2,3} %get-tuple-element.431, f32[1,1,256,256]{3,2,1,0} %arg46.47), kind=kLoop, calls=%fused_computation.132, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.686 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) %fusion.275), index=0
  %arg142.143 = f32[1,512,100,100]{3,2,1,0} parameter(142), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.144 = (f32[1,1,512,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %arg142.143, f32[1,256,100,100]{3,2,1,0} %get-tuple-element.687), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.440 = f32[1,1,512,256]{1,0,2,3} get-tuple-element((f32[1,1,512,256]{1,0,2,3}, u8[0]{0}) %custom-call.144), index=0
  %arg45.46 = f32[1,1,512,256]{3,2,1,0} parameter(45), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.131 = f32[1,1,512,256]{3,2,1,0} fusion(f32[1,1,512,256]{1,0,2,3} %get-tuple-element.440, f32[1,1,512,256]{3,2,1,0} %arg45.46), kind=kLoop, calls=%fused_computation.131, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.688 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.276), index=0
  %custom-call.154 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %arg143.144, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.689), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c4/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.450 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.154), index=0
  %fusion.130 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.450, f32[1,1,1024,256]{3,2,1,0} %arg44.45), kind=kLoop, calls=%fused_computation.130, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.690 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[1,256,25,25]{3,2,1,0}) %fusion.277), index=0
  %custom-call.170 = (f32[1,1,2048,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %arg144.145, f32[1,256,25,25]{3,2,1,0} %get-tuple-element.691), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c5/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.466 = f32[1,1,2048,256]{1,0,2,3} get-tuple-element((f32[1,1,2048,256]{1,0,2,3}, u8[0]{0}) %custom-call.170), index=0
  %fusion.129 = f32[1,1,2048,256]{3,2,1,0} fusion(f32[1,1,2048,256]{1,0,2,3} %get-tuple-element.466, f32[1,1,2048,256]{3,2,1,0} %arg43.44), kind=kLoop, calls=%fused_computation.129, metadata={op_name="XLA_Retvals"}
  %get-tuple-element.581 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.127), index=1
  %custom-call.172 = (f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %arg148.149, f32[1,2048,25,25]{3,2,1,0} %fusion.222), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block2/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.468 = f32[1,1,512,2048]{1,0,2,3} get-tuple-element((f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) %custom-call.172), index=0
  %fusion.128 = f32[1,1,512,2048]{3,2,1,0} fusion(f32[1,1,512,2048]{1,0,2,3} %get-tuple-element.468, f32[1,1,512,2048]{3,2,1,0} %arg42.43), kind=kLoop, calls=%fused_computation.128, metadata={op_name="XLA_Retvals"}
  %arg149.150 = f32[1,2048,1,1]{3,2,1,0} parameter(149), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg147.148 = f32[1,2048,1,1]{3,2,1,0} parameter(147), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.580 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.127), index=0
  %fusion.126 = f32[2048]{0} fusion(f32[1,2048,1,1]{3,2,1,0} %arg149.150, f32[1,2048,1,1]{3,2,1,0} %arg147.148, f32[2048]{0} %get-tuple-element.581, f32[2048]{0} %get-tuple-element.580), kind=kLoop, calls=%fused_computation.126, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.659 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.124), index=1
  %custom-call.174 = (f32[3,3,512,512]{1,0,2,3}, u8[44974080]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %arg153.154, f32[1,512,25,25]{3,2,1,0} %fusion.220), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block2/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.470 = f32[3,3,512,512]{1,0,2,3} get-tuple-element((f32[3,3,512,512]{1,0,2,3}, u8[44974080]{0}) %custom-call.174), index=0
  %fusion.125 = f32[3,3,512,512]{3,2,1,0} fusion(f32[3,3,512,512]{1,0,2,3} %get-tuple-element.470, f32[3,3,512,512]{3,2,1,0} %arg41.42), kind=kLoop, calls=%fused_computation.125, metadata={op_name="XLA_Retvals"}
  %arg154.155 = f32[1,512,1,1]{3,2,1,0} parameter(154), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg152.153 = f32[1,512,1,1]{3,2,1,0} parameter(152), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.658 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.124), index=0
  %fusion.123 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg154.155, f32[1,512,1,1]{3,2,1,0} %arg152.153, f32[512]{0} %get-tuple-element.659, f32[512]{0} %get-tuple-element.658), kind=kLoop, calls=%fused_computation.123, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.661 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.121), index=1
  %custom-call.176 = (f32[1,1,2048,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %arg158.159, f32[1,512,25,25]{3,2,1,0} %fusion.218), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block2/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.472 = f32[1,1,2048,512]{1,0,2,3} get-tuple-element((f32[1,1,2048,512]{1,0,2,3}, u8[0]{0}) %custom-call.176), index=0
  %fusion.122 = f32[1,1,2048,512]{3,2,1,0} fusion(f32[1,1,2048,512]{1,0,2,3} %get-tuple-element.472, f32[1,1,2048,512]{3,2,1,0} %arg40.41), kind=kLoop, calls=%fused_computation.122, metadata={op_name="XLA_Retvals"}
  %arg159.160 = f32[1,512,1,1]{3,2,1,0} parameter(159), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg157.158 = f32[1,512,1,1]{3,2,1,0} parameter(157), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.660 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.121), index=0
  %fusion.120 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg159.160, f32[1,512,1,1]{3,2,1,0} %arg157.158, f32[512]{0} %get-tuple-element.661, f32[512]{0} %get-tuple-element.660), kind=kLoop, calls=%fused_computation.120, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block2/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.604 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.118), index=1
  %custom-call.178 = (f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %arg163.164, f32[1,2048,25,25]{3,2,1,0} %fusion.216), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block1/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.474 = f32[1,1,512,2048]{1,0,2,3} get-tuple-element((f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) %custom-call.178), index=0
  %fusion.119 = f32[1,1,512,2048]{3,2,1,0} fusion(f32[1,1,512,2048]{1,0,2,3} %get-tuple-element.474, f32[1,1,512,2048]{3,2,1,0} %arg39.40), kind=kLoop, calls=%fused_computation.119, metadata={op_name="XLA_Retvals"}
  %arg164.165 = f32[1,2048,1,1]{3,2,1,0} parameter(164), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg162.163 = f32[1,2048,1,1]{3,2,1,0} parameter(162), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.603 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.118), index=0
  %fusion.117 = f32[2048]{0} fusion(f32[1,2048,1,1]{3,2,1,0} %arg164.165, f32[1,2048,1,1]{3,2,1,0} %arg162.163, f32[2048]{0} %get-tuple-element.604, f32[2048]{0} %get-tuple-element.603), kind=kLoop, calls=%fused_computation.117, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.655 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.115), index=1
  %custom-call.180 = (f32[3,3,512,512]{1,0,2,3}, u8[44974080]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %arg168.169, f32[1,512,25,25]{3,2,1,0} %fusion.214), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block1/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.476 = f32[3,3,512,512]{1,0,2,3} get-tuple-element((f32[3,3,512,512]{1,0,2,3}, u8[44974080]{0}) %custom-call.180), index=0
  %fusion.116 = f32[3,3,512,512]{3,2,1,0} fusion(f32[3,3,512,512]{1,0,2,3} %get-tuple-element.476, f32[3,3,512,512]{3,2,1,0} %arg38.39), kind=kLoop, calls=%fused_computation.116, metadata={op_name="XLA_Retvals"}
  %arg169.170 = f32[1,512,1,1]{3,2,1,0} parameter(169), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg167.168 = f32[1,512,1,1]{3,2,1,0} parameter(167), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.654 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.115), index=0
  %fusion.114 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg169.170, f32[1,512,1,1]{3,2,1,0} %arg167.168, f32[512]{0} %get-tuple-element.655, f32[512]{0} %get-tuple-element.654), kind=kLoop, calls=%fused_computation.114, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.657 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.112), index=1
  %custom-call.182 = (f32[1,1,2048,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %arg173.174, f32[1,512,25,25]{3,2,1,0} %fusion.212), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block1/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.478 = f32[1,1,2048,512]{1,0,2,3} get-tuple-element((f32[1,1,2048,512]{1,0,2,3}, u8[0]{0}) %custom-call.182), index=0
  %fusion.113 = f32[1,1,2048,512]{3,2,1,0} fusion(f32[1,1,2048,512]{1,0,2,3} %get-tuple-element.478, f32[1,1,2048,512]{3,2,1,0} %arg37.38), kind=kLoop, calls=%fused_computation.113, metadata={op_name="XLA_Retvals"}
  %arg174.175 = f32[1,512,1,1]{3,2,1,0} parameter(174), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg172.173 = f32[1,512,1,1]{3,2,1,0} parameter(172), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.656 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.112), index=0
  %fusion.111 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg174.175, f32[1,512,1,1]{3,2,1,0} %arg172.173, f32[512]{0} %get-tuple-element.657, f32[512]{0} %get-tuple-element.656), kind=kLoop, calls=%fused_computation.111, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block1/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.600 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.108), index=2
  %copy.484 = f32[2048]{0} copy(f32[2048]{0} %get-tuple-element.600)
  %custom-call.184 = (f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %arg181.182, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.601), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block0/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.480 = f32[1,1,512,2048]{1,0,2,3} get-tuple-element((f32[1,1,512,2048]{1,0,2,3}, u8[0]{0}) %custom-call.184), index=0
  %fusion.110 = f32[1,1,512,2048]{3,2,1,0} fusion(f32[1,1,512,2048]{1,0,2,3} %get-tuple-element.480, f32[1,1,512,2048]{3,2,1,0} %arg35.36), kind=kLoop, calls=%fused_computation.110, metadata={op_name="XLA_Retvals"}
  %custom-call.190 = (f32[1,1,1024,2048]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %arg143.144, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.602), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block0/convshortcut/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.486 = f32[1,1,1024,2048]{1,0,2,3} get-tuple-element((f32[1,1,1024,2048]{1,0,2,3}, u8[0]{0}) %custom-call.190), index=0
  %fusion.109 = f32[1,1,1024,2048]{3,2,1,0} fusion(f32[1,1,1024,2048]{1,0,2,3} %get-tuple-element.486, f32[1,1,1024,2048]{3,2,1,0} %arg36.37), kind=kLoop, calls=%fused_computation.109, metadata={op_name="XLA_Retvals"}
  %arg182.183 = f32[1,2048,1,1]{3,2,1,0} parameter(182), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg179.180 = f32[1,2048,1,1]{3,2,1,0} parameter(179), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.598 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.108), index=0
  %arg183.184 = f32[1,2048,1,1]{3,2,1,0} parameter(183), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg180.181 = f32[1,2048,1,1]{3,2,1,0} parameter(180), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.599 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,2048,25,25]{3,2,1,0}) %fusion.108), index=1
  %fusion.106 = (f32[2048]{0}, f32[2048]{0}) fusion(f32[1,2048,1,1]{3,2,1,0} %arg182.183, f32[1,2048,1,1]{3,2,1,0} %arg179.180, f32[2048]{0} %get-tuple-element.598, f32[2048]{0} %get-tuple-element.600, f32[1,2048,1,1]{3,2,1,0} %arg183.184, f32[1,2048,1,1]{3,2,1,0} %arg180.181, f32[2048]{0} %get-tuple-element.599), kind=kLoop, calls=%fused_computation.106, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.662 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}) %fusion.106), index=0
  %get-tuple-element.663 = f32[2048]{0} get-tuple-element((f32[2048]{0}, f32[2048]{0}) %fusion.106), index=1
  %get-tuple-element.653 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.102), index=1
  %arg187.188 = f32[1,512,51,51]{3,2,1,0} parameter(187), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.186 = (f32[3,3,512,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,51,51]{3,2,1,0} %arg187.188, f32[1,512,25,25]{3,2,1,0} %fusion.208), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block0/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.482 = f32[3,3,512,512]{1,0,2,3} get-tuple-element((f32[3,3,512,512]{1,0,2,3}, u8[0]{0}) %custom-call.186), index=0
  %fusion.103 = f32[3,3,512,512]{3,2,1,0} fusion(f32[3,3,512,512]{1,0,2,3} %get-tuple-element.482, f32[3,3,512,512]{3,2,1,0} %arg34.35), kind=kLoop, calls=%fused_computation.103, metadata={op_name="XLA_Retvals"}
  %arg188.189 = f32[1,512,1,1]{3,2,1,0} parameter(188), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg186.187 = f32[1,512,1,1]{3,2,1,0} parameter(186), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.652 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,25,25]{3,2,1,0}) %fusion.102), index=0
  %fusion.101 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg188.189, f32[1,512,1,1]{3,2,1,0} %arg186.187, f32[512]{0} %get-tuple-element.653, f32[512]{0} %get-tuple-element.652), kind=kLoop, calls=%fused_computation.101, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.583 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,50,50]{3,2,1,0}) %fusion.99), index=1
  %custom-call.188 = (f32[1,1,1024,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %arg143.144, f32[1,512,50,50]{3,2,1,0} %fusion.206), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group3/block0/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.484 = f32[1,1,1024,512]{1,0,2,3} get-tuple-element((f32[1,1,1024,512]{1,0,2,3}, u8[0]{0}) %custom-call.188), index=0
  %fusion.100 = f32[1,1,1024,512]{3,2,1,0} fusion(f32[1,1,1024,512]{1,0,2,3} %get-tuple-element.484, f32[1,1,1024,512]{3,2,1,0} %arg0.1), kind=kLoop, calls=%fused_computation.100, metadata={op_name="XLA_Retvals"}
  %arg193.194 = f32[1,512,1,1]{3,2,1,0} parameter(193), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg192.193 = f32[1,512,1,1]{3,2,1,0} parameter(192), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.582 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,50,50]{3,2,1,0}) %fusion.99), index=0
  %fusion.98 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg193.194, f32[1,512,1,1]{3,2,1,0} %arg192.193, f32[512]{0} %get-tuple-element.583, f32[512]{0} %get-tuple-element.582), kind=kLoop, calls=%fused_computation.98, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group3/block0/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.568 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.96), index=1
  %custom-call.192 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg197.198, f32[1,1024,50,50]{3,2,1,0} %fusion.203), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block5/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.488 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.192), index=0
  %fusion.97 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.488, f32[1,1,256,1024]{3,2,1,0} %arg1.2), kind=kLoop, calls=%fused_computation.97, metadata={op_name="XLA_Retvals"}
  %arg198.199 = f32[1,1024,1,1]{3,2,1,0} parameter(198), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg196.197 = f32[1,1024,1,1]{3,2,1,0} parameter(196), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.567 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.96), index=0
  %fusion.95 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg198.199, f32[1,1024,1,1]{3,2,1,0} %arg196.197, f32[1024]{0} %get-tuple-element.568, f32[1024]{0} %get-tuple-element.567), kind=kLoop, calls=%fused_computation.95, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.624 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.93), index=1
  %get-tuple-element.650 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=9
  %arg203.204 = f32[1,256,1,1]{3,2,1,0} parameter(203), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg201.202 = f32[1,256,1,1]{3,2,1,0} parameter(201), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.623 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.93), index=0
  %fusion.92 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg203.204, f32[1,256,1,1]{3,2,1,0} %arg201.202, f32[256]{0} %get-tuple-element.624, f32[256]{0} %get-tuple-element.623), kind=kLoop, calls=%fused_computation.92, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.626 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.90), index=1
  %custom-call.196 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %arg207.208, f32[1,256,50,50]{3,2,1,0} %fusion.199), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block5/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.492 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.196), index=0
  %fusion.91 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.492, f32[1,1,1024,256]{3,2,1,0} %arg4.5), kind=kLoop, calls=%fused_computation.91, metadata={op_name="XLA_Retvals"}
  %arg208.209 = f32[1,256,1,1]{3,2,1,0} parameter(208), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg206.207 = f32[1,256,1,1]{3,2,1,0} parameter(206), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.625 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.90), index=0
  %fusion.89 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg208.209, f32[1,256,1,1]{3,2,1,0} %arg206.207, f32[256]{0} %get-tuple-element.626, f32[256]{0} %get-tuple-element.625), kind=kLoop, calls=%fused_computation.89, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block5/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.570 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.87), index=1
  %custom-call.198 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg212.213, f32[1,1024,50,50]{3,2,1,0} %fusion.197), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block4/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.494 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.198), index=0
  %fusion.88 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.494, f32[1,1,256,1024]{3,2,1,0} %arg5.6), kind=kLoop, calls=%fused_computation.88, metadata={op_name="XLA_Retvals"}
  %arg213.214 = f32[1,1024,1,1]{3,2,1,0} parameter(213), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg211.212 = f32[1,1024,1,1]{3,2,1,0} parameter(211), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.569 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.87), index=0
  %fusion.86 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg213.214, f32[1,1024,1,1]{3,2,1,0} %arg211.212, f32[1024]{0} %get-tuple-element.570, f32[1024]{0} %get-tuple-element.569), kind=kLoop, calls=%fused_computation.86, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.618 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.84), index=1
  %get-tuple-element.631 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=0
  %arg218.219 = f32[1,256,1,1]{3,2,1,0} parameter(218), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg216.217 = f32[1,256,1,1]{3,2,1,0} parameter(216), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.617 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.84), index=0
  %fusion.83 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg218.219, f32[1,256,1,1]{3,2,1,0} %arg216.217, f32[256]{0} %get-tuple-element.618, f32[256]{0} %get-tuple-element.617), kind=kLoop, calls=%fused_computation.83, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.616 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.81), index=1
  %custom-call.202 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %arg222.223, f32[1,256,50,50]{3,2,1,0} %fusion.193), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block4/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.498 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.202), index=0
  %fusion.82 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.498, f32[1,1,1024,256]{3,2,1,0} %arg7.8), kind=kLoop, calls=%fused_computation.82, metadata={op_name="XLA_Retvals"}
  %arg223.224 = f32[1,256,1,1]{3,2,1,0} parameter(223), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg221.222 = f32[1,256,1,1]{3,2,1,0} parameter(221), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.615 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.81), index=0
  %fusion.80 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg223.224, f32[1,256,1,1]{3,2,1,0} %arg221.222, f32[256]{0} %get-tuple-element.616, f32[256]{0} %get-tuple-element.615), kind=kLoop, calls=%fused_computation.80, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block4/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.572 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.78), index=1
  %custom-call.204 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg227.228, f32[1,1024,50,50]{3,2,1,0} %fusion.191), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block3/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.500 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.204), index=0
  %fusion.79 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.500, f32[1,1,256,1024]{3,2,1,0} %arg8.9), kind=kLoop, calls=%fused_computation.79, metadata={op_name="XLA_Retvals"}
  %arg228.229 = f32[1,1024,1,1]{3,2,1,0} parameter(228), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg226.227 = f32[1,1024,1,1]{3,2,1,0} parameter(226), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.571 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.78), index=0
  %fusion.77 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg228.229, f32[1,1024,1,1]{3,2,1,0} %arg226.227, f32[1024]{0} %get-tuple-element.572, f32[1024]{0} %get-tuple-element.571), kind=kLoop, calls=%fused_computation.77, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.612 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.75), index=1
  %get-tuple-element.644 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=3
  %arg233.234 = f32[1,256,1,1]{3,2,1,0} parameter(233), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg231.232 = f32[1,256,1,1]{3,2,1,0} parameter(231), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.611 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.75), index=0
  %fusion.74 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg233.234, f32[1,256,1,1]{3,2,1,0} %arg231.232, f32[256]{0} %get-tuple-element.612, f32[256]{0} %get-tuple-element.611), kind=kLoop, calls=%fused_computation.74, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.610 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.72), index=1
  %custom-call.208 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %arg237.238, f32[1,256,50,50]{3,2,1,0} %fusion.187), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block3/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.504 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.208), index=0
  %fusion.73 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.504, f32[1,1,1024,256]{3,2,1,0} %arg10.11), kind=kLoop, calls=%fused_computation.73, metadata={op_name="XLA_Retvals"}
  %arg238.239 = f32[1,256,1,1]{3,2,1,0} parameter(238), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg236.237 = f32[1,256,1,1]{3,2,1,0} parameter(236), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.609 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.72), index=0
  %fusion.71 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg238.239, f32[1,256,1,1]{3,2,1,0} %arg236.237, f32[256]{0} %get-tuple-element.610, f32[256]{0} %get-tuple-element.609), kind=kLoop, calls=%fused_computation.71, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block3/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.564 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.69), index=1
  %custom-call.210 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg242.243, f32[1,1024,50,50]{3,2,1,0} %fusion.185), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block2/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.506 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.210), index=0
  %fusion.70 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.506, f32[1,1,256,1024]{3,2,1,0} %arg11.12), kind=kLoop, calls=%fused_computation.70, metadata={op_name="XLA_Retvals"}
  %arg243.244 = f32[1,1024,1,1]{3,2,1,0} parameter(243), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg241.242 = f32[1,1024,1,1]{3,2,1,0} parameter(241), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.563 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.69), index=0
  %fusion.68 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg243.244, f32[1,1024,1,1]{3,2,1,0} %arg241.242, f32[1024]{0} %get-tuple-element.564, f32[1024]{0} %get-tuple-element.563), kind=kLoop, calls=%fused_computation.68, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.606 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.66), index=1
  %get-tuple-element.651 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=10
  %arg248.249 = f32[1,256,1,1]{3,2,1,0} parameter(248), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg246.247 = f32[1,256,1,1]{3,2,1,0} parameter(246), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.605 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.66), index=0
  %fusion.65 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg248.249, f32[1,256,1,1]{3,2,1,0} %arg246.247, f32[256]{0} %get-tuple-element.606, f32[256]{0} %get-tuple-element.605), kind=kLoop, calls=%fused_computation.65, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.608 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.63), index=1
  %custom-call.214 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %arg252.253, f32[1,256,50,50]{3,2,1,0} %fusion.181), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block2/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.510 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.214), index=0
  %fusion.64 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.510, f32[1,1,1024,256]{3,2,1,0} %arg13.14), kind=kLoop, calls=%fused_computation.64, metadata={op_name="XLA_Retvals"}
  %arg253.254 = f32[1,256,1,1]{3,2,1,0} parameter(253), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg251.252 = f32[1,256,1,1]{3,2,1,0} parameter(251), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.607 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.63), index=0
  %fusion.62 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg253.254, f32[1,256,1,1]{3,2,1,0} %arg251.252, f32[256]{0} %get-tuple-element.608, f32[256]{0} %get-tuple-element.607), kind=kLoop, calls=%fused_computation.62, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block2/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.576 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.60), index=1
  %custom-call.216 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg257.258, f32[1,1024,50,50]{3,2,1,0} %fusion.179), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block1/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.512 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.216), index=0
  %fusion.61 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.512, f32[1,1,256,1024]{3,2,1,0} %arg14.15), kind=kLoop, calls=%fused_computation.61, metadata={op_name="XLA_Retvals"}
  %arg258.259 = f32[1,1024,1,1]{3,2,1,0} parameter(258), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg256.257 = f32[1,1024,1,1]{3,2,1,0} parameter(256), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.575 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.60), index=0
  %fusion.59 = f32[1024]{0} fusion(f32[1,1024,1,1]{3,2,1,0} %arg258.259, f32[1,1024,1,1]{3,2,1,0} %arg256.257, f32[1024]{0} %get-tuple-element.576, f32[1024]{0} %get-tuple-element.575), kind=kLoop, calls=%fused_computation.59, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv3/bn/Reshape_grad/Reshape"}
  %get-tuple-element.620 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.57), index=1
  %get-tuple-element.647 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=6
  %arg263.264 = f32[1,256,1,1]{3,2,1,0} parameter(263), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg261.262 = f32[1,256,1,1]{3,2,1,0} parameter(261), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.619 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.57), index=0
  %fusion.56 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg263.264, f32[1,256,1,1]{3,2,1,0} %arg261.262, f32[256]{0} %get-tuple-element.620, f32[256]{0} %get-tuple-element.619), kind=kLoop, calls=%fused_computation.56, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv2/bn/Reshape_grad/Reshape"}
  %get-tuple-element.622 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.54), index=1
  %custom-call.220 = (f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %arg267.268, f32[1,256,50,50]{3,2,1,0} %fusion.175), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block1/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.516 = f32[1,1,1024,256]{1,0,2,3} get-tuple-element((f32[1,1,1024,256]{1,0,2,3}, u8[0]{0}) %custom-call.220), index=0
  %fusion.55 = f32[1,1,1024,256]{3,2,1,0} fusion(f32[1,1,1024,256]{1,0,2,3} %get-tuple-element.516, f32[1,1,1024,256]{3,2,1,0} %arg16.17), kind=kLoop, calls=%fused_computation.55, metadata={op_name="XLA_Retvals"}
  %arg268.269 = f32[1,256,1,1]{3,2,1,0} parameter(268), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg266.267 = f32[1,256,1,1]{3,2,1,0} parameter(266), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.621 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.54), index=0
  %fusion.53 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg268.269, f32[1,256,1,1]{3,2,1,0} %arg266.267, f32[256]{0} %get-tuple-element.622, f32[256]{0} %get-tuple-element.621), kind=kLoop, calls=%fused_computation.53, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block1/conv1/bn/Reshape_grad/Reshape"}
  %get-tuple-element.578 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.50), index=1
  %copy.485 = f32[1024]{0} copy(f32[1024]{0} %get-tuple-element.578)
  %get-tuple-element.566 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.173), index=1
  %custom-call.228 = (f32[1,1,512,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %arg142.143, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.566), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block0/convshortcut/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.524 = f32[1,1,512,1024]{1,0,2,3} get-tuple-element((f32[1,1,512,1024]{1,0,2,3}, u8[0]{0}) %custom-call.228), index=0
  %arg17.18 = f32[1,1,512,1024]{3,2,1,0} parameter(17), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.52 = f32[1,1,512,1024]{3,2,1,0} fusion(f32[1,1,512,1024]{1,0,2,3} %get-tuple-element.524, f32[1,1,512,1024]{3,2,1,0} %arg17.18), kind=kLoop, calls=%fused_computation.52, metadata={op_name="XLA_Retvals"}
  %custom-call.222 = (f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %arg275.276, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.565), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block0/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.518 = f32[1,1,256,1024]{1,0,2,3} get-tuple-element((f32[1,1,256,1024]{1,0,2,3}, u8[0]{0}) %custom-call.222), index=0
  %fusion.51 = f32[1,1,256,1024]{3,2,1,0} fusion(f32[1,1,256,1024]{1,0,2,3} %get-tuple-element.518, f32[1,1,256,1024]{3,2,1,0} %arg31.32), kind=kLoop, calls=%fused_computation.51, metadata={op_name="XLA_Retvals"}
  %arg276.277 = f32[1,1024,1,1]{3,2,1,0} parameter(276), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg273.274 = f32[1,1024,1,1]{3,2,1,0} parameter(273), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.577 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.50), index=0
  %arg277.278 = f32[1,1024,1,1]{3,2,1,0} parameter(277), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg274.275 = f32[1,1024,1,1]{3,2,1,0} parameter(274), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.579 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1024,50,50]{3,2,1,0}) %fusion.50), index=2
  %fusion.48 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1,1024,1,1]{3,2,1,0} %arg276.277, f32[1,1024,1,1]{3,2,1,0} %arg273.274, f32[1024]{0} %get-tuple-element.577, f32[1024]{0} %get-tuple-element.578, f32[1,1024,1,1]{3,2,1,0} %arg277.278, f32[1,1024,1,1]{3,2,1,0} %arg274.275, f32[1024]{0} %get-tuple-element.579), kind=kLoop, calls=%fused_computation.48, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/convshortcut/bn/Reshape_grad/Reshape"}
  %get-tuple-element.664 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.48), index=0
  %get-tuple-element.665 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.48), index=1
  %get-tuple-element.614 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.44), index=1
  %get-tuple-element.632 = f32[3,3,256,256]{3,2,1,0} get-tuple-element((f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}) %fusion.85), index=1
  %arg282.283 = f32[1,256,1,1]{3,2,1,0} parameter(282), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg280.281 = f32[1,256,1,1]{3,2,1,0} parameter(280), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.613 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,50,50]{3,2,1,0}) %fusion.44), index=0
  %fusion.43 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg282.283, f32[1,256,1,1]{3,2,1,0} %arg280.281, f32[256]{0} %get-tuple-element.614, f32[256]{0} %get-tuple-element.613), kind=kLoop, calls=%fused_computation.43, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv2/bn/Reshape_grad/Reshape"}
  %arg285.286 = f32[1,256,100,100]{3,2,1,0} parameter(285), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.282 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg32.33), metadata={op_name="XLA_Args"}
  %custom-call.225 = (f32[1,256,101,101]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.171, f32[3,3,256,256]{1,0,2,3} %copy.282), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block0/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.521 = f32[1,256,101,101]{3,2,1,0} get-tuple-element((f32[1,256,101,101]{3,2,1,0}, u8[0]{0}) %custom-call.225), index=0
  %arg283.284 = f32[1,256,100,100]{3,2,1,0} parameter(283), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.41 = (f32[256]{0}, f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) fusion(f32[1,256,100,100]{3,2,1,0} %arg285.286, f32[1,256,101,101]{3,2,1,0} %get-tuple-element.521, f32[1,256,100,100]{3,2,1,0} %arg283.284), kind=kInput, calls=%fused_computation.41, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.574 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) %fusion.41), index=1
  %get-tuple-element.718 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) %fusion.41), index=2
  %arg284.285 = f32[1,256,1,1]{3,2,1,0} parameter(284), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.169 = f32[1,256,100,100]{3,2,1,0} fusion(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.718, f32[1,256,1,1]{3,2,1,0} %arg284.285), kind=kLoop, calls=%fused_computation.169, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.226 = (f32[1,1,512,256]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %arg142.143, f32[1,256,100,100]{3,2,1,0} %fusion.169), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group2/block0/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.522 = f32[1,1,512,256]{1,0,2,3} get-tuple-element((f32[1,1,512,256]{1,0,2,3}, u8[0]{0}) %custom-call.226), index=0
  %arg33.34 = f32[1,1,512,256]{3,2,1,0} parameter(33), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.42 = f32[1,1,512,256]{3,2,1,0} fusion(f32[1,1,512,256]{1,0,2,3} %get-tuple-element.522, f32[1,1,512,256]{3,2,1,0} %arg33.34), kind=kLoop, calls=%fused_computation.42, metadata={op_name="XLA_Retvals"}
  %arg287.288 = f32[1,256,1,1]{3,2,1,0} parameter(287), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg286.287 = f32[1,256,1,1]{3,2,1,0} parameter(286), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.573 = f32[256]{0} get-tuple-element((f32[256]{0}, f32[256]{0}, f32[1,256,100,100]{3,2,1,0}) %fusion.41), index=0
  %fusion.40 = f32[256]{0} fusion(f32[1,256,1,1]{3,2,1,0} %arg287.288, f32[1,256,1,1]{3,2,1,0} %arg286.287, f32[256]{0} %get-tuple-element.574, f32[256]{0} %get-tuple-element.573), kind=kLoop, calls=%fused_computation.40, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group2/block0/conv1/bn/Reshape_grad/Reshape"}
  %arg289.290 = f32[1,512,100,100]{3,2,1,0} parameter(289), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.286 = f32[1,1,512,1024]{1,0,2,3} copy(f32[1,1,512,1024]{3,2,1,0} %arg17.18), metadata={op_name="XLA_Args"}
  %custom-call.229 = (f32[1,512,100,100]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.566, f32[1,1,512,1024]{1,0,2,3} %copy.286), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block0/convshortcut/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.525 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[0]{0}) %custom-call.229), index=0
  %bitcast.166 = f32[1,1,512,256]{1,0,3,2} bitcast(f32[1,1,512,256]{3,2,1,0} %arg45.46), metadata={op_name="XLA_Args"}
  %custom-call.145 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.687, f32[1,1,512,256]{1,0,3,2} %bitcast.166), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/fpn/lateral_1x1_c3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.441 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.145), index=0
  %bitcast.167 = f32[1,1,512,256]{1,0,3,2} bitcast(f32[1,1,512,256]{3,2,1,0} %arg33.34), metadata={op_name="XLA_Args"}
  %custom-call.227 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %fusion.169, f32[1,1,512,256]{1,0,3,2} %bitcast.167), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group2/block0/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.523 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.227), index=0
  %fusion.38 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) fusion(f32[1,512,100,100]{3,2,1,0} %arg289.290, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.525, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.441, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.523, f32[1,512,100,100]{3,2,1,0} %arg142.143), kind=kInput, calls=%fused_computation.38, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.560 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.38), index=1
  %arg291.292 = f32[1,128,100,100]{3,2,1,0} parameter(291), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.719 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.38), index=2
  %arg288.289 = f32[1,512,1,1]{3,2,1,0} parameter(288), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.166 = f32[1,512,100,100]{3,2,1,0} fusion(f32[1,512,100,100]{3,2,1,0} %get-tuple-element.719, f32[1,512,1,1]{3,2,1,0} %arg288.289), kind=kLoop, calls=%fused_computation.166, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.230 = (f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %arg291.292, f32[1,512,100,100]{3,2,1,0} %fusion.166), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block3/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.526 = f32[1,1,128,512]{1,0,2,3} get-tuple-element((f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) %custom-call.230), index=0
  %arg18.19 = f32[1,1,128,512]{3,2,1,0} parameter(18), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.39 = f32[1,1,128,512]{3,2,1,0} fusion(f32[1,1,128,512]{1,0,2,3} %get-tuple-element.526, f32[1,1,128,512]{3,2,1,0} %arg18.19), kind=kLoop, calls=%fused_computation.39, metadata={op_name="XLA_Retvals"}
  %arg292.293 = f32[1,512,1,1]{3,2,1,0} parameter(292), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg290.291 = f32[1,512,1,1]{3,2,1,0} parameter(290), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.559 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.38), index=0
  %fusion.37 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg292.293, f32[1,512,1,1]{3,2,1,0} %arg290.291, f32[512]{0} %get-tuple-element.560, f32[512]{0} %get-tuple-element.559), kind=kLoop, calls=%fused_computation.37, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv3/bn/Reshape_grad/Reshape"}
  %arg294.295 = f32[1,128,100,100]{3,2,1,0} parameter(294), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.172 = f32[1,1,128,512]{1,0,3,2} bitcast(f32[1,1,128,512]{3,2,1,0} %arg18.19), metadata={op_name="XLA_Args"}
  %custom-call.231 = (f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %fusion.166, f32[1,1,128,512]{1,0,3,2} %bitcast.172), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block3/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.527 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.231), index=0
  %fusion.35 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) fusion(f32[1,128,100,100]{3,2,1,0} %arg294.295, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.527, f32[1,128,100,100]{3,2,1,0} %arg291.292), kind=kInput, calls=%fused_computation.35, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.597 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.35), index=1
  %arg296.297 = f32[1,128,100,100]{3,2,1,0} parameter(296), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.720 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.35), index=2
  %arg293.294 = f32[1,128,1,1]{3,2,1,0} parameter(293), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.164 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,100,100]{3,2,1,0} %get-tuple-element.720, f32[1,128,1,1]{3,2,1,0} %arg293.294), kind=kLoop, calls=%fused_computation.164, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.232 = (f32[3,3,128,128]{1,0,2,3}, u8[25399296]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %arg296.297, f32[1,128,100,100]{3,2,1,0} %fusion.164), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block3/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.528 = f32[3,3,128,128]{1,0,2,3} get-tuple-element((f32[3,3,128,128]{1,0,2,3}, u8[25399296]{0}) %custom-call.232), index=0
  %arg19.20 = f32[3,3,128,128]{3,2,1,0} parameter(19), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.36 = f32[3,3,128,128]{3,2,1,0} fusion(f32[3,3,128,128]{1,0,2,3} %get-tuple-element.528, f32[3,3,128,128]{3,2,1,0} %arg19.20), kind=kLoop, calls=%fused_computation.36, metadata={op_name="XLA_Retvals"}
  %arg297.298 = f32[1,128,1,1]{3,2,1,0} parameter(297), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg295.296 = f32[1,128,1,1]{3,2,1,0} parameter(295), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.596 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.35), index=0
  %fusion.34 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg297.298, f32[1,128,1,1]{3,2,1,0} %arg295.296, f32[128]{0} %get-tuple-element.597, f32[128]{0} %get-tuple-element.596), kind=kLoop, calls=%fused_computation.34, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv2/bn/Reshape_grad/Reshape"}
  %arg299.300 = f32[1,128,100,100]{3,2,1,0} parameter(299), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.290 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg19.20), metadata={op_name="XLA_Args"}
  %custom-call.233 = (f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.164, f32[3,3,128,128]{1,0,2,3} %copy.290), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block3/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.529 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) %custom-call.233), index=0
  %fusion.32 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) fusion(f32[1,128,100,100]{3,2,1,0} %arg299.300, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.529, f32[1,128,100,100]{3,2,1,0} %arg296.297), kind=kInput, calls=%fused_computation.32, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.595 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.32), index=1
  %arg301.302 = f32[1,512,100,100]{3,2,1,0} parameter(301), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.721 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.32), index=2
  %arg298.299 = f32[1,128,1,1]{3,2,1,0} parameter(298), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.162 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,100,100]{3,2,1,0} %get-tuple-element.721, f32[1,128,1,1]{3,2,1,0} %arg298.299), kind=kLoop, calls=%fused_computation.162, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.234 = (f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %arg301.302, f32[1,128,100,100]{3,2,1,0} %fusion.162), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block3/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.530 = f32[1,1,512,128]{1,0,2,3} get-tuple-element((f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) %custom-call.234), index=0
  %arg20.21 = f32[1,1,512,128]{3,2,1,0} parameter(20), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.33 = f32[1,1,512,128]{3,2,1,0} fusion(f32[1,1,512,128]{1,0,2,3} %get-tuple-element.530, f32[1,1,512,128]{3,2,1,0} %arg20.21), kind=kLoop, calls=%fused_computation.33, metadata={op_name="XLA_Retvals"}
  %arg302.303 = f32[1,128,1,1]{3,2,1,0} parameter(302), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg300.301 = f32[1,128,1,1]{3,2,1,0} parameter(300), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.594 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.32), index=0
  %fusion.31 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg302.303, f32[1,128,1,1]{3,2,1,0} %arg300.301, f32[128]{0} %get-tuple-element.595, f32[128]{0} %get-tuple-element.594), kind=kLoop, calls=%fused_computation.31, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block3/conv1/bn/Reshape_grad/Reshape"}
  %arg304.305 = f32[1,512,100,100]{3,2,1,0} parameter(304), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.181 = f32[1,1,512,128]{1,0,3,2} bitcast(f32[1,1,512,128]{3,2,1,0} %arg20.21), metadata={op_name="XLA_Args"}
  %custom-call.235 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.162, f32[1,1,512,128]{1,0,3,2} %bitcast.181), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block3/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.531 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.235), index=0
  %fusion.29 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) fusion(f32[1,512,100,100]{3,2,1,0} %arg304.305, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.531, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.719, f32[1,512,100,100]{3,2,1,0} %arg301.302), kind=kInput, calls=%fused_computation.29, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.562 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.29), index=1
  %arg306.307 = f32[1,128,100,100]{3,2,1,0} parameter(306), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.722 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.29), index=2
  %arg303.304 = f32[1,512,1,1]{3,2,1,0} parameter(303), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.160 = f32[1,512,100,100]{3,2,1,0} fusion(f32[1,512,100,100]{3,2,1,0} %get-tuple-element.722, f32[1,512,1,1]{3,2,1,0} %arg303.304), kind=kLoop, calls=%fused_computation.160, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.236 = (f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %arg306.307, f32[1,512,100,100]{3,2,1,0} %fusion.160), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block2/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.532 = f32[1,1,128,512]{1,0,2,3} get-tuple-element((f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) %custom-call.236), index=0
  %arg21.22 = f32[1,1,128,512]{3,2,1,0} parameter(21), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.30 = f32[1,1,128,512]{3,2,1,0} fusion(f32[1,1,128,512]{1,0,2,3} %get-tuple-element.532, f32[1,1,128,512]{3,2,1,0} %arg21.22), kind=kLoop, calls=%fused_computation.30, metadata={op_name="XLA_Retvals"}
  %arg307.308 = f32[1,512,1,1]{3,2,1,0} parameter(307), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg305.306 = f32[1,512,1,1]{3,2,1,0} parameter(305), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.561 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.29), index=0
  %fusion.28 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg307.308, f32[1,512,1,1]{3,2,1,0} %arg305.306, f32[512]{0} %get-tuple-element.562, f32[512]{0} %get-tuple-element.561), kind=kLoop, calls=%fused_computation.28, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv3/bn/Reshape_grad/Reshape"}
  %arg309.310 = f32[1,128,100,100]{3,2,1,0} parameter(309), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.186 = f32[1,1,128,512]{1,0,3,2} bitcast(f32[1,1,128,512]{3,2,1,0} %arg21.22), metadata={op_name="XLA_Args"}
  %custom-call.237 = (f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %fusion.160, f32[1,1,128,512]{1,0,3,2} %bitcast.186), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block2/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.533 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.237), index=0
  %fusion.26 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) fusion(f32[1,128,100,100]{3,2,1,0} %arg309.310, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.533, f32[1,128,100,100]{3,2,1,0} %arg306.307), kind=kInput, calls=%fused_computation.26, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.591 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.26), index=1
  %arg311.312 = f32[1,128,100,100]{3,2,1,0} parameter(311), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.723 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.26), index=2
  %arg308.309 = f32[1,128,1,1]{3,2,1,0} parameter(308), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.158 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,100,100]{3,2,1,0} %get-tuple-element.723, f32[1,128,1,1]{3,2,1,0} %arg308.309), kind=kLoop, calls=%fused_computation.158, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.238 = (f32[3,3,128,128]{1,0,2,3}, u8[25399296]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %arg311.312, f32[1,128,100,100]{3,2,1,0} %fusion.158), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block2/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.534 = f32[3,3,128,128]{1,0,2,3} get-tuple-element((f32[3,3,128,128]{1,0,2,3}, u8[25399296]{0}) %custom-call.238), index=0
  %arg22.23 = f32[3,3,128,128]{3,2,1,0} parameter(22), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.27 = f32[3,3,128,128]{3,2,1,0} fusion(f32[3,3,128,128]{1,0,2,3} %get-tuple-element.534, f32[3,3,128,128]{3,2,1,0} %arg22.23), kind=kLoop, calls=%fused_computation.27, metadata={op_name="XLA_Retvals"}
  %arg312.313 = f32[1,128,1,1]{3,2,1,0} parameter(312), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg310.311 = f32[1,128,1,1]{3,2,1,0} parameter(310), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.590 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.26), index=0
  %fusion.25 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg312.313, f32[1,128,1,1]{3,2,1,0} %arg310.311, f32[128]{0} %get-tuple-element.591, f32[128]{0} %get-tuple-element.590), kind=kLoop, calls=%fused_computation.25, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv2/bn/Reshape_grad/Reshape"}
  %arg314.315 = f32[1,128,100,100]{3,2,1,0} parameter(314), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.296 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg22.23), metadata={op_name="XLA_Args"}
  %custom-call.239 = (f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.158, f32[3,3,128,128]{1,0,2,3} %copy.296), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block2/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.535 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) %custom-call.239), index=0
  %fusion.23 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) fusion(f32[1,128,100,100]{3,2,1,0} %arg314.315, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.535, f32[1,128,100,100]{3,2,1,0} %arg311.312), kind=kInput, calls=%fused_computation.23, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.593 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.23), index=1
  %arg316.317 = f32[1,512,100,100]{3,2,1,0} parameter(316), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.724 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.23), index=2
  %arg313.314 = f32[1,128,1,1]{3,2,1,0} parameter(313), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.156 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,100,100]{3,2,1,0} %get-tuple-element.724, f32[1,128,1,1]{3,2,1,0} %arg313.314), kind=kLoop, calls=%fused_computation.156, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.240 = (f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %arg316.317, f32[1,128,100,100]{3,2,1,0} %fusion.156), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block2/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.536 = f32[1,1,512,128]{1,0,2,3} get-tuple-element((f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) %custom-call.240), index=0
  %arg23.24 = f32[1,1,512,128]{3,2,1,0} parameter(23), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.24 = f32[1,1,512,128]{3,2,1,0} fusion(f32[1,1,512,128]{1,0,2,3} %get-tuple-element.536, f32[1,1,512,128]{3,2,1,0} %arg23.24), kind=kLoop, calls=%fused_computation.24, metadata={op_name="XLA_Retvals"}
  %arg317.318 = f32[1,128,1,1]{3,2,1,0} parameter(317), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg315.316 = f32[1,128,1,1]{3,2,1,0} parameter(315), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.592 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.23), index=0
  %fusion.22 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg317.318, f32[1,128,1,1]{3,2,1,0} %arg315.316, f32[128]{0} %get-tuple-element.593, f32[128]{0} %get-tuple-element.592), kind=kLoop, calls=%fused_computation.22, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block2/conv1/bn/Reshape_grad/Reshape"}
  %arg319.320 = f32[1,512,100,100]{3,2,1,0} parameter(319), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.195 = f32[1,1,512,128]{1,0,3,2} bitcast(f32[1,1,512,128]{3,2,1,0} %arg23.24), metadata={op_name="XLA_Args"}
  %custom-call.241 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.156, f32[1,1,512,128]{1,0,3,2} %bitcast.195), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block2/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.537 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.241), index=0
  %fusion.20 = (f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) fusion(f32[1,512,100,100]{3,2,1,0} %arg319.320, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.537, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.722, f32[1,512,100,100]{3,2,1,0} %arg316.317), kind=kInput, calls=%fused_computation.20, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.553 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.20), index=1
  %arg321.322 = f32[1,128,100,100]{3,2,1,0} parameter(321), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.725 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.20), index=2
  %arg318.319 = f32[1,512,1,1]{3,2,1,0} parameter(318), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.154 = f32[1,512,100,100]{3,2,1,0} fusion(f32[1,512,100,100]{3,2,1,0} %get-tuple-element.725, f32[1,512,1,1]{3,2,1,0} %arg318.319), kind=kLoop, calls=%fused_computation.154, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.242 = (f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %arg321.322, f32[1,512,100,100]{3,2,1,0} %fusion.154), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block1/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.538 = f32[1,1,128,512]{1,0,2,3} get-tuple-element((f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) %custom-call.242), index=0
  %arg24.25 = f32[1,1,128,512]{3,2,1,0} parameter(24), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.21 = f32[1,1,128,512]{3,2,1,0} fusion(f32[1,1,128,512]{1,0,2,3} %get-tuple-element.538, f32[1,1,128,512]{3,2,1,0} %arg24.25), kind=kLoop, calls=%fused_computation.21, metadata={op_name="XLA_Retvals"}
  %arg322.323 = f32[1,512,1,1]{3,2,1,0} parameter(322), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg320.321 = f32[1,512,1,1]{3,2,1,0} parameter(320), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.552 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.20), index=0
  %fusion.19 = f32[512]{0} fusion(f32[1,512,1,1]{3,2,1,0} %arg322.323, f32[1,512,1,1]{3,2,1,0} %arg320.321, f32[512]{0} %get-tuple-element.553, f32[512]{0} %get-tuple-element.552), kind=kLoop, calls=%fused_computation.19, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv3/bn/Reshape_grad/Reshape"}
  %arg324.325 = f32[1,128,100,100]{3,2,1,0} parameter(324), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.200 = f32[1,1,128,512]{1,0,3,2} bitcast(f32[1,1,128,512]{3,2,1,0} %arg24.25), metadata={op_name="XLA_Args"}
  %custom-call.243 = (f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %fusion.154, f32[1,1,128,512]{1,0,3,2} %bitcast.200), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block1/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.539 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.243), index=0
  %fusion.17 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) fusion(f32[1,128,100,100]{3,2,1,0} %arg324.325, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.539, f32[1,128,100,100]{3,2,1,0} %arg321.322), kind=kInput, calls=%fused_computation.17, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.589 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.17), index=1
  %arg326.327 = f32[1,128,100,100]{3,2,1,0} parameter(326), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.726 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.17), index=2
  %arg323.324 = f32[1,128,1,1]{3,2,1,0} parameter(323), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.152 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,100,100]{3,2,1,0} %get-tuple-element.726, f32[1,128,1,1]{3,2,1,0} %arg323.324), kind=kLoop, calls=%fused_computation.152, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.244 = (f32[3,3,128,128]{1,0,2,3}, u8[25399296]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %arg326.327, f32[1,128,100,100]{3,2,1,0} %fusion.152), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block1/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"algorithm\":\"5\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.540 = f32[3,3,128,128]{1,0,2,3} get-tuple-element((f32[3,3,128,128]{1,0,2,3}, u8[25399296]{0}) %custom-call.244), index=0
  %arg25.26 = f32[3,3,128,128]{3,2,1,0} parameter(25), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.18 = f32[3,3,128,128]{3,2,1,0} fusion(f32[3,3,128,128]{1,0,2,3} %get-tuple-element.540, f32[3,3,128,128]{3,2,1,0} %arg25.26), kind=kLoop, calls=%fused_computation.18, metadata={op_name="XLA_Retvals"}
  %arg327.328 = f32[1,128,1,1]{3,2,1,0} parameter(327), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg325.326 = f32[1,128,1,1]{3,2,1,0} parameter(325), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.588 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.17), index=0
  %fusion.16 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg327.328, f32[1,128,1,1]{3,2,1,0} %arg325.326, f32[128]{0} %get-tuple-element.589, f32[128]{0} %get-tuple-element.588), kind=kLoop, calls=%fused_computation.16, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv2/bn/Reshape_grad/Reshape"}
  %arg329.330 = f32[1,128,100,100]{3,2,1,0} parameter(329), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.302 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg25.26), metadata={op_name="XLA_Args"}
  %custom-call.245 = (f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.152, f32[3,3,128,128]{1,0,2,3} %copy.302), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block1/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"4\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.541 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) %custom-call.245), index=0
  %fusion.14 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) fusion(f32[1,128,100,100]{3,2,1,0} %arg329.330, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.541, f32[1,128,100,100]{3,2,1,0} %arg326.327), kind=kInput, calls=%fused_computation.14, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.587 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.14), index=1
  %arg331.332 = f32[1,512,100,100]{3,2,1,0} parameter(331), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.727 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.14), index=2
  %arg328.329 = f32[1,128,1,1]{3,2,1,0} parameter(328), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.150 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,100,100]{3,2,1,0} %get-tuple-element.727, f32[1,128,1,1]{3,2,1,0} %arg328.329), kind=kLoop, calls=%fused_computation.150, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.246 = (f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %arg331.332, f32[1,128,100,100]{3,2,1,0} %fusion.150), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block1/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.542 = f32[1,1,512,128]{1,0,2,3} get-tuple-element((f32[1,1,512,128]{1,0,2,3}, u8[0]{0}) %custom-call.246), index=0
  %arg26.27 = f32[1,1,512,128]{3,2,1,0} parameter(26), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.15 = f32[1,1,512,128]{3,2,1,0} fusion(f32[1,1,512,128]{1,0,2,3} %get-tuple-element.542, f32[1,1,512,128]{3,2,1,0} %arg26.27), kind=kLoop, calls=%fused_computation.15, metadata={op_name="XLA_Retvals"}
  %arg332.333 = f32[1,128,1,1]{3,2,1,0} parameter(332), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg330.331 = f32[1,128,1,1]{3,2,1,0} parameter(330), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.586 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.14), index=0
  %fusion.13 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg332.333, f32[1,128,1,1]{3,2,1,0} %arg330.331, f32[128]{0} %get-tuple-element.587, f32[128]{0} %get-tuple-element.586), kind=kLoop, calls=%fused_computation.13, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block1/conv1/bn/Reshape_grad/Reshape"}
  %arg334.335 = f32[1,512,100,100]{3,2,1,0} parameter(334), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg335.336 = f32[1,512,100,100]{3,2,1,0} parameter(335), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.209 = f32[1,1,512,128]{1,0,3,2} bitcast(f32[1,1,512,128]{3,2,1,0} %arg26.27), metadata={op_name="XLA_Args"}
  %custom-call.247 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.150, f32[1,1,512,128]{1,0,3,2} %bitcast.209), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block1/conv1/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.543 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.247), index=0
  %fusion.10 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) fusion(f32[1,512,100,100]{3,2,1,0} %arg334.335, f32[1,512,100,100]{3,2,1,0} %arg335.336, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.543, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.725, f32[1,512,100,100]{3,2,1,0} %arg331.332), kind=kInput, calls=%fused_computation.10, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.555 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.10), index=1
  %copy.486 = f32[512]{0} copy(f32[512]{0} %get-tuple-element.555)
  %get-tuple-element.728 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.10), index=3
  %arg336.337 = f32[1,512,1,1]{3,2,1,0} parameter(336), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg333.334 = f32[1,512,1,1]{3,2,1,0} parameter(333), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.148 = (f32[1,512,100,100]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}) fusion(f32[1,512,100,100]{3,2,1,0} %get-tuple-element.728, f32[1,512,1,1]{3,2,1,0} %arg336.337, f32[1,512,1,1]{3,2,1,0} %arg333.334), kind=kLoop, calls=%fused_computation.148, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv3/bn/batchnorm/mul_1_grad/Mul"}
  %get-tuple-element.551 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}) %fusion.148), index=1
  %custom-call.253 = (f32[1,1,256,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %arg140.141, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.551), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block0/convshortcut/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.549 = f32[1,1,256,512]{1,0,2,3} get-tuple-element((f32[1,1,256,512]{1,0,2,3}, u8[0]{0}) %custom-call.253), index=0
  %arg27.28 = f32[1,1,256,512]{3,2,1,0} parameter(27), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.12 = f32[1,1,256,512]{3,2,1,0} fusion(f32[1,1,256,512]{1,0,2,3} %get-tuple-element.549, f32[1,1,256,512]{3,2,1,0} %arg27.28), kind=kLoop, calls=%fused_computation.12, metadata={op_name="XLA_Retvals"}
  %arg339.340 = f32[1,128,100,100]{3,2,1,0} parameter(339), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.550 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}) %fusion.148), index=0
  %custom-call.248 = (f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %arg339.340, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.550), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block0/conv3/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.544 = f32[1,1,128,512]{1,0,2,3} get-tuple-element((f32[1,1,128,512]{1,0,2,3}, u8[0]{0}) %custom-call.248), index=0
  %arg28.29 = f32[1,1,128,512]{3,2,1,0} parameter(28), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.11 = f32[1,1,128,512]{3,2,1,0} fusion(f32[1,1,128,512]{1,0,2,3} %get-tuple-element.544, f32[1,1,128,512]{3,2,1,0} %arg28.29), kind=kLoop, calls=%fused_computation.11, metadata={op_name="XLA_Retvals"}
  %arg340.341 = f32[1,512,1,1]{3,2,1,0} parameter(340), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg337.338 = f32[1,512,1,1]{3,2,1,0} parameter(337), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.554 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.10), index=0
  %arg341.342 = f32[1,512,1,1]{3,2,1,0} parameter(341), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg338.339 = f32[1,512,1,1]{3,2,1,0} parameter(338), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.558 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[1,512,100,100]{3,2,1,0}) %fusion.10), index=2
  %fusion.8 = (f32[512]{0}, f32[512]{0}) fusion(f32[1,512,1,1]{3,2,1,0} %arg340.341, f32[1,512,1,1]{3,2,1,0} %arg337.338, f32[512]{0} %get-tuple-element.554, f32[512]{0} %get-tuple-element.555, f32[1,512,1,1]{3,2,1,0} %arg341.342, f32[1,512,1,1]{3,2,1,0} %arg338.339, f32[512]{0} %get-tuple-element.558), kind=kLoop, calls=%fused_computation.8, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/convshortcut/bn/Reshape_grad/Reshape"}
  %get-tuple-element.666 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}) %fusion.8), index=0
  %get-tuple-element.667 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}) %fusion.8), index=1
  %arg342.343 = f32[1,128,100,100]{3,2,1,0} parameter(342), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %bitcast.217 = f32[1,1,128,512]{1,0,3,2} bitcast(f32[1,1,128,512]{3,2,1,0} %arg28.29), metadata={op_name="XLA_Args"}
  %custom-call.249 = (f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %get-tuple-element.550, f32[1,1,128,512]{1,0,3,2} %bitcast.217), window={size=1x1}, dim_labels=bf01_01oi->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block0/conv3/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.545 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.249), index=0
  %fusion.4 = (f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) fusion(f32[1,128,100,100]{3,2,1,0} %arg342.343, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.545, f32[1,128,100,100]{3,2,1,0} %arg339.340), kind=kInput, calls=%fused_computation.4, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.585 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.4), index=1
  %arg345.346 = f32[1,128,201,201]{3,2,1,0} parameter(345), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.729 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.4), index=2
  %arg343.344 = f32[1,128,1,1]{3,2,1,0} parameter(343), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.146 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,100,100]{3,2,1,0} %get-tuple-element.729, f32[1,128,1,1]{3,2,1,0} %arg343.344), kind=kLoop, calls=%fused_computation.146, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.250 = (f32[3,3,128,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,128,201,201]{3,2,1,0} %arg345.346, f32[1,128,100,100]{3,2,1,0} %fusion.146), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block0/conv2/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.546 = f32[3,3,128,128]{1,0,2,3} get-tuple-element((f32[3,3,128,128]{1,0,2,3}, u8[0]{0}) %custom-call.250), index=0
  %arg29.30 = f32[3,3,128,128]{3,2,1,0} parameter(29), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.5 = f32[3,3,128,128]{3,2,1,0} fusion(f32[3,3,128,128]{1,0,2,3} %get-tuple-element.546, f32[3,3,128,128]{3,2,1,0} %arg29.30), kind=kLoop, calls=%fused_computation.5, metadata={op_name="XLA_Retvals"}
  %arg346.347 = f32[1,128,1,1]{3,2,1,0} parameter(346), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg344.345 = f32[1,128,1,1]{3,2,1,0} parameter(344), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.584 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,100,100]{3,2,1,0}) %fusion.4), index=0
  %fusion.3 = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg346.347, f32[1,128,1,1]{3,2,1,0} %arg344.345, f32[128]{0} %get-tuple-element.585, f32[128]{0} %get-tuple-element.584), kind=kLoop, calls=%fused_computation.3, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv2/bn/Reshape_grad/Reshape"}
  %arg348.349 = f32[1,128,200,200]{3,2,1,0} parameter(348), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.309 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg29.30), metadata={op_name="XLA_Args"}
  %custom-call.251 = (f32[1,128,201,201]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.146, f32[3,3,128,128]{1,0,2,3} %copy.309), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardInput", metadata={op_type="Conv2DBackpropInput" op_name="tower0/gradients/tower0/group1/block0/conv2/Conv2D_grad/Conv2DBackpropInput"}, backend_config="{\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.547 = f32[1,128,201,201]{3,2,1,0} get-tuple-element((f32[1,128,201,201]{3,2,1,0}, u8[0]{0}) %custom-call.251), index=0
  %arg347.348 = f32[1,128,200,200]{3,2,1,0} parameter(347), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.1 = (f32[128]{0}, f32[128]{0}, f32[1,128,200,200]{3,2,1,0}) fusion(f32[1,128,200,200]{3,2,1,0} %arg348.349, f32[1,128,201,201]{3,2,1,0} %get-tuple-element.547, f32[1,128,200,200]{3,2,1,0} %arg347.348), kind=kInput, calls=%fused_computation.1, metadata={op_type="Sum" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Sum_1"}
  %get-tuple-element.557 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,200,200]{3,2,1,0}) %fusion.1), index=1
  %get-tuple-element.730 = f32[1,128,200,200]{3,2,1,0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,200,200]{3,2,1,0}) %fusion.1), index=2
  %arg349.350 = f32[1,128,1,1]{3,2,1,0} parameter(349), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.144 = f32[1,128,200,200]{3,2,1,0} fusion(f32[1,128,200,200]{3,2,1,0} %get-tuple-element.730, f32[1,128,1,1]{3,2,1,0} %arg349.350), kind=kLoop, calls=%fused_computation.144, metadata={op_type="Mul" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/batchnorm/mul_1_grad/Mul"}
  %custom-call.252 = (f32[1,1,256,128]{1,0,2,3}, u8[0]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %arg140.141, f32[1,128,200,200]{3,2,1,0} %fusion.144), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBackwardFilter", metadata={op_type="Conv2DBackpropFilter" op_name="tower0/gradients/tower0/group1/block0/conv1/Conv2D_grad/Conv2DBackpropFilter"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.548 = f32[1,1,256,128]{1,0,2,3} get-tuple-element((f32[1,1,256,128]{1,0,2,3}, u8[0]{0}) %custom-call.252), index=0
  %arg30.31 = f32[1,1,256,128]{3,2,1,0} parameter(30), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.2 = f32[1,1,256,128]{3,2,1,0} fusion(f32[1,1,256,128]{1,0,2,3} %get-tuple-element.548, f32[1,1,256,128]{3,2,1,0} %arg30.31), kind=kLoop, calls=%fused_computation.2, metadata={op_name="XLA_Retvals"}
  %arg351.352 = f32[1,128,1,1]{3,2,1,0} parameter(351), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg350.351 = f32[1,128,1,1]{3,2,1,0} parameter(350), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %get-tuple-element.556 = f32[128]{0} get-tuple-element((f32[128]{0}, f32[128]{0}, f32[1,128,200,200]{3,2,1,0}) %fusion.1), index=0
  %fusion = f32[128]{0} fusion(f32[1,128,1,1]{3,2,1,0} %arg351.352, f32[1,128,1,1]{3,2,1,0} %arg350.351, f32[128]{0} %get-tuple-element.557, f32[128]{0} %get-tuple-element.556), kind=kLoop, calls=%fused_computation, metadata={op_type="Reshape" op_name="tower0/gradients/tower0/group1/block0/conv1/bn/Reshape_grad/Reshape"}
  ROOT %tuple.238 = (f32[3]{0}, f32[1,1,256,3]{3,2,1,0}, f32[12]{0}, f32[1,1,256,12]{3,2,1,0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[1,1,256,256]{3,2,1,0}, f32[256]{0}, f32[1,1,512,256]{3,2,1,0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1,1,2048,256]{3,2,1,0}, f32[2048]{0}, f32[1,1,512,2048]{3,2,1,0}, f32[2048]{0}, f32[512]{0}, f32[3,3,512,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,2048,512]{3,2,1,0}, f32[512]{0}, f32[2048]{0}, f32[1,1,512,2048]{3,2,1,0}, f32[2048]{0}, f32[512]{0}, f32[3,3,512,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,2048,512]{3,2,1,0}, f32[512]{0}, f32[2048]{0}, f32[2048]{0}, f32[1,1,512,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[2048]{0}, f32[2048]{0}, f32[512]{0}, f32[3,3,512,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,1024,512]{3,2,1,0}, f32[512]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,1024,256]{3,2,1,0}, f32[256]{0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,256,1024]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[256]{0}, f32[3,3,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,512,256]{3,2,1,0}, f32[256]{0}, f32[512]{0}, f32[1,1,128,512]{3,2,1,0}, f32[512]{0}, f32[128]{0}, f32[3,3,128,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,512,128]{3,2,1,0}, f32[128]{0}, f32[512]{0}, f32[1,1,128,512]{3,2,1,0}, f32[512]{0}, f32[128]{0}, f32[3,3,128,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,512,128]{3,2,1,0}, f32[128]{0}, f32[512]{0}, f32[1,1,128,512]{3,2,1,0}, f32[512]{0}, f32[128]{0}, f32[3,3,128,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,512,128]{3,2,1,0}, f32[128]{0}, f32[512]{0}, f32[512]{0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,128,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[128]{0}, f32[3,3,128,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,256,128]{3,2,1,0}, f32[128]{0}) tuple(f32[3]{0} %fusion.142, f32[1,1,256,3]{3,2,1,0} %fusion.141, f32[12]{0} %fusion.140, f32[1,1,256,12]{3,2,1,0} %fusion.139, f32[256]{0} %fusion.138, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.646, f32[256]{0} %get-tuple-element.678, f32[256]{0} %get-tuple-element.680, f32[256]{0} %get-tuple-element.682, f32[256]{0} %get-tuple-element.684, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.649, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.643, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.645, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.648, f32[256]{0} %reduce.1290, f32[1,1,256,256]{3,2,1,0} %fusion.132, f32[256]{0} %get-tuple-element.686, f32[1,1,512,256]{3,2,1,0} %fusion.131, f32[256]{0} %get-tuple-element.688, f32[1,1,1024,256]{3,2,1,0} %fusion.130, f32[256]{0} %get-tuple-element.690, f32[1,1,2048,256]{3,2,1,0} %fusion.129, f32[2048]{0} %get-tuple-element.581, f32[1,1,512,2048]{3,2,1,0} %fusion.128, f32[2048]{0} %fusion.126, f32[512]{0} %get-tuple-element.659, f32[3,3,512,512]{3,2,1,0} %fusion.125, f32[512]{0} %fusion.123, f32[512]{0} %get-tuple-element.661, f32[1,1,2048,512]{3,2,1,0} %fusion.122, f32[512]{0} %fusion.120, f32[2048]{0} %get-tuple-element.604, f32[1,1,512,2048]{3,2,1,0} %fusion.119, f32[2048]{0} %fusion.117, f32[512]{0} %get-tuple-element.655, f32[3,3,512,512]{3,2,1,0} %fusion.116, f32[512]{0} %fusion.114, f32[512]{0} %get-tuple-element.657, f32[1,1,2048,512]{3,2,1,0} %fusion.113, f32[512]{0} %fusion.111, f32[2048]{0} %get-tuple-element.600, f32[2048]{0} %copy.484, f32[1,1,512,2048]{3,2,1,0} %fusion.110, f32[1,1,1024,2048]{3,2,1,0} %fusion.109, f32[2048]{0} %get-tuple-element.662, f32[2048]{0} %get-tuple-element.663, f32[512]{0} %get-tuple-element.653, f32[3,3,512,512]{3,2,1,0} %fusion.103, f32[512]{0} %fusion.101, f32[512]{0} %get-tuple-element.583, f32[1,1,1024,512]{3,2,1,0} %fusion.100, f32[512]{0} %fusion.98, f32[1024]{0} %get-tuple-element.568, f32[1,1,256,1024]{3,2,1,0} %fusion.97, f32[1024]{0} %fusion.95, f32[256]{0} %get-tuple-element.624, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.650, f32[256]{0} %fusion.92, f32[256]{0} %get-tuple-element.626, f32[1,1,1024,256]{3,2,1,0} %fusion.91, f32[256]{0} %fusion.89, f32[1024]{0} %get-tuple-element.570, f32[1,1,256,1024]{3,2,1,0} %fusion.88, f32[1024]{0} %fusion.86, f32[256]{0} %get-tuple-element.618, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.631, f32[256]{0} %fusion.83, f32[256]{0} %get-tuple-element.616, f32[1,1,1024,256]{3,2,1,0} %fusion.82, f32[256]{0} %fusion.80, f32[1024]{0} %get-tuple-element.572, f32[1,1,256,1024]{3,2,1,0} %fusion.79, f32[1024]{0} %fusion.77, f32[256]{0} %get-tuple-element.612, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.644, f32[256]{0} %fusion.74, f32[256]{0} %get-tuple-element.610, f32[1,1,1024,256]{3,2,1,0} %fusion.73, f32[256]{0} %fusion.71, f32[1024]{0} %get-tuple-element.564, f32[1,1,256,1024]{3,2,1,0} %fusion.70, f32[1024]{0} %fusion.68, f32[256]{0} %get-tuple-element.606, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.651, f32[256]{0} %fusion.65, f32[256]{0} %get-tuple-element.608, f32[1,1,1024,256]{3,2,1,0} %fusion.64, f32[256]{0} %fusion.62, f32[1024]{0} %get-tuple-element.576, f32[1,1,256,1024]{3,2,1,0} %fusion.61, f32[1024]{0} %fusion.59, f32[256]{0} %get-tuple-element.620, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.647, f32[256]{0} %fusion.56, f32[256]{0} %get-tuple-element.622, f32[1,1,1024,256]{3,2,1,0} %fusion.55, f32[256]{0} %fusion.53, f32[1024]{0} %get-tuple-element.578, f32[1024]{0} %copy.485, f32[1,1,512,1024]{3,2,1,0} %fusion.52, f32[1,1,256,1024]{3,2,1,0} %fusion.51, f32[1024]{0} %get-tuple-element.664, f32[1024]{0} %get-tuple-element.665, f32[256]{0} %get-tuple-element.614, f32[3,3,256,256]{3,2,1,0} %get-tuple-element.632, f32[256]{0} %fusion.43, f32[256]{0} %get-tuple-element.574, f32[1,1,512,256]{3,2,1,0} %fusion.42, f32[256]{0} %fusion.40, f32[512]{0} %get-tuple-element.560, f32[1,1,128,512]{3,2,1,0} %fusion.39, f32[512]{0} %fusion.37, f32[128]{0} %get-tuple-element.597, f32[3,3,128,128]{3,2,1,0} %fusion.36, f32[128]{0} %fusion.34, f32[128]{0} %get-tuple-element.595, f32[1,1,512,128]{3,2,1,0} %fusion.33, f32[128]{0} %fusion.31, f32[512]{0} %get-tuple-element.562, f32[1,1,128,512]{3,2,1,0} %fusion.30, f32[512]{0} %fusion.28, f32[128]{0} %get-tuple-element.591, f32[3,3,128,128]{3,2,1,0} %fusion.27, f32[128]{0} %fusion.25, f32[128]{0} %get-tuple-element.593, f32[1,1,512,128]{3,2,1,0} %fusion.24, f32[128]{0} %fusion.22, f32[512]{0} %get-tuple-element.553, f32[1,1,128,512]{3,2,1,0} %fusion.21, f32[512]{0} %fusion.19, f32[128]{0} %get-tuple-element.589, f32[3,3,128,128]{3,2,1,0} %fusion.18, f32[128]{0} %fusion.16, f32[128]{0} %get-tuple-element.587, f32[1,1,512,128]{3,2,1,0} %fusion.15, f32[128]{0} %fusion.13, f32[512]{0} %get-tuple-element.555, f32[512]{0} %copy.486, f32[1,1,256,512]{3,2,1,0} %fusion.12, f32[1,1,128,512]{3,2,1,0} %fusion.11, f32[512]{0} %get-tuple-element.666, f32[512]{0} %get-tuple-element.667, f32[128]{0} %get-tuple-element.585, f32[3,3,128,128]{3,2,1,0} %fusion.5, f32[128]{0} %fusion.3, f32[128]{0} %get-tuple-element.557, f32[1,1,256,128]{3,2,1,0} %fusion.2, f32[128]{0} %fusion)
}

