HloModule cluster_17__XlaCompiledKernel_true__XlaNumConstantArgs_39__XlaNumResourceArgs_0_.2788

%max_F32.1823 (lhs.1824: f32[], rhs.1825: f32[]) -> f32[] {
  %lhs.1824 = f32[] parameter(0)
  %rhs.1825 = f32[] parameter(1)
  ROOT %maximum.1826 = f32[] maximum(f32[] %lhs.1824, f32[] %rhs.1825)
}

%fused_computation (param_0.2: s32[], param_1.3: s32[], param_2.6: f32[], param_3.10: f32[]) -> pred[] {
  %param_3.10 = f32[] parameter(3)
  %bitcast-convert.35 = s32[] bitcast-convert(f32[] %param_3.10)
  %constant_130 = s32[] constant(0)
  %compare.30 = pred[] compare(s32[] %bitcast-convert.35, s32[] %constant_130), direction=LT
  %constant_129 = u32[] constant(2147483647)
  %bitcast-convert.34 = u32[] bitcast-convert(f32[] %param_3.10)
  %subtract.22 = u32[] subtract(u32[] %constant_129, u32[] %bitcast-convert.34)
  %bitcast-convert.33 = s32[] bitcast-convert(u32[] %subtract.22)
  %select.17 = s32[] select(pred[] %compare.30, s32[] %bitcast-convert.33, s32[] %bitcast-convert.35)
  %param_2.6 = f32[] parameter(2)
  %bitcast-convert.32 = s32[] bitcast-convert(f32[] %param_2.6)
  %compare.29 = pred[] compare(s32[] %bitcast-convert.32, s32[] %constant_130), direction=LT
  %bitcast-convert.31 = u32[] bitcast-convert(f32[] %param_2.6)
  %subtract.21 = u32[] subtract(u32[] %constant_129, u32[] %bitcast-convert.31)
  %bitcast-convert.30 = s32[] bitcast-convert(u32[] %subtract.21)
  %select.16 = s32[] select(pred[] %compare.29, s32[] %bitcast-convert.30, s32[] %bitcast-convert.32)
  %compare.28 = pred[] compare(s32[] %select.17, s32[] %select.16), direction=GT
  %compare.27 = pred[] compare(s32[] %select.16, s32[] %select.17), direction=GT
  %compare.26 = pred[] compare(pred[] %compare.28, pred[] %compare.27), direction=EQ
  %param_0.2 = s32[] parameter(0)
  %param_1.3 = s32[] parameter(1)
  %compare.25 = pred[] compare(s32[] %param_0.2, s32[] %param_1.3), direction=LT
  ROOT %select.15 = pred[] select(pred[] %compare.26, pred[] %compare.25, pred[] %compare.28)
}

%compare-greater-than.1898 (p.0.lhs.1899: f32[], p.0.rhs.1900: f32[], p.1.lhs.1901: s32[], p.1.rhs.1902: s32[]) -> pred[] {
  %p.1.lhs.1901 = s32[] parameter(2)
  %p.1.rhs.1902 = s32[] parameter(3)
  %p.0.rhs.1900 = f32[] parameter(1)
  %p.0.lhs.1899 = f32[] parameter(0)
  ROOT %fusion = pred[] fusion(s32[] %p.1.lhs.1901, s32[] %p.1.rhs.1902, f32[] %p.0.rhs.1900, f32[] %p.0.lhs.1899), kind=kLoop, calls=%fused_computation
}

%fused_computation.1 (param_0.5: s32[], param_1.7: s32[], param_2.13: f32[], param_3.21: f32[]) -> pred[] {
  %param_3.21 = f32[] parameter(3)
  %bitcast-convert.41 = s32[] bitcast-convert(f32[] %param_3.21)
  %constant_132 = s32[] constant(0)
  %compare.36 = pred[] compare(s32[] %bitcast-convert.41, s32[] %constant_132), direction=LT
  %constant_131 = u32[] constant(2147483647)
  %bitcast-convert.40 = u32[] bitcast-convert(f32[] %param_3.21)
  %subtract.24 = u32[] subtract(u32[] %constant_131, u32[] %bitcast-convert.40)
  %bitcast-convert.39 = s32[] bitcast-convert(u32[] %subtract.24)
  %select.20 = s32[] select(pred[] %compare.36, s32[] %bitcast-convert.39, s32[] %bitcast-convert.41)
  %param_2.13 = f32[] parameter(2)
  %bitcast-convert.38 = s32[] bitcast-convert(f32[] %param_2.13)
  %compare.35 = pred[] compare(s32[] %bitcast-convert.38, s32[] %constant_132), direction=LT
  %bitcast-convert.37 = u32[] bitcast-convert(f32[] %param_2.13)
  %subtract.23 = u32[] subtract(u32[] %constant_131, u32[] %bitcast-convert.37)
  %bitcast-convert.36 = s32[] bitcast-convert(u32[] %subtract.23)
  %select.19 = s32[] select(pred[] %compare.35, s32[] %bitcast-convert.36, s32[] %bitcast-convert.38)
  %compare.34 = pred[] compare(s32[] %select.20, s32[] %select.19), direction=GT
  %compare.33 = pred[] compare(s32[] %select.19, s32[] %select.20), direction=GT
  %compare.32 = pred[] compare(pred[] %compare.34, pred[] %compare.33), direction=EQ
  %param_0.5 = s32[] parameter(0)
  %param_1.7 = s32[] parameter(1)
  %compare.31 = pred[] compare(s32[] %param_0.5, s32[] %param_1.7), direction=LT
  ROOT %select.18 = pred[] select(pred[] %compare.32, pred[] %compare.31, pred[] %compare.34)
}

%compare-greater-than.1972 (p.0.lhs.1973: f32[], p.0.rhs.1974: f32[], p.1.lhs.1975: s32[], p.1.rhs.1976: s32[]) -> pred[] {
  %p.1.lhs.1975 = s32[] parameter(2)
  %p.1.rhs.1976 = s32[] parameter(3)
  %p.0.rhs.1974 = f32[] parameter(1)
  %p.0.lhs.1973 = f32[] parameter(0)
  ROOT %fusion.1 = pred[] fusion(s32[] %p.1.lhs.1975, s32[] %p.1.rhs.1976, f32[] %p.0.rhs.1974, f32[] %p.0.lhs.1973), kind=kLoop, calls=%fused_computation.1
}

%fused_computation.2 (param_0.8: s32[], param_1.11: s32[], param_2.20: f32[], param_3.32: f32[]) -> pred[] {
  %param_3.32 = f32[] parameter(3)
  %bitcast-convert.47 = s32[] bitcast-convert(f32[] %param_3.32)
  %constant_134 = s32[] constant(0)
  %compare.42 = pred[] compare(s32[] %bitcast-convert.47, s32[] %constant_134), direction=LT
  %constant_133 = u32[] constant(2147483647)
  %bitcast-convert.46 = u32[] bitcast-convert(f32[] %param_3.32)
  %subtract.26 = u32[] subtract(u32[] %constant_133, u32[] %bitcast-convert.46)
  %bitcast-convert.45 = s32[] bitcast-convert(u32[] %subtract.26)
  %select.23 = s32[] select(pred[] %compare.42, s32[] %bitcast-convert.45, s32[] %bitcast-convert.47)
  %param_2.20 = f32[] parameter(2)
  %bitcast-convert.44 = s32[] bitcast-convert(f32[] %param_2.20)
  %compare.41 = pred[] compare(s32[] %bitcast-convert.44, s32[] %constant_134), direction=LT
  %bitcast-convert.43 = u32[] bitcast-convert(f32[] %param_2.20)
  %subtract.25 = u32[] subtract(u32[] %constant_133, u32[] %bitcast-convert.43)
  %bitcast-convert.42 = s32[] bitcast-convert(u32[] %subtract.25)
  %select.22 = s32[] select(pred[] %compare.41, s32[] %bitcast-convert.42, s32[] %bitcast-convert.44)
  %compare.40 = pred[] compare(s32[] %select.23, s32[] %select.22), direction=GT
  %compare.39 = pred[] compare(s32[] %select.22, s32[] %select.23), direction=GT
  %compare.38 = pred[] compare(pred[] %compare.40, pred[] %compare.39), direction=EQ
  %param_0.8 = s32[] parameter(0)
  %param_1.11 = s32[] parameter(1)
  %compare.37 = pred[] compare(s32[] %param_0.8, s32[] %param_1.11), direction=LT
  ROOT %select.21 = pred[] select(pred[] %compare.38, pred[] %compare.37, pred[] %compare.40)
}

%compare-greater-than.2138 (p.0.lhs.2139: f32[], p.0.rhs.2140: f32[], p.1.lhs.2141: s32[], p.1.rhs.2142: s32[]) -> pred[] {
  %p.1.lhs.2141 = s32[] parameter(2)
  %p.1.rhs.2142 = s32[] parameter(3)
  %p.0.rhs.2140 = f32[] parameter(1)
  %p.0.lhs.2139 = f32[] parameter(0)
  ROOT %fusion.2 = pred[] fusion(s32[] %p.1.lhs.2141, s32[] %p.1.rhs.2142, f32[] %p.0.rhs.2140, f32[] %p.0.lhs.2139), kind=kLoop, calls=%fused_computation.2
}

%fused_computation.3 (param_0.11: s32[], param_1.15: s32[], param_2.27: f32[], param_3.43: f32[]) -> pred[] {
  %param_3.43 = f32[] parameter(3)
  %bitcast-convert.53 = s32[] bitcast-convert(f32[] %param_3.43)
  %constant_136 = s32[] constant(0)
  %compare.48 = pred[] compare(s32[] %bitcast-convert.53, s32[] %constant_136), direction=LT
  %constant_135 = u32[] constant(2147483647)
  %bitcast-convert.52 = u32[] bitcast-convert(f32[] %param_3.43)
  %subtract.28 = u32[] subtract(u32[] %constant_135, u32[] %bitcast-convert.52)
  %bitcast-convert.51 = s32[] bitcast-convert(u32[] %subtract.28)
  %select.26 = s32[] select(pred[] %compare.48, s32[] %bitcast-convert.51, s32[] %bitcast-convert.53)
  %param_2.27 = f32[] parameter(2)
  %bitcast-convert.50 = s32[] bitcast-convert(f32[] %param_2.27)
  %compare.47 = pred[] compare(s32[] %bitcast-convert.50, s32[] %constant_136), direction=LT
  %bitcast-convert.49 = u32[] bitcast-convert(f32[] %param_2.27)
  %subtract.27 = u32[] subtract(u32[] %constant_135, u32[] %bitcast-convert.49)
  %bitcast-convert.48 = s32[] bitcast-convert(u32[] %subtract.27)
  %select.25 = s32[] select(pred[] %compare.47, s32[] %bitcast-convert.48, s32[] %bitcast-convert.50)
  %compare.46 = pred[] compare(s32[] %select.26, s32[] %select.25), direction=GT
  %compare.45 = pred[] compare(s32[] %select.25, s32[] %select.26), direction=GT
  %compare.44 = pred[] compare(pred[] %compare.46, pred[] %compare.45), direction=EQ
  %param_0.11 = s32[] parameter(0)
  %param_1.15 = s32[] parameter(1)
  %compare.43 = pred[] compare(s32[] %param_0.11, s32[] %param_1.15), direction=LT
  ROOT %select.24 = pred[] select(pred[] %compare.44, pred[] %compare.43, pred[] %compare.46)
}

%compare-greater-than.2304 (p.0.lhs.2305: f32[], p.0.rhs.2306: f32[], p.1.lhs.2307: s32[], p.1.rhs.2308: s32[]) -> pred[] {
  %p.1.lhs.2307 = s32[] parameter(2)
  %p.1.rhs.2308 = s32[] parameter(3)
  %p.0.rhs.2306 = f32[] parameter(1)
  %p.0.lhs.2305 = f32[] parameter(0)
  ROOT %fusion.3 = pred[] fusion(s32[] %p.1.lhs.2307, s32[] %p.1.rhs.2308, f32[] %p.0.rhs.2306, f32[] %p.0.lhs.2305), kind=kLoop, calls=%fused_computation.3
}

%fused_computation.4 (param_0.14: s32[], param_1.19: s32[], param_2.34: f32[], param_3.54: f32[]) -> pred[] {
  %param_3.54 = f32[] parameter(3)
  %bitcast-convert.59 = s32[] bitcast-convert(f32[] %param_3.54)
  %constant_138 = s32[] constant(0)
  %compare.54 = pred[] compare(s32[] %bitcast-convert.59, s32[] %constant_138), direction=LT
  %constant_137 = u32[] constant(2147483647)
  %bitcast-convert.58 = u32[] bitcast-convert(f32[] %param_3.54)
  %subtract.30 = u32[] subtract(u32[] %constant_137, u32[] %bitcast-convert.58)
  %bitcast-convert.57 = s32[] bitcast-convert(u32[] %subtract.30)
  %select.29 = s32[] select(pred[] %compare.54, s32[] %bitcast-convert.57, s32[] %bitcast-convert.59)
  %param_2.34 = f32[] parameter(2)
  %bitcast-convert.56 = s32[] bitcast-convert(f32[] %param_2.34)
  %compare.53 = pred[] compare(s32[] %bitcast-convert.56, s32[] %constant_138), direction=LT
  %bitcast-convert.55 = u32[] bitcast-convert(f32[] %param_2.34)
  %subtract.29 = u32[] subtract(u32[] %constant_137, u32[] %bitcast-convert.55)
  %bitcast-convert.54 = s32[] bitcast-convert(u32[] %subtract.29)
  %select.28 = s32[] select(pred[] %compare.53, s32[] %bitcast-convert.54, s32[] %bitcast-convert.56)
  %compare.52 = pred[] compare(s32[] %select.29, s32[] %select.28), direction=GT
  %compare.51 = pred[] compare(s32[] %select.28, s32[] %select.29), direction=GT
  %compare.50 = pred[] compare(pred[] %compare.52, pred[] %compare.51), direction=EQ
  %param_0.14 = s32[] parameter(0)
  %param_1.19 = s32[] parameter(1)
  %compare.49 = pred[] compare(s32[] %param_0.14, s32[] %param_1.19), direction=LT
  ROOT %select.27 = pred[] select(pred[] %compare.50, pred[] %compare.49, pred[] %compare.52)
}

%compare-greater-than.2470 (p.0.lhs.2471: f32[], p.0.rhs.2472: f32[], p.1.lhs.2473: s32[], p.1.rhs.2474: s32[]) -> pred[] {
  %p.1.lhs.2473 = s32[] parameter(2)
  %p.1.rhs.2474 = s32[] parameter(3)
  %p.0.rhs.2472 = f32[] parameter(1)
  %p.0.lhs.2471 = f32[] parameter(0)
  ROOT %fusion.4 = pred[] fusion(s32[] %p.1.lhs.2473, s32[] %p.1.rhs.2474, f32[] %p.0.rhs.2472, f32[] %p.0.lhs.2471), kind=kLoop, calls=%fused_computation.4
}

%fused_computation.5 (param_0.402: s32[120000], param_1.423: f32[120000,1,2], param_2.344: f32[120000,1,2], param_3.305: f32[120000,1,2], param_4.241: f32[1,200,200,12]) -> f32[2000,4] {
  %param_4.241 = f32[1,200,200,12]{3,2,1,0} parameter(4)
  %bitcast.544 = f32[120000,2,2]{2,1,0} bitcast(f32[1,200,200,12]{3,2,1,0} %param_4.241), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target/Reshape"}
  %slice.1 = f32[120000,1,2]{2,1,0} slice(f32[120000,2,2]{2,1,0} %bitcast.544), slice={[0:120000], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target/split"}
  %param_2.344 = f32[120000,1,2]{2,1,0} parameter(2)
  %multiply.28 = f32[120000,1,2]{2,1,0} multiply(f32[120000,1,2]{2,1,0} %slice.1, f32[120000,1,2]{2,1,0} %param_2.344), metadata={op_type="Mul" op_name="tower0/decode_bbox_target/mul_2"}
  %constant_139 = f32[] constant(0.5), metadata={op_type="Mul" op_name="EMA/QueueInput/queue_size_EMA_apply/mul"}
  %broadcast.116 = f32[120000,1,2]{2,1,0} broadcast(f32[] %constant_139), dimensions={}, metadata={op_type="Mul" op_name="tower0/rpn_losses/encode_bbox_target/mul"}
  %param_3.305 = f32[120000,1,2]{2,1,0} parameter(3)
  %multiply.27 = f32[120000,1,2]{2,1,0} multiply(f32[120000,1,2]{2,1,0} %broadcast.116, f32[120000,1,2]{2,1,0} %param_3.305), metadata={op_type="Mul" op_name="tower0/rpn_losses/encode_bbox_target/mul"}
  %add.58 = f32[120000,1,2]{2,1,0} add(f32[120000,1,2]{2,1,0} %multiply.28, f32[120000,1,2]{2,1,0} %multiply.27), metadata={op_type="Add" op_name="tower0/decode_bbox_target/add_1"}
  %param_1.423 = f32[120000,1,2]{2,1,0} parameter(1)
  %multiply.26 = f32[120000,1,2]{2,1,0} multiply(f32[120000,1,2]{2,1,0} %broadcast.116, f32[120000,1,2]{2,1,0} %param_2.344), metadata={op_type="Mul" op_name="tower0/decode_bbox_target/mul_1"}
  %multiply.25 = f32[120000,1,2]{2,1,0} multiply(f32[120000,1,2]{2,1,0} %param_1.423, f32[120000,1,2]{2,1,0} %multiply.26), metadata={op_type="Mul" op_name="tower0/decode_bbox_target/mul_4"}
  %subtract.31 = f32[120000,1,2]{2,1,0} subtract(f32[120000,1,2]{2,1,0} %add.58, f32[120000,1,2]{2,1,0} %multiply.25), metadata={op_type="Sub" op_name="tower0/decode_bbox_target/sub_1"}
  %add.57 = f32[120000,1,2]{2,1,0} add(f32[120000,1,2]{2,1,0} %add.58, f32[120000,1,2]{2,1,0} %multiply.25), metadata={op_type="Add" op_name="tower0/decode_bbox_target/add_2"}
  %concatenate.0 = f32[120000,2,2]{2,1,0} concatenate(f32[120000,1,2]{2,1,0} %subtract.31, f32[120000,1,2]{2,1,0} %add.57), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower0/decode_bbox_target/concat"}
  %bitcast.306 = f32[120000,4]{1,0} bitcast(f32[120000,2,2]{2,1,0} %concatenate.0), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl2/Reshape"}
  %param_0.402 = s32[120000]{0} parameter(0)
  %slice.0 = s32[2000]{0} slice(s32[120000]{0} %param_0.402), slice={[0:2000]}, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %gather.0 = f32[2000,4]{1,0} gather(f32[120000,4]{1,0} %bitcast.306, s32[2000]{0} %slice.0), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/GatherV2"}
  %constant_171 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.272 = f32[2000,4]{1,0} broadcast(f32[] %constant_171), dimensions={}, metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Maximum"}
  %maximum.0 = f32[2000,4]{1,0} maximum(f32[2000,4]{1,0} %gather.0, f32[2000,4]{1,0} %broadcast.272), metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/clip_boxes/Maximum"}
  %constant_170 = f32[] constant(800), metadata={op_type="Cast" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Cast"}
  %broadcast.273 = f32[2000,4]{1,0} broadcast(f32[] %constant_170), dimensions={}, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Minimum"}
  ROOT %minimum.0 = f32[2000,4]{1,0} minimum(f32[2000,4]{1,0} %maximum.0, f32[2000,4]{1,0} %broadcast.273), metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/clip_boxes/Minimum"}
}

%fused_computation.6 (param_0.494: f32[1,3,200,200], param_1.591: f32[3]) -> f32[120000] {
  %param_0.494 = f32[1,3,200,200]{3,2,1,0} parameter(0)
  %param_1.591 = f32[3]{0} parameter(1)
  %broadcast.393 = f32[1,3,200,200]{3,2,1,0} broadcast(f32[3]{0} %param_1.591), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn/class/BiasAdd"}
  %add.228 = f32[1,3,200,200]{3,2,1,0} add(f32[1,3,200,200]{3,2,1,0} %param_0.494, f32[1,3,200,200]{3,2,1,0} %broadcast.393), metadata={op_type="BiasAdd" op_name="tower0/rpn/class/BiasAdd"}
  %bitcast.307 = f32[1,200,200,3]{2,1,3,0} bitcast(f32[1,3,200,200]{3,2,1,0} %add.228), metadata={op_type="Transpose" op_name="tower0/rpn/transpose"}
  ROOT %reshape.186 = f32[120000]{0} reshape(f32[1,200,200,3]{2,1,3,0} %bitcast.307), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl2/Reshape_1"}
}

%fused_computation.7 (param_0.403: f32[1,200,200,12]) -> f32[120000,1,2] {
  %param_0.403 = f32[1,200,200,12]{3,2,1,0} parameter(0)
  %bitcast.545 = f32[120000,2,2]{2,1,0} bitcast(f32[1,200,200,12]{3,2,1,0} %param_0.403), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target/Reshape"}
  %slice.2 = f32[120000,1,2]{2,1,0} slice(f32[120000,2,2]{2,1,0} %bitcast.545), slice={[0:120000], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target/split"}
  %constant_140 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_4/Minimum"}
  %broadcast.117 = f32[120000,1,2]{2,1,0} broadcast(f32[] %constant_140), dimensions={}, metadata={op_type="Minimum" op_name="tower0/decode_bbox_target/Minimum"}
  %minimum.1 = f32[120000,1,2]{2,1,0} minimum(f32[120000,1,2]{2,1,0} %slice.2, f32[120000,1,2]{2,1,0} %broadcast.117), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target/Minimum"}
  ROOT %exponential.0 = f32[120000,1,2]{2,1,0} exponential(f32[120000,1,2]{2,1,0} %minimum.1), metadata={op_type="Exp" op_name="tower0/decode_bbox_target/Exp"}
}

%fused_computation.8 (param_0.25: f32[1,12,200,200], param_1.27: f32[12]) -> f32[1,200,200,12] {
  %param_0.25 = f32[1,12,200,200]{3,2,1,0} parameter(0)
  %param_1.27 = f32[12]{0} parameter(1)
  %broadcast.118 = f32[1,12,200,200]{3,2,1,0} broadcast(f32[12]{0} %param_1.27), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn/box/BiasAdd"}
  %add.59 = f32[1,12,200,200]{3,2,1,0} add(f32[1,12,200,200]{3,2,1,0} %param_0.25, f32[1,12,200,200]{3,2,1,0} %broadcast.118), metadata={op_type="BiasAdd" op_name="tower0/rpn/box/BiasAdd"}
  %bitcast.308 = f32[1,200,200,12]{2,1,3,0} bitcast(f32[1,12,200,200]{3,2,1,0} %add.59), metadata={op_type="Transpose" op_name="tower0/rpn/transpose_1"}
  ROOT %copy.184 = f32[1,200,200,12]{3,2,1,0} copy(f32[1,200,200,12]{2,1,3,0} %bitcast.308), metadata={op_name="XLA_Retvals"}
}

%fused_computation.9 (param_0.496: f32[1,3,200,200], param_1.594: f32[3]) -> f32[200,200,3] {
  %param_0.496 = f32[1,3,200,200]{3,2,1,0} parameter(0)
  %param_1.594 = f32[3]{0} parameter(1)
  %broadcast.395 = f32[1,3,200,200]{3,2,1,0} broadcast(f32[3]{0} %param_1.594), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn/class/BiasAdd"}
  %add.230 = f32[1,3,200,200]{3,2,1,0} add(f32[1,3,200,200]{3,2,1,0} %param_0.496, f32[1,3,200,200]{3,2,1,0} %broadcast.395), metadata={op_type="BiasAdd" op_name="tower0/rpn/class/BiasAdd"}
  %bitcast.309 = f32[200,200,3]{1,0,2} bitcast(f32[1,3,200,200]{3,2,1,0} %add.230), metadata={op_type="Squeeze" op_name="tower0/rpn/Squeeze"}
  ROOT %copy.185 = f32[200,200,3]{2,1,0} copy(f32[200,200,3]{1,0,2} %bitcast.309), metadata={op_name="XLA_Retvals"}
}

%fused_computation.11 (param_0.404: s32[30000], param_1.425: f32[30000,1,2], param_2.346: f32[30000,1,2], param_3.307: f32[30000,1,2], param_4.243: f32[1,100,100,12]) -> f32[2000,4] {
  %constant_141 = f32[] constant(0.5), metadata={op_type="Mul" op_name="EMA/QueueInput/queue_size_EMA_apply/mul"}
  %broadcast.120 = f32[30000,1,2]{2,1,0} broadcast(f32[] %constant_141), dimensions={}, metadata={op_type="Mul" op_name="tower0/decode_bbox_target_1/mul"}
  %param_3.307 = f32[30000,1,2]{2,1,0} parameter(3)
  %multiply.32 = f32[30000,1,2]{2,1,0} multiply(f32[30000,1,2]{2,1,0} %broadcast.120, f32[30000,1,2]{2,1,0} %param_3.307), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_1/mul"}
  %param_4.243 = f32[1,100,100,12]{3,2,1,0} parameter(4)
  %bitcast.546 = f32[30000,2,2]{2,1,0} bitcast(f32[1,100,100,12]{3,2,1,0} %param_4.243), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target_1/Reshape"}
  %slice.4 = f32[30000,1,2]{2,1,0} slice(f32[30000,2,2]{2,1,0} %bitcast.546), slice={[0:30000], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target_1/split"}
  %param_2.346 = f32[30000,1,2]{2,1,0} parameter(2)
  %multiply.31 = f32[30000,1,2]{2,1,0} multiply(f32[30000,1,2]{2,1,0} %slice.4, f32[30000,1,2]{2,1,0} %param_2.346), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_1/mul_2"}
  %add.62 = f32[30000,1,2]{2,1,0} add(f32[30000,1,2]{2,1,0} %multiply.32, f32[30000,1,2]{2,1,0} %multiply.31), metadata={op_type="Add" op_name="tower0/decode_bbox_target_1/add_1"}
  %param_1.425 = f32[30000,1,2]{2,1,0} parameter(1)
  %multiply.30 = f32[30000,1,2]{2,1,0} multiply(f32[30000,1,2]{2,1,0} %broadcast.120, f32[30000,1,2]{2,1,0} %param_2.346), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_1/mul_1"}
  %multiply.29 = f32[30000,1,2]{2,1,0} multiply(f32[30000,1,2]{2,1,0} %param_1.425, f32[30000,1,2]{2,1,0} %multiply.30), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_1/mul_4"}
  %subtract.32 = f32[30000,1,2]{2,1,0} subtract(f32[30000,1,2]{2,1,0} %add.62, f32[30000,1,2]{2,1,0} %multiply.29), metadata={op_type="Sub" op_name="tower0/decode_bbox_target_1/sub_1"}
  %add.61 = f32[30000,1,2]{2,1,0} add(f32[30000,1,2]{2,1,0} %add.62, f32[30000,1,2]{2,1,0} %multiply.29), metadata={op_type="Add" op_name="tower0/decode_bbox_target_1/add_2"}
  %concatenate.1 = f32[30000,2,2]{2,1,0} concatenate(f32[30000,1,2]{2,1,0} %subtract.32, f32[30000,1,2]{2,1,0} %add.61), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower0/decode_bbox_target_1/concat"}
  %bitcast.310 = f32[30000,4]{1,0} bitcast(f32[30000,2,2]{2,1,0} %concatenate.1), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl3/Reshape"}
  %param_0.404 = s32[30000]{0} parameter(0)
  %slice.3 = s32[2000]{0} slice(s32[30000]{0} %param_0.404), slice={[0:2000]}, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %gather.1 = f32[2000,4]{1,0} gather(f32[30000,4]{1,0} %bitcast.310, s32[2000]{0} %slice.3), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/GatherV2"}
  %constant_173 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.274 = f32[2000,4]{1,0} broadcast(f32[] %constant_173), dimensions={}, metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Maximum"}
  %maximum.1 = f32[2000,4]{1,0} maximum(f32[2000,4]{1,0} %gather.1, f32[2000,4]{1,0} %broadcast.274), metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/clip_boxes/Maximum"}
  %constant_172 = f32[] constant(800), metadata={op_type="Cast" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Cast"}
  %broadcast.275 = f32[2000,4]{1,0} broadcast(f32[] %constant_172), dimensions={}, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Minimum"}
  ROOT %minimum.2 = f32[2000,4]{1,0} minimum(f32[2000,4]{1,0} %maximum.1, f32[2000,4]{1,0} %broadcast.275), metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/clip_boxes/Minimum"}
}

%fused_computation.12 (param_0.490: f32[1,3,100,100], param_1.585: f32[3]) -> f32[30000] {
  %param_0.490 = f32[1,3,100,100]{3,2,1,0} parameter(0)
  %param_1.585 = f32[3]{0} parameter(1)
  %broadcast.389 = f32[1,3,100,100]{3,2,1,0} broadcast(f32[3]{0} %param_1.585), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_1/class/BiasAdd"}
  %add.224 = f32[1,3,100,100]{3,2,1,0} add(f32[1,3,100,100]{3,2,1,0} %param_0.490, f32[1,3,100,100]{3,2,1,0} %broadcast.389), metadata={op_type="BiasAdd" op_name="tower0/rpn_1/class/BiasAdd"}
  %bitcast.311 = f32[1,100,100,3]{2,1,3,0} bitcast(f32[1,3,100,100]{3,2,1,0} %add.224), metadata={op_type="Transpose" op_name="tower0/rpn_1/transpose"}
  ROOT %reshape.187 = f32[30000]{0} reshape(f32[1,100,100,3]{2,1,3,0} %bitcast.311), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl3/Reshape_1"}
}

%fused_computation.13 (param_0.405: f32[1,100,100,12]) -> f32[30000,1,2] {
  %param_0.405 = f32[1,100,100,12]{3,2,1,0} parameter(0)
  %bitcast.547 = f32[30000,2,2]{2,1,0} bitcast(f32[1,100,100,12]{3,2,1,0} %param_0.405), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target_1/Reshape"}
  %slice.5 = f32[30000,1,2]{2,1,0} slice(f32[30000,2,2]{2,1,0} %bitcast.547), slice={[0:30000], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target_1/split"}
  %constant_142 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_4/Minimum"}
  %broadcast.121 = f32[30000,1,2]{2,1,0} broadcast(f32[] %constant_142), dimensions={}, metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_1/Minimum"}
  %minimum.3 = f32[30000,1,2]{2,1,0} minimum(f32[30000,1,2]{2,1,0} %slice.5, f32[30000,1,2]{2,1,0} %broadcast.121), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_1/Minimum"}
  ROOT %exponential.1 = f32[30000,1,2]{2,1,0} exponential(f32[30000,1,2]{2,1,0} %minimum.3), metadata={op_type="Exp" op_name="tower0/decode_bbox_target_1/Exp"}
}

%fused_computation.15 (param_0.407: s32[336,336,3]) -> (pred[200,200,3], pred[200,200,3]) {
  %param_0.407 = s32[336,336,3]{2,1,0} parameter(0)
  %slice.14 = s32[200,200,3]{2,1,0} slice(s32[336,336,3]{2,1,0} %param_0.407), slice={[0:200], [0:200], [0:3]}, metadata={op_type="Slice" op_name="tower0/FPN_slice_lvl0/narrow_to/Slice_1"}
  %constant_144 = s32[] constant(-1), metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level5/NotEqual"}
  %broadcast.123 = s32[200,200,3]{2,1,0} broadcast(s32[] %constant_144), dimensions={}, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level2/NotEqual"}
  %compare.56 = pred[200,200,3]{2,1,0} compare(s32[200,200,3]{2,1,0} %slice.14, s32[200,200,3]{2,1,0} %broadcast.123), direction=NE, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level2/NotEqual"}
  %constant_143_clone_1 = s32[] constant(1), metadata={op_type="Equal" op_name="tower0/rpn_losses/level5/Equal"}
  %broadcast.122.clone.1 = s32[200,200,3]{2,1,0} broadcast(s32[] %constant_143_clone_1), dimensions={}, metadata={op_type="Equal" op_name="tower0/rpn_losses/level2/Equal"}
  %compare.55.clone.1 = pred[200,200,3]{2,1,0} compare(s32[200,200,3]{2,1,0} %slice.14, s32[200,200,3]{2,1,0} %broadcast.122.clone.1), direction=EQ, metadata={op_type="Equal" op_name="tower0/rpn_losses/level2/Equal"}
  ROOT %tuple.78 = (pred[200,200,3]{2,1,0}, pred[200,200,3]{2,1,0}) tuple(pred[200,200,3]{2,1,0} %compare.56, pred[200,200,3]{2,1,0} %compare.55.clone.1)
}

%fused_computation.16 (param_0.41: f32[1,12,100,100], param_1.41: f32[12]) -> f32[1,100,100,12] {
  %param_0.41 = f32[1,12,100,100]{3,2,1,0} parameter(0)
  %param_1.41 = f32[12]{0} parameter(1)
  %broadcast.124 = f32[1,12,100,100]{3,2,1,0} broadcast(f32[12]{0} %param_1.41), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_1/box/BiasAdd"}
  %add.63 = f32[1,12,100,100]{3,2,1,0} add(f32[1,12,100,100]{3,2,1,0} %param_0.41, f32[1,12,100,100]{3,2,1,0} %broadcast.124), metadata={op_type="BiasAdd" op_name="tower0/rpn_1/box/BiasAdd"}
  %bitcast.312 = f32[1,100,100,12]{2,1,3,0} bitcast(f32[1,12,100,100]{3,2,1,0} %add.63), metadata={op_type="Transpose" op_name="tower0/rpn_1/transpose_1"}
  ROOT %copy.186 = f32[1,100,100,12]{3,2,1,0} copy(f32[1,100,100,12]{2,1,3,0} %bitcast.312), metadata={op_name="XLA_Retvals"}
}

%fused_computation.17 (param_0.492: f32[1,3,100,100], param_1.588: f32[3]) -> f32[100,100,3] {
  %param_0.492 = f32[1,3,100,100]{3,2,1,0} parameter(0)
  %param_1.588 = f32[3]{0} parameter(1)
  %broadcast.391 = f32[1,3,100,100]{3,2,1,0} broadcast(f32[3]{0} %param_1.588), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_1/class/BiasAdd"}
  %add.226 = f32[1,3,100,100]{3,2,1,0} add(f32[1,3,100,100]{3,2,1,0} %param_0.492, f32[1,3,100,100]{3,2,1,0} %broadcast.391), metadata={op_type="BiasAdd" op_name="tower0/rpn_1/class/BiasAdd"}
  %bitcast.313 = f32[100,100,3]{1,0,2} bitcast(f32[1,3,100,100]{3,2,1,0} %add.226), metadata={op_type="Squeeze" op_name="tower0/rpn_1/Squeeze"}
  ROOT %copy.187 = f32[100,100,3]{2,1,0} copy(f32[100,100,3]{1,0,2} %bitcast.313), metadata={op_name="XLA_Retvals"}
}

%fused_computation.19 (param_0.408: s32[7500], param_1.427: f32[7500,1,2], param_2.348: f32[7500,1,2], param_3.309: f32[7500,1,2], param_4.245: f32[1,50,50,12]) -> f32[2000,4] {
  %constant_145 = f32[] constant(0.5), metadata={op_type="Mul" op_name="EMA/QueueInput/queue_size_EMA_apply/mul"}
  %broadcast.126 = f32[7500,1,2]{2,1,0} broadcast(f32[] %constant_145), dimensions={}, metadata={op_type="Mul" op_name="tower0/decode_bbox_target_2/mul"}
  %param_3.309 = f32[7500,1,2]{2,1,0} parameter(3)
  %multiply.36 = f32[7500,1,2]{2,1,0} multiply(f32[7500,1,2]{2,1,0} %broadcast.126, f32[7500,1,2]{2,1,0} %param_3.309), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_2/mul"}
  %param_4.245 = f32[1,50,50,12]{3,2,1,0} parameter(4)
  %bitcast.548 = f32[7500,2,2]{2,1,0} bitcast(f32[1,50,50,12]{3,2,1,0} %param_4.245), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target_2/Reshape"}
  %slice.7 = f32[7500,1,2]{2,1,0} slice(f32[7500,2,2]{2,1,0} %bitcast.548), slice={[0:7500], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target_2/split"}
  %param_2.348 = f32[7500,1,2]{2,1,0} parameter(2)
  %multiply.35 = f32[7500,1,2]{2,1,0} multiply(f32[7500,1,2]{2,1,0} %slice.7, f32[7500,1,2]{2,1,0} %param_2.348), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_2/mul_2"}
  %add.66 = f32[7500,1,2]{2,1,0} add(f32[7500,1,2]{2,1,0} %multiply.36, f32[7500,1,2]{2,1,0} %multiply.35), metadata={op_type="Add" op_name="tower0/decode_bbox_target_2/add_1"}
  %param_1.427 = f32[7500,1,2]{2,1,0} parameter(1)
  %multiply.34 = f32[7500,1,2]{2,1,0} multiply(f32[7500,1,2]{2,1,0} %broadcast.126, f32[7500,1,2]{2,1,0} %param_2.348), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_2/mul_1"}
  %multiply.33 = f32[7500,1,2]{2,1,0} multiply(f32[7500,1,2]{2,1,0} %param_1.427, f32[7500,1,2]{2,1,0} %multiply.34), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_2/mul_4"}
  %subtract.33 = f32[7500,1,2]{2,1,0} subtract(f32[7500,1,2]{2,1,0} %add.66, f32[7500,1,2]{2,1,0} %multiply.33), metadata={op_type="Sub" op_name="tower0/decode_bbox_target_2/sub_1"}
  %add.65 = f32[7500,1,2]{2,1,0} add(f32[7500,1,2]{2,1,0} %add.66, f32[7500,1,2]{2,1,0} %multiply.33), metadata={op_type="Add" op_name="tower0/decode_bbox_target_2/add_2"}
  %concatenate.2 = f32[7500,2,2]{2,1,0} concatenate(f32[7500,1,2]{2,1,0} %subtract.33, f32[7500,1,2]{2,1,0} %add.65), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower0/decode_bbox_target_2/concat"}
  %bitcast.314 = f32[7500,4]{1,0} bitcast(f32[7500,2,2]{2,1,0} %concatenate.2), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl4/Reshape"}
  %param_0.408 = s32[7500]{0} parameter(0)
  %slice.6 = s32[2000]{0} slice(s32[7500]{0} %param_0.408), slice={[0:2000]}, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %gather.2 = f32[2000,4]{1,0} gather(f32[7500,4]{1,0} %bitcast.314, s32[2000]{0} %slice.6), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/GatherV2"}
  %constant_175 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.276 = f32[2000,4]{1,0} broadcast(f32[] %constant_175), dimensions={}, metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Maximum"}
  %maximum.2 = f32[2000,4]{1,0} maximum(f32[2000,4]{1,0} %gather.2, f32[2000,4]{1,0} %broadcast.276), metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Maximum"}
  %constant_174 = f32[] constant(800), metadata={op_type="Cast" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Cast"}
  %broadcast.277 = f32[2000,4]{1,0} broadcast(f32[] %constant_174), dimensions={}, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Minimum"}
  ROOT %minimum.4 = f32[2000,4]{1,0} minimum(f32[2000,4]{1,0} %maximum.2, f32[2000,4]{1,0} %broadcast.277), metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Minimum"}
}

%fused_computation.20 (param_0.486: f32[1,3,50,50], param_1.579: f32[3]) -> f32[7500] {
  %param_0.486 = f32[1,3,50,50]{3,2,1,0} parameter(0)
  %param_1.579 = f32[3]{0} parameter(1)
  %broadcast.385 = f32[1,3,50,50]{3,2,1,0} broadcast(f32[3]{0} %param_1.579), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_2/class/BiasAdd"}
  %add.220 = f32[1,3,50,50]{3,2,1,0} add(f32[1,3,50,50]{3,2,1,0} %param_0.486, f32[1,3,50,50]{3,2,1,0} %broadcast.385), metadata={op_type="BiasAdd" op_name="tower0/rpn_2/class/BiasAdd"}
  %bitcast.315 = f32[1,50,50,3]{2,1,3,0} bitcast(f32[1,3,50,50]{3,2,1,0} %add.220), metadata={op_type="Transpose" op_name="tower0/rpn_2/transpose"}
  ROOT %reshape.188 = f32[7500]{0} reshape(f32[1,50,50,3]{2,1,3,0} %bitcast.315), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl4/Reshape_1"}
}

%fused_computation.21 (param_0.409: f32[1,50,50,12]) -> f32[7500,1,2] {
  %param_0.409 = f32[1,50,50,12]{3,2,1,0} parameter(0)
  %bitcast.549 = f32[7500,2,2]{2,1,0} bitcast(f32[1,50,50,12]{3,2,1,0} %param_0.409), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target_2/Reshape"}
  %slice.8 = f32[7500,1,2]{2,1,0} slice(f32[7500,2,2]{2,1,0} %bitcast.549), slice={[0:7500], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target_2/split"}
  %constant_146 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_4/Minimum"}
  %broadcast.127 = f32[7500,1,2]{2,1,0} broadcast(f32[] %constant_146), dimensions={}, metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_2/Minimum"}
  %minimum.5 = f32[7500,1,2]{2,1,0} minimum(f32[7500,1,2]{2,1,0} %slice.8, f32[7500,1,2]{2,1,0} %broadcast.127), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_2/Minimum"}
  ROOT %exponential.2 = f32[7500,1,2]{2,1,0} exponential(f32[7500,1,2]{2,1,0} %minimum.5), metadata={op_type="Exp" op_name="tower0/decode_bbox_target_2/Exp"}
}

%fused_computation.23 (param_0.411: s32[168,168,3]) -> (pred[100,100,3], pred[100,100,3]) {
  %param_0.411 = s32[168,168,3]{2,1,0} parameter(0)
  %slice.16 = s32[100,100,3]{2,1,0} slice(s32[168,168,3]{2,1,0} %param_0.411), slice={[0:100], [0:100], [0:3]}, metadata={op_type="Slice" op_name="tower0/FPN_slice_lvl1/narrow_to/Slice_1"}
  %constant_148 = s32[] constant(-1), metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level5/NotEqual"}
  %broadcast.129 = s32[100,100,3]{2,1,0} broadcast(s32[] %constant_148), dimensions={}, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level3/NotEqual"}
  %compare.58 = pred[100,100,3]{2,1,0} compare(s32[100,100,3]{2,1,0} %slice.16, s32[100,100,3]{2,1,0} %broadcast.129), direction=NE, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level3/NotEqual"}
  %constant_147_clone_1 = s32[] constant(1), metadata={op_type="Equal" op_name="tower0/rpn_losses/level5/Equal"}
  %broadcast.128.clone.1 = s32[100,100,3]{2,1,0} broadcast(s32[] %constant_147_clone_1), dimensions={}, metadata={op_type="Equal" op_name="tower0/rpn_losses/level3/Equal"}
  %compare.57.clone.1 = pred[100,100,3]{2,1,0} compare(s32[100,100,3]{2,1,0} %slice.16, s32[100,100,3]{2,1,0} %broadcast.128.clone.1), direction=EQ, metadata={op_type="Equal" op_name="tower0/rpn_losses/level3/Equal"}
  ROOT %tuple.79 = (pred[100,100,3]{2,1,0}, pred[100,100,3]{2,1,0}) tuple(pred[100,100,3]{2,1,0} %compare.58, pred[100,100,3]{2,1,0} %compare.57.clone.1)
}

%fused_computation.24 (param_0.57: f32[1,12,50,50], param_1.55: f32[12]) -> f32[1,50,50,12] {
  %param_0.57 = f32[1,12,50,50]{3,2,1,0} parameter(0)
  %param_1.55 = f32[12]{0} parameter(1)
  %broadcast.130 = f32[1,12,50,50]{3,2,1,0} broadcast(f32[12]{0} %param_1.55), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_2/box/BiasAdd"}
  %add.67 = f32[1,12,50,50]{3,2,1,0} add(f32[1,12,50,50]{3,2,1,0} %param_0.57, f32[1,12,50,50]{3,2,1,0} %broadcast.130), metadata={op_type="BiasAdd" op_name="tower0/rpn_2/box/BiasAdd"}
  %bitcast.316 = f32[1,50,50,12]{2,1,3,0} bitcast(f32[1,12,50,50]{3,2,1,0} %add.67), metadata={op_type="Transpose" op_name="tower0/rpn_2/transpose_1"}
  ROOT %copy.188 = f32[1,50,50,12]{3,2,1,0} copy(f32[1,50,50,12]{2,1,3,0} %bitcast.316), metadata={op_name="XLA_Retvals"}
}

%fused_computation.25 (param_0.61: s32[507], param_1.61: f32[507,1,2], param_2.49: f32[507,1,2], param_3.310: f32[507,1,2], param_4.246: f32[1,13,13,12]) -> f32[507,4] {
  %constant_150 = f32[] constant(0.5), metadata={op_type="Mul" op_name="EMA/QueueInput/queue_size_EMA_apply/mul"}
  %broadcast.133 = f32[507,1,2]{2,1,0} broadcast(f32[] %constant_150), dimensions={}, metadata={op_type="Mul" op_name="tower0/decode_bbox_target_4/mul"}
  %param_3.310 = f32[507,1,2]{2,1,0} parameter(3)
  %multiply.40 = f32[507,1,2]{2,1,0} multiply(f32[507,1,2]{2,1,0} %broadcast.133, f32[507,1,2]{2,1,0} %param_3.310), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_4/mul"}
  %param_4.246 = f32[1,13,13,12]{3,2,1,0} parameter(4)
  %bitcast.550 = f32[507,2,2]{2,1,0} bitcast(f32[1,13,13,12]{3,2,1,0} %param_4.246), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target_4/Reshape"}
  %slice.9 = f32[507,1,2]{2,1,0} slice(f32[507,2,2]{2,1,0} %bitcast.550), slice={[0:507], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target_4/split"}
  %param_2.49 = f32[507,1,2]{2,1,0} parameter(2)
  %multiply.39 = f32[507,1,2]{2,1,0} multiply(f32[507,1,2]{2,1,0} %slice.9, f32[507,1,2]{2,1,0} %param_2.49), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_4/mul_2"}
  %add.69 = f32[507,1,2]{2,1,0} add(f32[507,1,2]{2,1,0} %multiply.40, f32[507,1,2]{2,1,0} %multiply.39), metadata={op_type="Add" op_name="tower0/decode_bbox_target_4/add_1"}
  %param_1.61 = f32[507,1,2]{2,1,0} parameter(1)
  %multiply.38 = f32[507,1,2]{2,1,0} multiply(f32[507,1,2]{2,1,0} %broadcast.133, f32[507,1,2]{2,1,0} %param_2.49), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_4/mul_1"}
  %multiply.37 = f32[507,1,2]{2,1,0} multiply(f32[507,1,2]{2,1,0} %param_1.61, f32[507,1,2]{2,1,0} %multiply.38), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_4/mul_4"}
  %subtract.34 = f32[507,1,2]{2,1,0} subtract(f32[507,1,2]{2,1,0} %add.69, f32[507,1,2]{2,1,0} %multiply.37), metadata={op_type="Sub" op_name="tower0/decode_bbox_target_4/sub_1"}
  %add.68 = f32[507,1,2]{2,1,0} add(f32[507,1,2]{2,1,0} %add.69, f32[507,1,2]{2,1,0} %multiply.37), metadata={op_type="Add" op_name="tower0/decode_bbox_target_4/add_2"}
  %concatenate.3 = f32[507,2,2]{2,1,0} concatenate(f32[507,1,2]{2,1,0} %subtract.34, f32[507,1,2]{2,1,0} %add.68), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower0/decode_bbox_target_4/concat"}
  %bitcast.317 = f32[507,4]{1,0} bitcast(f32[507,2,2]{2,1,0} %concatenate.3), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl6/Reshape"}
  %param_0.61 = s32[507]{0} parameter(0)
  %gather.3 = f32[507,4]{1,0} gather(f32[507,4]{1,0} %bitcast.317, s32[507]{0} %param_0.61), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/GatherV2"}
  %constant_151 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.132 = f32[507,4]{1,0} broadcast(f32[] %constant_151), dimensions={}, metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Maximum"}
  %maximum.3 = f32[507,4]{1,0} maximum(f32[507,4]{1,0} %gather.3, f32[507,4]{1,0} %broadcast.132), metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Maximum"}
  %constant_149 = f32[] constant(800), metadata={op_type="Cast" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Cast"}
  %broadcast.131 = f32[507,4]{1,0} broadcast(f32[] %constant_149), dimensions={}, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Minimum"}
  ROOT %minimum.6 = f32[507,4]{1,0} minimum(f32[507,4]{1,0} %maximum.3, f32[507,4]{1,0} %broadcast.131), metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Minimum"}
}

%fused_computation.26 (param_0.482: f32[1,3,13,13], param_1.573: f32[3]) -> f32[507] {
  %param_0.482 = f32[1,3,13,13]{3,2,1,0} parameter(0)
  %param_1.573 = f32[3]{0} parameter(1)
  %broadcast.381 = f32[1,3,13,13]{3,2,1,0} broadcast(f32[3]{0} %param_1.573), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_4/class/BiasAdd"}
  %add.216 = f32[1,3,13,13]{3,2,1,0} add(f32[1,3,13,13]{3,2,1,0} %param_0.482, f32[1,3,13,13]{3,2,1,0} %broadcast.381), metadata={op_type="BiasAdd" op_name="tower0/rpn_4/class/BiasAdd"}
  %bitcast.318 = f32[1,13,13,3]{2,1,3,0} bitcast(f32[1,3,13,13]{3,2,1,0} %add.216), metadata={op_type="Transpose" op_name="tower0/rpn_4/transpose"}
  ROOT %reshape.189 = f32[507]{0} reshape(f32[1,13,13,3]{2,1,3,0} %bitcast.318), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl6/Reshape_1"}
}

%fused_computation.27 (param_0.412: f32[1,13,13,12]) -> f32[507,1,2] {
  %param_0.412 = f32[1,13,13,12]{3,2,1,0} parameter(0)
  %bitcast.551 = f32[507,2,2]{2,1,0} bitcast(f32[1,13,13,12]{3,2,1,0} %param_0.412), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target_4/Reshape"}
  %slice.10 = f32[507,1,2]{2,1,0} slice(f32[507,2,2]{2,1,0} %bitcast.551), slice={[0:507], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target_4/split"}
  %constant_152 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_4/Minimum"}
  %broadcast.134 = f32[507,1,2]{2,1,0} broadcast(f32[] %constant_152), dimensions={}, metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_4/Minimum"}
  %minimum.7 = f32[507,1,2]{2,1,0} minimum(f32[507,1,2]{2,1,0} %slice.10, f32[507,1,2]{2,1,0} %broadcast.134), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_4/Minimum"}
  ROOT %exponential.3 = f32[507,1,2]{2,1,0} exponential(f32[507,1,2]{2,1,0} %minimum.7), metadata={op_type="Exp" op_name="tower0/decode_bbox_target_4/Exp"}
}

%fused_computation.28 (param_0.488: f32[1,3,50,50], param_1.582: f32[3]) -> f32[50,50,3] {
  %param_0.488 = f32[1,3,50,50]{3,2,1,0} parameter(0)
  %param_1.582 = f32[3]{0} parameter(1)
  %broadcast.387 = f32[1,3,50,50]{3,2,1,0} broadcast(f32[3]{0} %param_1.582), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_2/class/BiasAdd"}
  %add.222 = f32[1,3,50,50]{3,2,1,0} add(f32[1,3,50,50]{3,2,1,0} %param_0.488, f32[1,3,50,50]{3,2,1,0} %broadcast.387), metadata={op_type="BiasAdd" op_name="tower0/rpn_2/class/BiasAdd"}
  %bitcast.319 = f32[50,50,3]{1,0,2} bitcast(f32[1,3,50,50]{3,2,1,0} %add.222), metadata={op_type="Squeeze" op_name="tower0/rpn_2/Squeeze"}
  ROOT %copy.189 = f32[50,50,3]{2,1,0} copy(f32[50,50,3]{1,0,2} %bitcast.319), metadata={op_name="XLA_Retvals"}
}

%fused_computation.30 (param_0.74: s32[1875], param_1.73: f32[1875,1,2], param_2.55: f32[1875,1,2], param_3.311: f32[1875,1,2], param_4.247: f32[1,25,25,12]) -> f32[1875,4] {
  %constant_154 = f32[] constant(0.5), metadata={op_type="Mul" op_name="EMA/QueueInput/queue_size_EMA_apply/mul"}
  %broadcast.138 = f32[1875,1,2]{2,1,0} broadcast(f32[] %constant_154), dimensions={}, metadata={op_type="Mul" op_name="tower0/decode_bbox_target_3/mul"}
  %param_3.311 = f32[1875,1,2]{2,1,0} parameter(3)
  %multiply.44 = f32[1875,1,2]{2,1,0} multiply(f32[1875,1,2]{2,1,0} %broadcast.138, f32[1875,1,2]{2,1,0} %param_3.311), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_3/mul"}
  %param_4.247 = f32[1,25,25,12]{3,2,1,0} parameter(4)
  %bitcast.552 = f32[1875,2,2]{2,1,0} bitcast(f32[1,25,25,12]{3,2,1,0} %param_4.247), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target_3/Reshape"}
  %slice.11 = f32[1875,1,2]{2,1,0} slice(f32[1875,2,2]{2,1,0} %bitcast.552), slice={[0:1875], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target_3/split"}
  %param_2.55 = f32[1875,1,2]{2,1,0} parameter(2)
  %multiply.43 = f32[1875,1,2]{2,1,0} multiply(f32[1875,1,2]{2,1,0} %slice.11, f32[1875,1,2]{2,1,0} %param_2.55), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_3/mul_2"}
  %add.72 = f32[1875,1,2]{2,1,0} add(f32[1875,1,2]{2,1,0} %multiply.44, f32[1875,1,2]{2,1,0} %multiply.43), metadata={op_type="Add" op_name="tower0/decode_bbox_target_3/add_1"}
  %param_1.73 = f32[1875,1,2]{2,1,0} parameter(1)
  %multiply.42 = f32[1875,1,2]{2,1,0} multiply(f32[1875,1,2]{2,1,0} %broadcast.138, f32[1875,1,2]{2,1,0} %param_2.55), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_3/mul_1"}
  %multiply.41 = f32[1875,1,2]{2,1,0} multiply(f32[1875,1,2]{2,1,0} %param_1.73, f32[1875,1,2]{2,1,0} %multiply.42), metadata={op_type="Mul" op_name="tower0/decode_bbox_target_3/mul_4"}
  %subtract.35 = f32[1875,1,2]{2,1,0} subtract(f32[1875,1,2]{2,1,0} %add.72, f32[1875,1,2]{2,1,0} %multiply.41), metadata={op_type="Sub" op_name="tower0/decode_bbox_target_3/sub_1"}
  %add.71 = f32[1875,1,2]{2,1,0} add(f32[1875,1,2]{2,1,0} %add.72, f32[1875,1,2]{2,1,0} %multiply.41), metadata={op_type="Add" op_name="tower0/decode_bbox_target_3/add_2"}
  %concatenate.4 = f32[1875,2,2]{2,1,0} concatenate(f32[1875,1,2]{2,1,0} %subtract.35, f32[1875,1,2]{2,1,0} %add.71), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower0/decode_bbox_target_3/concat"}
  %bitcast.320 = f32[1875,4]{1,0} bitcast(f32[1875,2,2]{2,1,0} %concatenate.4), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl5/Reshape"}
  %param_0.74 = s32[1875]{0} parameter(0)
  %gather.4 = f32[1875,4]{1,0} gather(f32[1875,4]{1,0} %bitcast.320, s32[1875]{0} %param_0.74), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/GatherV2"}
  %constant_155 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.137 = f32[1875,4]{1,0} broadcast(f32[] %constant_155), dimensions={}, metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Maximum"}
  %maximum.4 = f32[1875,4]{1,0} maximum(f32[1875,4]{1,0} %gather.4, f32[1875,4]{1,0} %broadcast.137), metadata={op_type="Maximum" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Maximum"}
  %constant_153 = f32[] constant(800), metadata={op_type="Cast" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Cast"}
  %broadcast.136 = f32[1875,4]{1,0} broadcast(f32[] %constant_153), dimensions={}, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Minimum"}
  ROOT %minimum.8 = f32[1875,4]{1,0} minimum(f32[1875,4]{1,0} %maximum.4, f32[1875,4]{1,0} %broadcast.136), metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Minimum"}
}

%fused_computation.31 (param_0.478: f32[1,3,25,25], param_1.567: f32[3]) -> f32[1875] {
  %param_0.478 = f32[1,3,25,25]{3,2,1,0} parameter(0)
  %param_1.567 = f32[3]{0} parameter(1)
  %broadcast.377 = f32[1,3,25,25]{3,2,1,0} broadcast(f32[3]{0} %param_1.567), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_3/class/BiasAdd"}
  %add.212 = f32[1,3,25,25]{3,2,1,0} add(f32[1,3,25,25]{3,2,1,0} %param_0.478, f32[1,3,25,25]{3,2,1,0} %broadcast.377), metadata={op_type="BiasAdd" op_name="tower0/rpn_3/class/BiasAdd"}
  %bitcast.321 = f32[1,25,25,3]{2,1,3,0} bitcast(f32[1,3,25,25]{3,2,1,0} %add.212), metadata={op_type="Transpose" op_name="tower0/rpn_3/transpose"}
  ROOT %reshape.190 = f32[1875]{0} reshape(f32[1,25,25,3]{2,1,3,0} %bitcast.321), metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl5/Reshape_1"}
}

%fused_computation.32 (param_0.413: f32[1,25,25,12]) -> f32[1875,1,2] {
  %param_0.413 = f32[1,25,25,12]{3,2,1,0} parameter(0)
  %bitcast.553 = f32[1875,2,2]{2,1,0} bitcast(f32[1,25,25,12]{3,2,1,0} %param_0.413), metadata={op_type="Reshape" op_name="tower0/decode_bbox_target_3/Reshape"}
  %slice.12 = f32[1875,1,2]{2,1,0} slice(f32[1875,2,2]{2,1,0} %bitcast.553), slice={[0:1875], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower0/decode_bbox_target_3/split"}
  %constant_156 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_4/Minimum"}
  %broadcast.139 = f32[1875,1,2]{2,1,0} broadcast(f32[] %constant_156), dimensions={}, metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_3/Minimum"}
  %minimum.9 = f32[1875,1,2]{2,1,0} minimum(f32[1875,1,2]{2,1,0} %slice.12, f32[1875,1,2]{2,1,0} %broadcast.139), metadata={op_type="Minimum" op_name="tower0/decode_bbox_target_3/Minimum"}
  ROOT %exponential.4 = f32[1875,1,2]{2,1,0} exponential(f32[1875,1,2]{2,1,0} %minimum.9), metadata={op_type="Exp" op_name="tower0/decode_bbox_target_3/Exp"}
}

%fused_computation.34 (param_0.415: s32[84,84,3]) -> (pred[50,50,3], pred[50,50,3]) {
  %param_0.415 = s32[84,84,3]{2,1,0} parameter(0)
  %slice.18 = s32[50,50,3]{2,1,0} slice(s32[84,84,3]{2,1,0} %param_0.415), slice={[0:50], [0:50], [0:3]}, metadata={op_type="Slice" op_name="tower0/FPN_slice_lvl2/narrow_to/Slice_1"}
  %constant_158 = s32[] constant(-1), metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level5/NotEqual"}
  %broadcast.141 = s32[50,50,3]{2,1,0} broadcast(s32[] %constant_158), dimensions={}, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level4/NotEqual"}
  %compare.60 = pred[50,50,3]{2,1,0} compare(s32[50,50,3]{2,1,0} %slice.18, s32[50,50,3]{2,1,0} %broadcast.141), direction=NE, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level4/NotEqual"}
  %constant_157_clone_1 = s32[] constant(1), metadata={op_type="Equal" op_name="tower0/rpn_losses/level5/Equal"}
  %broadcast.140.clone.1 = s32[50,50,3]{2,1,0} broadcast(s32[] %constant_157_clone_1), dimensions={}, metadata={op_type="Equal" op_name="tower0/rpn_losses/level4/Equal"}
  %compare.59.clone.1 = pred[50,50,3]{2,1,0} compare(s32[50,50,3]{2,1,0} %slice.18, s32[50,50,3]{2,1,0} %broadcast.140.clone.1), direction=EQ, metadata={op_type="Equal" op_name="tower0/rpn_losses/level4/Equal"}
  ROOT %tuple.80 = (pred[50,50,3]{2,1,0}, pred[50,50,3]{2,1,0}) tuple(pred[50,50,3]{2,1,0} %compare.60, pred[50,50,3]{2,1,0} %compare.59.clone.1)
}

%fused_computation.35 (param_0.84: f32[1,12,13,13], param_1.81: f32[12]) -> f32[1,13,13,12] {
  %param_0.84 = f32[1,12,13,13]{3,2,1,0} parameter(0)
  %param_1.81 = f32[12]{0} parameter(1)
  %broadcast.142 = f32[1,12,13,13]{3,2,1,0} broadcast(f32[12]{0} %param_1.81), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_4/box/BiasAdd"}
  %add.73 = f32[1,12,13,13]{3,2,1,0} add(f32[1,12,13,13]{3,2,1,0} %param_0.84, f32[1,12,13,13]{3,2,1,0} %broadcast.142), metadata={op_type="BiasAdd" op_name="tower0/rpn_4/box/BiasAdd"}
  %bitcast.322 = f32[1,13,13,12]{2,1,3,0} bitcast(f32[1,12,13,13]{3,2,1,0} %add.73), metadata={op_type="Transpose" op_name="tower0/rpn_4/transpose_1"}
  ROOT %copy.190 = f32[1,13,13,12]{3,2,1,0} copy(f32[1,13,13,12]{2,1,3,0} %bitcast.322), metadata={op_name="XLA_Retvals"}
}

%fused_computation.36 (param_0.87: f32[1,12,25,25], param_1.83: f32[12]) -> f32[1,25,25,12] {
  %param_0.87 = f32[1,12,25,25]{3,2,1,0} parameter(0)
  %param_1.83 = f32[12]{0} parameter(1)
  %broadcast.143 = f32[1,12,25,25]{3,2,1,0} broadcast(f32[12]{0} %param_1.83), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_3/box/BiasAdd"}
  %add.74 = f32[1,12,25,25]{3,2,1,0} add(f32[1,12,25,25]{3,2,1,0} %param_0.87, f32[1,12,25,25]{3,2,1,0} %broadcast.143), metadata={op_type="BiasAdd" op_name="tower0/rpn_3/box/BiasAdd"}
  %bitcast.323 = f32[1,25,25,12]{2,1,3,0} bitcast(f32[1,12,25,25]{3,2,1,0} %add.74), metadata={op_type="Transpose" op_name="tower0/rpn_3/transpose_1"}
  ROOT %copy.191 = f32[1,25,25,12]{3,2,1,0} copy(f32[1,25,25,12]{2,1,3,0} %bitcast.323), metadata={op_name="XLA_Retvals"}
}

%fused_computation.37 (param_0.484: f32[1,3,13,13], param_1.576: f32[3]) -> f32[13,13,3] {
  %param_0.484 = f32[1,3,13,13]{3,2,1,0} parameter(0)
  %param_1.576 = f32[3]{0} parameter(1)
  %broadcast.383 = f32[1,3,13,13]{3,2,1,0} broadcast(f32[3]{0} %param_1.576), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_4/class/BiasAdd"}
  %add.218 = f32[1,3,13,13]{3,2,1,0} add(f32[1,3,13,13]{3,2,1,0} %param_0.484, f32[1,3,13,13]{3,2,1,0} %broadcast.383), metadata={op_type="BiasAdd" op_name="tower0/rpn_4/class/BiasAdd"}
  %bitcast.324 = f32[13,13,3]{1,0,2} bitcast(f32[1,3,13,13]{3,2,1,0} %add.218), metadata={op_type="Squeeze" op_name="tower0/rpn_4/Squeeze"}
  ROOT %copy.192 = f32[13,13,3]{2,1,0} copy(f32[13,13,3]{1,0,2} %bitcast.324), metadata={op_name="XLA_Retvals"}
}

%fused_computation.39 (param_0.480: f32[1,3,25,25], param_1.570: f32[3]) -> f32[25,25,3] {
  %param_0.480 = f32[1,3,25,25]{3,2,1,0} parameter(0)
  %param_1.570 = f32[3]{0} parameter(1)
  %broadcast.379 = f32[1,3,25,25]{3,2,1,0} broadcast(f32[3]{0} %param_1.570), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/rpn_3/class/BiasAdd"}
  %add.214 = f32[1,3,25,25]{3,2,1,0} add(f32[1,3,25,25]{3,2,1,0} %param_0.480, f32[1,3,25,25]{3,2,1,0} %broadcast.379), metadata={op_type="BiasAdd" op_name="tower0/rpn_3/class/BiasAdd"}
  %bitcast.325 = f32[25,25,3]{1,0,2} bitcast(f32[1,3,25,25]{3,2,1,0} %add.214), metadata={op_type="Squeeze" op_name="tower0/rpn_3/Squeeze"}
  ROOT %copy.193 = f32[25,25,3]{2,1,0} copy(f32[25,25,3]{1,0,2} %bitcast.325), metadata={op_name="XLA_Retvals"}
}

%fused_computation.42 (param_0.417: s32[21,21,3]) -> (pred[13,13,3], pred[13,13,3]) {
  %param_0.417 = s32[21,21,3]{2,1,0} parameter(0)
  %slice.20 = s32[13,13,3]{2,1,0} slice(s32[21,21,3]{2,1,0} %param_0.417), slice={[0:13], [0:13], [0:3]}, metadata={op_type="Slice" op_name="tower0/FPN_slice_lvl4/narrow_to/Slice_1"}
  %constant_160 = s32[] constant(-1), metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level5/NotEqual"}
  %broadcast.147 = s32[13,13,3]{2,1,0} broadcast(s32[] %constant_160), dimensions={}, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level6/NotEqual"}
  %compare.62 = pred[13,13,3]{2,1,0} compare(s32[13,13,3]{2,1,0} %slice.20, s32[13,13,3]{2,1,0} %broadcast.147), direction=NE, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level6/NotEqual"}
  %constant_159_clone_1 = s32[] constant(1), metadata={op_type="Equal" op_name="tower0/rpn_losses/level5/Equal"}
  %broadcast.146.clone.1 = s32[13,13,3]{2,1,0} broadcast(s32[] %constant_159_clone_1), dimensions={}, metadata={op_type="Equal" op_name="tower0/rpn_losses/level6/Equal"}
  %compare.61.clone.1 = pred[13,13,3]{2,1,0} compare(s32[13,13,3]{2,1,0} %slice.20, s32[13,13,3]{2,1,0} %broadcast.146.clone.1), direction=EQ, metadata={op_type="Equal" op_name="tower0/rpn_losses/level6/Equal"}
  ROOT %tuple.82 = (pred[13,13,3]{2,1,0}, pred[13,13,3]{2,1,0}) tuple(pred[13,13,3]{2,1,0} %compare.62, pred[13,13,3]{2,1,0} %compare.61.clone.1)
}

%fused_computation.44 (param_0.419: s32[42,42,3]) -> (pred[25,25,3], pred[25,25,3]) {
  %param_0.419 = s32[42,42,3]{2,1,0} parameter(0)
  %slice.22 = s32[25,25,3]{2,1,0} slice(s32[42,42,3]{2,1,0} %param_0.419), slice={[0:25], [0:25], [0:3]}, metadata={op_type="Slice" op_name="tower0/FPN_slice_lvl3/narrow_to/Slice_1"}
  %constant_162 = s32[] constant(-1), metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level5/NotEqual"}
  %broadcast.149 = s32[25,25,3]{2,1,0} broadcast(s32[] %constant_162), dimensions={}, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level5/NotEqual"}
  %compare.64 = pred[25,25,3]{2,1,0} compare(s32[25,25,3]{2,1,0} %slice.22, s32[25,25,3]{2,1,0} %broadcast.149), direction=NE, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level5/NotEqual"}
  %constant_161_clone_1 = s32[] constant(1), metadata={op_type="Equal" op_name="tower0/rpn_losses/level5/Equal"}
  %broadcast.148.clone.1 = s32[25,25,3]{2,1,0} broadcast(s32[] %constant_161_clone_1), dimensions={}, metadata={op_type="Equal" op_name="tower0/rpn_losses/level5/Equal"}
  %compare.63.clone.1 = pred[25,25,3]{2,1,0} compare(s32[25,25,3]{2,1,0} %slice.22, s32[25,25,3]{2,1,0} %broadcast.148.clone.1), direction=EQ, metadata={op_type="Equal" op_name="tower0/rpn_losses/level5/Equal"}
  ROOT %tuple.81 = (pred[25,25,3]{2,1,0}, pred[25,25,3]{2,1,0}) tuple(pred[25,25,3]{2,1,0} %compare.64, pred[25,25,3]{2,1,0} %compare.63.clone.1)
}

%fused_computation.45 (param_0.99: f32[], param_1.98: s32[]) -> f32[] {
  %constant_163 = f32[] constant(0.5), metadata={op_type="Mul" op_name="EMA/QueueInput/queue_size_EMA_apply/mul"}
  %param_0.99 = f32[] parameter(0)
  %param_1.98 = s32[] parameter(1)
  %convert.3 = f32[] convert(s32[] %param_1.98), metadata={op_type="Cast" op_name="QueueInput/queue_size"}
  %subtract.36 = f32[] subtract(f32[] %param_0.99, f32[] %convert.3), metadata={op_type="Sub" op_name="EMA/QueueInput/queue_size_EMA_apply/sub"}
  ROOT %multiply.45 = f32[] multiply(f32[] %constant_163, f32[] %subtract.36), metadata={op_type="Mul" op_name="EMA/QueueInput/queue_size_EMA_apply/mul"}
}

%fused_computation.46 (param_0.100: f32[1,256,200,200], param_1.100: f32[256]) -> f32[1,256,200,200] {
  %param_0.100 = f32[1,256,200,200]{3,2,1,0} parameter(0)
  %param_1.100 = f32[256]{0} parameter(1)
  %broadcast.150 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[256]{0} %param_1.100), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p2/BiasAdd"}
  ROOT %add.77 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %param_0.100, f32[1,256,200,200]{3,2,1,0} %broadcast.150), metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p2/BiasAdd"}
}

%fused_computation.47 (param_0.102: f32[1,256,100,100], param_1.106: f32[1,256,200,200], param_2.59: f32[256]) -> f32[1,256,200,200] {
  %param_1.106 = f32[1,256,200,200]{3,2,1,0} parameter(1)
  %param_2.59 = f32[256]{0} parameter(2)
  %broadcast.152 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[256]{0} %param_2.59), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/fpn/lateral_1x1_c2/BiasAdd"}
  %add.79 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %param_1.106, f32[1,256,200,200]{3,2,1,0} %broadcast.152), metadata={op_type="BiasAdd" op_name="tower0/fpn/lateral_1x1_c2/BiasAdd"}
  %param_0.102 = f32[1,256,100,100]{3,2,1,0} parameter(0)
  %bitcast.327 = f32[2560000]{0} bitcast(f32[1,256,100,100]{3,2,1,0} %param_0.102)
  %broadcast.151 = f32[2560000,4]{1,0} broadcast(f32[2560000]{0} %bitcast.327), dimensions={0}, metadata={op_type="MatMul" op_name="tower0/fpn/upsample_lat3/Tensordot/MatMul"}
  %reshape.191 = f32[1,256,100,100,2,2]{5,3,4,2,1,0} reshape(f32[2560000,4]{1,0} %broadcast.151), metadata={op_type="Reshape" op_name="tower0/fpn/upsample_lat3/Tensordot"}
  %bitcast.326 = f32[1,256,200,200]{3,2,1,0} bitcast(f32[1,256,100,100,2,2]{5,3,4,2,1,0} %reshape.191), metadata={op_type="Reshape" op_name="tower0/fpn/upsample_lat3/Reshape"}
  ROOT %add.78 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %add.79, f32[1,256,200,200]{3,2,1,0} %bitcast.326), metadata={op_type="Add" op_name="tower0/fpn/add_2"}
}

%fused_computation.48 (param_0.103: f32[1,256,100,100], param_1.108: f32[256]) -> f32[1,256,100,100] {
  %param_0.103 = f32[1,256,100,100]{3,2,1,0} parameter(0)
  %param_1.108 = f32[256]{0} parameter(1)
  %broadcast.153 = f32[1,256,100,100]{3,2,1,0} broadcast(f32[256]{0} %param_1.108), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p3/BiasAdd"}
  ROOT %add.80 = f32[1,256,100,100]{3,2,1,0} add(f32[1,256,100,100]{3,2,1,0} %param_0.103, f32[1,256,100,100]{3,2,1,0} %broadcast.153), metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p3/BiasAdd"}
}

%fused_computation.49 (param_0.105: f32[1,256,50,50], param_1.114: f32[1,256,100,100], param_2.61: f32[256]) -> f32[1,256,100,100] {
  %param_1.114 = f32[1,256,100,100]{3,2,1,0} parameter(1)
  %param_2.61 = f32[256]{0} parameter(2)
  %broadcast.155 = f32[1,256,100,100]{3,2,1,0} broadcast(f32[256]{0} %param_2.61), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/fpn/lateral_1x1_c3/BiasAdd"}
  %add.82 = f32[1,256,100,100]{3,2,1,0} add(f32[1,256,100,100]{3,2,1,0} %param_1.114, f32[1,256,100,100]{3,2,1,0} %broadcast.155), metadata={op_type="BiasAdd" op_name="tower0/fpn/lateral_1x1_c3/BiasAdd"}
  %param_0.105 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %bitcast.329 = f32[640000]{0} bitcast(f32[1,256,50,50]{3,2,1,0} %param_0.105)
  %broadcast.154 = f32[640000,4]{1,0} broadcast(f32[640000]{0} %bitcast.329), dimensions={0}, metadata={op_type="MatMul" op_name="tower0/fpn/upsample_lat4/Tensordot/MatMul"}
  %reshape.192 = f32[1,256,50,50,2,2]{5,3,4,2,1,0} reshape(f32[640000,4]{1,0} %broadcast.154), metadata={op_type="Reshape" op_name="tower0/fpn/upsample_lat4/Tensordot"}
  %bitcast.328 = f32[1,256,100,100]{3,2,1,0} bitcast(f32[1,256,50,50,2,2]{5,3,4,2,1,0} %reshape.192), metadata={op_type="Reshape" op_name="tower0/fpn/upsample_lat4/Reshape"}
  ROOT %add.81 = f32[1,256,100,100]{3,2,1,0} add(f32[1,256,100,100]{3,2,1,0} %add.82, f32[1,256,100,100]{3,2,1,0} %bitcast.328), metadata={op_type="Add" op_name="tower0/fpn/add_1"}
}

%fused_computation.50 (param_0.106: f32[1,256,50,50], param_1.116: f32[256]) -> f32[1,256,50,50] {
  %param_0.106 = f32[1,256,50,50]{3,2,1,0} parameter(0)
  %param_1.116 = f32[256]{0} parameter(1)
  %broadcast.156 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[256]{0} %param_1.116), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p4/BiasAdd"}
  ROOT %add.83 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %param_0.106, f32[1,256,50,50]{3,2,1,0} %broadcast.156), metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p4/BiasAdd"}
}

%fused_computation.51 (param_0.108: f32[1,256,25,25], param_1.122: f32[1,256,50,50], param_2.63: f32[256]) -> f32[1,256,50,50] {
  %param_1.122 = f32[1,256,50,50]{3,2,1,0} parameter(1)
  %param_2.63 = f32[256]{0} parameter(2)
  %broadcast.158 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[256]{0} %param_2.63), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/fpn/lateral_1x1_c4/BiasAdd"}
  %add.85 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %param_1.122, f32[1,256,50,50]{3,2,1,0} %broadcast.158), metadata={op_type="BiasAdd" op_name="tower0/fpn/lateral_1x1_c4/BiasAdd"}
  %param_0.108 = f32[1,256,25,25]{3,2,1,0} parameter(0)
  %bitcast.331 = f32[160000]{0} bitcast(f32[1,256,25,25]{3,2,1,0} %param_0.108)
  %broadcast.157 = f32[160000,4]{1,0} broadcast(f32[160000]{0} %bitcast.331), dimensions={0}, metadata={op_type="MatMul" op_name="tower0/fpn/upsample_lat5/Tensordot/MatMul"}
  %reshape.193 = f32[1,256,25,25,2,2]{5,3,4,2,1,0} reshape(f32[160000,4]{1,0} %broadcast.157), metadata={op_type="Reshape" op_name="tower0/fpn/upsample_lat5/Tensordot"}
  %bitcast.330 = f32[1,256,50,50]{3,2,1,0} bitcast(f32[1,256,25,25,2,2]{5,3,4,2,1,0} %reshape.193), metadata={op_type="Reshape" op_name="tower0/fpn/upsample_lat5/Reshape"}
  ROOT %add.84 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %add.85, f32[1,256,50,50]{3,2,1,0} %bitcast.330), metadata={op_type="Add" op_name="tower0/fpn/add"}
}

%fused_computation.52 (param_0.109: f32[1,256,25,25], param_1.124: f32[256]) -> f32[1,256,25,25] {
  %param_0.109 = f32[1,256,25,25]{3,2,1,0} parameter(0)
  %param_1.124 = f32[256]{0} parameter(1)
  %broadcast.159 = f32[1,256,25,25]{3,2,1,0} broadcast(f32[256]{0} %param_1.124), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p5/BiasAdd"}
  ROOT %add.86 = f32[1,256,25,25]{3,2,1,0} add(f32[1,256,25,25]{3,2,1,0} %param_0.109, f32[1,256,25,25]{3,2,1,0} %broadcast.159), metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p5/BiasAdd"}
}

%fused_computation.53 (param_0.110: f32[1,256,25,25], param_1.126: f32[256]) -> f32[1,256,25,25] {
  %param_0.110 = f32[1,256,25,25]{3,2,1,0} parameter(0)
  %param_1.126 = f32[256]{0} parameter(1)
  %broadcast.160 = f32[1,256,25,25]{3,2,1,0} broadcast(f32[256]{0} %param_1.126), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower0/fpn/lateral_1x1_c5/BiasAdd"}
  ROOT %add.87 = f32[1,256,25,25]{3,2,1,0} add(f32[1,256,25,25]{3,2,1,0} %param_0.110, f32[1,256,25,25]{3,2,1,0} %broadcast.160), metadata={op_type="BiasAdd" op_name="tower0/fpn/lateral_1x1_c5/BiasAdd"}
}

%fused_computation.54 (param_0.420: f32[1,2048,1,1], param_1.429: f32[2048], param_2.350: f32[1,2048,25,25], param_3.313: f32[1,2048,25,25], param_4.249: f32[2048]) -> f32[1,2048,25,25] {
  %constant_176 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.278 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[] %constant_176), dimensions={}, metadata={op_type="Relu" op_name="tower0/group3/block0/output"}
  %param_2.350 = f32[1,2048,25,25]{3,2,1,0} parameter(2)
  %param_3.313 = f32[1,2048,25,25]{3,2,1,0} parameter(3)
  %param_0.420 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  %bitcast.334 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_0.420), metadata={op_type="Mul" op_name="tower0/group3/block2/conv3/bn/batchnorm/mul_1"}
  %broadcast.162 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.334), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block2/conv3/bn/batchnorm/mul_1"}
  %multiply.47 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %param_3.313, f32[1,2048,25,25]{3,2,1,0} %broadcast.162), metadata={op_type="Mul" op_name="tower0/group3/block2/conv3/bn/batchnorm/mul_1"}
  %add.89 = f32[1,2048,25,25]{3,2,1,0} add(f32[1,2048,25,25]{3,2,1,0} %param_2.350, f32[1,2048,25,25]{3,2,1,0} %multiply.47), metadata={op_type="AddN" op_name="tower0/group3/block2/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.429 = f32[2048]{0} parameter(1)
  %bitcast.333 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_1.429), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv3/bn/Reshape_1"}
  %param_4.249 = f32[2048]{0} parameter(4)
  %bitcast.554 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_4.249), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv3/bn/Reshape_2"}
  %multiply.46 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.554, f32[1,2048,1,1]{3,2,1,0} %param_0.420), metadata={op_type="Mul" op_name="tower0/group3/block2/conv3/bn/batchnorm/mul_2"}
  %subtract.37 = f32[1,2048,1,1]{3,2,1,0} subtract(f32[1,2048,1,1]{3,2,1,0} %bitcast.333, f32[1,2048,1,1]{3,2,1,0} %multiply.46), metadata={op_type="Sub" op_name="tower0/group3/block2/conv3/bn/batchnorm/sub"}
  %bitcast.332 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %subtract.37), metadata={op_type="Add" op_name="tower0/group3/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.161 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.332), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.88 = f32[1,2048,25,25]{3,2,1,0} add(f32[1,2048,25,25]{3,2,1,0} %add.89, f32[1,2048,25,25]{3,2,1,0} %broadcast.161), metadata={op_type="Add" op_name="tower0/group3/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.5 = f32[1,2048,25,25]{3,2,1,0} maximum(f32[1,2048,25,25]{3,2,1,0} %broadcast.278, f32[1,2048,25,25]{3,2,1,0} %add.88), metadata={op_type="Relu" op_name="tower0/group3/block2/output"}
}

%fused_computation.55 (param_0.113: f32[1,2048,1,1], param_1.131: f32[2048]) -> f32[1,2048,1,1] {
  %param_1.131 = f32[2048]{0} parameter(1)
  %bitcast.335 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_1.131), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv3/bn/Reshape"}
  %param_0.113 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.48 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.335, f32[1,2048,1,1]{3,2,1,0} %param_0.113), metadata={op_type="Mul" op_name="tower0/group3/block2/conv3/bn/batchnorm/mul"}
}

%fused_computation.56 (param_0.421: f32[2048]) -> f32[1,2048,1,1] {
  %constant_177 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.279 = f32[2048]{0} broadcast(f32[] %constant_177), dimensions={}, metadata={op_type="Add" op_name="tower0/group3/block0/conv3/bn/batchnorm/add"}
  %param_0.421 = f32[2048]{0} parameter(0)
  %add.90 = f32[2048]{0} add(f32[2048]{0} %broadcast.279, f32[2048]{0} %param_0.421), metadata={op_type="Add" op_name="tower0/group3/block2/conv3/bn/batchnorm/add"}
  %rsqrt.53 = f32[2048]{0} rsqrt(f32[2048]{0} %add.90), metadata={op_type="Rsqrt" op_name="tower0/group3/block2/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.336 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %rsqrt.53), metadata={op_type="Rsqrt" op_name="tower0/group3/block2/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.57 (param_0.422: f32[1,512,1,1], param_1.432: f32[512], param_2.352: f32[1,512,25,25], param_3.315: f32[512]) -> f32[1,512,25,25] {
  %constant_178 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.280 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_178), dimensions={}, metadata={op_type="Relu" op_name="tower0/group3/block0/conv2/Relu"}
  %param_2.352 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %param_0.422 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.339 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.422), metadata={op_type="Mul" op_name="tower0/group3/block2/conv2/bn/batchnorm/mul_1"}
  %broadcast.164 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.339), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block2/conv2/bn/batchnorm/mul_1"}
  %multiply.50 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_2.352, f32[1,512,25,25]{3,2,1,0} %broadcast.164), metadata={op_type="Mul" op_name="tower0/group3/block2/conv2/bn/batchnorm/mul_1"}
  %param_1.432 = f32[512]{0} parameter(1)
  %bitcast.338 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.432), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv2/bn/Reshape_1"}
  %param_3.315 = f32[512]{0} parameter(3)
  %bitcast.555 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.315), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv2/bn/Reshape_2"}
  %multiply.49 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.555, f32[1,512,1,1]{3,2,1,0} %param_0.422), metadata={op_type="Mul" op_name="tower0/group3/block2/conv2/bn/batchnorm/mul_2"}
  %subtract.38 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.338, f32[1,512,1,1]{3,2,1,0} %multiply.49), metadata={op_type="Sub" op_name="tower0/group3/block2/conv2/bn/batchnorm/sub"}
  %bitcast.337 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.38), metadata={op_type="Add" op_name="tower0/group3/block2/conv2/bn/batchnorm/add_1"}
  %broadcast.163 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.337), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block2/conv2/bn/batchnorm/add_1"}
  %add.91 = f32[1,512,25,25]{3,2,1,0} add(f32[1,512,25,25]{3,2,1,0} %multiply.50, f32[1,512,25,25]{3,2,1,0} %broadcast.163), metadata={op_type="Add" op_name="tower0/group3/block2/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.6 = f32[1,512,25,25]{3,2,1,0} maximum(f32[1,512,25,25]{3,2,1,0} %broadcast.280, f32[1,512,25,25]{3,2,1,0} %add.91), metadata={op_type="Relu" op_name="tower0/group3/block2/conv2/Relu"}
}

%fused_computation.58 (param_0.119: f32[1,512,1,1], param_1.137: f32[512]) -> f32[1,512,1,1] {
  %param_1.137 = f32[512]{0} parameter(1)
  %bitcast.340 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.137), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv2/bn/Reshape"}
  %param_0.119 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.51 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.340, f32[1,512,1,1]{3,2,1,0} %param_0.119), metadata={op_type="Mul" op_name="tower0/group3/block2/conv2/bn/batchnorm/mul"}
}

%fused_computation.59 (param_0.423: f32[512]) -> f32[1,512,1,1] {
  %constant_179 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.281 = f32[512]{0} broadcast(f32[] %constant_179), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %param_0.423 = f32[512]{0} parameter(0)
  %add.92 = f32[512]{0} add(f32[512]{0} %broadcast.281, f32[512]{0} %param_0.423), metadata={op_type="Add" op_name="tower0/group3/block2/conv2/bn/batchnorm/add"}
  %rsqrt.54 = f32[512]{0} rsqrt(f32[512]{0} %add.92), metadata={op_type="Rsqrt" op_name="tower0/group3/block2/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.341 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.54), metadata={op_type="Rsqrt" op_name="tower0/group3/block2/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.60 (param_0.424: f32[1,512,1,1], param_1.435: f32[512], param_2.354: f32[1,512,25,25], param_3.317: f32[512]) -> f32[1,512,25,25] {
  %constant_180 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.282 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_180), dimensions={}, metadata={op_type="Relu" op_name="tower0/group3/block0/conv2/Relu"}
  %param_2.354 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %param_0.424 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.344 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.424), metadata={op_type="Mul" op_name="tower0/group3/block2/conv1/bn/batchnorm/mul_1"}
  %broadcast.166 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.344), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block2/conv1/bn/batchnorm/mul_1"}
  %multiply.53 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_2.354, f32[1,512,25,25]{3,2,1,0} %broadcast.166), metadata={op_type="Mul" op_name="tower0/group3/block2/conv1/bn/batchnorm/mul_1"}
  %param_1.435 = f32[512]{0} parameter(1)
  %bitcast.343 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.435), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv1/bn/Reshape_1"}
  %param_3.317 = f32[512]{0} parameter(3)
  %bitcast.556 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.317), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv1/bn/Reshape_2"}
  %multiply.52 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.556, f32[1,512,1,1]{3,2,1,0} %param_0.424), metadata={op_type="Mul" op_name="tower0/group3/block2/conv1/bn/batchnorm/mul_2"}
  %subtract.39 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.343, f32[1,512,1,1]{3,2,1,0} %multiply.52), metadata={op_type="Sub" op_name="tower0/group3/block2/conv1/bn/batchnorm/sub"}
  %bitcast.342 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.39), metadata={op_type="Add" op_name="tower0/group3/block2/conv1/bn/batchnorm/add_1"}
  %broadcast.165 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.342), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block2/conv1/bn/batchnorm/add_1"}
  %add.93 = f32[1,512,25,25]{3,2,1,0} add(f32[1,512,25,25]{3,2,1,0} %multiply.53, f32[1,512,25,25]{3,2,1,0} %broadcast.165), metadata={op_type="Add" op_name="tower0/group3/block2/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.7 = f32[1,512,25,25]{3,2,1,0} maximum(f32[1,512,25,25]{3,2,1,0} %broadcast.282, f32[1,512,25,25]{3,2,1,0} %add.93), metadata={op_type="Relu" op_name="tower0/group3/block2/conv1/Relu"}
}

%fused_computation.61 (param_0.125: f32[1,512,1,1], param_1.143: f32[512]) -> f32[1,512,1,1] {
  %param_1.143 = f32[512]{0} parameter(1)
  %bitcast.345 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.143), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv1/bn/Reshape"}
  %param_0.125 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.54 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.345, f32[1,512,1,1]{3,2,1,0} %param_0.125), metadata={op_type="Mul" op_name="tower0/group3/block2/conv1/bn/batchnorm/mul"}
}

%fused_computation.62 (param_0.425: f32[512]) -> f32[1,512,1,1] {
  %constant_181 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.283 = f32[512]{0} broadcast(f32[] %constant_181), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %param_0.425 = f32[512]{0} parameter(0)
  %add.94 = f32[512]{0} add(f32[512]{0} %broadcast.283, f32[512]{0} %param_0.425), metadata={op_type="Add" op_name="tower0/group3/block2/conv1/bn/batchnorm/add"}
  %rsqrt.55 = f32[512]{0} rsqrt(f32[512]{0} %add.94), metadata={op_type="Rsqrt" op_name="tower0/group3/block2/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.346 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.55), metadata={op_type="Rsqrt" op_name="tower0/group3/block2/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.63 (param_0.426: f32[1,2048,1,1], param_1.438: f32[2048], param_2.356: f32[1,2048,25,25], param_3.319: f32[1,2048,25,25], param_4.255: f32[2048]) -> f32[1,2048,25,25] {
  %constant_182 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.284 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[] %constant_182), dimensions={}, metadata={op_type="Relu" op_name="tower0/group3/block0/output"}
  %param_2.356 = f32[1,2048,25,25]{3,2,1,0} parameter(2)
  %param_3.319 = f32[1,2048,25,25]{3,2,1,0} parameter(3)
  %param_0.426 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  %bitcast.349 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_0.426), metadata={op_type="Mul" op_name="tower0/group3/block1/conv3/bn/batchnorm/mul_1"}
  %broadcast.168 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.349), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block1/conv3/bn/batchnorm/mul_1"}
  %multiply.56 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %param_3.319, f32[1,2048,25,25]{3,2,1,0} %broadcast.168), metadata={op_type="Mul" op_name="tower0/group3/block1/conv3/bn/batchnorm/mul_1"}
  %add.96 = f32[1,2048,25,25]{3,2,1,0} add(f32[1,2048,25,25]{3,2,1,0} %param_2.356, f32[1,2048,25,25]{3,2,1,0} %multiply.56), metadata={op_type="AddN" op_name="tower0/group3/block1/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.438 = f32[2048]{0} parameter(1)
  %bitcast.348 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_1.438), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv3/bn/Reshape_1"}
  %param_4.255 = f32[2048]{0} parameter(4)
  %bitcast.557 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_4.255), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv3/bn/Reshape_2"}
  %multiply.55 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.557, f32[1,2048,1,1]{3,2,1,0} %param_0.426), metadata={op_type="Mul" op_name="tower0/group3/block1/conv3/bn/batchnorm/mul_2"}
  %subtract.40 = f32[1,2048,1,1]{3,2,1,0} subtract(f32[1,2048,1,1]{3,2,1,0} %bitcast.348, f32[1,2048,1,1]{3,2,1,0} %multiply.55), metadata={op_type="Sub" op_name="tower0/group3/block1/conv3/bn/batchnorm/sub"}
  %bitcast.347 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %subtract.40), metadata={op_type="Add" op_name="tower0/group3/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.167 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.347), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.95 = f32[1,2048,25,25]{3,2,1,0} add(f32[1,2048,25,25]{3,2,1,0} %add.96, f32[1,2048,25,25]{3,2,1,0} %broadcast.167), metadata={op_type="Add" op_name="tower0/group3/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.8 = f32[1,2048,25,25]{3,2,1,0} maximum(f32[1,2048,25,25]{3,2,1,0} %broadcast.284, f32[1,2048,25,25]{3,2,1,0} %add.95), metadata={op_type="Relu" op_name="tower0/group3/block1/output"}
}

%fused_computation.64 (param_0.131: f32[1,2048,1,1], param_1.149: f32[2048]) -> f32[1,2048,1,1] {
  %param_1.149 = f32[2048]{0} parameter(1)
  %bitcast.350 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_1.149), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv3/bn/Reshape"}
  %param_0.131 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.57 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.350, f32[1,2048,1,1]{3,2,1,0} %param_0.131), metadata={op_type="Mul" op_name="tower0/group3/block1/conv3/bn/batchnorm/mul"}
}

%fused_computation.65 (param_0.427: f32[2048]) -> f32[1,2048,1,1] {
  %constant_183 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.285 = f32[2048]{0} broadcast(f32[] %constant_183), dimensions={}, metadata={op_type="Add" op_name="tower0/group3/block0/conv3/bn/batchnorm/add"}
  %param_0.427 = f32[2048]{0} parameter(0)
  %add.97 = f32[2048]{0} add(f32[2048]{0} %broadcast.285, f32[2048]{0} %param_0.427), metadata={op_type="Add" op_name="tower0/group3/block1/conv3/bn/batchnorm/add"}
  %rsqrt.56 = f32[2048]{0} rsqrt(f32[2048]{0} %add.97), metadata={op_type="Rsqrt" op_name="tower0/group3/block1/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.351 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %rsqrt.56), metadata={op_type="Rsqrt" op_name="tower0/group3/block1/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.66 (param_0.428: f32[1,512,1,1], param_1.441: f32[512], param_2.358: f32[1,512,25,25], param_3.321: f32[512]) -> f32[1,512,25,25] {
  %constant_184 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.286 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_184), dimensions={}, metadata={op_type="Relu" op_name="tower0/group3/block0/conv2/Relu"}
  %param_2.358 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %param_0.428 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.354 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.428), metadata={op_type="Mul" op_name="tower0/group3/block1/conv2/bn/batchnorm/mul_1"}
  %broadcast.170 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.354), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block1/conv2/bn/batchnorm/mul_1"}
  %multiply.59 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_2.358, f32[1,512,25,25]{3,2,1,0} %broadcast.170), metadata={op_type="Mul" op_name="tower0/group3/block1/conv2/bn/batchnorm/mul_1"}
  %param_1.441 = f32[512]{0} parameter(1)
  %bitcast.353 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.441), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv2/bn/Reshape_1"}
  %param_3.321 = f32[512]{0} parameter(3)
  %bitcast.558 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.321), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv2/bn/Reshape_2"}
  %multiply.58 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.558, f32[1,512,1,1]{3,2,1,0} %param_0.428), metadata={op_type="Mul" op_name="tower0/group3/block1/conv2/bn/batchnorm/mul_2"}
  %subtract.41 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.353, f32[1,512,1,1]{3,2,1,0} %multiply.58), metadata={op_type="Sub" op_name="tower0/group3/block1/conv2/bn/batchnorm/sub"}
  %bitcast.352 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.41), metadata={op_type="Add" op_name="tower0/group3/block1/conv2/bn/batchnorm/add_1"}
  %broadcast.169 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.352), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block1/conv2/bn/batchnorm/add_1"}
  %add.98 = f32[1,512,25,25]{3,2,1,0} add(f32[1,512,25,25]{3,2,1,0} %multiply.59, f32[1,512,25,25]{3,2,1,0} %broadcast.169), metadata={op_type="Add" op_name="tower0/group3/block1/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.9 = f32[1,512,25,25]{3,2,1,0} maximum(f32[1,512,25,25]{3,2,1,0} %broadcast.286, f32[1,512,25,25]{3,2,1,0} %add.98), metadata={op_type="Relu" op_name="tower0/group3/block1/conv2/Relu"}
}

%fused_computation.67 (param_0.137: f32[1,512,1,1], param_1.155: f32[512]) -> f32[1,512,1,1] {
  %param_1.155 = f32[512]{0} parameter(1)
  %bitcast.355 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.155), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv2/bn/Reshape"}
  %param_0.137 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.60 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.355, f32[1,512,1,1]{3,2,1,0} %param_0.137), metadata={op_type="Mul" op_name="tower0/group3/block1/conv2/bn/batchnorm/mul"}
}

%fused_computation.68 (param_0.429: f32[512]) -> f32[1,512,1,1] {
  %constant_185 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.287 = f32[512]{0} broadcast(f32[] %constant_185), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %param_0.429 = f32[512]{0} parameter(0)
  %add.99 = f32[512]{0} add(f32[512]{0} %broadcast.287, f32[512]{0} %param_0.429), metadata={op_type="Add" op_name="tower0/group3/block1/conv2/bn/batchnorm/add"}
  %rsqrt.57 = f32[512]{0} rsqrt(f32[512]{0} %add.99), metadata={op_type="Rsqrt" op_name="tower0/group3/block1/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.356 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.57), metadata={op_type="Rsqrt" op_name="tower0/group3/block1/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.69 (param_0.430: f32[1,512,1,1], param_1.444: f32[512], param_2.360: f32[1,512,25,25], param_3.323: f32[512]) -> f32[1,512,25,25] {
  %constant_186 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.288 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_186), dimensions={}, metadata={op_type="Relu" op_name="tower0/group3/block0/conv2/Relu"}
  %param_2.360 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %param_0.430 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.359 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.430), metadata={op_type="Mul" op_name="tower0/group3/block1/conv1/bn/batchnorm/mul_1"}
  %broadcast.172 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.359), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block1/conv1/bn/batchnorm/mul_1"}
  %multiply.62 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_2.360, f32[1,512,25,25]{3,2,1,0} %broadcast.172), metadata={op_type="Mul" op_name="tower0/group3/block1/conv1/bn/batchnorm/mul_1"}
  %param_1.444 = f32[512]{0} parameter(1)
  %bitcast.358 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.444), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv1/bn/Reshape_1"}
  %param_3.323 = f32[512]{0} parameter(3)
  %bitcast.559 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.323), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv1/bn/Reshape_2"}
  %multiply.61 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.559, f32[1,512,1,1]{3,2,1,0} %param_0.430), metadata={op_type="Mul" op_name="tower0/group3/block1/conv1/bn/batchnorm/mul_2"}
  %subtract.42 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.358, f32[1,512,1,1]{3,2,1,0} %multiply.61), metadata={op_type="Sub" op_name="tower0/group3/block1/conv1/bn/batchnorm/sub"}
  %bitcast.357 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.42), metadata={op_type="Add" op_name="tower0/group3/block1/conv1/bn/batchnorm/add_1"}
  %broadcast.171 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.357), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block1/conv1/bn/batchnorm/add_1"}
  %add.100 = f32[1,512,25,25]{3,2,1,0} add(f32[1,512,25,25]{3,2,1,0} %multiply.62, f32[1,512,25,25]{3,2,1,0} %broadcast.171), metadata={op_type="Add" op_name="tower0/group3/block1/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.10 = f32[1,512,25,25]{3,2,1,0} maximum(f32[1,512,25,25]{3,2,1,0} %broadcast.288, f32[1,512,25,25]{3,2,1,0} %add.100), metadata={op_type="Relu" op_name="tower0/group3/block1/conv1/Relu"}
}

%fused_computation.70 (param_0.143: f32[1,512,1,1], param_1.161: f32[512]) -> f32[1,512,1,1] {
  %param_1.161 = f32[512]{0} parameter(1)
  %bitcast.360 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.161), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv1/bn/Reshape"}
  %param_0.143 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.63 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.360, f32[1,512,1,1]{3,2,1,0} %param_0.143), metadata={op_type="Mul" op_name="tower0/group3/block1/conv1/bn/batchnorm/mul"}
}

%fused_computation.71 (param_0.431: f32[512]) -> f32[1,512,1,1] {
  %constant_187 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.289 = f32[512]{0} broadcast(f32[] %constant_187), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %param_0.431 = f32[512]{0} parameter(0)
  %add.101 = f32[512]{0} add(f32[512]{0} %broadcast.289, f32[512]{0} %param_0.431), metadata={op_type="Add" op_name="tower0/group3/block1/conv1/bn/batchnorm/add"}
  %rsqrt.58 = f32[512]{0} rsqrt(f32[512]{0} %add.101), metadata={op_type="Rsqrt" op_name="tower0/group3/block1/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.361 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.58), metadata={op_type="Rsqrt" op_name="tower0/group3/block1/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.72 (param_0.432: f32[1,2048,1,1], param_1.447: f32[2048], param_2.362: f32[1,2048,25,25], param_3.325: f32[1,2048,1,1], param_4.261: f32[2048], param_5.196: f32[1,2048,25,25], param_6.119: f32[2048], param_7.59: f32[2048]) -> f32[1,2048,25,25] {
  %constant_188 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.290 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[] %constant_188), dimensions={}, metadata={op_type="Relu" op_name="tower0/group3/block0/output"}
  %param_5.196 = f32[1,2048,25,25]{3,2,1,0} parameter(5)
  %param_3.325 = f32[1,2048,1,1]{3,2,1,0} parameter(3)
  %bitcast.367 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_3.325), metadata={op_type="Mul" op_name="tower0/group3/block0/conv3/bn/batchnorm/mul_1"}
  %broadcast.176 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.367), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block0/conv3/bn/batchnorm/mul_1"}
  %multiply.67 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %param_5.196, f32[1,2048,25,25]{3,2,1,0} %broadcast.176), metadata={op_type="Mul" op_name="tower0/group3/block0/conv3/bn/batchnorm/mul_1"}
  %param_4.261 = f32[2048]{0} parameter(4)
  %bitcast.366 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_4.261), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv3/bn/Reshape_1"}
  %param_6.119 = f32[2048]{0} parameter(6)
  %bitcast.560 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_6.119), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv3/bn/Reshape_2"}
  %multiply.66 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.560, f32[1,2048,1,1]{3,2,1,0} %param_3.325), metadata={op_type="Mul" op_name="tower0/group3/block0/conv3/bn/batchnorm/mul_2"}
  %subtract.44 = f32[1,2048,1,1]{3,2,1,0} subtract(f32[1,2048,1,1]{3,2,1,0} %bitcast.366, f32[1,2048,1,1]{3,2,1,0} %multiply.66), metadata={op_type="Sub" op_name="tower0/group3/block0/conv3/bn/batchnorm/sub"}
  %bitcast.365 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %subtract.44), metadata={op_type="Add" op_name="tower0/group3/block0/conv3/bn/batchnorm/add_1"}
  %broadcast.175 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.365), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block0/conv3/bn/batchnorm/add_1"}
  %add.104 = f32[1,2048,25,25]{3,2,1,0} add(f32[1,2048,25,25]{3,2,1,0} %multiply.67, f32[1,2048,25,25]{3,2,1,0} %broadcast.175), metadata={op_type="Add" op_name="tower0/group3/block0/conv3/bn/batchnorm/add_1"}
  %param_2.362 = f32[1,2048,25,25]{3,2,1,0} parameter(2)
  %param_0.432 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  %bitcast.364 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %param_0.432), metadata={op_type="Mul" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/mul_1"}
  %broadcast.174 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.364), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/mul_1"}
  %multiply.65 = f32[1,2048,25,25]{3,2,1,0} multiply(f32[1,2048,25,25]{3,2,1,0} %param_2.362, f32[1,2048,25,25]{3,2,1,0} %broadcast.174), metadata={op_type="Mul" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/mul_1"}
  %param_1.447 = f32[2048]{0} parameter(1)
  %bitcast.363 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_1.447), metadata={op_type="Reshape" op_name="tower0/group3/block0/convshortcut/bn/Reshape_1"}
  %param_7.59 = f32[2048]{0} parameter(7)
  %bitcast.561 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_7.59), metadata={op_type="Reshape" op_name="tower0/group3/block0/convshortcut/bn/Reshape_2"}
  %multiply.64 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.561, f32[1,2048,1,1]{3,2,1,0} %param_0.432), metadata={op_type="Mul" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/mul_2"}
  %subtract.43 = f32[1,2048,1,1]{3,2,1,0} subtract(f32[1,2048,1,1]{3,2,1,0} %bitcast.363, f32[1,2048,1,1]{3,2,1,0} %multiply.64), metadata={op_type="Sub" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/sub"}
  %bitcast.362 = f32[1,2048]{1,0} bitcast(f32[1,2048,1,1]{3,2,1,0} %subtract.43), metadata={op_type="Add" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/add_1"}
  %broadcast.173 = f32[1,2048,25,25]{3,2,1,0} broadcast(f32[1,2048]{1,0} %bitcast.362), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/add_1"}
  %add.103 = f32[1,2048,25,25]{3,2,1,0} add(f32[1,2048,25,25]{3,2,1,0} %multiply.65, f32[1,2048,25,25]{3,2,1,0} %broadcast.173), metadata={op_type="Add" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/add_1"}
  %add.102 = f32[1,2048,25,25]{3,2,1,0} add(f32[1,2048,25,25]{3,2,1,0} %add.104, f32[1,2048,25,25]{3,2,1,0} %add.103), metadata={op_type="Add" op_name="tower0/group3/block0/add"}
  ROOT %maximum.11 = f32[1,2048,25,25]{3,2,1,0} maximum(f32[1,2048,25,25]{3,2,1,0} %broadcast.290, f32[1,2048,25,25]{3,2,1,0} %add.102), metadata={op_type="Relu" op_name="tower0/group3/block0/output"}
}

%fused_computation.73 (param_0.149: f32[1,2048,1,1], param_1.167: f32[2048]) -> f32[1,2048,1,1] {
  %param_1.167 = f32[2048]{0} parameter(1)
  %bitcast.368 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_1.167), metadata={op_type="Reshape" op_name="tower0/group3/block0/convshortcut/bn/Reshape"}
  %param_0.149 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.68 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.368, f32[1,2048,1,1]{3,2,1,0} %param_0.149), metadata={op_type="Mul" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/mul"}
}

%fused_computation.74 (param_0.434: f32[2048]) -> f32[1,2048,1,1] {
  %constant_190 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.292 = f32[2048]{0} broadcast(f32[] %constant_190), dimensions={}, metadata={op_type="Add" op_name="tower0/group3/block0/conv3/bn/batchnorm/add"}
  %param_0.434 = f32[2048]{0} parameter(0)
  %add.105 = f32[2048]{0} add(f32[2048]{0} %broadcast.292, f32[2048]{0} %param_0.434), metadata={op_type="Add" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/add"}
  %rsqrt.59 = f32[2048]{0} rsqrt(f32[2048]{0} %add.105), metadata={op_type="Rsqrt" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.369 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %rsqrt.59), metadata={op_type="Rsqrt" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/Rsqrt"}
}

%fused_computation.75 (param_0.154: f32[1,2048,1,1], param_1.170: f32[2048]) -> f32[1,2048,1,1] {
  %param_1.170 = f32[2048]{0} parameter(1)
  %bitcast.370 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %param_1.170), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv3/bn/Reshape"}
  %param_0.154 = f32[1,2048,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.69 = f32[1,2048,1,1]{3,2,1,0} multiply(f32[1,2048,1,1]{3,2,1,0} %bitcast.370, f32[1,2048,1,1]{3,2,1,0} %param_0.154), metadata={op_type="Mul" op_name="tower0/group3/block0/conv3/bn/batchnorm/mul"}
}

%fused_computation.76 (param_0.433: f32[2048]) -> f32[1,2048,1,1] {
  %constant_189 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.291 = f32[2048]{0} broadcast(f32[] %constant_189), dimensions={}, metadata={op_type="Add" op_name="tower0/group3/block0/conv3/bn/batchnorm/add"}
  %param_0.433 = f32[2048]{0} parameter(0)
  %add.106 = f32[2048]{0} add(f32[2048]{0} %broadcast.291, f32[2048]{0} %param_0.433), metadata={op_type="Add" op_name="tower0/group3/block0/conv3/bn/batchnorm/add"}
  %rsqrt.60 = f32[2048]{0} rsqrt(f32[2048]{0} %add.106), metadata={op_type="Rsqrt" op_name="tower0/group3/block0/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.371 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %rsqrt.60), metadata={op_type="Rsqrt" op_name="tower0/group3/block0/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.77 (param_0.435: f32[1,512,1,1], param_1.451: f32[512], param_2.364: f32[1,512,25,25], param_3.327: f32[512]) -> f32[1,512,25,25] {
  %constant_191 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.293 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[] %constant_191), dimensions={}, metadata={op_type="Relu" op_name="tower0/group3/block0/conv2/Relu"}
  %param_2.364 = f32[1,512,25,25]{3,2,1,0} parameter(2)
  %param_0.435 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.374 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.435), metadata={op_type="Mul" op_name="tower0/group3/block0/conv2/bn/batchnorm/mul_1"}
  %broadcast.178 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.374), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block0/conv2/bn/batchnorm/mul_1"}
  %multiply.71 = f32[1,512,25,25]{3,2,1,0} multiply(f32[1,512,25,25]{3,2,1,0} %param_2.364, f32[1,512,25,25]{3,2,1,0} %broadcast.178), metadata={op_type="Mul" op_name="tower0/group3/block0/conv2/bn/batchnorm/mul_1"}
  %param_1.451 = f32[512]{0} parameter(1)
  %bitcast.373 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.451), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv2/bn/Reshape_1"}
  %param_3.327 = f32[512]{0} parameter(3)
  %bitcast.562 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.327), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv2/bn/Reshape_2"}
  %multiply.70 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.562, f32[1,512,1,1]{3,2,1,0} %param_0.435), metadata={op_type="Mul" op_name="tower0/group3/block0/conv2/bn/batchnorm/mul_2"}
  %subtract.45 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.373, f32[1,512,1,1]{3,2,1,0} %multiply.70), metadata={op_type="Sub" op_name="tower0/group3/block0/conv2/bn/batchnorm/sub"}
  %bitcast.372 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.45), metadata={op_type="Add" op_name="tower0/group3/block0/conv2/bn/batchnorm/add_1"}
  %broadcast.177 = f32[1,512,25,25]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.372), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block0/conv2/bn/batchnorm/add_1"}
  %add.107 = f32[1,512,25,25]{3,2,1,0} add(f32[1,512,25,25]{3,2,1,0} %multiply.71, f32[1,512,25,25]{3,2,1,0} %broadcast.177), metadata={op_type="Add" op_name="tower0/group3/block0/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.12 = f32[1,512,25,25]{3,2,1,0} maximum(f32[1,512,25,25]{3,2,1,0} %broadcast.293, f32[1,512,25,25]{3,2,1,0} %add.107), metadata={op_type="Relu" op_name="tower0/group3/block0/conv2/Relu"}
}

%fused_computation.78 (param_0.160: f32[1,512,1,1], param_1.176: f32[512]) -> f32[1,512,1,1] {
  %param_1.176 = f32[512]{0} parameter(1)
  %bitcast.375 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.176), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv2/bn/Reshape"}
  %param_0.160 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.72 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.375, f32[1,512,1,1]{3,2,1,0} %param_0.160), metadata={op_type="Mul" op_name="tower0/group3/block0/conv2/bn/batchnorm/mul"}
}

%fused_computation.79 (param_0.437: f32[512]) -> f32[1,512,1,1] {
  %constant_193 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.295 = f32[512]{0} broadcast(f32[] %constant_193), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %param_0.437 = f32[512]{0} parameter(0)
  %add.108 = f32[512]{0} add(f32[512]{0} %broadcast.295, f32[512]{0} %param_0.437), metadata={op_type="Add" op_name="tower0/group3/block0/conv2/bn/batchnorm/add"}
  %rsqrt.61 = f32[512]{0} rsqrt(f32[512]{0} %add.108), metadata={op_type="Rsqrt" op_name="tower0/group3/block0/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.376 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.61), metadata={op_type="Rsqrt" op_name="tower0/group3/block0/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.80 (param_0.436: f32[1,512,1,1], param_1.452: f32[512], param_2.365: f32[1,512,50,50], param_3.328: f32[512]) -> f32[1,512,50,50] {
  %constant_164 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.181 = f32[1,512,50,50]{3,2,1,0} broadcast(f32[] %constant_164), dimensions={}, metadata={op_type="Relu" op_name="tower0/group3/block0/conv1/Relu"}
  %param_2.365 = f32[1,512,50,50]{3,2,1,0} parameter(2)
  %param_0.436 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.379 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.436), metadata={op_type="Mul" op_name="tower0/group3/block0/conv1/bn/batchnorm/mul_1"}
  %broadcast.180 = f32[1,512,50,50]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.379), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group3/block0/conv1/bn/batchnorm/mul_1"}
  %multiply.74 = f32[1,512,50,50]{3,2,1,0} multiply(f32[1,512,50,50]{3,2,1,0} %param_2.365, f32[1,512,50,50]{3,2,1,0} %broadcast.180), metadata={op_type="Mul" op_name="tower0/group3/block0/conv1/bn/batchnorm/mul_1"}
  %param_1.452 = f32[512]{0} parameter(1)
  %bitcast.378 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.452), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv1/bn/Reshape_1"}
  %param_3.328 = f32[512]{0} parameter(3)
  %bitcast.563 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_3.328), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv1/bn/Reshape_2"}
  %multiply.73 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.563, f32[1,512,1,1]{3,2,1,0} %param_0.436), metadata={op_type="Mul" op_name="tower0/group3/block0/conv1/bn/batchnorm/mul_2"}
  %subtract.46 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.378, f32[1,512,1,1]{3,2,1,0} %multiply.73), metadata={op_type="Sub" op_name="tower0/group3/block0/conv1/bn/batchnorm/sub"}
  %bitcast.377 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.46), metadata={op_type="Add" op_name="tower0/group3/block0/conv1/bn/batchnorm/add_1"}
  %broadcast.179 = f32[1,512,50,50]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.377), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group3/block0/conv1/bn/batchnorm/add_1"}
  %add.109 = f32[1,512,50,50]{3,2,1,0} add(f32[1,512,50,50]{3,2,1,0} %multiply.74, f32[1,512,50,50]{3,2,1,0} %broadcast.179), metadata={op_type="Add" op_name="tower0/group3/block0/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.13 = f32[1,512,50,50]{3,2,1,0} maximum(f32[1,512,50,50]{3,2,1,0} %broadcast.181, f32[1,512,50,50]{3,2,1,0} %add.109), metadata={op_type="Relu" op_name="tower0/group3/block0/conv1/Relu"}
}

%fused_computation.81 (param_0.167: f32[1,512,1,1], param_1.183: f32[512]) -> f32[1,512,1,1] {
  %param_1.183 = f32[512]{0} parameter(1)
  %bitcast.380 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.183), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv1/bn/Reshape"}
  %param_0.167 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.75 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.380, f32[1,512,1,1]{3,2,1,0} %param_0.167), metadata={op_type="Mul" op_name="tower0/group3/block0/conv1/bn/batchnorm/mul"}
}

%fused_computation.82 (param_0.170: f32[512]) -> f32[1,512,1,1] {
  %param_0.170 = f32[512]{0} parameter(0)
  %constant_192 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.294 = f32[512]{0} broadcast(f32[] %constant_192), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %add.110 = f32[512]{0} add(f32[512]{0} %param_0.170, f32[512]{0} %broadcast.294), metadata={op_type="Add" op_name="tower0/group3/block0/conv1/bn/batchnorm/add"}
  %rsqrt.62 = f32[512]{0} rsqrt(f32[512]{0} %add.110), metadata={op_type="Rsqrt" op_name="tower0/group3/block0/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.381 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.62), metadata={op_type="Rsqrt" op_name="tower0/group3/block0/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.83 (param_0.438: f32[1,1024,1,1], param_1.456: f32[1024], param_2.367: f32[1,1024,50,50], param_3.330: f32[1,1024,50,50], param_4.265: f32[1024]) -> f32[1,1024,50,50] {
  %constant_194 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.296 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_194), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/output"}
  %param_2.367 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %param_3.330 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %param_0.438 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %bitcast.384 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_0.438), metadata={op_type="Mul" op_name="tower0/group2/block5/conv3/bn/batchnorm/mul_1"}
  %broadcast.183 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.384), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block5/conv3/bn/batchnorm/mul_1"}
  %multiply.77 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_3.330, f32[1,1024,50,50]{3,2,1,0} %broadcast.183), metadata={op_type="Mul" op_name="tower0/group2/block5/conv3/bn/batchnorm/mul_1"}
  %add.112 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_2.367, f32[1,1024,50,50]{3,2,1,0} %multiply.77), metadata={op_type="AddN" op_name="tower0/group2/block5/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.456 = f32[1024]{0} parameter(1)
  %bitcast.383 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.456), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv3/bn/Reshape_1"}
  %param_4.265 = f32[1024]{0} parameter(4)
  %bitcast.564 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_4.265), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv3/bn/Reshape_2"}
  %multiply.76 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.564, f32[1,1024,1,1]{3,2,1,0} %param_0.438), metadata={op_type="Mul" op_name="tower0/group2/block5/conv3/bn/batchnorm/mul_2"}
  %subtract.47 = f32[1,1024,1,1]{3,2,1,0} subtract(f32[1,1024,1,1]{3,2,1,0} %bitcast.383, f32[1,1024,1,1]{3,2,1,0} %multiply.76), metadata={op_type="Sub" op_name="tower0/group2/block5/conv3/bn/batchnorm/sub"}
  %bitcast.382 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %subtract.47), metadata={op_type="Add" op_name="tower0/group2/block5/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.182 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.382), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block5/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.111 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %add.112, f32[1,1024,50,50]{3,2,1,0} %broadcast.182), metadata={op_type="Add" op_name="tower0/group2/block5/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.14 = f32[1,1024,50,50]{3,2,1,0} maximum(f32[1,1024,50,50]{3,2,1,0} %broadcast.296, f32[1,1024,50,50]{3,2,1,0} %add.111), metadata={op_type="Relu" op_name="tower0/group2/block5/output"}
}

%fused_computation.84 (param_0.173: f32[1,1024,1,1], param_1.189: f32[1024]) -> f32[1,1024,1,1] {
  %param_1.189 = f32[1024]{0} parameter(1)
  %bitcast.385 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.189), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv3/bn/Reshape"}
  %param_0.173 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.78 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.385, f32[1,1024,1,1]{3,2,1,0} %param_0.173), metadata={op_type="Mul" op_name="tower0/group2/block5/conv3/bn/batchnorm/mul"}
}

%fused_computation.85 (param_0.176: f32[1024]) -> f32[1,1024,1,1] {
  %param_0.176 = f32[1024]{0} parameter(0)
  %constant_195 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.297 = f32[1024]{0} broadcast(f32[] %constant_195), dimensions={}, metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add"}
  %add.113 = f32[1024]{0} add(f32[1024]{0} %param_0.176, f32[1024]{0} %broadcast.297), metadata={op_type="Add" op_name="tower0/group2/block5/conv3/bn/batchnorm/add"}
  %rsqrt.63 = f32[1024]{0} rsqrt(f32[1024]{0} %add.113), metadata={op_type="Rsqrt" op_name="tower0/group2/block5/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.386 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %rsqrt.63), metadata={op_type="Rsqrt" op_name="tower0/group2/block5/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.86 (param_0.439: f32[1,256,1,1], param_1.459: f32[256], param_2.369: f32[1,256,50,50], param_3.332: f32[256]) -> f32[1,256,50,50] {
  %constant_196 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.298 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_196), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.369 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.439 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.389 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.439), metadata={op_type="Mul" op_name="tower0/group2/block5/conv2/bn/batchnorm/mul_1"}
  %broadcast.185 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.389), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block5/conv2/bn/batchnorm/mul_1"}
  %multiply.80 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.369, f32[1,256,50,50]{3,2,1,0} %broadcast.185), metadata={op_type="Mul" op_name="tower0/group2/block5/conv2/bn/batchnorm/mul_1"}
  %param_1.459 = f32[256]{0} parameter(1)
  %bitcast.388 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.459), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv2/bn/Reshape_1"}
  %param_3.332 = f32[256]{0} parameter(3)
  %bitcast.565 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.332), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv2/bn/Reshape_2"}
  %multiply.79 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.565, f32[1,256,1,1]{3,2,1,0} %param_0.439), metadata={op_type="Mul" op_name="tower0/group2/block5/conv2/bn/batchnorm/mul_2"}
  %subtract.48 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.388, f32[1,256,1,1]{3,2,1,0} %multiply.79), metadata={op_type="Sub" op_name="tower0/group2/block5/conv2/bn/batchnorm/sub"}
  %bitcast.387 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.48), metadata={op_type="Add" op_name="tower0/group2/block5/conv2/bn/batchnorm/add_1"}
  %broadcast.184 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.387), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block5/conv2/bn/batchnorm/add_1"}
  %add.114 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.80, f32[1,256,50,50]{3,2,1,0} %broadcast.184), metadata={op_type="Add" op_name="tower0/group2/block5/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.15 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.298, f32[1,256,50,50]{3,2,1,0} %add.114), metadata={op_type="Relu" op_name="tower0/group2/block5/conv2/Relu"}
}

%fused_computation.87 (param_0.179: f32[1,256,1,1], param_1.195: f32[256]) -> f32[1,256,1,1] {
  %param_1.195 = f32[256]{0} parameter(1)
  %bitcast.390 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.195), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv2/bn/Reshape"}
  %param_0.179 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.81 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.390, f32[1,256,1,1]{3,2,1,0} %param_0.179), metadata={op_type="Mul" op_name="tower0/group2/block5/conv2/bn/batchnorm/mul"}
}

%fused_computation.88 (param_0.182: f32[256]) -> f32[1,256,1,1] {
  %param_0.182 = f32[256]{0} parameter(0)
  %constant_197 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.299 = f32[256]{0} broadcast(f32[] %constant_197), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.115 = f32[256]{0} add(f32[256]{0} %param_0.182, f32[256]{0} %broadcast.299), metadata={op_type="Add" op_name="tower0/group2/block5/conv2/bn/batchnorm/add"}
  %rsqrt.64 = f32[256]{0} rsqrt(f32[256]{0} %add.115), metadata={op_type="Rsqrt" op_name="tower0/group2/block5/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.391 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.64), metadata={op_type="Rsqrt" op_name="tower0/group2/block5/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.89 (param_0.440: f32[1,256,1,1], param_1.462: f32[256], param_2.371: f32[1,256,50,50], param_3.334: f32[256]) -> f32[1,256,50,50] {
  %constant_198 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.300 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_198), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.371 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.440 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.394 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.440), metadata={op_type="Mul" op_name="tower0/group2/block5/conv1/bn/batchnorm/mul_1"}
  %broadcast.187 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.394), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block5/conv1/bn/batchnorm/mul_1"}
  %multiply.83 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.371, f32[1,256,50,50]{3,2,1,0} %broadcast.187), metadata={op_type="Mul" op_name="tower0/group2/block5/conv1/bn/batchnorm/mul_1"}
  %param_1.462 = f32[256]{0} parameter(1)
  %bitcast.393 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.462), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv1/bn/Reshape_1"}
  %param_3.334 = f32[256]{0} parameter(3)
  %bitcast.566 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.334), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv1/bn/Reshape_2"}
  %multiply.82 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.566, f32[1,256,1,1]{3,2,1,0} %param_0.440), metadata={op_type="Mul" op_name="tower0/group2/block5/conv1/bn/batchnorm/mul_2"}
  %subtract.49 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.393, f32[1,256,1,1]{3,2,1,0} %multiply.82), metadata={op_type="Sub" op_name="tower0/group2/block5/conv1/bn/batchnorm/sub"}
  %bitcast.392 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.49), metadata={op_type="Add" op_name="tower0/group2/block5/conv1/bn/batchnorm/add_1"}
  %broadcast.186 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.392), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block5/conv1/bn/batchnorm/add_1"}
  %add.116 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.83, f32[1,256,50,50]{3,2,1,0} %broadcast.186), metadata={op_type="Add" op_name="tower0/group2/block5/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.16 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.300, f32[1,256,50,50]{3,2,1,0} %add.116), metadata={op_type="Relu" op_name="tower0/group2/block5/conv1/Relu"}
}

%fused_computation.90 (param_0.185: f32[1,256,1,1], param_1.201: f32[256]) -> f32[1,256,1,1] {
  %param_1.201 = f32[256]{0} parameter(1)
  %bitcast.395 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.201), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv1/bn/Reshape"}
  %param_0.185 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.84 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.395, f32[1,256,1,1]{3,2,1,0} %param_0.185), metadata={op_type="Mul" op_name="tower0/group2/block5/conv1/bn/batchnorm/mul"}
}

%fused_computation.91 (param_0.188: f32[256]) -> f32[1,256,1,1] {
  %param_0.188 = f32[256]{0} parameter(0)
  %constant_199 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.301 = f32[256]{0} broadcast(f32[] %constant_199), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.117 = f32[256]{0} add(f32[256]{0} %param_0.188, f32[256]{0} %broadcast.301), metadata={op_type="Add" op_name="tower0/group2/block5/conv1/bn/batchnorm/add"}
  %rsqrt.65 = f32[256]{0} rsqrt(f32[256]{0} %add.117), metadata={op_type="Rsqrt" op_name="tower0/group2/block5/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.396 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.65), metadata={op_type="Rsqrt" op_name="tower0/group2/block5/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.92 (param_0.441: f32[1,1024,1,1], param_1.465: f32[1024], param_2.373: f32[1,1024,50,50], param_3.336: f32[1,1024,50,50], param_4.271: f32[1024]) -> f32[1,1024,50,50] {
  %constant_200 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.302 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_200), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/output"}
  %param_2.373 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %param_3.336 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %param_0.441 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %bitcast.399 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_0.441), metadata={op_type="Mul" op_name="tower0/group2/block4/conv3/bn/batchnorm/mul_1"}
  %broadcast.189 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.399), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block4/conv3/bn/batchnorm/mul_1"}
  %multiply.86 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_3.336, f32[1,1024,50,50]{3,2,1,0} %broadcast.189), metadata={op_type="Mul" op_name="tower0/group2/block4/conv3/bn/batchnorm/mul_1"}
  %add.119 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_2.373, f32[1,1024,50,50]{3,2,1,0} %multiply.86), metadata={op_type="AddN" op_name="tower0/group2/block4/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.465 = f32[1024]{0} parameter(1)
  %bitcast.398 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.465), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv3/bn/Reshape_1"}
  %param_4.271 = f32[1024]{0} parameter(4)
  %bitcast.567 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_4.271), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv3/bn/Reshape_2"}
  %multiply.85 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.567, f32[1,1024,1,1]{3,2,1,0} %param_0.441), metadata={op_type="Mul" op_name="tower0/group2/block4/conv3/bn/batchnorm/mul_2"}
  %subtract.50 = f32[1,1024,1,1]{3,2,1,0} subtract(f32[1,1024,1,1]{3,2,1,0} %bitcast.398, f32[1,1024,1,1]{3,2,1,0} %multiply.85), metadata={op_type="Sub" op_name="tower0/group2/block4/conv3/bn/batchnorm/sub"}
  %bitcast.397 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %subtract.50), metadata={op_type="Add" op_name="tower0/group2/block4/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.188 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.397), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block4/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.118 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %add.119, f32[1,1024,50,50]{3,2,1,0} %broadcast.188), metadata={op_type="Add" op_name="tower0/group2/block4/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.17 = f32[1,1024,50,50]{3,2,1,0} maximum(f32[1,1024,50,50]{3,2,1,0} %broadcast.302, f32[1,1024,50,50]{3,2,1,0} %add.118), metadata={op_type="Relu" op_name="tower0/group2/block4/output"}
}

%fused_computation.93 (param_0.191: f32[1,1024,1,1], param_1.207: f32[1024]) -> f32[1,1024,1,1] {
  %param_1.207 = f32[1024]{0} parameter(1)
  %bitcast.400 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.207), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv3/bn/Reshape"}
  %param_0.191 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.87 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.400, f32[1,1024,1,1]{3,2,1,0} %param_0.191), metadata={op_type="Mul" op_name="tower0/group2/block4/conv3/bn/batchnorm/mul"}
}

%fused_computation.94 (param_0.194: f32[1024]) -> f32[1,1024,1,1] {
  %param_0.194 = f32[1024]{0} parameter(0)
  %constant_201 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.303 = f32[1024]{0} broadcast(f32[] %constant_201), dimensions={}, metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add"}
  %add.120 = f32[1024]{0} add(f32[1024]{0} %param_0.194, f32[1024]{0} %broadcast.303), metadata={op_type="Add" op_name="tower0/group2/block4/conv3/bn/batchnorm/add"}
  %rsqrt.66 = f32[1024]{0} rsqrt(f32[1024]{0} %add.120), metadata={op_type="Rsqrt" op_name="tower0/group2/block4/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.401 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %rsqrt.66), metadata={op_type="Rsqrt" op_name="tower0/group2/block4/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.95 (param_0.442: f32[1,256,1,1], param_1.468: f32[256], param_2.375: f32[1,256,50,50], param_3.338: f32[256]) -> f32[1,256,50,50] {
  %constant_202 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.304 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_202), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.375 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.442 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.404 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.442), metadata={op_type="Mul" op_name="tower0/group2/block4/conv2/bn/batchnorm/mul_1"}
  %broadcast.191 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.404), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block4/conv2/bn/batchnorm/mul_1"}
  %multiply.89 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.375, f32[1,256,50,50]{3,2,1,0} %broadcast.191), metadata={op_type="Mul" op_name="tower0/group2/block4/conv2/bn/batchnorm/mul_1"}
  %param_1.468 = f32[256]{0} parameter(1)
  %bitcast.403 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.468), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv2/bn/Reshape_1"}
  %param_3.338 = f32[256]{0} parameter(3)
  %bitcast.568 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.338), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv2/bn/Reshape_2"}
  %multiply.88 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.568, f32[1,256,1,1]{3,2,1,0} %param_0.442), metadata={op_type="Mul" op_name="tower0/group2/block4/conv2/bn/batchnorm/mul_2"}
  %subtract.51 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.403, f32[1,256,1,1]{3,2,1,0} %multiply.88), metadata={op_type="Sub" op_name="tower0/group2/block4/conv2/bn/batchnorm/sub"}
  %bitcast.402 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.51), metadata={op_type="Add" op_name="tower0/group2/block4/conv2/bn/batchnorm/add_1"}
  %broadcast.190 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.402), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block4/conv2/bn/batchnorm/add_1"}
  %add.121 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.89, f32[1,256,50,50]{3,2,1,0} %broadcast.190), metadata={op_type="Add" op_name="tower0/group2/block4/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.18 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.304, f32[1,256,50,50]{3,2,1,0} %add.121), metadata={op_type="Relu" op_name="tower0/group2/block4/conv2/Relu"}
}

%fused_computation.96 (param_0.197: f32[1,256,1,1], param_1.213: f32[256]) -> f32[1,256,1,1] {
  %param_1.213 = f32[256]{0} parameter(1)
  %bitcast.405 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.213), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv2/bn/Reshape"}
  %param_0.197 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.90 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.405, f32[1,256,1,1]{3,2,1,0} %param_0.197), metadata={op_type="Mul" op_name="tower0/group2/block4/conv2/bn/batchnorm/mul"}
}

%fused_computation.97 (param_0.200: f32[256]) -> f32[1,256,1,1] {
  %param_0.200 = f32[256]{0} parameter(0)
  %constant_203 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.305 = f32[256]{0} broadcast(f32[] %constant_203), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.122 = f32[256]{0} add(f32[256]{0} %param_0.200, f32[256]{0} %broadcast.305), metadata={op_type="Add" op_name="tower0/group2/block4/conv2/bn/batchnorm/add"}
  %rsqrt.67 = f32[256]{0} rsqrt(f32[256]{0} %add.122), metadata={op_type="Rsqrt" op_name="tower0/group2/block4/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.406 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.67), metadata={op_type="Rsqrt" op_name="tower0/group2/block4/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.98 (param_0.443: f32[1,256,1,1], param_1.471: f32[256], param_2.377: f32[1,256,50,50], param_3.340: f32[256]) -> f32[1,256,50,50] {
  %constant_204 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.306 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_204), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.377 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.443 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.409 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.443), metadata={op_type="Mul" op_name="tower0/group2/block4/conv1/bn/batchnorm/mul_1"}
  %broadcast.193 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.409), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block4/conv1/bn/batchnorm/mul_1"}
  %multiply.92 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.377, f32[1,256,50,50]{3,2,1,0} %broadcast.193), metadata={op_type="Mul" op_name="tower0/group2/block4/conv1/bn/batchnorm/mul_1"}
  %param_1.471 = f32[256]{0} parameter(1)
  %bitcast.408 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.471), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv1/bn/Reshape_1"}
  %param_3.340 = f32[256]{0} parameter(3)
  %bitcast.569 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.340), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv1/bn/Reshape_2"}
  %multiply.91 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.569, f32[1,256,1,1]{3,2,1,0} %param_0.443), metadata={op_type="Mul" op_name="tower0/group2/block4/conv1/bn/batchnorm/mul_2"}
  %subtract.52 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.408, f32[1,256,1,1]{3,2,1,0} %multiply.91), metadata={op_type="Sub" op_name="tower0/group2/block4/conv1/bn/batchnorm/sub"}
  %bitcast.407 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.52), metadata={op_type="Add" op_name="tower0/group2/block4/conv1/bn/batchnorm/add_1"}
  %broadcast.192 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.407), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block4/conv1/bn/batchnorm/add_1"}
  %add.123 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.92, f32[1,256,50,50]{3,2,1,0} %broadcast.192), metadata={op_type="Add" op_name="tower0/group2/block4/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.19 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.306, f32[1,256,50,50]{3,2,1,0} %add.123), metadata={op_type="Relu" op_name="tower0/group2/block4/conv1/Relu"}
}

%fused_computation.99 (param_0.203: f32[1,256,1,1], param_1.219: f32[256]) -> f32[1,256,1,1] {
  %param_1.219 = f32[256]{0} parameter(1)
  %bitcast.410 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.219), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv1/bn/Reshape"}
  %param_0.203 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.93 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.410, f32[1,256,1,1]{3,2,1,0} %param_0.203), metadata={op_type="Mul" op_name="tower0/group2/block4/conv1/bn/batchnorm/mul"}
}

%fused_computation.100 (param_0.206: f32[256]) -> f32[1,256,1,1] {
  %param_0.206 = f32[256]{0} parameter(0)
  %constant_205 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.307 = f32[256]{0} broadcast(f32[] %constant_205), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.124 = f32[256]{0} add(f32[256]{0} %param_0.206, f32[256]{0} %broadcast.307), metadata={op_type="Add" op_name="tower0/group2/block4/conv1/bn/batchnorm/add"}
  %rsqrt.68 = f32[256]{0} rsqrt(f32[256]{0} %add.124), metadata={op_type="Rsqrt" op_name="tower0/group2/block4/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.411 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.68), metadata={op_type="Rsqrt" op_name="tower0/group2/block4/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.101 (param_0.444: f32[1,1024,1,1], param_1.474: f32[1024], param_2.379: f32[1,1024,50,50], param_3.342: f32[1,1024,50,50], param_4.277: f32[1024]) -> f32[1,1024,50,50] {
  %constant_206 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.308 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_206), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/output"}
  %param_2.379 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %param_3.342 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %param_0.444 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %bitcast.414 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_0.444), metadata={op_type="Mul" op_name="tower0/group2/block3/conv3/bn/batchnorm/mul_1"}
  %broadcast.195 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.414), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block3/conv3/bn/batchnorm/mul_1"}
  %multiply.95 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_3.342, f32[1,1024,50,50]{3,2,1,0} %broadcast.195), metadata={op_type="Mul" op_name="tower0/group2/block3/conv3/bn/batchnorm/mul_1"}
  %add.126 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_2.379, f32[1,1024,50,50]{3,2,1,0} %multiply.95), metadata={op_type="AddN" op_name="tower0/group2/block3/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.474 = f32[1024]{0} parameter(1)
  %bitcast.413 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.474), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv3/bn/Reshape_1"}
  %param_4.277 = f32[1024]{0} parameter(4)
  %bitcast.570 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_4.277), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv3/bn/Reshape_2"}
  %multiply.94 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.570, f32[1,1024,1,1]{3,2,1,0} %param_0.444), metadata={op_type="Mul" op_name="tower0/group2/block3/conv3/bn/batchnorm/mul_2"}
  %subtract.53 = f32[1,1024,1,1]{3,2,1,0} subtract(f32[1,1024,1,1]{3,2,1,0} %bitcast.413, f32[1,1024,1,1]{3,2,1,0} %multiply.94), metadata={op_type="Sub" op_name="tower0/group2/block3/conv3/bn/batchnorm/sub"}
  %bitcast.412 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %subtract.53), metadata={op_type="Add" op_name="tower0/group2/block3/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.194 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.412), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block3/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.125 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %add.126, f32[1,1024,50,50]{3,2,1,0} %broadcast.194), metadata={op_type="Add" op_name="tower0/group2/block3/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.20 = f32[1,1024,50,50]{3,2,1,0} maximum(f32[1,1024,50,50]{3,2,1,0} %broadcast.308, f32[1,1024,50,50]{3,2,1,0} %add.125), metadata={op_type="Relu" op_name="tower0/group2/block3/output"}
}

%fused_computation.102 (param_0.209: f32[1,1024,1,1], param_1.225: f32[1024]) -> f32[1,1024,1,1] {
  %param_1.225 = f32[1024]{0} parameter(1)
  %bitcast.415 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.225), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv3/bn/Reshape"}
  %param_0.209 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.96 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.415, f32[1,1024,1,1]{3,2,1,0} %param_0.209), metadata={op_type="Mul" op_name="tower0/group2/block3/conv3/bn/batchnorm/mul"}
}

%fused_computation.103 (param_0.212: f32[1024]) -> f32[1,1024,1,1] {
  %param_0.212 = f32[1024]{0} parameter(0)
  %constant_207 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.309 = f32[1024]{0} broadcast(f32[] %constant_207), dimensions={}, metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add"}
  %add.127 = f32[1024]{0} add(f32[1024]{0} %param_0.212, f32[1024]{0} %broadcast.309), metadata={op_type="Add" op_name="tower0/group2/block3/conv3/bn/batchnorm/add"}
  %rsqrt.69 = f32[1024]{0} rsqrt(f32[1024]{0} %add.127), metadata={op_type="Rsqrt" op_name="tower0/group2/block3/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.416 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %rsqrt.69), metadata={op_type="Rsqrt" op_name="tower0/group2/block3/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.104 (param_0.445: f32[1,256,1,1], param_1.477: f32[256], param_2.381: f32[1,256,50,50], param_3.344: f32[256]) -> f32[1,256,50,50] {
  %constant_208 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.310 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_208), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.381 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.445 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.419 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.445), metadata={op_type="Mul" op_name="tower0/group2/block3/conv2/bn/batchnorm/mul_1"}
  %broadcast.197 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.419), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block3/conv2/bn/batchnorm/mul_1"}
  %multiply.98 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.381, f32[1,256,50,50]{3,2,1,0} %broadcast.197), metadata={op_type="Mul" op_name="tower0/group2/block3/conv2/bn/batchnorm/mul_1"}
  %param_1.477 = f32[256]{0} parameter(1)
  %bitcast.418 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.477), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv2/bn/Reshape_1"}
  %param_3.344 = f32[256]{0} parameter(3)
  %bitcast.571 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.344), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv2/bn/Reshape_2"}
  %multiply.97 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.571, f32[1,256,1,1]{3,2,1,0} %param_0.445), metadata={op_type="Mul" op_name="tower0/group2/block3/conv2/bn/batchnorm/mul_2"}
  %subtract.54 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.418, f32[1,256,1,1]{3,2,1,0} %multiply.97), metadata={op_type="Sub" op_name="tower0/group2/block3/conv2/bn/batchnorm/sub"}
  %bitcast.417 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.54), metadata={op_type="Add" op_name="tower0/group2/block3/conv2/bn/batchnorm/add_1"}
  %broadcast.196 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.417), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block3/conv2/bn/batchnorm/add_1"}
  %add.128 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.98, f32[1,256,50,50]{3,2,1,0} %broadcast.196), metadata={op_type="Add" op_name="tower0/group2/block3/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.21 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.310, f32[1,256,50,50]{3,2,1,0} %add.128), metadata={op_type="Relu" op_name="tower0/group2/block3/conv2/Relu"}
}

%fused_computation.105 (param_0.215: f32[1,256,1,1], param_1.231: f32[256]) -> f32[1,256,1,1] {
  %param_1.231 = f32[256]{0} parameter(1)
  %bitcast.420 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.231), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv2/bn/Reshape"}
  %param_0.215 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.99 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.420, f32[1,256,1,1]{3,2,1,0} %param_0.215), metadata={op_type="Mul" op_name="tower0/group2/block3/conv2/bn/batchnorm/mul"}
}

%fused_computation.106 (param_0.218: f32[256]) -> f32[1,256,1,1] {
  %param_0.218 = f32[256]{0} parameter(0)
  %constant_209 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.311 = f32[256]{0} broadcast(f32[] %constant_209), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.129 = f32[256]{0} add(f32[256]{0} %param_0.218, f32[256]{0} %broadcast.311), metadata={op_type="Add" op_name="tower0/group2/block3/conv2/bn/batchnorm/add"}
  %rsqrt.70 = f32[256]{0} rsqrt(f32[256]{0} %add.129), metadata={op_type="Rsqrt" op_name="tower0/group2/block3/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.421 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.70), metadata={op_type="Rsqrt" op_name="tower0/group2/block3/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.107 (param_0.446: f32[1,256,1,1], param_1.480: f32[256], param_2.383: f32[1,256,50,50], param_3.346: f32[256]) -> f32[1,256,50,50] {
  %constant_210 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.312 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_210), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.383 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.446 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.424 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.446), metadata={op_type="Mul" op_name="tower0/group2/block3/conv1/bn/batchnorm/mul_1"}
  %broadcast.199 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.424), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block3/conv1/bn/batchnorm/mul_1"}
  %multiply.101 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.383, f32[1,256,50,50]{3,2,1,0} %broadcast.199), metadata={op_type="Mul" op_name="tower0/group2/block3/conv1/bn/batchnorm/mul_1"}
  %param_1.480 = f32[256]{0} parameter(1)
  %bitcast.423 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.480), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv1/bn/Reshape_1"}
  %param_3.346 = f32[256]{0} parameter(3)
  %bitcast.572 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.346), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv1/bn/Reshape_2"}
  %multiply.100 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.572, f32[1,256,1,1]{3,2,1,0} %param_0.446), metadata={op_type="Mul" op_name="tower0/group2/block3/conv1/bn/batchnorm/mul_2"}
  %subtract.55 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.423, f32[1,256,1,1]{3,2,1,0} %multiply.100), metadata={op_type="Sub" op_name="tower0/group2/block3/conv1/bn/batchnorm/sub"}
  %bitcast.422 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.55), metadata={op_type="Add" op_name="tower0/group2/block3/conv1/bn/batchnorm/add_1"}
  %broadcast.198 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.422), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block3/conv1/bn/batchnorm/add_1"}
  %add.130 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.101, f32[1,256,50,50]{3,2,1,0} %broadcast.198), metadata={op_type="Add" op_name="tower0/group2/block3/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.22 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.312, f32[1,256,50,50]{3,2,1,0} %add.130), metadata={op_type="Relu" op_name="tower0/group2/block3/conv1/Relu"}
}

%fused_computation.108 (param_0.221: f32[1,256,1,1], param_1.237: f32[256]) -> f32[1,256,1,1] {
  %param_1.237 = f32[256]{0} parameter(1)
  %bitcast.425 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.237), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv1/bn/Reshape"}
  %param_0.221 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.102 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.425, f32[1,256,1,1]{3,2,1,0} %param_0.221), metadata={op_type="Mul" op_name="tower0/group2/block3/conv1/bn/batchnorm/mul"}
}

%fused_computation.109 (param_0.224: f32[256]) -> f32[1,256,1,1] {
  %param_0.224 = f32[256]{0} parameter(0)
  %constant_211 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.313 = f32[256]{0} broadcast(f32[] %constant_211), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.131 = f32[256]{0} add(f32[256]{0} %param_0.224, f32[256]{0} %broadcast.313), metadata={op_type="Add" op_name="tower0/group2/block3/conv1/bn/batchnorm/add"}
  %rsqrt.71 = f32[256]{0} rsqrt(f32[256]{0} %add.131), metadata={op_type="Rsqrt" op_name="tower0/group2/block3/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.426 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.71), metadata={op_type="Rsqrt" op_name="tower0/group2/block3/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.110 (param_0.447: f32[1,1024,1,1], param_1.483: f32[1024], param_2.385: f32[1,1024,50,50], param_3.348: f32[1,1024,50,50], param_4.283: f32[1024]) -> f32[1,1024,50,50] {
  %constant_212 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.314 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_212), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/output"}
  %param_2.385 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %param_3.348 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %param_0.447 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %bitcast.429 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_0.447), metadata={op_type="Mul" op_name="tower0/group2/block2/conv3/bn/batchnorm/mul_1"}
  %broadcast.201 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.429), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block2/conv3/bn/batchnorm/mul_1"}
  %multiply.104 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_3.348, f32[1,1024,50,50]{3,2,1,0} %broadcast.201), metadata={op_type="Mul" op_name="tower0/group2/block2/conv3/bn/batchnorm/mul_1"}
  %add.133 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_2.385, f32[1,1024,50,50]{3,2,1,0} %multiply.104), metadata={op_type="AddN" op_name="tower0/group2/block2/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.483 = f32[1024]{0} parameter(1)
  %bitcast.428 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.483), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv3/bn/Reshape_1"}
  %param_4.283 = f32[1024]{0} parameter(4)
  %bitcast.573 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_4.283), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv3/bn/Reshape_2"}
  %multiply.103 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.573, f32[1,1024,1,1]{3,2,1,0} %param_0.447), metadata={op_type="Mul" op_name="tower0/group2/block2/conv3/bn/batchnorm/mul_2"}
  %subtract.56 = f32[1,1024,1,1]{3,2,1,0} subtract(f32[1,1024,1,1]{3,2,1,0} %bitcast.428, f32[1,1024,1,1]{3,2,1,0} %multiply.103), metadata={op_type="Sub" op_name="tower0/group2/block2/conv3/bn/batchnorm/sub"}
  %bitcast.427 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %subtract.56), metadata={op_type="Add" op_name="tower0/group2/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.200 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.427), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.132 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %add.133, f32[1,1024,50,50]{3,2,1,0} %broadcast.200), metadata={op_type="Add" op_name="tower0/group2/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.23 = f32[1,1024,50,50]{3,2,1,0} maximum(f32[1,1024,50,50]{3,2,1,0} %broadcast.314, f32[1,1024,50,50]{3,2,1,0} %add.132), metadata={op_type="Relu" op_name="tower0/group2/block2/output"}
}

%fused_computation.111 (param_0.227: f32[1,1024,1,1], param_1.243: f32[1024]) -> f32[1,1024,1,1] {
  %param_1.243 = f32[1024]{0} parameter(1)
  %bitcast.430 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.243), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv3/bn/Reshape"}
  %param_0.227 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.105 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.430, f32[1,1024,1,1]{3,2,1,0} %param_0.227), metadata={op_type="Mul" op_name="tower0/group2/block2/conv3/bn/batchnorm/mul"}
}

%fused_computation.112 (param_0.230: f32[1024]) -> f32[1,1024,1,1] {
  %param_0.230 = f32[1024]{0} parameter(0)
  %constant_213 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.315 = f32[1024]{0} broadcast(f32[] %constant_213), dimensions={}, metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add"}
  %add.134 = f32[1024]{0} add(f32[1024]{0} %param_0.230, f32[1024]{0} %broadcast.315), metadata={op_type="Add" op_name="tower0/group2/block2/conv3/bn/batchnorm/add"}
  %rsqrt.72 = f32[1024]{0} rsqrt(f32[1024]{0} %add.134), metadata={op_type="Rsqrt" op_name="tower0/group2/block2/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.431 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %rsqrt.72), metadata={op_type="Rsqrt" op_name="tower0/group2/block2/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.113 (param_0.448: f32[1,256,1,1], param_1.486: f32[256], param_2.387: f32[1,256,50,50], param_3.350: f32[256]) -> f32[1,256,50,50] {
  %constant_214 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.316 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_214), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.387 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.448 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.434 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.448), metadata={op_type="Mul" op_name="tower0/group2/block2/conv2/bn/batchnorm/mul_1"}
  %broadcast.203 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.434), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block2/conv2/bn/batchnorm/mul_1"}
  %multiply.107 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.387, f32[1,256,50,50]{3,2,1,0} %broadcast.203), metadata={op_type="Mul" op_name="tower0/group2/block2/conv2/bn/batchnorm/mul_1"}
  %param_1.486 = f32[256]{0} parameter(1)
  %bitcast.433 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.486), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv2/bn/Reshape_1"}
  %param_3.350 = f32[256]{0} parameter(3)
  %bitcast.574 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.350), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv2/bn/Reshape_2"}
  %multiply.106 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.574, f32[1,256,1,1]{3,2,1,0} %param_0.448), metadata={op_type="Mul" op_name="tower0/group2/block2/conv2/bn/batchnorm/mul_2"}
  %subtract.57 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.433, f32[1,256,1,1]{3,2,1,0} %multiply.106), metadata={op_type="Sub" op_name="tower0/group2/block2/conv2/bn/batchnorm/sub"}
  %bitcast.432 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.57), metadata={op_type="Add" op_name="tower0/group2/block2/conv2/bn/batchnorm/add_1"}
  %broadcast.202 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.432), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block2/conv2/bn/batchnorm/add_1"}
  %add.135 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.107, f32[1,256,50,50]{3,2,1,0} %broadcast.202), metadata={op_type="Add" op_name="tower0/group2/block2/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.24 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.316, f32[1,256,50,50]{3,2,1,0} %add.135), metadata={op_type="Relu" op_name="tower0/group2/block2/conv2/Relu"}
}

%fused_computation.114 (param_0.233: f32[1,256,1,1], param_1.249: f32[256]) -> f32[1,256,1,1] {
  %param_1.249 = f32[256]{0} parameter(1)
  %bitcast.435 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.249), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv2/bn/Reshape"}
  %param_0.233 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.108 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.435, f32[1,256,1,1]{3,2,1,0} %param_0.233), metadata={op_type="Mul" op_name="tower0/group2/block2/conv2/bn/batchnorm/mul"}
}

%fused_computation.115 (param_0.236: f32[256]) -> f32[1,256,1,1] {
  %param_0.236 = f32[256]{0} parameter(0)
  %constant_215 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.317 = f32[256]{0} broadcast(f32[] %constant_215), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.136 = f32[256]{0} add(f32[256]{0} %param_0.236, f32[256]{0} %broadcast.317), metadata={op_type="Add" op_name="tower0/group2/block2/conv2/bn/batchnorm/add"}
  %rsqrt.73 = f32[256]{0} rsqrt(f32[256]{0} %add.136), metadata={op_type="Rsqrt" op_name="tower0/group2/block2/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.436 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.73), metadata={op_type="Rsqrt" op_name="tower0/group2/block2/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.116 (param_0.449: f32[1,256,1,1], param_1.489: f32[256], param_2.389: f32[1,256,50,50], param_3.352: f32[256]) -> f32[1,256,50,50] {
  %constant_216 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.318 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_216), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.389 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.449 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.439 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.449), metadata={op_type="Mul" op_name="tower0/group2/block2/conv1/bn/batchnorm/mul_1"}
  %broadcast.205 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.439), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block2/conv1/bn/batchnorm/mul_1"}
  %multiply.110 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.389, f32[1,256,50,50]{3,2,1,0} %broadcast.205), metadata={op_type="Mul" op_name="tower0/group2/block2/conv1/bn/batchnorm/mul_1"}
  %param_1.489 = f32[256]{0} parameter(1)
  %bitcast.438 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.489), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv1/bn/Reshape_1"}
  %param_3.352 = f32[256]{0} parameter(3)
  %bitcast.575 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.352), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv1/bn/Reshape_2"}
  %multiply.109 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.575, f32[1,256,1,1]{3,2,1,0} %param_0.449), metadata={op_type="Mul" op_name="tower0/group2/block2/conv1/bn/batchnorm/mul_2"}
  %subtract.58 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.438, f32[1,256,1,1]{3,2,1,0} %multiply.109), metadata={op_type="Sub" op_name="tower0/group2/block2/conv1/bn/batchnorm/sub"}
  %bitcast.437 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.58), metadata={op_type="Add" op_name="tower0/group2/block2/conv1/bn/batchnorm/add_1"}
  %broadcast.204 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.437), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block2/conv1/bn/batchnorm/add_1"}
  %add.137 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.110, f32[1,256,50,50]{3,2,1,0} %broadcast.204), metadata={op_type="Add" op_name="tower0/group2/block2/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.25 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.318, f32[1,256,50,50]{3,2,1,0} %add.137), metadata={op_type="Relu" op_name="tower0/group2/block2/conv1/Relu"}
}

%fused_computation.117 (param_0.239: f32[1,256,1,1], param_1.255: f32[256]) -> f32[1,256,1,1] {
  %param_1.255 = f32[256]{0} parameter(1)
  %bitcast.440 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.255), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv1/bn/Reshape"}
  %param_0.239 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.111 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.440, f32[1,256,1,1]{3,2,1,0} %param_0.239), metadata={op_type="Mul" op_name="tower0/group2/block2/conv1/bn/batchnorm/mul"}
}

%fused_computation.118 (param_0.242: f32[256]) -> f32[1,256,1,1] {
  %param_0.242 = f32[256]{0} parameter(0)
  %constant_217 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.319 = f32[256]{0} broadcast(f32[] %constant_217), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.138 = f32[256]{0} add(f32[256]{0} %param_0.242, f32[256]{0} %broadcast.319), metadata={op_type="Add" op_name="tower0/group2/block2/conv1/bn/batchnorm/add"}
  %rsqrt.74 = f32[256]{0} rsqrt(f32[256]{0} %add.138), metadata={op_type="Rsqrt" op_name="tower0/group2/block2/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.441 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.74), metadata={op_type="Rsqrt" op_name="tower0/group2/block2/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.119 (param_0.450: f32[1,1024,1,1], param_1.492: f32[1024], param_2.391: f32[1,1024,50,50], param_3.354: f32[1,1024,50,50], param_4.289: f32[1024]) -> f32[1,1024,50,50] {
  %constant_218 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.320 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_218), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/output"}
  %param_2.391 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %param_3.354 = f32[1,1024,50,50]{3,2,1,0} parameter(3)
  %param_0.450 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %bitcast.444 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_0.450), metadata={op_type="Mul" op_name="tower0/group2/block1/conv3/bn/batchnorm/mul_1"}
  %broadcast.207 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.444), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block1/conv3/bn/batchnorm/mul_1"}
  %multiply.113 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_3.354, f32[1,1024,50,50]{3,2,1,0} %broadcast.207), metadata={op_type="Mul" op_name="tower0/group2/block1/conv3/bn/batchnorm/mul_1"}
  %add.140 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %param_2.391, f32[1,1024,50,50]{3,2,1,0} %multiply.113), metadata={op_type="AddN" op_name="tower0/group2/block1/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.492 = f32[1024]{0} parameter(1)
  %bitcast.443 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.492), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv3/bn/Reshape_1"}
  %param_4.289 = f32[1024]{0} parameter(4)
  %bitcast.576 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_4.289), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv3/bn/Reshape_2"}
  %multiply.112 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.576, f32[1,1024,1,1]{3,2,1,0} %param_0.450), metadata={op_type="Mul" op_name="tower0/group2/block1/conv3/bn/batchnorm/mul_2"}
  %subtract.59 = f32[1,1024,1,1]{3,2,1,0} subtract(f32[1,1024,1,1]{3,2,1,0} %bitcast.443, f32[1,1024,1,1]{3,2,1,0} %multiply.112), metadata={op_type="Sub" op_name="tower0/group2/block1/conv3/bn/batchnorm/sub"}
  %bitcast.442 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %subtract.59), metadata={op_type="Add" op_name="tower0/group2/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.206 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.442), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.139 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %add.140, f32[1,1024,50,50]{3,2,1,0} %broadcast.206), metadata={op_type="Add" op_name="tower0/group2/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.26 = f32[1,1024,50,50]{3,2,1,0} maximum(f32[1,1024,50,50]{3,2,1,0} %broadcast.320, f32[1,1024,50,50]{3,2,1,0} %add.139), metadata={op_type="Relu" op_name="tower0/group2/block1/output"}
}

%fused_computation.120 (param_0.245: f32[1,1024,1,1], param_1.261: f32[1024]) -> f32[1,1024,1,1] {
  %param_1.261 = f32[1024]{0} parameter(1)
  %bitcast.445 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.261), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv3/bn/Reshape"}
  %param_0.245 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.114 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.445, f32[1,1024,1,1]{3,2,1,0} %param_0.245), metadata={op_type="Mul" op_name="tower0/group2/block1/conv3/bn/batchnorm/mul"}
}

%fused_computation.121 (param_0.248: f32[1024]) -> f32[1,1024,1,1] {
  %param_0.248 = f32[1024]{0} parameter(0)
  %constant_219 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.321 = f32[1024]{0} broadcast(f32[] %constant_219), dimensions={}, metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add"}
  %add.141 = f32[1024]{0} add(f32[1024]{0} %param_0.248, f32[1024]{0} %broadcast.321), metadata={op_type="Add" op_name="tower0/group2/block1/conv3/bn/batchnorm/add"}
  %rsqrt.75 = f32[1024]{0} rsqrt(f32[1024]{0} %add.141), metadata={op_type="Rsqrt" op_name="tower0/group2/block1/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.446 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %rsqrt.75), metadata={op_type="Rsqrt" op_name="tower0/group2/block1/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.122 (param_0.451: f32[1,256,1,1], param_1.495: f32[256], param_2.393: f32[1,256,50,50], param_3.356: f32[256]) -> f32[1,256,50,50] {
  %constant_220 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.322 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_220), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.393 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.451 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.449 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.451), metadata={op_type="Mul" op_name="tower0/group2/block1/conv2/bn/batchnorm/mul_1"}
  %broadcast.209 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.449), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block1/conv2/bn/batchnorm/mul_1"}
  %multiply.116 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.393, f32[1,256,50,50]{3,2,1,0} %broadcast.209), metadata={op_type="Mul" op_name="tower0/group2/block1/conv2/bn/batchnorm/mul_1"}
  %param_1.495 = f32[256]{0} parameter(1)
  %bitcast.448 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.495), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv2/bn/Reshape_1"}
  %param_3.356 = f32[256]{0} parameter(3)
  %bitcast.577 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.356), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv2/bn/Reshape_2"}
  %multiply.115 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.577, f32[1,256,1,1]{3,2,1,0} %param_0.451), metadata={op_type="Mul" op_name="tower0/group2/block1/conv2/bn/batchnorm/mul_2"}
  %subtract.60 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.448, f32[1,256,1,1]{3,2,1,0} %multiply.115), metadata={op_type="Sub" op_name="tower0/group2/block1/conv2/bn/batchnorm/sub"}
  %bitcast.447 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.60), metadata={op_type="Add" op_name="tower0/group2/block1/conv2/bn/batchnorm/add_1"}
  %broadcast.208 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.447), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block1/conv2/bn/batchnorm/add_1"}
  %add.142 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.116, f32[1,256,50,50]{3,2,1,0} %broadcast.208), metadata={op_type="Add" op_name="tower0/group2/block1/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.27 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.322, f32[1,256,50,50]{3,2,1,0} %add.142), metadata={op_type="Relu" op_name="tower0/group2/block1/conv2/Relu"}
}

%fused_computation.123 (param_0.251: f32[1,256,1,1], param_1.267: f32[256]) -> f32[1,256,1,1] {
  %param_1.267 = f32[256]{0} parameter(1)
  %bitcast.450 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.267), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv2/bn/Reshape"}
  %param_0.251 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.117 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.450, f32[1,256,1,1]{3,2,1,0} %param_0.251), metadata={op_type="Mul" op_name="tower0/group2/block1/conv2/bn/batchnorm/mul"}
}

%fused_computation.124 (param_0.254: f32[256]) -> f32[1,256,1,1] {
  %param_0.254 = f32[256]{0} parameter(0)
  %constant_221 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.323 = f32[256]{0} broadcast(f32[] %constant_221), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.143 = f32[256]{0} add(f32[256]{0} %param_0.254, f32[256]{0} %broadcast.323), metadata={op_type="Add" op_name="tower0/group2/block1/conv2/bn/batchnorm/add"}
  %rsqrt.76 = f32[256]{0} rsqrt(f32[256]{0} %add.143), metadata={op_type="Rsqrt" op_name="tower0/group2/block1/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.451 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.76), metadata={op_type="Rsqrt" op_name="tower0/group2/block1/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.125 (param_0.452: f32[1,256,1,1], param_1.498: f32[256], param_2.395: f32[1,256,50,50], param_3.358: f32[256]) -> f32[1,256,50,50] {
  %constant_222 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.324 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_222), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.395 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.452 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.454 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.452), metadata={op_type="Mul" op_name="tower0/group2/block1/conv1/bn/batchnorm/mul_1"}
  %broadcast.211 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.454), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block1/conv1/bn/batchnorm/mul_1"}
  %multiply.119 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.395, f32[1,256,50,50]{3,2,1,0} %broadcast.211), metadata={op_type="Mul" op_name="tower0/group2/block1/conv1/bn/batchnorm/mul_1"}
  %param_1.498 = f32[256]{0} parameter(1)
  %bitcast.453 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.498), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv1/bn/Reshape_1"}
  %param_3.358 = f32[256]{0} parameter(3)
  %bitcast.578 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.358), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv1/bn/Reshape_2"}
  %multiply.118 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.578, f32[1,256,1,1]{3,2,1,0} %param_0.452), metadata={op_type="Mul" op_name="tower0/group2/block1/conv1/bn/batchnorm/mul_2"}
  %subtract.61 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.453, f32[1,256,1,1]{3,2,1,0} %multiply.118), metadata={op_type="Sub" op_name="tower0/group2/block1/conv1/bn/batchnorm/sub"}
  %bitcast.452 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.61), metadata={op_type="Add" op_name="tower0/group2/block1/conv1/bn/batchnorm/add_1"}
  %broadcast.210 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.452), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block1/conv1/bn/batchnorm/add_1"}
  %add.144 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.119, f32[1,256,50,50]{3,2,1,0} %broadcast.210), metadata={op_type="Add" op_name="tower0/group2/block1/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.28 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.324, f32[1,256,50,50]{3,2,1,0} %add.144), metadata={op_type="Relu" op_name="tower0/group2/block1/conv1/Relu"}
}

%fused_computation.126 (param_0.257: f32[1,256,1,1], param_1.273: f32[256]) -> f32[1,256,1,1] {
  %param_1.273 = f32[256]{0} parameter(1)
  %bitcast.455 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.273), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv1/bn/Reshape"}
  %param_0.257 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.120 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.455, f32[1,256,1,1]{3,2,1,0} %param_0.257), metadata={op_type="Mul" op_name="tower0/group2/block1/conv1/bn/batchnorm/mul"}
}

%fused_computation.127 (param_0.260: f32[256]) -> f32[1,256,1,1] {
  %param_0.260 = f32[256]{0} parameter(0)
  %constant_223 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.325 = f32[256]{0} broadcast(f32[] %constant_223), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.145 = f32[256]{0} add(f32[256]{0} %param_0.260, f32[256]{0} %broadcast.325), metadata={op_type="Add" op_name="tower0/group2/block1/conv1/bn/batchnorm/add"}
  %rsqrt.77 = f32[256]{0} rsqrt(f32[256]{0} %add.145), metadata={op_type="Rsqrt" op_name="tower0/group2/block1/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.456 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.77), metadata={op_type="Rsqrt" op_name="tower0/group2/block1/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.128 (param_0.453: f32[1,1024,1,1], param_1.501: f32[1024], param_2.397: f32[1,1024,50,50], param_3.360: f32[1,1024,1,1], param_4.295: f32[1024], param_5.209: f32[1,1024,50,50], param_6.122: f32[1024], param_7.62: f32[1024]) -> f32[1,1024,50,50] {
  %constant_224 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.326 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[] %constant_224), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/output"}
  %param_5.209 = f32[1,1024,50,50]{3,2,1,0} parameter(5)
  %param_3.360 = f32[1,1024,1,1]{3,2,1,0} parameter(3)
  %bitcast.462 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_3.360), metadata={op_type="Mul" op_name="tower0/group2/block0/conv3/bn/batchnorm/mul_1"}
  %broadcast.215 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.462), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block0/conv3/bn/batchnorm/mul_1"}
  %multiply.124 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_5.209, f32[1,1024,50,50]{3,2,1,0} %broadcast.215), metadata={op_type="Mul" op_name="tower0/group2/block0/conv3/bn/batchnorm/mul_1"}
  %param_4.295 = f32[1024]{0} parameter(4)
  %bitcast.461 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_4.295), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv3/bn/Reshape_1"}
  %param_6.122 = f32[1024]{0} parameter(6)
  %bitcast.579 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_6.122), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv3/bn/Reshape_2"}
  %multiply.123 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.579, f32[1,1024,1,1]{3,2,1,0} %param_3.360), metadata={op_type="Mul" op_name="tower0/group2/block0/conv3/bn/batchnorm/mul_2"}
  %subtract.63 = f32[1,1024,1,1]{3,2,1,0} subtract(f32[1,1024,1,1]{3,2,1,0} %bitcast.461, f32[1,1024,1,1]{3,2,1,0} %multiply.123), metadata={op_type="Sub" op_name="tower0/group2/block0/conv3/bn/batchnorm/sub"}
  %bitcast.460 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %subtract.63), metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add_1"}
  %broadcast.214 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.460), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add_1"}
  %add.148 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %multiply.124, f32[1,1024,50,50]{3,2,1,0} %broadcast.214), metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add_1"}
  %param_2.397 = f32[1,1024,50,50]{3,2,1,0} parameter(2)
  %param_0.453 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  %bitcast.459 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %param_0.453), metadata={op_type="Mul" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/mul_1"}
  %broadcast.213 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.459), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/mul_1"}
  %multiply.122 = f32[1,1024,50,50]{3,2,1,0} multiply(f32[1,1024,50,50]{3,2,1,0} %param_2.397, f32[1,1024,50,50]{3,2,1,0} %broadcast.213), metadata={op_type="Mul" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/mul_1"}
  %param_1.501 = f32[1024]{0} parameter(1)
  %bitcast.458 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.501), metadata={op_type="Reshape" op_name="tower0/group2/block0/convshortcut/bn/Reshape_1"}
  %param_7.62 = f32[1024]{0} parameter(7)
  %bitcast.580 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_7.62), metadata={op_type="Reshape" op_name="tower0/group2/block0/convshortcut/bn/Reshape_2"}
  %multiply.121 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.580, f32[1,1024,1,1]{3,2,1,0} %param_0.453), metadata={op_type="Mul" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/mul_2"}
  %subtract.62 = f32[1,1024,1,1]{3,2,1,0} subtract(f32[1,1024,1,1]{3,2,1,0} %bitcast.458, f32[1,1024,1,1]{3,2,1,0} %multiply.121), metadata={op_type="Sub" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/sub"}
  %bitcast.457 = f32[1,1024]{1,0} bitcast(f32[1,1024,1,1]{3,2,1,0} %subtract.62), metadata={op_type="Add" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/add_1"}
  %broadcast.212 = f32[1,1024,50,50]{3,2,1,0} broadcast(f32[1,1024]{1,0} %bitcast.457), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/add_1"}
  %add.147 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %multiply.122, f32[1,1024,50,50]{3,2,1,0} %broadcast.212), metadata={op_type="Add" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/add_1"}
  %add.146 = f32[1,1024,50,50]{3,2,1,0} add(f32[1,1024,50,50]{3,2,1,0} %add.148, f32[1,1024,50,50]{3,2,1,0} %add.147), metadata={op_type="Add" op_name="tower0/group2/block0/add"}
  ROOT %maximum.29 = f32[1,1024,50,50]{3,2,1,0} maximum(f32[1,1024,50,50]{3,2,1,0} %broadcast.326, f32[1,1024,50,50]{3,2,1,0} %add.146), metadata={op_type="Relu" op_name="tower0/group2/block0/output"}
}

%fused_computation.129 (param_0.263: f32[1,1024,1,1], param_1.279: f32[1024]) -> f32[1,1024,1,1] {
  %param_1.279 = f32[1024]{0} parameter(1)
  %bitcast.463 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.279), metadata={op_type="Reshape" op_name="tower0/group2/block0/convshortcut/bn/Reshape"}
  %param_0.263 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.125 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.463, f32[1,1024,1,1]{3,2,1,0} %param_0.263), metadata={op_type="Mul" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/mul"}
}

%fused_computation.130 (param_0.266: f32[1024]) -> f32[1,1024,1,1] {
  %param_0.266 = f32[1024]{0} parameter(0)
  %constant_226 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.328 = f32[1024]{0} broadcast(f32[] %constant_226), dimensions={}, metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add"}
  %add.149 = f32[1024]{0} add(f32[1024]{0} %param_0.266, f32[1024]{0} %broadcast.328), metadata={op_type="Add" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/add"}
  %rsqrt.78 = f32[1024]{0} rsqrt(f32[1024]{0} %add.149), metadata={op_type="Rsqrt" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.464 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %rsqrt.78), metadata={op_type="Rsqrt" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/Rsqrt"}
}

%fused_computation.131 (param_0.268: f32[1,1024,1,1], param_1.282: f32[1024]) -> f32[1,1024,1,1] {
  %param_1.282 = f32[1024]{0} parameter(1)
  %bitcast.465 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %param_1.282), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv3/bn/Reshape"}
  %param_0.268 = f32[1,1024,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.126 = f32[1,1024,1,1]{3,2,1,0} multiply(f32[1,1024,1,1]{3,2,1,0} %bitcast.465, f32[1,1024,1,1]{3,2,1,0} %param_0.268), metadata={op_type="Mul" op_name="tower0/group2/block0/conv3/bn/batchnorm/mul"}
}

%fused_computation.132 (param_0.271: f32[1024]) -> f32[1,1024,1,1] {
  %param_0.271 = f32[1024]{0} parameter(0)
  %constant_225 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.327 = f32[1024]{0} broadcast(f32[] %constant_225), dimensions={}, metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add"}
  %add.150 = f32[1024]{0} add(f32[1024]{0} %param_0.271, f32[1024]{0} %broadcast.327), metadata={op_type="Add" op_name="tower0/group2/block0/conv3/bn/batchnorm/add"}
  %rsqrt.79 = f32[1024]{0} rsqrt(f32[1024]{0} %add.150), metadata={op_type="Rsqrt" op_name="tower0/group2/block0/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.466 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %rsqrt.79), metadata={op_type="Rsqrt" op_name="tower0/group2/block0/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.133 (param_0.454: f32[1,256,1,1], param_1.505: f32[256], param_2.399: f32[1,256,50,50], param_3.362: f32[256]) -> f32[1,256,50,50] {
  %constant_227 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.329 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[] %constant_227), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %param_2.399 = f32[1,256,50,50]{3,2,1,0} parameter(2)
  %param_0.454 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.469 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.454), metadata={op_type="Mul" op_name="tower0/group2/block0/conv2/bn/batchnorm/mul_1"}
  %broadcast.217 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.469), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block0/conv2/bn/batchnorm/mul_1"}
  %multiply.128 = f32[1,256,50,50]{3,2,1,0} multiply(f32[1,256,50,50]{3,2,1,0} %param_2.399, f32[1,256,50,50]{3,2,1,0} %broadcast.217), metadata={op_type="Mul" op_name="tower0/group2/block0/conv2/bn/batchnorm/mul_1"}
  %param_1.505 = f32[256]{0} parameter(1)
  %bitcast.468 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.505), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv2/bn/Reshape_1"}
  %param_3.362 = f32[256]{0} parameter(3)
  %bitcast.581 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.362), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv2/bn/Reshape_2"}
  %multiply.127 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.581, f32[1,256,1,1]{3,2,1,0} %param_0.454), metadata={op_type="Mul" op_name="tower0/group2/block0/conv2/bn/batchnorm/mul_2"}
  %subtract.64 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.468, f32[1,256,1,1]{3,2,1,0} %multiply.127), metadata={op_type="Sub" op_name="tower0/group2/block0/conv2/bn/batchnorm/sub"}
  %bitcast.467 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.64), metadata={op_type="Add" op_name="tower0/group2/block0/conv2/bn/batchnorm/add_1"}
  %broadcast.216 = f32[1,256,50,50]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.467), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block0/conv2/bn/batchnorm/add_1"}
  %add.151 = f32[1,256,50,50]{3,2,1,0} add(f32[1,256,50,50]{3,2,1,0} %multiply.128, f32[1,256,50,50]{3,2,1,0} %broadcast.216), metadata={op_type="Add" op_name="tower0/group2/block0/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.30 = f32[1,256,50,50]{3,2,1,0} maximum(f32[1,256,50,50]{3,2,1,0} %broadcast.329, f32[1,256,50,50]{3,2,1,0} %add.151), metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
}

%fused_computation.134 (param_0.274: f32[1,256,1,1], param_1.288: f32[256]) -> f32[1,256,1,1] {
  %param_1.288 = f32[256]{0} parameter(1)
  %bitcast.470 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.288), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv2/bn/Reshape"}
  %param_0.274 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.129 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.470, f32[1,256,1,1]{3,2,1,0} %param_0.274), metadata={op_type="Mul" op_name="tower0/group2/block0/conv2/bn/batchnorm/mul"}
}

%fused_computation.135 (param_0.277: f32[256]) -> f32[1,256,1,1] {
  %param_0.277 = f32[256]{0} parameter(0)
  %constant_229 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.331 = f32[256]{0} broadcast(f32[] %constant_229), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.152 = f32[256]{0} add(f32[256]{0} %param_0.277, f32[256]{0} %broadcast.331), metadata={op_type="Add" op_name="tower0/group2/block0/conv2/bn/batchnorm/add"}
  %rsqrt.80 = f32[256]{0} rsqrt(f32[256]{0} %add.152), metadata={op_type="Rsqrt" op_name="tower0/group2/block0/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.471 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.80), metadata={op_type="Rsqrt" op_name="tower0/group2/block0/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.136 (param_0.455: f32[1,256,1,1], param_1.506: f32[256], param_2.400: f32[1,256,100,100], param_3.363: f32[256]) -> f32[1,256,100,100] {
  %constant_165 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.220 = f32[1,256,100,100]{3,2,1,0} broadcast(f32[] %constant_165), dimensions={}, metadata={op_type="Relu" op_name="tower0/group2/block0/conv1/Relu"}
  %param_2.400 = f32[1,256,100,100]{3,2,1,0} parameter(2)
  %param_0.455 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  %bitcast.474 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %param_0.455), metadata={op_type="Mul" op_name="tower0/group2/block0/conv1/bn/batchnorm/mul_1"}
  %broadcast.219 = f32[1,256,100,100]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.474), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group2/block0/conv1/bn/batchnorm/mul_1"}
  %multiply.131 = f32[1,256,100,100]{3,2,1,0} multiply(f32[1,256,100,100]{3,2,1,0} %param_2.400, f32[1,256,100,100]{3,2,1,0} %broadcast.219), metadata={op_type="Mul" op_name="tower0/group2/block0/conv1/bn/batchnorm/mul_1"}
  %param_1.506 = f32[256]{0} parameter(1)
  %bitcast.473 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.506), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv1/bn/Reshape_1"}
  %param_3.363 = f32[256]{0} parameter(3)
  %bitcast.582 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_3.363), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv1/bn/Reshape_2"}
  %multiply.130 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.582, f32[1,256,1,1]{3,2,1,0} %param_0.455), metadata={op_type="Mul" op_name="tower0/group2/block0/conv1/bn/batchnorm/mul_2"}
  %subtract.65 = f32[1,256,1,1]{3,2,1,0} subtract(f32[1,256,1,1]{3,2,1,0} %bitcast.473, f32[1,256,1,1]{3,2,1,0} %multiply.130), metadata={op_type="Sub" op_name="tower0/group2/block0/conv1/bn/batchnorm/sub"}
  %bitcast.472 = f32[1,256]{1,0} bitcast(f32[1,256,1,1]{3,2,1,0} %subtract.65), metadata={op_type="Add" op_name="tower0/group2/block0/conv1/bn/batchnorm/add_1"}
  %broadcast.218 = f32[1,256,100,100]{3,2,1,0} broadcast(f32[1,256]{1,0} %bitcast.472), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group2/block0/conv1/bn/batchnorm/add_1"}
  %add.153 = f32[1,256,100,100]{3,2,1,0} add(f32[1,256,100,100]{3,2,1,0} %multiply.131, f32[1,256,100,100]{3,2,1,0} %broadcast.218), metadata={op_type="Add" op_name="tower0/group2/block0/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.31 = f32[1,256,100,100]{3,2,1,0} maximum(f32[1,256,100,100]{3,2,1,0} %broadcast.220, f32[1,256,100,100]{3,2,1,0} %add.153), metadata={op_type="Relu" op_name="tower0/group2/block0/conv1/Relu"}
}

%fused_computation.137 (param_0.281: f32[1,256,1,1], param_1.295: f32[256]) -> f32[1,256,1,1] {
  %param_1.295 = f32[256]{0} parameter(1)
  %bitcast.475 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %param_1.295), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv1/bn/Reshape"}
  %param_0.281 = f32[1,256,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.132 = f32[1,256,1,1]{3,2,1,0} multiply(f32[1,256,1,1]{3,2,1,0} %bitcast.475, f32[1,256,1,1]{3,2,1,0} %param_0.281), metadata={op_type="Mul" op_name="tower0/group2/block0/conv1/bn/batchnorm/mul"}
}

%fused_computation.138 (param_0.284: f32[256]) -> f32[1,256,1,1] {
  %param_0.284 = f32[256]{0} parameter(0)
  %constant_228 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.330 = f32[256]{0} broadcast(f32[] %constant_228), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.154 = f32[256]{0} add(f32[256]{0} %param_0.284, f32[256]{0} %broadcast.330), metadata={op_type="Add" op_name="tower0/group2/block0/conv1/bn/batchnorm/add"}
  %rsqrt.81 = f32[256]{0} rsqrt(f32[256]{0} %add.154), metadata={op_type="Rsqrt" op_name="tower0/group2/block0/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.476 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %rsqrt.81), metadata={op_type="Rsqrt" op_name="tower0/group2/block0/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.139 (param_0.456: f32[1,512,1,1], param_1.510: f32[512], param_2.402: f32[1,512,100,100], param_3.365: f32[1,512,100,100], param_4.299: f32[512]) -> f32[1,512,100,100] {
  %constant_230 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.332 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[] %constant_230), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/output"}
  %param_2.402 = f32[1,512,100,100]{3,2,1,0} parameter(2)
  %param_3.365 = f32[1,512,100,100]{3,2,1,0} parameter(3)
  %param_0.456 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.479 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.456), metadata={op_type="Mul" op_name="tower0/group1/block3/conv3/bn/batchnorm/mul_1"}
  %broadcast.222 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.479), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block3/conv3/bn/batchnorm/mul_1"}
  %multiply.134 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_3.365, f32[1,512,100,100]{3,2,1,0} %broadcast.222), metadata={op_type="Mul" op_name="tower0/group1/block3/conv3/bn/batchnorm/mul_1"}
  %add.156 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %param_2.402, f32[1,512,100,100]{3,2,1,0} %multiply.134), metadata={op_type="AddN" op_name="tower0/group1/block3/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.510 = f32[512]{0} parameter(1)
  %bitcast.478 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.510), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv3/bn/Reshape_1"}
  %param_4.299 = f32[512]{0} parameter(4)
  %bitcast.583 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_4.299), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv3/bn/Reshape_2"}
  %multiply.133 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.583, f32[1,512,1,1]{3,2,1,0} %param_0.456), metadata={op_type="Mul" op_name="tower0/group1/block3/conv3/bn/batchnorm/mul_2"}
  %subtract.66 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.478, f32[1,512,1,1]{3,2,1,0} %multiply.133), metadata={op_type="Sub" op_name="tower0/group1/block3/conv3/bn/batchnorm/sub"}
  %bitcast.477 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.66), metadata={op_type="Add" op_name="tower0/group1/block3/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.221 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.477), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block3/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.155 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %add.156, f32[1,512,100,100]{3,2,1,0} %broadcast.221), metadata={op_type="Add" op_name="tower0/group1/block3/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.32 = f32[1,512,100,100]{3,2,1,0} maximum(f32[1,512,100,100]{3,2,1,0} %broadcast.332, f32[1,512,100,100]{3,2,1,0} %add.155), metadata={op_type="Relu" op_name="tower0/group1/block3/output"}
}

%fused_computation.140 (param_0.287: f32[1,512,1,1], param_1.301: f32[512]) -> f32[1,512,1,1] {
  %param_1.301 = f32[512]{0} parameter(1)
  %bitcast.480 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.301), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv3/bn/Reshape"}
  %param_0.287 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.135 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.480, f32[1,512,1,1]{3,2,1,0} %param_0.287), metadata={op_type="Mul" op_name="tower0/group1/block3/conv3/bn/batchnorm/mul"}
}

%fused_computation.141 (param_0.290: f32[512]) -> f32[1,512,1,1] {
  %param_0.290 = f32[512]{0} parameter(0)
  %constant_231 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.333 = f32[512]{0} broadcast(f32[] %constant_231), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %add.157 = f32[512]{0} add(f32[512]{0} %param_0.290, f32[512]{0} %broadcast.333), metadata={op_type="Add" op_name="tower0/group1/block3/conv3/bn/batchnorm/add"}
  %rsqrt.82 = f32[512]{0} rsqrt(f32[512]{0} %add.157), metadata={op_type="Rsqrt" op_name="tower0/group1/block3/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.481 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.82), metadata={op_type="Rsqrt" op_name="tower0/group1/block3/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.142 (param_0.457: f32[1,128,1,1], param_1.513: f32[128], param_2.404: f32[1,128,100,100], param_3.367: f32[128]) -> f32[1,128,100,100] {
  %constant_232 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.334 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_232), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/conv2/Relu"}
  %param_2.404 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %param_0.457 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %bitcast.484 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_0.457), metadata={op_type="Mul" op_name="tower0/group1/block3/conv2/bn/batchnorm/mul_1"}
  %broadcast.224 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.484), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block3/conv2/bn/batchnorm/mul_1"}
  %multiply.137 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_2.404, f32[1,128,100,100]{3,2,1,0} %broadcast.224), metadata={op_type="Mul" op_name="tower0/group1/block3/conv2/bn/batchnorm/mul_1"}
  %param_1.513 = f32[128]{0} parameter(1)
  %bitcast.483 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.513), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv2/bn/Reshape_1"}
  %param_3.367 = f32[128]{0} parameter(3)
  %bitcast.584 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.367), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv2/bn/Reshape_2"}
  %multiply.136 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.584, f32[1,128,1,1]{3,2,1,0} %param_0.457), metadata={op_type="Mul" op_name="tower0/group1/block3/conv2/bn/batchnorm/mul_2"}
  %subtract.67 = f32[1,128,1,1]{3,2,1,0} subtract(f32[1,128,1,1]{3,2,1,0} %bitcast.483, f32[1,128,1,1]{3,2,1,0} %multiply.136), metadata={op_type="Sub" op_name="tower0/group1/block3/conv2/bn/batchnorm/sub"}
  %bitcast.482 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %subtract.67), metadata={op_type="Add" op_name="tower0/group1/block3/conv2/bn/batchnorm/add_1"}
  %broadcast.223 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.482), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block3/conv2/bn/batchnorm/add_1"}
  %add.158 = f32[1,128,100,100]{3,2,1,0} add(f32[1,128,100,100]{3,2,1,0} %multiply.137, f32[1,128,100,100]{3,2,1,0} %broadcast.223), metadata={op_type="Add" op_name="tower0/group1/block3/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.33 = f32[1,128,100,100]{3,2,1,0} maximum(f32[1,128,100,100]{3,2,1,0} %broadcast.334, f32[1,128,100,100]{3,2,1,0} %add.158), metadata={op_type="Relu" op_name="tower0/group1/block3/conv2/Relu"}
}

%fused_computation.143 (param_0.293: f32[1,128,1,1], param_1.307: f32[128]) -> f32[1,128,1,1] {
  %param_1.307 = f32[128]{0} parameter(1)
  %bitcast.485 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.307), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv2/bn/Reshape"}
  %param_0.293 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.138 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.485, f32[1,128,1,1]{3,2,1,0} %param_0.293), metadata={op_type="Mul" op_name="tower0/group1/block3/conv2/bn/batchnorm/mul"}
}

%fused_computation.144 (param_0.296: f32[128]) -> f32[1,128,1,1] {
  %param_0.296 = f32[128]{0} parameter(0)
  %constant_233 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.335 = f32[128]{0} broadcast(f32[] %constant_233), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add"}
  %add.159 = f32[128]{0} add(f32[128]{0} %param_0.296, f32[128]{0} %broadcast.335), metadata={op_type="Add" op_name="tower0/group1/block3/conv2/bn/batchnorm/add"}
  %rsqrt.83 = f32[128]{0} rsqrt(f32[128]{0} %add.159), metadata={op_type="Rsqrt" op_name="tower0/group1/block3/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.486 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %rsqrt.83), metadata={op_type="Rsqrt" op_name="tower0/group1/block3/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.145 (param_0.458: f32[1,128,1,1], param_1.516: f32[128], param_2.406: f32[1,128,100,100], param_3.369: f32[128]) -> f32[1,128,100,100] {
  %constant_234 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.336 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_234), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/conv2/Relu"}
  %param_2.406 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %param_0.458 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %bitcast.489 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_0.458), metadata={op_type="Mul" op_name="tower0/group1/block3/conv1/bn/batchnorm/mul_1"}
  %broadcast.226 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.489), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block3/conv1/bn/batchnorm/mul_1"}
  %multiply.140 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_2.406, f32[1,128,100,100]{3,2,1,0} %broadcast.226), metadata={op_type="Mul" op_name="tower0/group1/block3/conv1/bn/batchnorm/mul_1"}
  %param_1.516 = f32[128]{0} parameter(1)
  %bitcast.488 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.516), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv1/bn/Reshape_1"}
  %param_3.369 = f32[128]{0} parameter(3)
  %bitcast.585 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.369), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv1/bn/Reshape_2"}
  %multiply.139 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.585, f32[1,128,1,1]{3,2,1,0} %param_0.458), metadata={op_type="Mul" op_name="tower0/group1/block3/conv1/bn/batchnorm/mul_2"}
  %subtract.68 = f32[1,128,1,1]{3,2,1,0} subtract(f32[1,128,1,1]{3,2,1,0} %bitcast.488, f32[1,128,1,1]{3,2,1,0} %multiply.139), metadata={op_type="Sub" op_name="tower0/group1/block3/conv1/bn/batchnorm/sub"}
  %bitcast.487 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %subtract.68), metadata={op_type="Add" op_name="tower0/group1/block3/conv1/bn/batchnorm/add_1"}
  %broadcast.225 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.487), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block3/conv1/bn/batchnorm/add_1"}
  %add.160 = f32[1,128,100,100]{3,2,1,0} add(f32[1,128,100,100]{3,2,1,0} %multiply.140, f32[1,128,100,100]{3,2,1,0} %broadcast.225), metadata={op_type="Add" op_name="tower0/group1/block3/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.34 = f32[1,128,100,100]{3,2,1,0} maximum(f32[1,128,100,100]{3,2,1,0} %broadcast.336, f32[1,128,100,100]{3,2,1,0} %add.160), metadata={op_type="Relu" op_name="tower0/group1/block3/conv1/Relu"}
}

%fused_computation.146 (param_0.299: f32[1,128,1,1], param_1.313: f32[128]) -> f32[1,128,1,1] {
  %param_1.313 = f32[128]{0} parameter(1)
  %bitcast.490 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.313), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv1/bn/Reshape"}
  %param_0.299 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.141 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.490, f32[1,128,1,1]{3,2,1,0} %param_0.299), metadata={op_type="Mul" op_name="tower0/group1/block3/conv1/bn/batchnorm/mul"}
}

%fused_computation.147 (param_0.302: f32[128]) -> f32[1,128,1,1] {
  %param_0.302 = f32[128]{0} parameter(0)
  %constant_235 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.337 = f32[128]{0} broadcast(f32[] %constant_235), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add"}
  %add.161 = f32[128]{0} add(f32[128]{0} %param_0.302, f32[128]{0} %broadcast.337), metadata={op_type="Add" op_name="tower0/group1/block3/conv1/bn/batchnorm/add"}
  %rsqrt.84 = f32[128]{0} rsqrt(f32[128]{0} %add.161), metadata={op_type="Rsqrt" op_name="tower0/group1/block3/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.491 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %rsqrt.84), metadata={op_type="Rsqrt" op_name="tower0/group1/block3/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.148 (param_0.459: f32[1,512,1,1], param_1.519: f32[512], param_2.408: f32[1,512,100,100], param_3.371: f32[1,512,100,100], param_4.305: f32[512]) -> f32[1,512,100,100] {
  %constant_236 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.338 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[] %constant_236), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/output"}
  %param_2.408 = f32[1,512,100,100]{3,2,1,0} parameter(2)
  %param_3.371 = f32[1,512,100,100]{3,2,1,0} parameter(3)
  %param_0.459 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.494 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.459), metadata={op_type="Mul" op_name="tower0/group1/block2/conv3/bn/batchnorm/mul_1"}
  %broadcast.228 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.494), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block2/conv3/bn/batchnorm/mul_1"}
  %multiply.143 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_3.371, f32[1,512,100,100]{3,2,1,0} %broadcast.228), metadata={op_type="Mul" op_name="tower0/group1/block2/conv3/bn/batchnorm/mul_1"}
  %add.163 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %param_2.408, f32[1,512,100,100]{3,2,1,0} %multiply.143), metadata={op_type="AddN" op_name="tower0/group1/block2/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.519 = f32[512]{0} parameter(1)
  %bitcast.493 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.519), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv3/bn/Reshape_1"}
  %param_4.305 = f32[512]{0} parameter(4)
  %bitcast.586 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_4.305), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv3/bn/Reshape_2"}
  %multiply.142 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.586, f32[1,512,1,1]{3,2,1,0} %param_0.459), metadata={op_type="Mul" op_name="tower0/group1/block2/conv3/bn/batchnorm/mul_2"}
  %subtract.69 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.493, f32[1,512,1,1]{3,2,1,0} %multiply.142), metadata={op_type="Sub" op_name="tower0/group1/block2/conv3/bn/batchnorm/sub"}
  %bitcast.492 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.69), metadata={op_type="Add" op_name="tower0/group1/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.227 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.492), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.162 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %add.163, f32[1,512,100,100]{3,2,1,0} %broadcast.227), metadata={op_type="Add" op_name="tower0/group1/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.35 = f32[1,512,100,100]{3,2,1,0} maximum(f32[1,512,100,100]{3,2,1,0} %broadcast.338, f32[1,512,100,100]{3,2,1,0} %add.162), metadata={op_type="Relu" op_name="tower0/group1/block2/output"}
}

%fused_computation.149 (param_0.305: f32[1,512,1,1], param_1.319: f32[512]) -> f32[1,512,1,1] {
  %param_1.319 = f32[512]{0} parameter(1)
  %bitcast.495 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.319), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv3/bn/Reshape"}
  %param_0.305 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.144 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.495, f32[1,512,1,1]{3,2,1,0} %param_0.305), metadata={op_type="Mul" op_name="tower0/group1/block2/conv3/bn/batchnorm/mul"}
}

%fused_computation.150 (param_0.308: f32[512]) -> f32[1,512,1,1] {
  %param_0.308 = f32[512]{0} parameter(0)
  %constant_237 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.339 = f32[512]{0} broadcast(f32[] %constant_237), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %add.164 = f32[512]{0} add(f32[512]{0} %param_0.308, f32[512]{0} %broadcast.339), metadata={op_type="Add" op_name="tower0/group1/block2/conv3/bn/batchnorm/add"}
  %rsqrt.85 = f32[512]{0} rsqrt(f32[512]{0} %add.164), metadata={op_type="Rsqrt" op_name="tower0/group1/block2/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.496 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.85), metadata={op_type="Rsqrt" op_name="tower0/group1/block2/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.151 (param_0.460: f32[1,128,1,1], param_1.522: f32[128], param_2.410: f32[1,128,100,100], param_3.373: f32[128]) -> f32[1,128,100,100] {
  %constant_238 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.340 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_238), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/conv2/Relu"}
  %param_2.410 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %param_0.460 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %bitcast.499 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_0.460), metadata={op_type="Mul" op_name="tower0/group1/block2/conv2/bn/batchnorm/mul_1"}
  %broadcast.230 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.499), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block2/conv2/bn/batchnorm/mul_1"}
  %multiply.146 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_2.410, f32[1,128,100,100]{3,2,1,0} %broadcast.230), metadata={op_type="Mul" op_name="tower0/group1/block2/conv2/bn/batchnorm/mul_1"}
  %param_1.522 = f32[128]{0} parameter(1)
  %bitcast.498 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.522), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv2/bn/Reshape_1"}
  %param_3.373 = f32[128]{0} parameter(3)
  %bitcast.587 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.373), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv2/bn/Reshape_2"}
  %multiply.145 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.587, f32[1,128,1,1]{3,2,1,0} %param_0.460), metadata={op_type="Mul" op_name="tower0/group1/block2/conv2/bn/batchnorm/mul_2"}
  %subtract.70 = f32[1,128,1,1]{3,2,1,0} subtract(f32[1,128,1,1]{3,2,1,0} %bitcast.498, f32[1,128,1,1]{3,2,1,0} %multiply.145), metadata={op_type="Sub" op_name="tower0/group1/block2/conv2/bn/batchnorm/sub"}
  %bitcast.497 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %subtract.70), metadata={op_type="Add" op_name="tower0/group1/block2/conv2/bn/batchnorm/add_1"}
  %broadcast.229 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.497), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block2/conv2/bn/batchnorm/add_1"}
  %add.165 = f32[1,128,100,100]{3,2,1,0} add(f32[1,128,100,100]{3,2,1,0} %multiply.146, f32[1,128,100,100]{3,2,1,0} %broadcast.229), metadata={op_type="Add" op_name="tower0/group1/block2/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.36 = f32[1,128,100,100]{3,2,1,0} maximum(f32[1,128,100,100]{3,2,1,0} %broadcast.340, f32[1,128,100,100]{3,2,1,0} %add.165), metadata={op_type="Relu" op_name="tower0/group1/block2/conv2/Relu"}
}

%fused_computation.152 (param_0.311: f32[1,128,1,1], param_1.325: f32[128]) -> f32[1,128,1,1] {
  %param_1.325 = f32[128]{0} parameter(1)
  %bitcast.500 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.325), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv2/bn/Reshape"}
  %param_0.311 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.147 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.500, f32[1,128,1,1]{3,2,1,0} %param_0.311), metadata={op_type="Mul" op_name="tower0/group1/block2/conv2/bn/batchnorm/mul"}
}

%fused_computation.153 (param_0.314: f32[128]) -> f32[1,128,1,1] {
  %param_0.314 = f32[128]{0} parameter(0)
  %constant_239 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.341 = f32[128]{0} broadcast(f32[] %constant_239), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add"}
  %add.166 = f32[128]{0} add(f32[128]{0} %param_0.314, f32[128]{0} %broadcast.341), metadata={op_type="Add" op_name="tower0/group1/block2/conv2/bn/batchnorm/add"}
  %rsqrt.86 = f32[128]{0} rsqrt(f32[128]{0} %add.166), metadata={op_type="Rsqrt" op_name="tower0/group1/block2/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.501 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %rsqrt.86), metadata={op_type="Rsqrt" op_name="tower0/group1/block2/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.154 (param_0.461: f32[1,128,1,1], param_1.525: f32[128], param_2.412: f32[1,128,100,100], param_3.375: f32[128]) -> f32[1,128,100,100] {
  %constant_240 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.342 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_240), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/conv2/Relu"}
  %param_2.412 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %param_0.461 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %bitcast.504 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_0.461), metadata={op_type="Mul" op_name="tower0/group1/block2/conv1/bn/batchnorm/mul_1"}
  %broadcast.232 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.504), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block2/conv1/bn/batchnorm/mul_1"}
  %multiply.149 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_2.412, f32[1,128,100,100]{3,2,1,0} %broadcast.232), metadata={op_type="Mul" op_name="tower0/group1/block2/conv1/bn/batchnorm/mul_1"}
  %param_1.525 = f32[128]{0} parameter(1)
  %bitcast.503 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.525), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv1/bn/Reshape_1"}
  %param_3.375 = f32[128]{0} parameter(3)
  %bitcast.588 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.375), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv1/bn/Reshape_2"}
  %multiply.148 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.588, f32[1,128,1,1]{3,2,1,0} %param_0.461), metadata={op_type="Mul" op_name="tower0/group1/block2/conv1/bn/batchnorm/mul_2"}
  %subtract.71 = f32[1,128,1,1]{3,2,1,0} subtract(f32[1,128,1,1]{3,2,1,0} %bitcast.503, f32[1,128,1,1]{3,2,1,0} %multiply.148), metadata={op_type="Sub" op_name="tower0/group1/block2/conv1/bn/batchnorm/sub"}
  %bitcast.502 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %subtract.71), metadata={op_type="Add" op_name="tower0/group1/block2/conv1/bn/batchnorm/add_1"}
  %broadcast.231 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.502), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block2/conv1/bn/batchnorm/add_1"}
  %add.167 = f32[1,128,100,100]{3,2,1,0} add(f32[1,128,100,100]{3,2,1,0} %multiply.149, f32[1,128,100,100]{3,2,1,0} %broadcast.231), metadata={op_type="Add" op_name="tower0/group1/block2/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.37 = f32[1,128,100,100]{3,2,1,0} maximum(f32[1,128,100,100]{3,2,1,0} %broadcast.342, f32[1,128,100,100]{3,2,1,0} %add.167), metadata={op_type="Relu" op_name="tower0/group1/block2/conv1/Relu"}
}

%fused_computation.155 (param_0.317: f32[1,128,1,1], param_1.331: f32[128]) -> f32[1,128,1,1] {
  %param_1.331 = f32[128]{0} parameter(1)
  %bitcast.505 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.331), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv1/bn/Reshape"}
  %param_0.317 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.150 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.505, f32[1,128,1,1]{3,2,1,0} %param_0.317), metadata={op_type="Mul" op_name="tower0/group1/block2/conv1/bn/batchnorm/mul"}
}

%fused_computation.156 (param_0.320: f32[128]) -> f32[1,128,1,1] {
  %param_0.320 = f32[128]{0} parameter(0)
  %constant_241 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.343 = f32[128]{0} broadcast(f32[] %constant_241), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add"}
  %add.168 = f32[128]{0} add(f32[128]{0} %param_0.320, f32[128]{0} %broadcast.343), metadata={op_type="Add" op_name="tower0/group1/block2/conv1/bn/batchnorm/add"}
  %rsqrt.87 = f32[128]{0} rsqrt(f32[128]{0} %add.168), metadata={op_type="Rsqrt" op_name="tower0/group1/block2/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.506 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %rsqrt.87), metadata={op_type="Rsqrt" op_name="tower0/group1/block2/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.157 (param_0.462: f32[1,512,1,1], param_1.528: f32[512], param_2.414: f32[1,512,100,100], param_3.377: f32[1,512,100,100], param_4.311: f32[512]) -> f32[1,512,100,100] {
  %constant_242 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.344 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[] %constant_242), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/output"}
  %param_2.414 = f32[1,512,100,100]{3,2,1,0} parameter(2)
  %param_3.377 = f32[1,512,100,100]{3,2,1,0} parameter(3)
  %param_0.462 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.509 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.462), metadata={op_type="Mul" op_name="tower0/group1/block1/conv3/bn/batchnorm/mul_1"}
  %broadcast.234 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.509), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block1/conv3/bn/batchnorm/mul_1"}
  %multiply.152 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_3.377, f32[1,512,100,100]{3,2,1,0} %broadcast.234), metadata={op_type="Mul" op_name="tower0/group1/block1/conv3/bn/batchnorm/mul_1"}
  %add.170 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %param_2.414, f32[1,512,100,100]{3,2,1,0} %multiply.152), metadata={op_type="AddN" op_name="tower0/group1/block1/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_1.528 = f32[512]{0} parameter(1)
  %bitcast.508 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.528), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv3/bn/Reshape_1"}
  %param_4.311 = f32[512]{0} parameter(4)
  %bitcast.589 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_4.311), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv3/bn/Reshape_2"}
  %multiply.151 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.589, f32[1,512,1,1]{3,2,1,0} %param_0.462), metadata={op_type="Mul" op_name="tower0/group1/block1/conv3/bn/batchnorm/mul_2"}
  %subtract.72 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.508, f32[1,512,1,1]{3,2,1,0} %multiply.151), metadata={op_type="Sub" op_name="tower0/group1/block1/conv3/bn/batchnorm/sub"}
  %bitcast.507 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.72), metadata={op_type="Add" op_name="tower0/group1/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  %broadcast.233 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.507), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.169 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %add.170, f32[1,512,100,100]{3,2,1,0} %broadcast.233), metadata={op_type="Add" op_name="tower0/group1/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.38 = f32[1,512,100,100]{3,2,1,0} maximum(f32[1,512,100,100]{3,2,1,0} %broadcast.344, f32[1,512,100,100]{3,2,1,0} %add.169), metadata={op_type="Relu" op_name="tower0/group1/block1/output"}
}

%fused_computation.158 (param_0.323: f32[1,512,1,1], param_1.337: f32[512]) -> f32[1,512,1,1] {
  %param_1.337 = f32[512]{0} parameter(1)
  %bitcast.510 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.337), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv3/bn/Reshape"}
  %param_0.323 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.153 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.510, f32[1,512,1,1]{3,2,1,0} %param_0.323), metadata={op_type="Mul" op_name="tower0/group1/block1/conv3/bn/batchnorm/mul"}
}

%fused_computation.159 (param_0.326: f32[512]) -> f32[1,512,1,1] {
  %param_0.326 = f32[512]{0} parameter(0)
  %constant_243 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.345 = f32[512]{0} broadcast(f32[] %constant_243), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %add.171 = f32[512]{0} add(f32[512]{0} %param_0.326, f32[512]{0} %broadcast.345), metadata={op_type="Add" op_name="tower0/group1/block1/conv3/bn/batchnorm/add"}
  %rsqrt.88 = f32[512]{0} rsqrt(f32[512]{0} %add.171), metadata={op_type="Rsqrt" op_name="tower0/group1/block1/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.511 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.88), metadata={op_type="Rsqrt" op_name="tower0/group1/block1/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.160 (param_0.463: f32[1,128,1,1], param_1.531: f32[128], param_2.416: f32[1,128,100,100], param_3.379: f32[128]) -> f32[1,128,100,100] {
  %constant_244 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.346 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_244), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/conv2/Relu"}
  %param_2.416 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %param_0.463 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %bitcast.514 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_0.463), metadata={op_type="Mul" op_name="tower0/group1/block1/conv2/bn/batchnorm/mul_1"}
  %broadcast.236 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.514), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block1/conv2/bn/batchnorm/mul_1"}
  %multiply.155 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_2.416, f32[1,128,100,100]{3,2,1,0} %broadcast.236), metadata={op_type="Mul" op_name="tower0/group1/block1/conv2/bn/batchnorm/mul_1"}
  %param_1.531 = f32[128]{0} parameter(1)
  %bitcast.513 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.531), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv2/bn/Reshape_1"}
  %param_3.379 = f32[128]{0} parameter(3)
  %bitcast.590 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.379), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv2/bn/Reshape_2"}
  %multiply.154 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.590, f32[1,128,1,1]{3,2,1,0} %param_0.463), metadata={op_type="Mul" op_name="tower0/group1/block1/conv2/bn/batchnorm/mul_2"}
  %subtract.73 = f32[1,128,1,1]{3,2,1,0} subtract(f32[1,128,1,1]{3,2,1,0} %bitcast.513, f32[1,128,1,1]{3,2,1,0} %multiply.154), metadata={op_type="Sub" op_name="tower0/group1/block1/conv2/bn/batchnorm/sub"}
  %bitcast.512 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %subtract.73), metadata={op_type="Add" op_name="tower0/group1/block1/conv2/bn/batchnorm/add_1"}
  %broadcast.235 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.512), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block1/conv2/bn/batchnorm/add_1"}
  %add.172 = f32[1,128,100,100]{3,2,1,0} add(f32[1,128,100,100]{3,2,1,0} %multiply.155, f32[1,128,100,100]{3,2,1,0} %broadcast.235), metadata={op_type="Add" op_name="tower0/group1/block1/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.39 = f32[1,128,100,100]{3,2,1,0} maximum(f32[1,128,100,100]{3,2,1,0} %broadcast.346, f32[1,128,100,100]{3,2,1,0} %add.172), metadata={op_type="Relu" op_name="tower0/group1/block1/conv2/Relu"}
}

%fused_computation.161 (param_0.329: f32[1,128,1,1], param_1.343: f32[128]) -> f32[1,128,1,1] {
  %param_1.343 = f32[128]{0} parameter(1)
  %bitcast.515 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.343), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv2/bn/Reshape"}
  %param_0.329 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.156 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.515, f32[1,128,1,1]{3,2,1,0} %param_0.329), metadata={op_type="Mul" op_name="tower0/group1/block1/conv2/bn/batchnorm/mul"}
}

%fused_computation.162 (param_0.332: f32[128]) -> f32[1,128,1,1] {
  %param_0.332 = f32[128]{0} parameter(0)
  %constant_245 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.347 = f32[128]{0} broadcast(f32[] %constant_245), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add"}
  %add.173 = f32[128]{0} add(f32[128]{0} %param_0.332, f32[128]{0} %broadcast.347), metadata={op_type="Add" op_name="tower0/group1/block1/conv2/bn/batchnorm/add"}
  %rsqrt.89 = f32[128]{0} rsqrt(f32[128]{0} %add.173), metadata={op_type="Rsqrt" op_name="tower0/group1/block1/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.516 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %rsqrt.89), metadata={op_type="Rsqrt" op_name="tower0/group1/block1/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.163 (param_0.464: f32[1,128,1,1], param_1.534: f32[128], param_2.418: f32[1,128,100,100], param_3.381: f32[128]) -> f32[1,128,100,100] {
  %constant_246 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.348 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_246), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/conv2/Relu"}
  %param_2.418 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %param_0.464 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %bitcast.519 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_0.464), metadata={op_type="Mul" op_name="tower0/group1/block1/conv1/bn/batchnorm/mul_1"}
  %broadcast.238 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.519), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block1/conv1/bn/batchnorm/mul_1"}
  %multiply.158 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_2.418, f32[1,128,100,100]{3,2,1,0} %broadcast.238), metadata={op_type="Mul" op_name="tower0/group1/block1/conv1/bn/batchnorm/mul_1"}
  %param_1.534 = f32[128]{0} parameter(1)
  %bitcast.518 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.534), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv1/bn/Reshape_1"}
  %param_3.381 = f32[128]{0} parameter(3)
  %bitcast.591 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.381), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv1/bn/Reshape_2"}
  %multiply.157 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.591, f32[1,128,1,1]{3,2,1,0} %param_0.464), metadata={op_type="Mul" op_name="tower0/group1/block1/conv1/bn/batchnorm/mul_2"}
  %subtract.74 = f32[1,128,1,1]{3,2,1,0} subtract(f32[1,128,1,1]{3,2,1,0} %bitcast.518, f32[1,128,1,1]{3,2,1,0} %multiply.157), metadata={op_type="Sub" op_name="tower0/group1/block1/conv1/bn/batchnorm/sub"}
  %bitcast.517 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %subtract.74), metadata={op_type="Add" op_name="tower0/group1/block1/conv1/bn/batchnorm/add_1"}
  %broadcast.237 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.517), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block1/conv1/bn/batchnorm/add_1"}
  %add.174 = f32[1,128,100,100]{3,2,1,0} add(f32[1,128,100,100]{3,2,1,0} %multiply.158, f32[1,128,100,100]{3,2,1,0} %broadcast.237), metadata={op_type="Add" op_name="tower0/group1/block1/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.40 = f32[1,128,100,100]{3,2,1,0} maximum(f32[1,128,100,100]{3,2,1,0} %broadcast.348, f32[1,128,100,100]{3,2,1,0} %add.174), metadata={op_type="Relu" op_name="tower0/group1/block1/conv1/Relu"}
}

%fused_computation.164 (param_0.335: f32[1,128,1,1], param_1.349: f32[128]) -> f32[1,128,1,1] {
  %param_1.349 = f32[128]{0} parameter(1)
  %bitcast.520 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.349), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv1/bn/Reshape"}
  %param_0.335 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.159 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.520, f32[1,128,1,1]{3,2,1,0} %param_0.335), metadata={op_type="Mul" op_name="tower0/group1/block1/conv1/bn/batchnorm/mul"}
}

%fused_computation.165 (param_0.338: f32[128]) -> f32[1,128,1,1] {
  %param_0.338 = f32[128]{0} parameter(0)
  %constant_247 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.349 = f32[128]{0} broadcast(f32[] %constant_247), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add"}
  %add.175 = f32[128]{0} add(f32[128]{0} %param_0.338, f32[128]{0} %broadcast.349), metadata={op_type="Add" op_name="tower0/group1/block1/conv1/bn/batchnorm/add"}
  %rsqrt.90 = f32[128]{0} rsqrt(f32[128]{0} %add.175), metadata={op_type="Rsqrt" op_name="tower0/group1/block1/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.521 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %rsqrt.90), metadata={op_type="Rsqrt" op_name="tower0/group1/block1/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.166 (param_0.465: f32[1,512,1,1], param_1.537: f32[512], param_2.420: f32[1,512,100,100], param_3.383: f32[1,512,1,1], param_4.317: f32[512], param_5.218: f32[1,512,100,100], param_6.125: f32[512], param_7.65: f32[512]) -> f32[1,512,100,100] {
  %constant_248 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.350 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[] %constant_248), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/output"}
  %param_5.218 = f32[1,512,100,100]{3,2,1,0} parameter(5)
  %param_3.383 = f32[1,512,1,1]{3,2,1,0} parameter(3)
  %bitcast.527 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_3.383), metadata={op_type="Mul" op_name="tower0/group1/block0/conv3/bn/batchnorm/mul_1"}
  %broadcast.242 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.527), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block0/conv3/bn/batchnorm/mul_1"}
  %multiply.163 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_5.218, f32[1,512,100,100]{3,2,1,0} %broadcast.242), metadata={op_type="Mul" op_name="tower0/group1/block0/conv3/bn/batchnorm/mul_1"}
  %param_4.317 = f32[512]{0} parameter(4)
  %bitcast.526 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_4.317), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv3/bn/Reshape_1"}
  %param_6.125 = f32[512]{0} parameter(6)
  %bitcast.592 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_6.125), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv3/bn/Reshape_2"}
  %multiply.162 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.592, f32[1,512,1,1]{3,2,1,0} %param_3.383), metadata={op_type="Mul" op_name="tower0/group1/block0/conv3/bn/batchnorm/mul_2"}
  %subtract.76 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.526, f32[1,512,1,1]{3,2,1,0} %multiply.162), metadata={op_type="Sub" op_name="tower0/group1/block0/conv3/bn/batchnorm/sub"}
  %bitcast.525 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.76), metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add_1"}
  %broadcast.241 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.525), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add_1"}
  %add.178 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %multiply.163, f32[1,512,100,100]{3,2,1,0} %broadcast.241), metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add_1"}
  %param_2.420 = f32[1,512,100,100]{3,2,1,0} parameter(2)
  %param_0.465 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  %bitcast.524 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %param_0.465), metadata={op_type="Mul" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/mul_1"}
  %broadcast.240 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.524), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/mul_1"}
  %multiply.161 = f32[1,512,100,100]{3,2,1,0} multiply(f32[1,512,100,100]{3,2,1,0} %param_2.420, f32[1,512,100,100]{3,2,1,0} %broadcast.240), metadata={op_type="Mul" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/mul_1"}
  %param_1.537 = f32[512]{0} parameter(1)
  %bitcast.523 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.537), metadata={op_type="Reshape" op_name="tower0/group1/block0/convshortcut/bn/Reshape_1"}
  %param_7.65 = f32[512]{0} parameter(7)
  %bitcast.593 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_7.65), metadata={op_type="Reshape" op_name="tower0/group1/block0/convshortcut/bn/Reshape_2"}
  %multiply.160 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.593, f32[1,512,1,1]{3,2,1,0} %param_0.465), metadata={op_type="Mul" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/mul_2"}
  %subtract.75 = f32[1,512,1,1]{3,2,1,0} subtract(f32[1,512,1,1]{3,2,1,0} %bitcast.523, f32[1,512,1,1]{3,2,1,0} %multiply.160), metadata={op_type="Sub" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/sub"}
  %bitcast.522 = f32[1,512]{1,0} bitcast(f32[1,512,1,1]{3,2,1,0} %subtract.75), metadata={op_type="Add" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/add_1"}
  %broadcast.239 = f32[1,512,100,100]{3,2,1,0} broadcast(f32[1,512]{1,0} %bitcast.522), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/add_1"}
  %add.177 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %multiply.161, f32[1,512,100,100]{3,2,1,0} %broadcast.239), metadata={op_type="Add" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/add_1"}
  %add.176 = f32[1,512,100,100]{3,2,1,0} add(f32[1,512,100,100]{3,2,1,0} %add.178, f32[1,512,100,100]{3,2,1,0} %add.177), metadata={op_type="Add" op_name="tower0/group1/block0/add"}
  ROOT %maximum.41 = f32[1,512,100,100]{3,2,1,0} maximum(f32[1,512,100,100]{3,2,1,0} %broadcast.350, f32[1,512,100,100]{3,2,1,0} %add.176), metadata={op_type="Relu" op_name="tower0/group1/block0/output"}
}

%fused_computation.167 (param_0.341: f32[1,512,1,1], param_1.355: f32[512]) -> f32[1,512,1,1] {
  %param_1.355 = f32[512]{0} parameter(1)
  %bitcast.528 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.355), metadata={op_type="Reshape" op_name="tower0/group1/block0/convshortcut/bn/Reshape"}
  %param_0.341 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.164 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.528, f32[1,512,1,1]{3,2,1,0} %param_0.341), metadata={op_type="Mul" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/mul"}
}

%fused_computation.168 (param_0.344: f32[512]) -> f32[1,512,1,1] {
  %param_0.344 = f32[512]{0} parameter(0)
  %constant_250 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.352 = f32[512]{0} broadcast(f32[] %constant_250), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %add.179 = f32[512]{0} add(f32[512]{0} %param_0.344, f32[512]{0} %broadcast.352), metadata={op_type="Add" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/add"}
  %rsqrt.91 = f32[512]{0} rsqrt(f32[512]{0} %add.179), metadata={op_type="Rsqrt" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.529 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.91), metadata={op_type="Rsqrt" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/Rsqrt"}
}

%fused_computation.169 (param_0.346: f32[1,512,1,1], param_1.358: f32[512]) -> f32[1,512,1,1] {
  %param_1.358 = f32[512]{0} parameter(1)
  %bitcast.530 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %param_1.358), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv3/bn/Reshape"}
  %param_0.346 = f32[1,512,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.165 = f32[1,512,1,1]{3,2,1,0} multiply(f32[1,512,1,1]{3,2,1,0} %bitcast.530, f32[1,512,1,1]{3,2,1,0} %param_0.346), metadata={op_type="Mul" op_name="tower0/group1/block0/conv3/bn/batchnorm/mul"}
}

%fused_computation.170 (param_0.349: f32[512]) -> f32[1,512,1,1] {
  %param_0.349 = f32[512]{0} parameter(0)
  %constant_249 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.351 = f32[512]{0} broadcast(f32[] %constant_249), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %add.180 = f32[512]{0} add(f32[512]{0} %param_0.349, f32[512]{0} %broadcast.351), metadata={op_type="Add" op_name="tower0/group1/block0/conv3/bn/batchnorm/add"}
  %rsqrt.92 = f32[512]{0} rsqrt(f32[512]{0} %add.180), metadata={op_type="Rsqrt" op_name="tower0/group1/block0/conv3/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.531 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %rsqrt.92), metadata={op_type="Rsqrt" op_name="tower0/group1/block0/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.171 (param_0.466: f32[1,128,1,1], param_1.541: f32[128], param_2.422: f32[1,128,100,100], param_3.385: f32[128]) -> f32[1,128,100,100] {
  %constant_251 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.353 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[] %constant_251), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/conv2/Relu"}
  %param_2.422 = f32[1,128,100,100]{3,2,1,0} parameter(2)
  %param_0.466 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %bitcast.534 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_0.466), metadata={op_type="Mul" op_name="tower0/group1/block0/conv2/bn/batchnorm/mul_1"}
  %broadcast.244 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.534), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block0/conv2/bn/batchnorm/mul_1"}
  %multiply.167 = f32[1,128,100,100]{3,2,1,0} multiply(f32[1,128,100,100]{3,2,1,0} %param_2.422, f32[1,128,100,100]{3,2,1,0} %broadcast.244), metadata={op_type="Mul" op_name="tower0/group1/block0/conv2/bn/batchnorm/mul_1"}
  %param_1.541 = f32[128]{0} parameter(1)
  %bitcast.533 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.541), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv2/bn/Reshape_1"}
  %param_3.385 = f32[128]{0} parameter(3)
  %bitcast.594 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.385), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv2/bn/Reshape_2"}
  %multiply.166 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.594, f32[1,128,1,1]{3,2,1,0} %param_0.466), metadata={op_type="Mul" op_name="tower0/group1/block0/conv2/bn/batchnorm/mul_2"}
  %subtract.77 = f32[1,128,1,1]{3,2,1,0} subtract(f32[1,128,1,1]{3,2,1,0} %bitcast.533, f32[1,128,1,1]{3,2,1,0} %multiply.166), metadata={op_type="Sub" op_name="tower0/group1/block0/conv2/bn/batchnorm/sub"}
  %bitcast.532 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %subtract.77), metadata={op_type="Add" op_name="tower0/group1/block0/conv2/bn/batchnorm/add_1"}
  %broadcast.243 = f32[1,128,100,100]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.532), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block0/conv2/bn/batchnorm/add_1"}
  %add.181 = f32[1,128,100,100]{3,2,1,0} add(f32[1,128,100,100]{3,2,1,0} %multiply.167, f32[1,128,100,100]{3,2,1,0} %broadcast.243), metadata={op_type="Add" op_name="tower0/group1/block0/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.42 = f32[1,128,100,100]{3,2,1,0} maximum(f32[1,128,100,100]{3,2,1,0} %broadcast.353, f32[1,128,100,100]{3,2,1,0} %add.181), metadata={op_type="Relu" op_name="tower0/group1/block0/conv2/Relu"}
}

%fused_computation.172 (param_0.352: f32[1,128,1,1], param_1.364: f32[128]) -> f32[1,128,1,1] {
  %param_1.364 = f32[128]{0} parameter(1)
  %bitcast.535 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.364), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv2/bn/Reshape"}
  %param_0.352 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.168 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.535, f32[1,128,1,1]{3,2,1,0} %param_0.352), metadata={op_type="Mul" op_name="tower0/group1/block0/conv2/bn/batchnorm/mul"}
}

%fused_computation.173 (param_0.355: f32[128]) -> f32[1,128,1,1] {
  %param_0.355 = f32[128]{0} parameter(0)
  %constant_253 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.355 = f32[128]{0} broadcast(f32[] %constant_253), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add"}
  %add.182 = f32[128]{0} add(f32[128]{0} %param_0.355, f32[128]{0} %broadcast.355), metadata={op_type="Add" op_name="tower0/group1/block0/conv2/bn/batchnorm/add"}
  %rsqrt.93 = f32[128]{0} rsqrt(f32[128]{0} %add.182), metadata={op_type="Rsqrt" op_name="tower0/group1/block0/conv2/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.536 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %rsqrt.93), metadata={op_type="Rsqrt" op_name="tower0/group1/block0/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.174 (param_0.467: f32[1,128,1,1], param_1.542: f32[128], param_2.423: f32[1,128,200,200], param_3.386: f32[128]) -> f32[1,128,200,200] {
  %constant_166 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.247 = f32[1,128,200,200]{3,2,1,0} broadcast(f32[] %constant_166), dimensions={}, metadata={op_type="Relu" op_name="tower0/group1/block0/conv1/Relu"}
  %param_2.423 = f32[1,128,200,200]{3,2,1,0} parameter(2)
  %param_0.467 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  %bitcast.539 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %param_0.467), metadata={op_type="Mul" op_name="tower0/group1/block0/conv1/bn/batchnorm/mul_1"}
  %broadcast.246 = f32[1,128,200,200]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.539), dimensions={0,1}, metadata={op_type="Mul" op_name="tower0/group1/block0/conv1/bn/batchnorm/mul_1"}
  %multiply.170 = f32[1,128,200,200]{3,2,1,0} multiply(f32[1,128,200,200]{3,2,1,0} %param_2.423, f32[1,128,200,200]{3,2,1,0} %broadcast.246), metadata={op_type="Mul" op_name="tower0/group1/block0/conv1/bn/batchnorm/mul_1"}
  %param_1.542 = f32[128]{0} parameter(1)
  %bitcast.538 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.542), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv1/bn/Reshape_1"}
  %param_3.386 = f32[128]{0} parameter(3)
  %bitcast.595 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_3.386), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv1/bn/Reshape_2"}
  %multiply.169 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.595, f32[1,128,1,1]{3,2,1,0} %param_0.467), metadata={op_type="Mul" op_name="tower0/group1/block0/conv1/bn/batchnorm/mul_2"}
  %subtract.78 = f32[1,128,1,1]{3,2,1,0} subtract(f32[1,128,1,1]{3,2,1,0} %bitcast.538, f32[1,128,1,1]{3,2,1,0} %multiply.169), metadata={op_type="Sub" op_name="tower0/group1/block0/conv1/bn/batchnorm/sub"}
  %bitcast.537 = f32[1,128]{1,0} bitcast(f32[1,128,1,1]{3,2,1,0} %subtract.78), metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add_1"}
  %broadcast.245 = f32[1,128,200,200]{3,2,1,0} broadcast(f32[1,128]{1,0} %bitcast.537), dimensions={0,1}, metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add_1"}
  %add.183 = f32[1,128,200,200]{3,2,1,0} add(f32[1,128,200,200]{3,2,1,0} %multiply.170, f32[1,128,200,200]{3,2,1,0} %broadcast.245), metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.43 = f32[1,128,200,200]{3,2,1,0} maximum(f32[1,128,200,200]{3,2,1,0} %broadcast.247, f32[1,128,200,200]{3,2,1,0} %add.183), metadata={op_type="Relu" op_name="tower0/group1/block0/conv1/Relu"}
}

%fused_computation.175 (param_0.359: f32[1,128,1,1], param_1.371: f32[128]) -> f32[1,128,1,1] {
  %param_1.371 = f32[128]{0} parameter(1)
  %bitcast.540 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %param_1.371), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv1/bn/Reshape"}
  %param_0.359 = f32[1,128,1,1]{3,2,1,0} parameter(0)
  ROOT %multiply.171 = f32[1,128,1,1]{3,2,1,0} multiply(f32[1,128,1,1]{3,2,1,0} %bitcast.540, f32[1,128,1,1]{3,2,1,0} %param_0.359), metadata={op_type="Mul" op_name="tower0/group1/block0/conv1/bn/batchnorm/mul"}
}

%fused_computation.176 (param_0.362: f32[128]) -> f32[1,128,1,1] {
  %param_0.362 = f32[128]{0} parameter(0)
  %constant_252 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.354 = f32[128]{0} broadcast(f32[] %constant_252), dimensions={}, metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add"}
  %add.184 = f32[128]{0} add(f32[128]{0} %param_0.362, f32[128]{0} %broadcast.354), metadata={op_type="Add" op_name="tower0/group1/block0/conv1/bn/batchnorm/add"}
  %rsqrt.94 = f32[128]{0} rsqrt(f32[128]{0} %add.184), metadata={op_type="Rsqrt" op_name="tower0/group1/block0/conv1/bn/batchnorm/Rsqrt"}
  ROOT %bitcast.541 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %rsqrt.94), metadata={op_type="Rsqrt" op_name="tower0/group1/block0/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.177 (param_0.468: f32[256], param_1.545: f32[256], param_2.424: f32[1,256,200,200], param_3.387: f32[1,256,200,200], param_4.320: f32[256], param_5.219: f32[256]) -> f32[1,256,200,200] {
  %constant_254 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.356 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[] %constant_254), dimensions={}, metadata={op_type="Relu" op_name="tower0/group0/block2/output"}
  %param_2.424 = f32[1,256,200,200]{3,2,1,0} parameter(2)
  %param_3.387 = f32[1,256,200,200]{3,2,1,0} parameter(3)
  %param_4.320 = f32[256]{0} parameter(4)
  %param_5.219 = f32[256]{0} parameter(5)
  %multiply.174 = f32[256]{0} multiply(f32[256]{0} %param_4.320, f32[256]{0} %param_5.219), metadata={op_type="Mul" op_name="tower0/group0/block2/conv3/bn/batchnorm/mul"}
  %broadcast.249 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[256]{0} %multiply.174), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block2/conv3/bn/batchnorm/mul_1"}
  %multiply.173 = f32[1,256,200,200]{3,2,1,0} multiply(f32[1,256,200,200]{3,2,1,0} %param_3.387, f32[1,256,200,200]{3,2,1,0} %broadcast.249), metadata={op_type="Mul" op_name="tower0/group0/block2/conv3/bn/batchnorm/mul_1"}
  %add.186 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %param_2.424, f32[1,256,200,200]{3,2,1,0} %multiply.173), metadata={op_type="AddN" op_name="tower0/group0/block2/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_0.468 = f32[256]{0} parameter(0)
  %param_1.545 = f32[256]{0} parameter(1)
  %multiply.172 = f32[256]{0} multiply(f32[256]{0} %param_1.545, f32[256]{0} %multiply.174), metadata={op_type="Mul" op_name="tower0/group0/block2/conv3/bn/batchnorm/mul_2"}
  %subtract.79 = f32[256]{0} subtract(f32[256]{0} %param_0.468, f32[256]{0} %multiply.172), metadata={op_type="Sub" op_name="tower0/group0/block2/conv3/bn/batchnorm/sub"}
  %broadcast.248 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[256]{0} %subtract.79), dimensions={1}, metadata={op_type="Add" op_name="tower0/group0/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.185 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %add.186, f32[1,256,200,200]{3,2,1,0} %broadcast.248), metadata={op_type="Add" op_name="tower0/group0/block2/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.44 = f32[1,256,200,200]{3,2,1,0} maximum(f32[1,256,200,200]{3,2,1,0} %broadcast.356, f32[1,256,200,200]{3,2,1,0} %add.185), metadata={op_type="Relu" op_name="tower0/group0/block2/output"}
}

%fused_computation.178 (param_0.365: f32[256]) -> f32[256] {
  %param_0.365 = f32[256]{0} parameter(0)
  %constant_255 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.357 = f32[256]{0} broadcast(f32[] %constant_255), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.187 = f32[256]{0} add(f32[256]{0} %param_0.365, f32[256]{0} %broadcast.357), metadata={op_type="Add" op_name="tower0/group0/block2/conv3/bn/batchnorm/add"}
  ROOT %rsqrt.95 = f32[256]{0} rsqrt(f32[256]{0} %add.187), metadata={op_type="Rsqrt" op_name="tower0/group0/block2/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.179 (param_0.469: f32[64], param_1.547: f32[64], param_2.425: f32[1,64,200,200], param_3.388: f32[64], param_4.321: f32[64]) -> f32[1,64,200,200] {
  %constant_256 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.358 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[] %constant_256), dimensions={}, metadata={op_type="Relu" op_name="tower0/group0/block0/conv2/Relu"}
  %param_2.425 = f32[1,64,200,200]{3,2,1,0} parameter(2)
  %param_3.388 = f32[64]{0} parameter(3)
  %param_4.321 = f32[64]{0} parameter(4)
  %multiply.177 = f32[64]{0} multiply(f32[64]{0} %param_3.388, f32[64]{0} %param_4.321), metadata={op_type="Mul" op_name="tower0/group0/block2/conv2/bn/batchnorm/mul"}
  %broadcast.251 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %multiply.177), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block2/conv2/bn/batchnorm/mul_1"}
  %multiply.176 = f32[1,64,200,200]{3,2,1,0} multiply(f32[1,64,200,200]{3,2,1,0} %param_2.425, f32[1,64,200,200]{3,2,1,0} %broadcast.251), metadata={op_type="Mul" op_name="tower0/group0/block2/conv2/bn/batchnorm/mul_1"}
  %param_0.469 = f32[64]{0} parameter(0)
  %param_1.547 = f32[64]{0} parameter(1)
  %multiply.175 = f32[64]{0} multiply(f32[64]{0} %param_1.547, f32[64]{0} %multiply.177), metadata={op_type="Mul" op_name="tower0/group0/block2/conv2/bn/batchnorm/mul_2"}
  %subtract.80 = f32[64]{0} subtract(f32[64]{0} %param_0.469, f32[64]{0} %multiply.175), metadata={op_type="Sub" op_name="tower0/group0/block2/conv2/bn/batchnorm/sub"}
  %broadcast.250 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %subtract.80), dimensions={1}, metadata={op_type="Add" op_name="tower0/group0/block2/conv2/bn/batchnorm/add_1"}
  %add.188 = f32[1,64,200,200]{3,2,1,0} add(f32[1,64,200,200]{3,2,1,0} %multiply.176, f32[1,64,200,200]{3,2,1,0} %broadcast.250), metadata={op_type="Add" op_name="tower0/group0/block2/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.45 = f32[1,64,200,200]{3,2,1,0} maximum(f32[1,64,200,200]{3,2,1,0} %broadcast.358, f32[1,64,200,200]{3,2,1,0} %add.188), metadata={op_type="Relu" op_name="tower0/group0/block2/conv2/Relu"}
}

%fused_computation.180 (param_0.368: f32[64]) -> f32[64] {
  %param_0.368 = f32[64]{0} parameter(0)
  %constant_257 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.359 = f32[64]{0} broadcast(f32[] %constant_257), dimensions={}, metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %add.189 = f32[64]{0} add(f32[64]{0} %param_0.368, f32[64]{0} %broadcast.359), metadata={op_type="Add" op_name="tower0/group0/block2/conv2/bn/batchnorm/add"}
  ROOT %rsqrt.96 = f32[64]{0} rsqrt(f32[64]{0} %add.189), metadata={op_type="Rsqrt" op_name="tower0/group0/block2/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.181 (param_0.470: f32[64], param_1.549: f32[64], param_2.426: f32[1,64,200,200], param_3.389: f32[64], param_4.322: f32[64]) -> f32[1,64,200,200] {
  %constant_258 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.360 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[] %constant_258), dimensions={}, metadata={op_type="Relu" op_name="tower0/group0/block0/conv2/Relu"}
  %param_2.426 = f32[1,64,200,200]{3,2,1,0} parameter(2)
  %param_3.389 = f32[64]{0} parameter(3)
  %param_4.322 = f32[64]{0} parameter(4)
  %multiply.180 = f32[64]{0} multiply(f32[64]{0} %param_3.389, f32[64]{0} %param_4.322), metadata={op_type="Mul" op_name="tower0/group0/block2/conv1/bn/batchnorm/mul"}
  %broadcast.253 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %multiply.180), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block2/conv1/bn/batchnorm/mul_1"}
  %multiply.179 = f32[1,64,200,200]{3,2,1,0} multiply(f32[1,64,200,200]{3,2,1,0} %param_2.426, f32[1,64,200,200]{3,2,1,0} %broadcast.253), metadata={op_type="Mul" op_name="tower0/group0/block2/conv1/bn/batchnorm/mul_1"}
  %param_0.470 = f32[64]{0} parameter(0)
  %param_1.549 = f32[64]{0} parameter(1)
  %multiply.178 = f32[64]{0} multiply(f32[64]{0} %param_1.549, f32[64]{0} %multiply.180), metadata={op_type="Mul" op_name="tower0/group0/block2/conv1/bn/batchnorm/mul_2"}
  %subtract.81 = f32[64]{0} subtract(f32[64]{0} %param_0.470, f32[64]{0} %multiply.178), metadata={op_type="Sub" op_name="tower0/group0/block2/conv1/bn/batchnorm/sub"}
  %broadcast.252 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %subtract.81), dimensions={1}, metadata={op_type="Add" op_name="tower0/group0/block2/conv1/bn/batchnorm/add_1"}
  %add.190 = f32[1,64,200,200]{3,2,1,0} add(f32[1,64,200,200]{3,2,1,0} %multiply.179, f32[1,64,200,200]{3,2,1,0} %broadcast.252), metadata={op_type="Add" op_name="tower0/group0/block2/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.46 = f32[1,64,200,200]{3,2,1,0} maximum(f32[1,64,200,200]{3,2,1,0} %broadcast.360, f32[1,64,200,200]{3,2,1,0} %add.190), metadata={op_type="Relu" op_name="tower0/group0/block2/conv1/Relu"}
}

%fused_computation.182 (param_0.371: f32[64]) -> f32[64] {
  %param_0.371 = f32[64]{0} parameter(0)
  %constant_259 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.361 = f32[64]{0} broadcast(f32[] %constant_259), dimensions={}, metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %add.191 = f32[64]{0} add(f32[64]{0} %param_0.371, f32[64]{0} %broadcast.361), metadata={op_type="Add" op_name="tower0/group0/block2/conv1/bn/batchnorm/add"}
  ROOT %rsqrt.97 = f32[64]{0} rsqrt(f32[64]{0} %add.191), metadata={op_type="Rsqrt" op_name="tower0/group0/block2/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.183 (param_0.471: f32[256], param_1.551: f32[256], param_2.427: f32[1,256,200,200], param_3.390: f32[1,256,200,200], param_4.323: f32[256], param_5.222: f32[256]) -> f32[1,256,200,200] {
  %constant_260 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.362 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[] %constant_260), dimensions={}, metadata={op_type="Relu" op_name="tower0/group0/block2/output"}
  %param_2.427 = f32[1,256,200,200]{3,2,1,0} parameter(2)
  %param_3.390 = f32[1,256,200,200]{3,2,1,0} parameter(3)
  %param_4.323 = f32[256]{0} parameter(4)
  %param_5.222 = f32[256]{0} parameter(5)
  %multiply.183 = f32[256]{0} multiply(f32[256]{0} %param_4.323, f32[256]{0} %param_5.222), metadata={op_type="Mul" op_name="tower0/group0/block1/conv3/bn/batchnorm/mul"}
  %broadcast.255 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[256]{0} %multiply.183), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block1/conv3/bn/batchnorm/mul_1"}
  %multiply.182 = f32[1,256,200,200]{3,2,1,0} multiply(f32[1,256,200,200]{3,2,1,0} %param_3.390, f32[1,256,200,200]{3,2,1,0} %broadcast.255), metadata={op_type="Mul" op_name="tower0/group0/block1/conv3/bn/batchnorm/mul_1"}
  %add.193 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %param_2.427, f32[1,256,200,200]{3,2,1,0} %multiply.182), metadata={op_type="AddN" op_name="tower0/group0/block1/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %param_0.471 = f32[256]{0} parameter(0)
  %param_1.551 = f32[256]{0} parameter(1)
  %multiply.181 = f32[256]{0} multiply(f32[256]{0} %param_1.551, f32[256]{0} %multiply.183), metadata={op_type="Mul" op_name="tower0/group0/block1/conv3/bn/batchnorm/mul_2"}
  %subtract.82 = f32[256]{0} subtract(f32[256]{0} %param_0.471, f32[256]{0} %multiply.181), metadata={op_type="Sub" op_name="tower0/group0/block1/conv3/bn/batchnorm/sub"}
  %broadcast.254 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[256]{0} %subtract.82), dimensions={1}, metadata={op_type="Add" op_name="tower0/group0/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  %add.192 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %add.193, f32[1,256,200,200]{3,2,1,0} %broadcast.254), metadata={op_type="Add" op_name="tower0/group0/block1/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.47 = f32[1,256,200,200]{3,2,1,0} maximum(f32[1,256,200,200]{3,2,1,0} %broadcast.362, f32[1,256,200,200]{3,2,1,0} %add.192), metadata={op_type="Relu" op_name="tower0/group0/block1/output"}
}

%fused_computation.184 (param_0.374: f32[256]) -> f32[256] {
  %param_0.374 = f32[256]{0} parameter(0)
  %constant_261 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.363 = f32[256]{0} broadcast(f32[] %constant_261), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.194 = f32[256]{0} add(f32[256]{0} %param_0.374, f32[256]{0} %broadcast.363), metadata={op_type="Add" op_name="tower0/group0/block1/conv3/bn/batchnorm/add"}
  ROOT %rsqrt.98 = f32[256]{0} rsqrt(f32[256]{0} %add.194), metadata={op_type="Rsqrt" op_name="tower0/group0/block1/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.185 (param_0.472: f32[64], param_1.553: f32[64], param_2.428: f32[1,64,200,200], param_3.391: f32[64], param_4.324: f32[64]) -> f32[1,64,200,200] {
  %constant_262 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.364 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[] %constant_262), dimensions={}, metadata={op_type="Relu" op_name="tower0/group0/block0/conv2/Relu"}
  %param_2.428 = f32[1,64,200,200]{3,2,1,0} parameter(2)
  %param_3.391 = f32[64]{0} parameter(3)
  %param_4.324 = f32[64]{0} parameter(4)
  %multiply.186 = f32[64]{0} multiply(f32[64]{0} %param_3.391, f32[64]{0} %param_4.324), metadata={op_type="Mul" op_name="tower0/group0/block1/conv2/bn/batchnorm/mul"}
  %broadcast.257 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %multiply.186), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block1/conv2/bn/batchnorm/mul_1"}
  %multiply.185 = f32[1,64,200,200]{3,2,1,0} multiply(f32[1,64,200,200]{3,2,1,0} %param_2.428, f32[1,64,200,200]{3,2,1,0} %broadcast.257), metadata={op_type="Mul" op_name="tower0/group0/block1/conv2/bn/batchnorm/mul_1"}
  %param_0.472 = f32[64]{0} parameter(0)
  %param_1.553 = f32[64]{0} parameter(1)
  %multiply.184 = f32[64]{0} multiply(f32[64]{0} %param_1.553, f32[64]{0} %multiply.186), metadata={op_type="Mul" op_name="tower0/group0/block1/conv2/bn/batchnorm/mul_2"}
  %subtract.83 = f32[64]{0} subtract(f32[64]{0} %param_0.472, f32[64]{0} %multiply.184), metadata={op_type="Sub" op_name="tower0/group0/block1/conv2/bn/batchnorm/sub"}
  %broadcast.256 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %subtract.83), dimensions={1}, metadata={op_type="Add" op_name="tower0/group0/block1/conv2/bn/batchnorm/add_1"}
  %add.195 = f32[1,64,200,200]{3,2,1,0} add(f32[1,64,200,200]{3,2,1,0} %multiply.185, f32[1,64,200,200]{3,2,1,0} %broadcast.256), metadata={op_type="Add" op_name="tower0/group0/block1/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.48 = f32[1,64,200,200]{3,2,1,0} maximum(f32[1,64,200,200]{3,2,1,0} %broadcast.364, f32[1,64,200,200]{3,2,1,0} %add.195), metadata={op_type="Relu" op_name="tower0/group0/block1/conv2/Relu"}
}

%fused_computation.186 (param_0.377: f32[64]) -> f32[64] {
  %param_0.377 = f32[64]{0} parameter(0)
  %constant_263 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.365 = f32[64]{0} broadcast(f32[] %constant_263), dimensions={}, metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %add.196 = f32[64]{0} add(f32[64]{0} %param_0.377, f32[64]{0} %broadcast.365), metadata={op_type="Add" op_name="tower0/group0/block1/conv2/bn/batchnorm/add"}
  ROOT %rsqrt.99 = f32[64]{0} rsqrt(f32[64]{0} %add.196), metadata={op_type="Rsqrt" op_name="tower0/group0/block1/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.187 (param_0.473: f32[64], param_1.555: f32[64], param_2.429: f32[1,64,200,200], param_3.392: f32[64], param_4.325: f32[64]) -> f32[1,64,200,200] {
  %constant_264 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.366 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[] %constant_264), dimensions={}, metadata={op_type="Relu" op_name="tower0/group0/block0/conv2/Relu"}
  %param_2.429 = f32[1,64,200,200]{3,2,1,0} parameter(2)
  %param_3.392 = f32[64]{0} parameter(3)
  %param_4.325 = f32[64]{0} parameter(4)
  %multiply.189 = f32[64]{0} multiply(f32[64]{0} %param_3.392, f32[64]{0} %param_4.325), metadata={op_type="Mul" op_name="tower0/group0/block1/conv1/bn/batchnorm/mul"}
  %broadcast.259 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %multiply.189), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block1/conv1/bn/batchnorm/mul_1"}
  %multiply.188 = f32[1,64,200,200]{3,2,1,0} multiply(f32[1,64,200,200]{3,2,1,0} %param_2.429, f32[1,64,200,200]{3,2,1,0} %broadcast.259), metadata={op_type="Mul" op_name="tower0/group0/block1/conv1/bn/batchnorm/mul_1"}
  %param_0.473 = f32[64]{0} parameter(0)
  %param_1.555 = f32[64]{0} parameter(1)
  %multiply.187 = f32[64]{0} multiply(f32[64]{0} %param_1.555, f32[64]{0} %multiply.189), metadata={op_type="Mul" op_name="tower0/group0/block1/conv1/bn/batchnorm/mul_2"}
  %subtract.84 = f32[64]{0} subtract(f32[64]{0} %param_0.473, f32[64]{0} %multiply.187), metadata={op_type="Sub" op_name="tower0/group0/block1/conv1/bn/batchnorm/sub"}
  %broadcast.258 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %subtract.84), dimensions={1}, metadata={op_type="Add" op_name="tower0/group0/block1/conv1/bn/batchnorm/add_1"}
  %add.197 = f32[1,64,200,200]{3,2,1,0} add(f32[1,64,200,200]{3,2,1,0} %multiply.188, f32[1,64,200,200]{3,2,1,0} %broadcast.258), metadata={op_type="Add" op_name="tower0/group0/block1/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.49 = f32[1,64,200,200]{3,2,1,0} maximum(f32[1,64,200,200]{3,2,1,0} %broadcast.366, f32[1,64,200,200]{3,2,1,0} %add.197), metadata={op_type="Relu" op_name="tower0/group0/block1/conv1/Relu"}
}

%fused_computation.188 (param_0.380: f32[64]) -> f32[64] {
  %param_0.380 = f32[64]{0} parameter(0)
  %constant_265 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.367 = f32[64]{0} broadcast(f32[] %constant_265), dimensions={}, metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %add.198 = f32[64]{0} add(f32[64]{0} %param_0.380, f32[64]{0} %broadcast.367), metadata={op_type="Add" op_name="tower0/group0/block1/conv1/bn/batchnorm/add"}
  ROOT %rsqrt.100 = f32[64]{0} rsqrt(f32[64]{0} %add.198), metadata={op_type="Rsqrt" op_name="tower0/group0/block1/conv1/bn/batchnorm/Rsqrt"}
}

%fused_computation.189 (param_0.474: f32[1,256,200,200], param_1.557: f32[1,256,200,200], param_2.430: f32[256], param_3.393: f32[256], param_4.326: f32[256], param_5.225: f32[256], param_6.128: f32[256], param_7.66: f32[256], param_8.21: f32[256], param_9.9: f32[256]) -> f32[1,256,200,200] {
  %constant_266 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.368 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[] %constant_266), dimensions={}, metadata={op_type="Relu" op_name="tower0/group0/block2/output"}
  %param_6.128 = f32[256]{0} parameter(6)
  %param_7.66 = f32[256]{0} parameter(7)
  %param_8.21 = f32[256]{0} parameter(8)
  %param_9.9 = f32[256]{0} parameter(9)
  %multiply.195 = f32[256]{0} multiply(f32[256]{0} %param_8.21, f32[256]{0} %param_9.9), metadata={op_type="Mul" op_name="tower0/group0/block0/conv3/bn/batchnorm/mul"}
  %multiply.194 = f32[256]{0} multiply(f32[256]{0} %param_7.66, f32[256]{0} %multiply.195), metadata={op_type="Mul" op_name="tower0/group0/block0/conv3/bn/batchnorm/mul_2"}
  %subtract.86 = f32[256]{0} subtract(f32[256]{0} %param_6.128, f32[256]{0} %multiply.194), metadata={op_type="Sub" op_name="tower0/group0/block0/conv3/bn/batchnorm/sub"}
  %param_2.430 = f32[256]{0} parameter(2)
  %param_3.393 = f32[256]{0} parameter(3)
  %param_4.326 = f32[256]{0} parameter(4)
  %param_5.225 = f32[256]{0} parameter(5)
  %multiply.193 = f32[256]{0} multiply(f32[256]{0} %param_4.326, f32[256]{0} %param_5.225), metadata={op_type="Mul" op_name="tower0/group0/block0/convshortcut/bn/batchnorm/mul"}
  %multiply.192 = f32[256]{0} multiply(f32[256]{0} %param_3.393, f32[256]{0} %multiply.193), metadata={op_type="Mul" op_name="tower0/group0/block0/convshortcut/bn/batchnorm/mul_2"}
  %subtract.85 = f32[256]{0} subtract(f32[256]{0} %param_2.430, f32[256]{0} %multiply.192), metadata={op_type="Sub" op_name="tower0/group0/block0/convshortcut/bn/batchnorm/sub"}
  %add.201 = f32[256]{0} add(f32[256]{0} %subtract.86, f32[256]{0} %subtract.85), metadata={op_type="AddN" op_name="tower0/group0/block0/ArithmeticOptimizer/AddOpsRewrite_Leaf_0_add"}
  %broadcast.262 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[256]{0} %add.201), dimensions={1}, metadata={op_type="Add" op_name="tower0/group0/block0/ArithmeticOptimizer/AddOpsRewrite_add"}
  %param_1.557 = f32[1,256,200,200]{3,2,1,0} parameter(1)
  %broadcast.261 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[256]{0} %multiply.195), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block0/conv3/bn/batchnorm/mul_1"}
  %multiply.191 = f32[1,256,200,200]{3,2,1,0} multiply(f32[1,256,200,200]{3,2,1,0} %param_1.557, f32[1,256,200,200]{3,2,1,0} %broadcast.261), metadata={op_type="Mul" op_name="tower0/group0/block0/conv3/bn/batchnorm/mul_1"}
  %param_0.474 = f32[1,256,200,200]{3,2,1,0} parameter(0)
  %broadcast.260 = f32[1,256,200,200]{3,2,1,0} broadcast(f32[256]{0} %multiply.193), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block0/convshortcut/bn/batchnorm/mul_1"}
  %multiply.190 = f32[1,256,200,200]{3,2,1,0} multiply(f32[1,256,200,200]{3,2,1,0} %param_0.474, f32[1,256,200,200]{3,2,1,0} %broadcast.260), metadata={op_type="Mul" op_name="tower0/group0/block0/convshortcut/bn/batchnorm/mul_1"}
  %add.200 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %multiply.191, f32[1,256,200,200]{3,2,1,0} %multiply.190), metadata={op_type="AddN" op_name="tower0/group0/block0/ArithmeticOptimizer/AddOpsRewrite_Leaf_1_add"}
  %add.199 = f32[1,256,200,200]{3,2,1,0} add(f32[1,256,200,200]{3,2,1,0} %broadcast.262, f32[1,256,200,200]{3,2,1,0} %add.200), metadata={op_type="Add" op_name="tower0/group0/block0/ArithmeticOptimizer/AddOpsRewrite_add"}
  ROOT %maximum.50 = f32[1,256,200,200]{3,2,1,0} maximum(f32[1,256,200,200]{3,2,1,0} %broadcast.368, f32[1,256,200,200]{3,2,1,0} %add.199), metadata={op_type="Relu" op_name="tower0/group0/block0/output"}
}

%fused_computation.190 (param_0.383: f32[256]) -> f32[256] {
  %param_0.383 = f32[256]{0} parameter(0)
  %constant_268 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.370 = f32[256]{0} broadcast(f32[] %constant_268), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.202 = f32[256]{0} add(f32[256]{0} %param_0.383, f32[256]{0} %broadcast.370), metadata={op_type="Add" op_name="tower0/group0/block0/convshortcut/bn/batchnorm/add"}
  ROOT %rsqrt.101 = f32[256]{0} rsqrt(f32[256]{0} %add.202), metadata={op_type="Rsqrt" op_name="tower0/group0/block0/convshortcut/bn/batchnorm/Rsqrt"}
}

%fused_computation.191 (param_0.385: f32[256]) -> f32[256] {
  %param_0.385 = f32[256]{0} parameter(0)
  %constant_267 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.369 = f32[256]{0} broadcast(f32[] %constant_267), dimensions={}, metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  %add.203 = f32[256]{0} add(f32[256]{0} %param_0.385, f32[256]{0} %broadcast.369), metadata={op_type="Add" op_name="tower0/group0/block0/conv3/bn/batchnorm/add"}
  ROOT %rsqrt.102 = f32[256]{0} rsqrt(f32[256]{0} %add.203), metadata={op_type="Rsqrt" op_name="tower0/group0/block0/conv3/bn/batchnorm/Rsqrt"}
}

%fused_computation.192 (param_0.475: f32[64], param_1.560: f32[64], param_2.431: f32[1,64,200,200], param_3.394: f32[64], param_4.327: f32[64]) -> f32[1,64,200,200] {
  %constant_269 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.371 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[] %constant_269), dimensions={}, metadata={op_type="Relu" op_name="tower0/group0/block0/conv2/Relu"}
  %param_2.431 = f32[1,64,200,200]{3,2,1,0} parameter(2)
  %param_3.394 = f32[64]{0} parameter(3)
  %param_4.327 = f32[64]{0} parameter(4)
  %multiply.198 = f32[64]{0} multiply(f32[64]{0} %param_3.394, f32[64]{0} %param_4.327), metadata={op_type="Mul" op_name="tower0/group0/block0/conv2/bn/batchnorm/mul"}
  %broadcast.264 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %multiply.198), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block0/conv2/bn/batchnorm/mul_1"}
  %multiply.197 = f32[1,64,200,200]{3,2,1,0} multiply(f32[1,64,200,200]{3,2,1,0} %param_2.431, f32[1,64,200,200]{3,2,1,0} %broadcast.264), metadata={op_type="Mul" op_name="tower0/group0/block0/conv2/bn/batchnorm/mul_1"}
  %param_0.475 = f32[64]{0} parameter(0)
  %param_1.560 = f32[64]{0} parameter(1)
  %multiply.196 = f32[64]{0} multiply(f32[64]{0} %param_1.560, f32[64]{0} %multiply.198), metadata={op_type="Mul" op_name="tower0/group0/block0/conv2/bn/batchnorm/mul_2"}
  %subtract.87 = f32[64]{0} subtract(f32[64]{0} %param_0.475, f32[64]{0} %multiply.196), metadata={op_type="Sub" op_name="tower0/group0/block0/conv2/bn/batchnorm/sub"}
  %broadcast.263 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %subtract.87), dimensions={1}, metadata={op_type="Add" op_name="tower0/group0/block0/conv2/bn/batchnorm/add_1"}
  %add.204 = f32[1,64,200,200]{3,2,1,0} add(f32[1,64,200,200]{3,2,1,0} %multiply.197, f32[1,64,200,200]{3,2,1,0} %broadcast.263), metadata={op_type="Add" op_name="tower0/group0/block0/conv2/bn/batchnorm/add_1"}
  ROOT %maximum.51 = f32[1,64,200,200]{3,2,1,0} maximum(f32[1,64,200,200]{3,2,1,0} %broadcast.371, f32[1,64,200,200]{3,2,1,0} %add.204), metadata={op_type="Relu" op_name="tower0/group0/block0/conv2/Relu"}
}

%fused_computation.193 (param_0.388: f32[64]) -> f32[64] {
  %param_0.388 = f32[64]{0} parameter(0)
  %constant_270 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.372 = f32[64]{0} broadcast(f32[] %constant_270), dimensions={}, metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %add.205 = f32[64]{0} add(f32[64]{0} %param_0.388, f32[64]{0} %broadcast.372), metadata={op_type="Add" op_name="tower0/group0/block0/conv2/bn/batchnorm/add"}
  ROOT %rsqrt.103 = f32[64]{0} rsqrt(f32[64]{0} %add.205), metadata={op_type="Rsqrt" op_name="tower0/group0/block0/conv2/bn/batchnorm/Rsqrt"}
}

%fused_computation.194 (param_0.476: f32[64], param_1.562: f32[64], param_2.432: f32[1,64,200,200], param_3.395: f32[64], param_4.328: f32[64]) -> f32[1,64,200,200] {
  %constant_271 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.373 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[] %constant_271), dimensions={}, metadata={op_type="Relu" op_name="tower0/group0/block0/conv2/Relu"}
  %param_2.432 = f32[1,64,200,200]{3,2,1,0} parameter(2)
  %param_3.395 = f32[64]{0} parameter(3)
  %param_4.328 = f32[64]{0} parameter(4)
  %multiply.201 = f32[64]{0} multiply(f32[64]{0} %param_3.395, f32[64]{0} %param_4.328), metadata={op_type="Mul" op_name="tower0/group0/block0/conv1/bn/batchnorm/mul"}
  %broadcast.266 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %multiply.201), dimensions={1}, metadata={op_type="Mul" op_name="tower0/group0/block0/conv1/bn/batchnorm/mul_1"}
  %multiply.200 = f32[1,64,200,200]{3,2,1,0} multiply(f32[1,64,200,200]{3,2,1,0} %param_2.432, f32[1,64,200,200]{3,2,1,0} %broadcast.266), metadata={op_type="Mul" op_name="tower0/group0/block0/conv1/bn/batchnorm/mul_1"}
  %param_0.476 = f32[64]{0} parameter(0)
  %param_1.562 = f32[64]{0} parameter(1)
  %multiply.199 = f32[64]{0} multiply(f32[64]{0} %param_1.562, f32[64]{0} %multiply.201), metadata={op_type="Mul" op_name="tower0/group0/block0/conv1/bn/batchnorm/mul_2"}
  %subtract.88 = f32[64]{0} subtract(f32[64]{0} %param_0.476, f32[64]{0} %multiply.199), metadata={op_type="Sub" op_name="tower0/group0/block0/conv1/bn/batchnorm/sub"}
  %broadcast.265 = f32[1,64,200,200]{3,2,1,0} broadcast(f32[64]{0} %subtract.88), dimensions={1}, metadata={op_type="Add" op_name="tower0/group0/block0/conv1/bn/batchnorm/add_1"}
  %add.206 = f32[1,64,200,200]{3,2,1,0} add(f32[1,64,200,200]{3,2,1,0} %multiply.200, f32[1,64,200,200]{3,2,1,0} %broadcast.265), metadata={op_type="Add" op_name="tower0/group0/block0/conv1/bn/batchnorm/add_1"}
  ROOT %maximum.52 = f32[1,64,200,200]{3,2,1,0} maximum(f32[1,64,200,200]{3,2,1,0} %broadcast.373, f32[1,64,200,200]{3,2,1,0} %add.206), metadata={op_type="Relu" op_name="tower0/group0/block0/conv1/Relu"}
}

%fused_computation.195 (param_0.391: f32[64]) -> f32[64] {
  %param_0.391 = f32[64]{0} parameter(0)
  %constant_272 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.374 = f32[64]{0} broadcast(f32[] %constant_272), dimensions={}, metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %add.207 = f32[64]{0} add(f32[64]{0} %param_0.391, f32[64]{0} %broadcast.374), metadata={op_type="Add" op_name="tower0/group0/block0/conv1/bn/batchnorm/add"}
  ROOT %rsqrt.104 = f32[64]{0} rsqrt(f32[64]{0} %add.207), metadata={op_type="Rsqrt" op_name="tower0/group0/block0/conv1/bn/batchnorm/Rsqrt"}
}

%max_F32.1243 (lhs.1244: f32[], rhs.1245: f32[]) -> f32[] {
  %lhs.1244 = f32[] parameter(0)
  %rhs.1245 = f32[] parameter(1)
  ROOT %maximum.1246 = f32[] maximum(f32[] %lhs.1244, f32[] %rhs.1245)
}

%fused_computation.196 (param_0.396: f32[64], param_1.415: f32[64], param_2.337: f32[1,64,400,400], param_3.298: f32[64], param_4.239: f32[64]) -> f32[1,64,200,200] {
  %constant_168 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %broadcast.269 = f32[1,64,400,400]{3,2,1,0} broadcast(f32[] %constant_168), dimensions={}, metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %param_2.337 = f32[1,64,400,400]{3,2,1,0} parameter(2)
  %param_3.298 = f32[64]{0} parameter(3)
  %param_4.239 = f32[64]{0} parameter(4)
  %multiply.204 = f32[64]{0} multiply(f32[64]{0} %param_3.298, f32[64]{0} %param_4.239), metadata={op_type="Mul" op_name="tower0/conv0/bn/batchnorm/mul"}
  %broadcast.268 = f32[1,64,400,400]{3,2,1,0} broadcast(f32[64]{0} %multiply.204), dimensions={1}, metadata={op_type="Mul" op_name="tower0/conv0/bn/batchnorm/mul_1"}
  %multiply.203 = f32[1,64,400,400]{3,2,1,0} multiply(f32[1,64,400,400]{3,2,1,0} %param_2.337, f32[1,64,400,400]{3,2,1,0} %broadcast.268), metadata={op_type="Mul" op_name="tower0/conv0/bn/batchnorm/mul_1"}
  %param_0.396 = f32[64]{0} parameter(0)
  %param_1.415 = f32[64]{0} parameter(1)
  %multiply.202 = f32[64]{0} multiply(f32[64]{0} %param_1.415, f32[64]{0} %multiply.204), metadata={op_type="Mul" op_name="tower0/conv0/bn/batchnorm/mul_2"}
  %subtract.89 = f32[64]{0} subtract(f32[64]{0} %param_0.396, f32[64]{0} %multiply.202), metadata={op_type="Sub" op_name="tower0/conv0/bn/batchnorm/sub"}
  %broadcast.267 = f32[1,64,400,400]{3,2,1,0} broadcast(f32[64]{0} %subtract.89), dimensions={1}, metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add_1"}
  %add.208 = f32[1,64,400,400]{3,2,1,0} add(f32[1,64,400,400]{3,2,1,0} %multiply.203, f32[1,64,400,400]{3,2,1,0} %broadcast.267), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add_1"}
  %maximum.53 = f32[1,64,400,400]{3,2,1,0} maximum(f32[1,64,400,400]{3,2,1,0} %broadcast.269, f32[1,64,400,400]{3,2,1,0} %add.208), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %pad.12 = f32[1,64,401,401]{3,2,1,0} pad(f32[1,64,400,400]{3,2,1,0} %maximum.53, f32[] %constant_168), padding=0_0x0_0x1_0x1_0, metadata={op_type="Pad" op_name="tower0/Pad_1"}
  %constant_167 = f32[] constant(-inf), metadata={op_type="MaxPool" op_name="tower0/pool0/MaxPool"}
  ROOT %reduce-window.0 = f32[1,64,200,200]{3,2,1,0} reduce-window(f32[1,64,401,401]{3,2,1,0} %pad.12, f32[] %constant_167), window={size=1x1x3x3 stride=1x1x2x2}, to_apply=%max_F32.1243, metadata={op_type="MaxPool" op_name="tower0/pool0/MaxPool"}
}

%fused_computation.197 (param_0.398: f32[64]) -> f32[64] {
  %param_0.398 = f32[64]{0} parameter(0)
  %constant_273 = f32[] constant(1e-05), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %broadcast.375 = f32[64]{0} broadcast(f32[] %constant_273), dimensions={}, metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  %add.209 = f32[64]{0} add(f32[64]{0} %param_0.398, f32[64]{0} %broadcast.375), metadata={op_type="Add" op_name="tower0/conv0/bn/batchnorm/add"}
  ROOT %rsqrt.105 = f32[64]{0} rsqrt(f32[64]{0} %add.209), metadata={op_type="Rsqrt" op_name="tower0/conv0/bn/batchnorm/Rsqrt"}
}

%fused_computation.198 (param_0.401: f32[3], param_1.421: f32[800,800,3], param_2.342: f32[3]) -> f32[1,3,805,805] {
  %param_2.342 = f32[3]{0} parameter(2)
  %broadcast.271 = f32[1,800,800,3]{2,1,3,0} broadcast(f32[3]{0} %param_2.342), dimensions={3}, metadata={op_type="Mul" op_name="tower0/image_preprocess/mul"}
  %param_1.421 = f32[800,800,3]{2,1,0} parameter(1)
  %copy.194 = f32[800,800,3]{1,0,2} copy(f32[800,800,3]{2,1,0} %param_1.421), metadata={op_name="XLA_Args"}
  %bitcast.543 = f32[1,800,800,3]{2,1,3,0} bitcast(f32[800,800,3]{1,0,2} %copy.194), metadata={op_type="ExpandDims" op_name="tower0/ExpandDims_1"}
  %param_0.401 = f32[3]{0} parameter(0)
  %broadcast.270 = f32[1,800,800,3]{2,1,3,0} broadcast(f32[3]{0} %param_0.401), dimensions={3}
  %add.210 = f32[1,800,800,3]{2,1,3,0} add(f32[1,800,800,3]{2,1,3,0} %bitcast.543, f32[1,800,800,3]{2,1,3,0} %broadcast.270), metadata={op_type="Sub" op_name="tower0/image_preprocess/sub"}
  %multiply.205 = f32[1,800,800,3]{2,1,3,0} multiply(f32[1,800,800,3]{2,1,3,0} %broadcast.271, f32[1,800,800,3]{2,1,3,0} %add.210), metadata={op_type="Mul" op_name="tower0/image_preprocess/mul"}
  %bitcast.542 = f32[1,3,800,800]{3,2,1,0} bitcast(f32[1,800,800,3]{2,1,3,0} %multiply.205), metadata={op_type="Transpose" op_name="tower0/transpose"}
  %constant_169 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  ROOT %pad.13 = f32[1,3,805,805]{3,2,1,0} pad(f32[1,3,800,800]{3,2,1,0} %bitcast.542, f32[] %constant_169), padding=0_0x0_0x3_2x3_2
}

ENTRY %cluster_17__XlaCompiledKernel_true__XlaNumConstantArgs_39__XlaNumResourceArgs_0_.2788 (arg0.1: f32[512], arg1.2: f32[512], arg2.3: f32[1024], arg3.4: f32[1024], arg4.5: f32[256], arg5.6: f32[256], arg6.7: f32[256], arg7.8: f32[256], arg8.9: f32[256], arg9.10: f32[256], arg10.11: f32[256], arg11.12: f32[256], arg12.13: f32[1024], arg13.14: f32[1024], arg14.15: f32[256], arg15.16: f32[256], arg16.17: f32[256], arg17.18: f32[256], arg18.19: f32[256], arg19.20: f32[256], arg20.21: f32[256], arg21.22: f32[256], arg22.23: f32[1024], arg23.24: f32[1024], arg24.25: f32[256], arg25.26: f32[256], arg26.27: f32[256], arg27.28: f32[256], arg28.29: f32[256], arg29.30: f32[256], arg30.31: f32[256], arg31.32: f32[256], arg32.33: f32[1024], arg33.34: f32[1024], arg34.35: f32[256], arg35.36: f32[256], arg36.37: f32[256], arg37.38: f32[256], arg38.39: f32[256], arg39.40: f32[256], arg40.41: f32[256], arg41.42: f32[256], arg42.43: f32[1024], arg43.44: f32[1024], arg44.45: f32[256], arg45.46: f32[256], arg46.47: f32[256], arg47.48: f32[256], arg48.49: f32[256], arg49.50: f32[256], arg50.51: f32[256], arg51.52: f32[256], arg52.53: f32[1024], arg53.54: f32[1024], arg54.55: f32[1024], arg55.56: f32[1024], arg56.57: f32[512], arg57.58: f32[512], arg58.59: f32[128], arg59.60: f32[128], arg60.61: f32[128], arg61.62: f32[128], arg62.63: f32[128], arg63.64: f32[128], arg64.65: f32[128], arg65.66: f32[128], arg66.67: f32[512], arg67.68: f32[512], arg68.69: f32[128], arg69.70: f32[128], arg70.71: f32[128], arg71.72: f32[128], arg72.73: f32[128], arg73.74: f32[128], arg74.75: f32[128], arg75.76: f32[128], arg76.77: f32[512], arg77.78: f32[512], arg78.79: f32[128], arg79.80: f32[128], arg80.81: f32[128], arg81.82: f32[128], arg82.83: f32[128], arg83.84: f32[128], arg84.85: f32[128], arg85.86: f32[128], arg86.87: f32[512], arg87.88: f32[512], arg88.89: f32[512], arg89.90: f32[512], arg90.91: f32[256], arg91.92: f32[256], arg92.93: f32[64], arg93.94: f32[64], arg94.95: f32[64], arg95.96: f32[64], arg96.97: f32[64], arg97.98: f32[64], arg98.99: f32[64], arg99.100: f32[64], arg100.101: f32[256], arg101.102: f32[256], arg102.103: f32[64], arg103.104: f32[64], arg104.105: f32[64], arg105.106: f32[64], arg106.107: f32[64], arg107.108: f32[64], arg108.109: f32[64], arg109.110: f32[64], arg110.111: f32[256], arg111.112: f32[256], arg112.113: f32[64], arg113.114: f32[64], arg114.115: f32[64], arg115.116: f32[64], arg116.117: f32[800,800,3], arg117.118: s32[], arg118.119: f32[256], arg119.120: f32[256], arg120.121: f32[64], arg121.122: f32[64], arg122.123: f32[64], arg123.124: f32[64], arg124.125: f32[64], arg125.126: f32[64], arg126.127: f32[64], arg127.128: f32[64], arg128.129: f32[256], arg129.130: f32[256], arg130.131: f32[256], arg131.132: f32[256], arg132.133: f32[256], arg133.134: f32[256], arg134.135: f32[256], arg135.136: f32[256], arg136.137: f32[512], arg137.138: f32[512], arg138.139: f32[512], arg139.140: f32[512], arg140.141: f32[128], arg141.142: f32[128], arg142.143: f32[128], arg143.144: f32[128], arg144.145: f32[128], arg145.146: f32[128], arg146.147: f32[128], arg147.148: f32[128], arg148.149: f32[512], arg149.150: f32[512], arg150.151: f32[512], arg151.152: f32[512], arg152.153: f32[512], arg153.154: f32[512], arg154.155: f32[1024], arg155.156: f32[1024], arg156.157: f32[1024], arg157.158: f32[1024], arg158.159: f32[256], arg159.160: f32[256], arg160.161: f32[256], arg161.162: f32[256], arg162.163: f32[256], arg163.164: f32[256], arg164.165: f32[256], arg165.166: f32[256], arg166.167: f32[1024], arg167.168: f32[1024], arg168.169: f32[1024], arg169.170: f32[1024], arg170.171: f32[1024], arg171.172: f32[1024], arg172.173: f32[1024], arg173.174: f32[1024], arg174.175: f32[1024], arg175.176: f32[1024], arg176.177: f32[512], arg177.178: f32[512], arg178.179: f32[512], arg179.180: f32[512], arg180.181: f32[512], arg181.182: f32[512], arg182.183: f32[2048], arg183.184: f32[2048], arg184.185: f32[2048], arg185.186: f32[2048], arg186.187: f32[2048], arg187.188: f32[2048], arg188.189: f32[2048], arg189.190: f32[2048], arg190.191: f32[512], arg191.192: f32[512], arg192.193: f32[512], arg193.194: f32[512], arg194.195: f32[512], arg195.196: f32[512], arg196.197: f32[512], arg197.198: f32[512], arg198.199: f32[2048], arg199.200: f32[2048], arg200.201: f32[2048], arg201.202: f32[2048], arg202.203: f32[512], arg203.204: f32[512], arg204.205: f32[512], arg205.206: f32[512], arg206.207: f32[512], arg207.208: f32[512], arg208.209: f32[512], arg209.210: f32[512], arg210.211: f32[2048], arg211.212: f32[2048], arg212.213: f32[2048], arg213.214: f32[2048], arg214.215: f32[], arg215.216: f32[7,7,3,64], arg216.217: f32[1,1,64,256], arg217.218: f32[1,1,64,64], arg218.219: f32[3,3,64,64], arg219.220: f32[1,1,64,256], arg220.221: f32[1,1,256,64], arg221.222: f32[3,3,64,64], arg222.223: f32[1,1,64,256], arg223.224: f32[1,1,256,64], arg224.225: f32[3,3,64,64], arg225.226: f32[1,1,64,256], arg226.227: f32[1,1,256,512], arg227.228: f32[1,1,256,128], arg228.229: f32[1,1,256,256], arg229.230: f32[256], arg230.231: f32[3,3,128,128], arg231.232: f32[1,1,128,512], arg232.233: f32[1,1,512,128], arg233.234: f32[3,3,128,128], arg234.235: f32[1,1,128,512], arg235.236: f32[1,1,512,128], arg236.237: f32[3,3,128,128], arg237.238: f32[1,1,128,512], arg238.239: f32[1,1,512,128], arg239.240: f32[3,3,128,128], arg240.241: f32[1,1,128,512], arg241.242: f32[1,1,512,1024], arg242.243: f32[1,1,512,256], arg243.244: f32[1,1,512,256], arg244.245: f32[256], arg245.246: f32[3,3,256,256], arg246.247: f32[1,1,256,1024], arg247.248: f32[1,1,1024,256], arg248.249: f32[3,3,256,256], arg249.250: f32[1,1,256,1024], arg250.251: f32[1,1,1024,256], arg251.252: f32[3,3,256,256], arg252.253: f32[1,1,256,1024], arg253.254: f32[1,1,1024,256], arg254.255: f32[3,3,256,256], arg255.256: f32[1,1,256,1024], arg256.257: f32[1,1,1024,256], arg257.258: f32[3,3,256,256], arg258.259: f32[1,1,256,1024], arg259.260: f32[1,1,1024,256], arg260.261: f32[3,3,256,256], arg261.262: f32[1,1,256,1024], arg262.263: f32[1,1,1024,512], arg263.264: f32[1,1,1024,2048], arg264.265: f32[1,1,1024,256], arg265.266: f32[256], arg266.267: f32[3,3,512,512], arg267.268: f32[1,1,512,2048], arg268.269: f32[1,1,2048,512], arg269.270: f32[3,3,512,512], arg270.271: f32[1,1,512,2048], arg271.272: f32[1,1,2048,512], arg272.273: f32[3,3,512,512], arg273.274: f32[1,1,512,2048], arg274.275: f32[1,1,2048,256], arg275.276: f32[256], arg276.277: f32[3,3,256,256], arg277.278: f32[256], arg278.279: f32[3,3,256,256], arg279.280: f32[256], arg280.281: f32[1,1,256,3], arg281.282: f32[1,1,256,12], arg282.283: s32[42,42,3], arg283.284: s32[21,21,3], arg284.285: f32[3], arg285.286: f32[12], arg286.287: f32[3,3,256,256], arg287.288: f32[256], arg288.289: s32[84,84,3], arg289.290: f32[3,3,256,256], arg290.291: f32[256], arg291.292: s32[168,168,3], arg292.293: f32[3,3,256,256], arg293.294: f32[256], arg294.295: s32[336,336,3]) -> (f32[], f32[1,128,201,201], f32[1,128,100,100], f32[1,512,100,100], f32[1,128,100,100], f32[1,128,100,100], f32[1,512,100,100], f32[1,128,100,100], f32[1,128,100,100], f32[1,512,100,100], f32[1,128,100,100], f32[1,128,100,100], f32[1,512,100,100], f32[1,256,101,101], f32[1,256,50,50], f32[1,1024,50,50], f32[1,256,50,50], f32[1,256,50,50], f32[1,1024,50,50], f32[1,256,50,50], f32[1,256,50,50], f32[1,1024,50,50], f32[1,256,50,50], f32[1,256,50,50], f32[1,1024,50,50], f32[1,256,50,50], f32[1,256,50,50], f32[1,1024,50,50], f32[1,256,50,50], f32[1,256,50,50], f32[1,1024,50,50], f32[1,512,51,51], f32[1,512,25,25], f32[1,2048,25,25], f32[1,512,25,25], f32[1,512,25,25], f32[1,2048,25,25], f32[1,512,25,25], f32[1,512,25,25], f32[1,2048,25,25], f32[1,256,25,25], f32[1,256,25,25], f32[1,256,13,13], f32[1,256,25,25], f32[1,256,13,13], pred[25,25,3], pred[25,25,3], pred[13,13,3], pred[13,13,3], pred[1875], pred[1875], pred[1875], pred[507], pred[507], pred[507], f32[25,25,3], f32[13,13,3], f32[1,25,25,12], f32[1,256,50,50], f32[1,13,13,12], f32[1,256,50,50], f32[1,256,50,50], pred[50,50,3], pred[50,50,3], pred[7500], pred[7500], pred[7500], f32[1875,4], f32[1875], f32[50,50,3], f32[507,4], f32[507], f32[1,50,50,12], f32[1,256,100,100], f32[1,256,100,100], f32[1,256,100,100], pred[100,100,3], pred[100,100,3], pred[30000], pred[30000], pred[30000], f32[2000,4], f32[2000], f32[100,100,3], f32[1,100,100,12], f32[1,256,200,200], f32[1,256,200,200], f32[1,256,200,200], pred[200,200,3], pred[200,200,3], pred[120000], pred[120000], pred[120000], f32[2000,4], f32[2000], f32[200,200,3], f32[1,200,200,12], f32[2000,4], f32[2000], f32[1,256,200,200], f32[1,2048,1,1], f32[1,2048,25,25], f32[1,2048,1,1], f32[1,2048,1,1], f32[1,512,1,1], f32[1,512,25,25], f32[1,512,1,1], f32[1,512,1,1], f32[1,512,1,1], f32[1,512,25,25], f32[1,512,1,1], f32[1,512,1,1], f32[1,2048,1,1], f32[1,2048,25,25], f32[1,2048,1,1], f32[1,2048,1,1], f32[1,512,1,1], f32[1,512,25,25], f32[1,512,1,1], f32[1,512,1,1], f32[1,512,1,1], f32[1,512,25,25], f32[1,512,1,1], f32[1,512,1,1], f32[1,2048,1,1], f32[1,2048,25,25], f32[1,2048,1,1], f32[1,2048,25,25], f32[1,2048,1,1], f32[1,2048,1,1], f32[1,2048,1,1], f32[1,2048,1,1], f32[1,512,1,1], f32[1,512,25,25], f32[1,512,1,1], f32[1,512,1,1], f32[1,512,50,50], f32[1,512,50,50], f32[1,512,1,1], f32[1,512,1,1], f32[1,512,1,1], f32[1,1024,50,50], f32[1,1024,1,1], f32[1,1024,1,1], f32[1,1024,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,1024,1,1], f32[1,1024,50,50], f32[1,1024,1,1], f32[1,1024,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,1024,1,1], f32[1,1024,50,50], f32[1,1024,1,1], f32[1,1024,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,1024,1,1], f32[1,1024,50,50], f32[1,1024,1,1], f32[1,1024,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,1024,1,1], f32[1,1024,50,50], f32[1,1024,1,1], f32[1,1024,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,1024,1,1], f32[1,1024,50,50], f32[1,1024,1,1], f32[1,1024,50,50], f32[1,1024,1,1], f32[1,1024,1,1], f32[1,1024,1,1], f32[1,1024,1,1], f32[1,256,1,1], f32[1,256,50,50], f32[1,256,1,1], f32[1,256,1,1], f32[1,256,100,100], f32[1,256,1,1], f32[1,256,100,100], f32[1,256,1,1], f32[1,256,1,1], f32[1,512,1,1], f32[1,512,100,100], f32[1,512,1,1], f32[1,512,1,1], f32[1,128,1,1], f32[1,128,100,100], f32[1,128,1,1], f32[1,128,1,1], f32[1,128,1,1], f32[1,128,100,100], f32[1,128,1,1], f32[1,128,1,1], f32[1,512,1,1], f32[1,512,100,100], f32[1,512,1,1], f32[1,512,1,1], f32[1,128,1,1], f32[1,128,100,100], f32[1,128,1,1], f32[1,128,1,1], f32[1,128,1,1], f32[1,128,100,100], f32[1,128,1,1], f32[1,128,1,1], f32[1,512,1,1], f32[1,512,100,100], f32[1,512,1,1], f32[1,512,1,1], f32[1,128,1,1], f32[1,128,100,100], f32[1,128,1,1], f32[1,128,1,1], f32[1,128,1,1], f32[1,128,100,100], f32[1,128,1,1], f32[1,128,1,1], f32[1,512,1,1], f32[1,512,100,100], f32[1,512,100,100], f32[1,512,1,1], f32[1,512,1,1], f32[1,512,1,1], f32[1,512,1,1], f32[1,512,1,1], f32[1,128,100,100], f32[1,128,1,1], f32[1,128,1,1], f32[1,128,1,1], f32[1,128,200,200], f32[1,128,200,200], f32[1,128,1,1], f32[1,128,1,1], f32[1,128,1,1]) {
  %arg214.215 = f32[] parameter(214), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg117.118 = s32[] parameter(117), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.45 = f32[] fusion(f32[] %arg214.215, s32[] %arg117.118), kind=kLoop, calls=%fused_computation.45, metadata={op_type="Mul" op_name="EMA/QueueInput/queue_size_EMA_apply/mul"}
  %arg144.145 = f32[128]{0} parameter(144), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.176 = f32[1,128,1,1]{3,2,1,0} fusion(f32[128]{0} %arg144.145), kind=kLoop, calls=%fused_computation.176, metadata={op_type="Rsqrt" op_name="tower0/group1/block0/conv1/bn/batchnorm/Rsqrt"}
  %arg145.146 = f32[128]{0} parameter(145), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.175 = f32[1,128,1,1]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.176, f32[128]{0} %arg145.146), kind=kLoop, calls=%fused_computation.175, metadata={op_type="Mul" op_name="tower0/group1/block0/conv1/bn/batchnorm/mul"}
  %arg147.148 = f32[128]{0} parameter(147), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg135.136 = f32[256]{0} parameter(135), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg134.135 = f32[256]{0} parameter(134), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg133.134 = f32[256]{0} parameter(133), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg132.133 = f32[256]{0} parameter(132), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg115.116 = f32[64]{0} parameter(115), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg114.115 = f32[64]{0} parameter(114), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %constant_13 = f32[3]{0} constant({-103.53, -116.28, -123.675})
  %arg116.117 = f32[800,800,3]{2,1,0} parameter(116), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %constant_676 = f32[3]{0} constant({0.0174292, 0.017507, 0.0171248}), metadata={op_type="Mul" op_name="tower0/image_preprocess/mul"}
  %fusion.198 = f32[1,3,805,805]{3,2,1,0} fusion(f32[3]{0} %constant_13, f32[800,800,3]{2,1,0} %arg116.117, f32[3]{0} %constant_676), kind=kLoop, calls=%fused_computation.198
  %arg215.216 = f32[7,7,3,64]{3,2,1,0} parameter(215), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.93 = f32[7,7,3,64]{1,0,2,3} copy(f32[7,7,3,64]{3,2,1,0} %arg215.216), metadata={op_name="XLA_Args"}
  %custom-call.157 = (f32[1,64,400,400]{3,2,1,0}, u8[960008]{0}) custom-call(f32[1,3,805,805]{3,2,1,0} %fusion.198, f32[7,7,3,64]{1,0,2,3} %copy.93), window={size=7x7 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/conv0/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.695 = f32[1,64,400,400]{3,2,1,0} get-tuple-element((f32[1,64,400,400]{3,2,1,0}, u8[960008]{0}) %custom-call.157), index=0
  %arg113.114 = f32[64]{0} parameter(113), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg112.113 = f32[64]{0} parameter(112), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.197 = f32[64]{0} fusion(f32[64]{0} %arg112.113), kind=kLoop, calls=%fused_computation.197, metadata={op_type="Rsqrt" op_name="tower0/conv0/bn/batchnorm/Rsqrt"}
  %fusion.196 = f32[1,64,200,200]{3,2,1,0} fusion(f32[64]{0} %arg115.116, f32[64]{0} %arg114.115, f32[1,64,400,400]{3,2,1,0} %get-tuple-element.695, f32[64]{0} %arg113.114, f32[64]{0} %fusion.197), kind=kLoop, calls=%fused_computation.196, metadata={op_type="MaxPool" op_name="tower0/pool0/MaxPool"}
  %arg216.217 = f32[1,1,64,256]{3,2,1,0} parameter(216), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.97 = f32[1,1,64,256]{1,0,2,3} copy(f32[1,1,64,256]{3,2,1,0} %arg216.217), metadata={op_name="XLA_Args"}
  %custom-call.88 = (f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,64,200,200]{3,2,1,0} %fusion.196, f32[1,1,64,256]{1,0,2,3} %copy.97), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block0/convshortcut/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.626 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.88), index=0
  %arg123.124 = f32[64]{0} parameter(123), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg122.123 = f32[64]{0} parameter(122), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg127.128 = f32[64]{0} parameter(127), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg126.127 = f32[64]{0} parameter(126), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg217.218 = f32[1,1,64,64]{3,2,1,0} parameter(217), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.94 = f32[1,1,64,64]{1,0,2,3} copy(f32[1,1,64,64]{3,2,1,0} %arg217.218), metadata={op_name="XLA_Args"}
  %custom-call.85 = (f32[1,64,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,64,200,200]{3,2,1,0} %fusion.196, f32[1,1,64,64]{1,0,2,3} %copy.94), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block0/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.623 = f32[1,64,200,200]{3,2,1,0} get-tuple-element((f32[1,64,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.85), index=0
  %arg125.126 = f32[64]{0} parameter(125), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg124.125 = f32[64]{0} parameter(124), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.195 = f32[64]{0} fusion(f32[64]{0} %arg124.125), kind=kLoop, calls=%fused_computation.195, metadata={op_type="Rsqrt" op_name="tower0/group0/block0/conv1/bn/batchnorm/Rsqrt"}
  %fusion.194 = f32[1,64,200,200]{3,2,1,0} fusion(f32[64]{0} %arg127.128, f32[64]{0} %arg126.127, f32[1,64,200,200]{3,2,1,0} %get-tuple-element.623, f32[64]{0} %arg125.126, f32[64]{0} %fusion.195), kind=kLoop, calls=%fused_computation.194, metadata={op_type="Relu" op_name="tower0/group0/block0/conv1/Relu"}
  %arg218.219 = f32[3,3,64,64]{3,2,1,0} parameter(218), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.95 = f32[3,3,64,64]{1,0,2,3} copy(f32[3,3,64,64]{3,2,1,0} %arg218.219), metadata={op_name="XLA_Args"}
  %custom-call.86 = (f32[1,64,200,200]{3,2,1,0}, u8[409856]{0}) custom-call(f32[1,64,200,200]{3,2,1,0} %fusion.194, f32[3,3,64,64]{1,0,2,3} %copy.95), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block0/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.624 = f32[1,64,200,200]{3,2,1,0} get-tuple-element((f32[1,64,200,200]{3,2,1,0}, u8[409856]{0}) %custom-call.86), index=0
  %arg121.122 = f32[64]{0} parameter(121), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg120.121 = f32[64]{0} parameter(120), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.193 = f32[64]{0} fusion(f32[64]{0} %arg120.121), kind=kLoop, calls=%fused_computation.193, metadata={op_type="Rsqrt" op_name="tower0/group0/block0/conv2/bn/batchnorm/Rsqrt"}
  %fusion.192 = f32[1,64,200,200]{3,2,1,0} fusion(f32[64]{0} %arg123.124, f32[64]{0} %arg122.123, f32[1,64,200,200]{3,2,1,0} %get-tuple-element.624, f32[64]{0} %arg121.122, f32[64]{0} %fusion.193), kind=kLoop, calls=%fused_computation.192, metadata={op_type="Relu" op_name="tower0/group0/block0/conv2/Relu"}
  %arg219.220 = f32[1,1,64,256]{3,2,1,0} parameter(219), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.96 = f32[1,1,64,256]{1,0,2,3} copy(f32[1,1,64,256]{3,2,1,0} %arg219.220), metadata={op_name="XLA_Args"}
  %custom-call.87 = (f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,64,200,200]{3,2,1,0} %fusion.192, f32[1,1,64,256]{1,0,2,3} %copy.96), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block0/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.625 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.87), index=0
  %arg129.130 = f32[256]{0} parameter(129), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg128.129 = f32[256]{0} parameter(128), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg111.112 = f32[256]{0} parameter(111), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg110.111 = f32[256]{0} parameter(110), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.190 = f32[256]{0} fusion(f32[256]{0} %arg110.111), kind=kLoop, calls=%fused_computation.190, metadata={op_type="Rsqrt" op_name="tower0/group0/block0/convshortcut/bn/batchnorm/Rsqrt"}
  %arg131.132 = f32[256]{0} parameter(131), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg130.131 = f32[256]{0} parameter(130), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg119.120 = f32[256]{0} parameter(119), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg118.119 = f32[256]{0} parameter(118), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.191 = f32[256]{0} fusion(f32[256]{0} %arg118.119), kind=kLoop, calls=%fused_computation.191, metadata={op_type="Rsqrt" op_name="tower0/group0/block0/conv3/bn/batchnorm/Rsqrt"}
  %fusion.189 = f32[1,256,200,200]{3,2,1,0} fusion(f32[1,256,200,200]{3,2,1,0} %get-tuple-element.626, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.625, f32[256]{0} %arg129.130, f32[256]{0} %arg128.129, f32[256]{0} %arg111.112, f32[256]{0} %fusion.190, f32[256]{0} %arg131.132, f32[256]{0} %arg130.131, f32[256]{0} %arg119.120, f32[256]{0} %fusion.191), kind=kLoop, calls=%fused_computation.189, metadata={op_type="Relu" op_name="tower0/group0/block0/output"}
  %arg105.106 = f32[64]{0} parameter(105), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg104.105 = f32[64]{0} parameter(104), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg109.110 = f32[64]{0} parameter(109), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg108.109 = f32[64]{0} parameter(108), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg220.221 = f32[1,1,256,64]{3,2,1,0} parameter(220), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.98 = f32[1,1,256,64]{1,0,2,3} copy(f32[1,1,256,64]{3,2,1,0} %arg220.221), metadata={op_name="XLA_Args"}
  %custom-call.89 = (f32[1,64,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %fusion.189, f32[1,1,256,64]{1,0,2,3} %copy.98), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block1/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.627 = f32[1,64,200,200]{3,2,1,0} get-tuple-element((f32[1,64,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.89), index=0
  %arg107.108 = f32[64]{0} parameter(107), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg106.107 = f32[64]{0} parameter(106), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.188 = f32[64]{0} fusion(f32[64]{0} %arg106.107), kind=kLoop, calls=%fused_computation.188, metadata={op_type="Rsqrt" op_name="tower0/group0/block1/conv1/bn/batchnorm/Rsqrt"}
  %fusion.187 = f32[1,64,200,200]{3,2,1,0} fusion(f32[64]{0} %arg109.110, f32[64]{0} %arg108.109, f32[1,64,200,200]{3,2,1,0} %get-tuple-element.627, f32[64]{0} %arg107.108, f32[64]{0} %fusion.188), kind=kLoop, calls=%fused_computation.187, metadata={op_type="Relu" op_name="tower0/group0/block1/conv1/Relu"}
  %arg221.222 = f32[3,3,64,64]{3,2,1,0} parameter(221), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.99 = f32[3,3,64,64]{1,0,2,3} copy(f32[3,3,64,64]{3,2,1,0} %arg221.222), metadata={op_name="XLA_Args"}
  %custom-call.90 = (f32[1,64,200,200]{3,2,1,0}, u8[409856]{0}) custom-call(f32[1,64,200,200]{3,2,1,0} %fusion.187, f32[3,3,64,64]{1,0,2,3} %copy.99), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block1/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.628 = f32[1,64,200,200]{3,2,1,0} get-tuple-element((f32[1,64,200,200]{3,2,1,0}, u8[409856]{0}) %custom-call.90), index=0
  %arg103.104 = f32[64]{0} parameter(103), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg102.103 = f32[64]{0} parameter(102), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.186 = f32[64]{0} fusion(f32[64]{0} %arg102.103), kind=kLoop, calls=%fused_computation.186, metadata={op_type="Rsqrt" op_name="tower0/group0/block1/conv2/bn/batchnorm/Rsqrt"}
  %fusion.185 = f32[1,64,200,200]{3,2,1,0} fusion(f32[64]{0} %arg105.106, f32[64]{0} %arg104.105, f32[1,64,200,200]{3,2,1,0} %get-tuple-element.628, f32[64]{0} %arg103.104, f32[64]{0} %fusion.186), kind=kLoop, calls=%fused_computation.185, metadata={op_type="Relu" op_name="tower0/group0/block1/conv2/Relu"}
  %arg222.223 = f32[1,1,64,256]{3,2,1,0} parameter(222), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.100 = f32[1,1,64,256]{1,0,2,3} copy(f32[1,1,64,256]{3,2,1,0} %arg222.223), metadata={op_name="XLA_Args"}
  %custom-call.91 = (f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,64,200,200]{3,2,1,0} %fusion.185, f32[1,1,64,256]{1,0,2,3} %copy.100), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block1/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.629 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.91), index=0
  %arg101.102 = f32[256]{0} parameter(101), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg100.101 = f32[256]{0} parameter(100), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.184 = f32[256]{0} fusion(f32[256]{0} %arg100.101), kind=kLoop, calls=%fused_computation.184, metadata={op_type="Rsqrt" op_name="tower0/group0/block1/conv3/bn/batchnorm/Rsqrt"}
  %fusion.183 = f32[1,256,200,200]{3,2,1,0} fusion(f32[256]{0} %arg133.134, f32[256]{0} %arg132.133, f32[1,256,200,200]{3,2,1,0} %fusion.189, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.629, f32[256]{0} %arg101.102, f32[256]{0} %fusion.184), kind=kLoop, calls=%fused_computation.183, metadata={op_type="Relu" op_name="tower0/group0/block1/output"}
  %arg95.96 = f32[64]{0} parameter(95), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg94.95 = f32[64]{0} parameter(94), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg99.100 = f32[64]{0} parameter(99), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg98.99 = f32[64]{0} parameter(98), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg223.224 = f32[1,1,256,64]{3,2,1,0} parameter(223), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.101 = f32[1,1,256,64]{1,0,2,3} copy(f32[1,1,256,64]{3,2,1,0} %arg223.224), metadata={op_name="XLA_Args"}
  %custom-call.92 = (f32[1,64,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %fusion.183, f32[1,1,256,64]{1,0,2,3} %copy.101), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block2/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.630 = f32[1,64,200,200]{3,2,1,0} get-tuple-element((f32[1,64,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.92), index=0
  %arg97.98 = f32[64]{0} parameter(97), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg96.97 = f32[64]{0} parameter(96), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.182 = f32[64]{0} fusion(f32[64]{0} %arg96.97), kind=kLoop, calls=%fused_computation.182, metadata={op_type="Rsqrt" op_name="tower0/group0/block2/conv1/bn/batchnorm/Rsqrt"}
  %fusion.181 = f32[1,64,200,200]{3,2,1,0} fusion(f32[64]{0} %arg99.100, f32[64]{0} %arg98.99, f32[1,64,200,200]{3,2,1,0} %get-tuple-element.630, f32[64]{0} %arg97.98, f32[64]{0} %fusion.182), kind=kLoop, calls=%fused_computation.181, metadata={op_type="Relu" op_name="tower0/group0/block2/conv1/Relu"}
  %arg224.225 = f32[3,3,64,64]{3,2,1,0} parameter(224), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.102 = f32[3,3,64,64]{1,0,2,3} copy(f32[3,3,64,64]{3,2,1,0} %arg224.225), metadata={op_name="XLA_Args"}
  %custom-call.93 = (f32[1,64,200,200]{3,2,1,0}, u8[409856]{0}) custom-call(f32[1,64,200,200]{3,2,1,0} %fusion.181, f32[3,3,64,64]{1,0,2,3} %copy.102), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block2/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.631 = f32[1,64,200,200]{3,2,1,0} get-tuple-element((f32[1,64,200,200]{3,2,1,0}, u8[409856]{0}) %custom-call.93), index=0
  %arg93.94 = f32[64]{0} parameter(93), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg92.93 = f32[64]{0} parameter(92), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.180 = f32[64]{0} fusion(f32[64]{0} %arg92.93), kind=kLoop, calls=%fused_computation.180, metadata={op_type="Rsqrt" op_name="tower0/group0/block2/conv2/bn/batchnorm/Rsqrt"}
  %fusion.179 = f32[1,64,200,200]{3,2,1,0} fusion(f32[64]{0} %arg95.96, f32[64]{0} %arg94.95, f32[1,64,200,200]{3,2,1,0} %get-tuple-element.631, f32[64]{0} %arg93.94, f32[64]{0} %fusion.180), kind=kLoop, calls=%fused_computation.179, metadata={op_type="Relu" op_name="tower0/group0/block2/conv2/Relu"}
  %arg225.226 = f32[1,1,64,256]{3,2,1,0} parameter(225), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.103 = f32[1,1,64,256]{1,0,2,3} copy(f32[1,1,64,256]{3,2,1,0} %arg225.226), metadata={op_name="XLA_Args"}
  %custom-call.94 = (f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,64,200,200]{3,2,1,0} %fusion.179, f32[1,1,64,256]{1,0,2,3} %copy.103), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group0/block2/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.632 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.94), index=0
  %arg91.92 = f32[256]{0} parameter(91), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg90.91 = f32[256]{0} parameter(90), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.178 = f32[256]{0} fusion(f32[256]{0} %arg90.91), kind=kLoop, calls=%fused_computation.178, metadata={op_type="Rsqrt" op_name="tower0/group0/block2/conv3/bn/batchnorm/Rsqrt"}
  %fusion.177 = f32[1,256,200,200]{3,2,1,0} fusion(f32[256]{0} %arg135.136, f32[256]{0} %arg134.135, f32[1,256,200,200]{3,2,1,0} %fusion.183, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.632, f32[256]{0} %arg91.92, f32[256]{0} %fusion.178), kind=kLoop, calls=%fused_computation.177, metadata={op_type="Relu" op_name="tower0/group0/block2/output"}
  %arg227.228 = f32[1,1,256,128]{3,2,1,0} parameter(227), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.104 = f32[1,1,256,128]{1,0,2,3} copy(f32[1,1,256,128]{3,2,1,0} %arg227.228), metadata={op_name="XLA_Args"}
  %custom-call.96 = (f32[1,128,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %fusion.177, f32[1,1,256,128]{1,0,2,3} %copy.104), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block0/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.634 = f32[1,128,200,200]{3,2,1,0} get-tuple-element((f32[1,128,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.96), index=0
  %arg146.147 = f32[128]{0} parameter(146), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.174 = f32[1,128,200,200]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.175, f32[128]{0} %arg147.148, f32[1,128,200,200]{3,2,1,0} %get-tuple-element.634, f32[128]{0} %arg146.147), kind=kLoop, calls=%fused_computation.174, metadata={op_type="Relu" op_name="tower0/group1/block0/conv1/Relu"}
  %constant_1237 = f32[] constant(0), metadata={op_type="Relu" op_name="tower0/conv0/Relu"}
  %pad.9 = f32[1,128,201,201]{3,2,1,0} pad(f32[1,128,200,200]{3,2,1,0} %fusion.174, f32[] %constant_1237), padding=0_0x0_0x1_0x1_0
  %arg140.141 = f32[128]{0} parameter(140), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.173 = f32[1,128,1,1]{3,2,1,0} fusion(f32[128]{0} %arg140.141), kind=kLoop, calls=%fused_computation.173, metadata={op_type="Rsqrt" op_name="tower0/group1/block0/conv2/bn/batchnorm/Rsqrt"}
  %arg141.142 = f32[128]{0} parameter(141), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.172 = f32[1,128,1,1]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.173, f32[128]{0} %arg141.142), kind=kLoop, calls=%fused_computation.172, metadata={op_type="Mul" op_name="tower0/group1/block0/conv2/bn/batchnorm/mul"}
  %arg143.144 = f32[128]{0} parameter(143), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg230.231 = f32[3,3,128,128]{3,2,1,0} parameter(230), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.105 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg230.231), metadata={op_name="XLA_Args"}
  %custom-call.158 = (f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,128,201,201]{3,2,1,0} %pad.9, f32[3,3,128,128]{1,0,2,3} %copy.105), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block0/conv2/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.696 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.158), index=0
  %arg142.143 = f32[128]{0} parameter(142), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.171 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.172, f32[128]{0} %arg143.144, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.696, f32[128]{0} %arg142.143), kind=kLoop, calls=%fused_computation.171, metadata={op_type="Relu" op_name="tower0/group1/block0/conv2/Relu"}
  %arg86.87 = f32[512]{0} parameter(86), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.168 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg86.87), kind=kLoop, calls=%fused_computation.168, metadata={op_type="Rsqrt" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/Rsqrt"}
  %arg87.88 = f32[512]{0} parameter(87), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.167 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.168, f32[512]{0} %arg87.88), kind=kLoop, calls=%fused_computation.167, metadata={op_type="Mul" op_name="tower0/group1/block0/convshortcut/bn/batchnorm/mul"}
  %arg89.90 = f32[512]{0} parameter(89), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg226.227 = f32[1,1,256,512]{3,2,1,0} parameter(226), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.107 = f32[1,1,256,512]{1,0,2,3} copy(f32[1,1,256,512]{3,2,1,0} %arg226.227), metadata={op_name="XLA_Args"}
  %custom-call.98 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %fusion.177, f32[1,1,256,512]{1,0,2,3} %copy.107), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block0/convshortcut/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.636 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.98), index=0
  %arg136.137 = f32[512]{0} parameter(136), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.170 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg136.137), kind=kLoop, calls=%fused_computation.170, metadata={op_type="Rsqrt" op_name="tower0/group1/block0/conv3/bn/batchnorm/Rsqrt"}
  %arg137.138 = f32[512]{0} parameter(137), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.169 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.170, f32[512]{0} %arg137.138), kind=kLoop, calls=%fused_computation.169, metadata={op_type="Mul" op_name="tower0/group1/block0/conv3/bn/batchnorm/mul"}
  %arg139.140 = f32[512]{0} parameter(139), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg231.232 = f32[1,1,128,512]{3,2,1,0} parameter(231), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.106 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %arg231.232), metadata={op_name="XLA_Args"}
  %custom-call.97 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.171, f32[1,1,128,512]{1,0,2,3} %copy.106), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block0/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.635 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.97), index=0
  %arg138.139 = f32[512]{0} parameter(138), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg88.89 = f32[512]{0} parameter(88), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.166 = f32[1,512,100,100]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.167, f32[512]{0} %arg89.90, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.636, f32[1,512,1,1]{3,2,1,0} %fusion.169, f32[512]{0} %arg139.140, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.635, f32[512]{0} %arg138.139, f32[512]{0} %arg88.89), kind=kLoop, calls=%fused_computation.166, metadata={op_type="Relu" op_name="tower0/group1/block0/output"}
  %arg82.83 = f32[128]{0} parameter(82), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.165 = f32[1,128,1,1]{3,2,1,0} fusion(f32[128]{0} %arg82.83), kind=kLoop, calls=%fused_computation.165, metadata={op_type="Rsqrt" op_name="tower0/group1/block1/conv1/bn/batchnorm/Rsqrt"}
  %arg83.84 = f32[128]{0} parameter(83), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.164 = f32[1,128,1,1]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.165, f32[128]{0} %arg83.84), kind=kLoop, calls=%fused_computation.164, metadata={op_type="Mul" op_name="tower0/group1/block1/conv1/bn/batchnorm/mul"}
  %arg85.86 = f32[128]{0} parameter(85), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg232.233 = f32[1,1,512,128]{3,2,1,0} parameter(232), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.108 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %arg232.233), metadata={op_name="XLA_Args"}
  %custom-call.99 = (f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %fusion.166, f32[1,1,512,128]{1,0,2,3} %copy.108), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block1/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.637 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.99), index=0
  %arg84.85 = f32[128]{0} parameter(84), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.163 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.164, f32[128]{0} %arg85.86, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.637, f32[128]{0} %arg84.85), kind=kLoop, calls=%fused_computation.163, metadata={op_type="Relu" op_name="tower0/group1/block1/conv1/Relu"}
  %arg78.79 = f32[128]{0} parameter(78), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.162 = f32[1,128,1,1]{3,2,1,0} fusion(f32[128]{0} %arg78.79), kind=kLoop, calls=%fused_computation.162, metadata={op_type="Rsqrt" op_name="tower0/group1/block1/conv2/bn/batchnorm/Rsqrt"}
  %arg79.80 = f32[128]{0} parameter(79), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.161 = f32[1,128,1,1]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.162, f32[128]{0} %arg79.80), kind=kLoop, calls=%fused_computation.161, metadata={op_type="Mul" op_name="tower0/group1/block1/conv2/bn/batchnorm/mul"}
  %arg81.82 = f32[128]{0} parameter(81), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg233.234 = f32[3,3,128,128]{3,2,1,0} parameter(233), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.109 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg233.234), metadata={op_name="XLA_Args"}
  %custom-call.100 = (f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.163, f32[3,3,128,128]{1,0,2,3} %copy.109), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block1/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.638 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) %custom-call.100), index=0
  %arg80.81 = f32[128]{0} parameter(80), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.160 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.161, f32[128]{0} %arg81.82, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.638, f32[128]{0} %arg80.81), kind=kLoop, calls=%fused_computation.160, metadata={op_type="Relu" op_name="tower0/group1/block1/conv2/Relu"}
  %arg76.77 = f32[512]{0} parameter(76), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.159 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg76.77), kind=kLoop, calls=%fused_computation.159, metadata={op_type="Rsqrt" op_name="tower0/group1/block1/conv3/bn/batchnorm/Rsqrt"}
  %arg77.78 = f32[512]{0} parameter(77), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.158 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.159, f32[512]{0} %arg77.78), kind=kLoop, calls=%fused_computation.158, metadata={op_type="Mul" op_name="tower0/group1/block1/conv3/bn/batchnorm/mul"}
  %arg149.150 = f32[512]{0} parameter(149), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg234.235 = f32[1,1,128,512]{3,2,1,0} parameter(234), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.110 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %arg234.235), metadata={op_name="XLA_Args"}
  %custom-call.101 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.160, f32[1,1,128,512]{1,0,2,3} %copy.110), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block1/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.639 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.101), index=0
  %arg148.149 = f32[512]{0} parameter(148), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.157 = f32[1,512,100,100]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.158, f32[512]{0} %arg149.150, f32[1,512,100,100]{3,2,1,0} %fusion.166, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.639, f32[512]{0} %arg148.149), kind=kLoop, calls=%fused_computation.157, metadata={op_type="Relu" op_name="tower0/group1/block1/output"}
  %arg72.73 = f32[128]{0} parameter(72), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.156 = f32[1,128,1,1]{3,2,1,0} fusion(f32[128]{0} %arg72.73), kind=kLoop, calls=%fused_computation.156, metadata={op_type="Rsqrt" op_name="tower0/group1/block2/conv1/bn/batchnorm/Rsqrt"}
  %arg73.74 = f32[128]{0} parameter(73), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.155 = f32[1,128,1,1]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.156, f32[128]{0} %arg73.74), kind=kLoop, calls=%fused_computation.155, metadata={op_type="Mul" op_name="tower0/group1/block2/conv1/bn/batchnorm/mul"}
  %arg75.76 = f32[128]{0} parameter(75), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg235.236 = f32[1,1,512,128]{3,2,1,0} parameter(235), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.111 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %arg235.236), metadata={op_name="XLA_Args"}
  %custom-call.102 = (f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %fusion.157, f32[1,1,512,128]{1,0,2,3} %copy.111), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block2/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.640 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.102), index=0
  %arg74.75 = f32[128]{0} parameter(74), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.154 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.155, f32[128]{0} %arg75.76, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.640, f32[128]{0} %arg74.75), kind=kLoop, calls=%fused_computation.154, metadata={op_type="Relu" op_name="tower0/group1/block2/conv1/Relu"}
  %arg68.69 = f32[128]{0} parameter(68), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.153 = f32[1,128,1,1]{3,2,1,0} fusion(f32[128]{0} %arg68.69), kind=kLoop, calls=%fused_computation.153, metadata={op_type="Rsqrt" op_name="tower0/group1/block2/conv2/bn/batchnorm/Rsqrt"}
  %arg69.70 = f32[128]{0} parameter(69), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.152 = f32[1,128,1,1]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.153, f32[128]{0} %arg69.70), kind=kLoop, calls=%fused_computation.152, metadata={op_type="Mul" op_name="tower0/group1/block2/conv2/bn/batchnorm/mul"}
  %arg71.72 = f32[128]{0} parameter(71), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg236.237 = f32[3,3,128,128]{3,2,1,0} parameter(236), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.112 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg236.237), metadata={op_name="XLA_Args"}
  %custom-call.103 = (f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.154, f32[3,3,128,128]{1,0,2,3} %copy.112), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block2/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.641 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) %custom-call.103), index=0
  %arg70.71 = f32[128]{0} parameter(70), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.151 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.152, f32[128]{0} %arg71.72, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.641, f32[128]{0} %arg70.71), kind=kLoop, calls=%fused_computation.151, metadata={op_type="Relu" op_name="tower0/group1/block2/conv2/Relu"}
  %arg66.67 = f32[512]{0} parameter(66), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.150 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg66.67), kind=kLoop, calls=%fused_computation.150, metadata={op_type="Rsqrt" op_name="tower0/group1/block2/conv3/bn/batchnorm/Rsqrt"}
  %arg67.68 = f32[512]{0} parameter(67), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.149 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.150, f32[512]{0} %arg67.68), kind=kLoop, calls=%fused_computation.149, metadata={op_type="Mul" op_name="tower0/group1/block2/conv3/bn/batchnorm/mul"}
  %arg151.152 = f32[512]{0} parameter(151), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg237.238 = f32[1,1,128,512]{3,2,1,0} parameter(237), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.113 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %arg237.238), metadata={op_name="XLA_Args"}
  %custom-call.104 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.151, f32[1,1,128,512]{1,0,2,3} %copy.113), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block2/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.642 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.104), index=0
  %arg150.151 = f32[512]{0} parameter(150), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.148 = f32[1,512,100,100]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.149, f32[512]{0} %arg151.152, f32[1,512,100,100]{3,2,1,0} %fusion.157, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.642, f32[512]{0} %arg150.151), kind=kLoop, calls=%fused_computation.148, metadata={op_type="Relu" op_name="tower0/group1/block2/output"}
  %arg62.63 = f32[128]{0} parameter(62), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.147 = f32[1,128,1,1]{3,2,1,0} fusion(f32[128]{0} %arg62.63), kind=kLoop, calls=%fused_computation.147, metadata={op_type="Rsqrt" op_name="tower0/group1/block3/conv1/bn/batchnorm/Rsqrt"}
  %arg63.64 = f32[128]{0} parameter(63), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.146 = f32[1,128,1,1]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.147, f32[128]{0} %arg63.64), kind=kLoop, calls=%fused_computation.146, metadata={op_type="Mul" op_name="tower0/group1/block3/conv1/bn/batchnorm/mul"}
  %arg65.66 = f32[128]{0} parameter(65), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg238.239 = f32[1,1,512,128]{3,2,1,0} parameter(238), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.114 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %arg238.239), metadata={op_name="XLA_Args"}
  %custom-call.105 = (f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %fusion.148, f32[1,1,512,128]{1,0,2,3} %copy.114), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block3/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.643 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.105), index=0
  %arg64.65 = f32[128]{0} parameter(64), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.145 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.146, f32[128]{0} %arg65.66, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.643, f32[128]{0} %arg64.65), kind=kLoop, calls=%fused_computation.145, metadata={op_type="Relu" op_name="tower0/group1/block3/conv1/Relu"}
  %arg58.59 = f32[128]{0} parameter(58), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.144 = f32[1,128,1,1]{3,2,1,0} fusion(f32[128]{0} %arg58.59), kind=kLoop, calls=%fused_computation.144, metadata={op_type="Rsqrt" op_name="tower0/group1/block3/conv2/bn/batchnorm/Rsqrt"}
  %arg59.60 = f32[128]{0} parameter(59), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.143 = f32[1,128,1,1]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.144, f32[128]{0} %arg59.60), kind=kLoop, calls=%fused_computation.143, metadata={op_type="Mul" op_name="tower0/group1/block3/conv2/bn/batchnorm/mul"}
  %arg61.62 = f32[128]{0} parameter(61), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg239.240 = f32[3,3,128,128]{3,2,1,0} parameter(239), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.115 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg239.240), metadata={op_name="XLA_Args"}
  %custom-call.106 = (f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.145, f32[3,3,128,128]{1,0,2,3} %copy.115), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block3/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.644 = f32[1,128,100,100]{3,2,1,0} get-tuple-element((f32[1,128,100,100]{3,2,1,0}, u8[1638912]{0}) %custom-call.106), index=0
  %arg60.61 = f32[128]{0} parameter(60), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.142 = f32[1,128,100,100]{3,2,1,0} fusion(f32[1,128,1,1]{3,2,1,0} %fusion.143, f32[128]{0} %arg61.62, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.644, f32[128]{0} %arg60.61), kind=kLoop, calls=%fused_computation.142, metadata={op_type="Relu" op_name="tower0/group1/block3/conv2/Relu"}
  %arg56.57 = f32[512]{0} parameter(56), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.141 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg56.57), kind=kLoop, calls=%fused_computation.141, metadata={op_type="Rsqrt" op_name="tower0/group1/block3/conv3/bn/batchnorm/Rsqrt"}
  %arg57.58 = f32[512]{0} parameter(57), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.140 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.141, f32[512]{0} %arg57.58), kind=kLoop, calls=%fused_computation.140, metadata={op_type="Mul" op_name="tower0/group1/block3/conv3/bn/batchnorm/mul"}
  %arg153.154 = f32[512]{0} parameter(153), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg240.241 = f32[1,1,128,512]{3,2,1,0} parameter(240), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.116 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %arg240.241), metadata={op_name="XLA_Args"}
  %custom-call.107 = (f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,128,100,100]{3,2,1,0} %fusion.142, f32[1,1,128,512]{1,0,2,3} %copy.116), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group1/block3/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.645 = f32[1,512,100,100]{3,2,1,0} get-tuple-element((f32[1,512,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.107), index=0
  %arg152.153 = f32[512]{0} parameter(152), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.139 = f32[1,512,100,100]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.140, f32[512]{0} %arg153.154, f32[1,512,100,100]{3,2,1,0} %fusion.148, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.645, f32[512]{0} %arg152.153), kind=kLoop, calls=%fused_computation.139, metadata={op_type="Relu" op_name="tower0/group1/block3/output"}
  %arg162.163 = f32[256]{0} parameter(162), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.138 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg162.163), kind=kLoop, calls=%fused_computation.138, metadata={op_type="Rsqrt" op_name="tower0/group2/block0/conv1/bn/batchnorm/Rsqrt"}
  %arg163.164 = f32[256]{0} parameter(163), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.137 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.138, f32[256]{0} %arg163.164), kind=kLoop, calls=%fused_computation.137, metadata={op_type="Mul" op_name="tower0/group2/block0/conv1/bn/batchnorm/mul"}
  %arg165.166 = f32[256]{0} parameter(165), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg242.243 = f32[1,1,512,256]{3,2,1,0} parameter(242), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.117 = f32[1,1,512,256]{1,0,2,3} copy(f32[1,1,512,256]{3,2,1,0} %arg242.243), metadata={op_name="XLA_Args"}
  %custom-call.109 = (f32[1,256,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %fusion.139, f32[1,1,512,256]{1,0,2,3} %copy.117), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block0/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.647 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[1,256,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.109), index=0
  %arg164.165 = f32[256]{0} parameter(164), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.136 = f32[1,256,100,100]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.137, f32[256]{0} %arg165.166, f32[1,256,100,100]{3,2,1,0} %get-tuple-element.647, f32[256]{0} %arg164.165), kind=kLoop, calls=%fused_computation.136, metadata={op_type="Relu" op_name="tower0/group2/block0/conv1/Relu"}
  %pad.10 = f32[1,256,101,101]{3,2,1,0} pad(f32[1,256,100,100]{3,2,1,0} %fusion.136, f32[] %constant_1237), padding=0_0x0_0x1_0x1_0
  %arg158.159 = f32[256]{0} parameter(158), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.135 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg158.159), kind=kLoop, calls=%fused_computation.135, metadata={op_type="Rsqrt" op_name="tower0/group2/block0/conv2/bn/batchnorm/Rsqrt"}
  %arg159.160 = f32[256]{0} parameter(159), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.134 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.135, f32[256]{0} %arg159.160), kind=kLoop, calls=%fused_computation.134, metadata={op_type="Mul" op_name="tower0/group2/block0/conv2/bn/batchnorm/mul"}
  %arg161.162 = f32[256]{0} parameter(161), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg245.246 = f32[3,3,256,256]{3,2,1,0} parameter(245), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.118 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg245.246), metadata={op_name="XLA_Args"}
  %custom-call.159 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,101,101]{3,2,1,0} %pad.10, f32[3,3,256,256]{1,0,2,3} %copy.118), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block0/conv2/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.697 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.159), index=0
  %arg160.161 = f32[256]{0} parameter(160), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.133 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.134, f32[256]{0} %arg161.162, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.697, f32[256]{0} %arg160.161), kind=kLoop, calls=%fused_computation.133, metadata={op_type="Relu" op_name="tower0/group2/block0/conv2/Relu"}
  %arg52.53 = f32[1024]{0} parameter(52), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.130 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1024]{0} %arg52.53), kind=kLoop, calls=%fused_computation.130, metadata={op_type="Rsqrt" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/Rsqrt"}
  %arg53.54 = f32[1024]{0} parameter(53), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.129 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.130, f32[1024]{0} %arg53.54), kind=kLoop, calls=%fused_computation.129, metadata={op_type="Mul" op_name="tower0/group2/block0/convshortcut/bn/batchnorm/mul"}
  %arg55.56 = f32[1024]{0} parameter(55), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg241.242 = f32[1,1,512,1024]{3,2,1,0} parameter(241), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.120 = f32[1,1,512,1024]{1,0,2,3} copy(f32[1,1,512,1024]{3,2,1,0} %arg241.242), metadata={op_name="XLA_Args"}
  %custom-call.111 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %fusion.139, f32[1,1,512,1024]{1,0,2,3} %copy.120), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block0/convshortcut/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.649 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.111), index=0
  %arg154.155 = f32[1024]{0} parameter(154), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.132 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1024]{0} %arg154.155), kind=kLoop, calls=%fused_computation.132, metadata={op_type="Rsqrt" op_name="tower0/group2/block0/conv3/bn/batchnorm/Rsqrt"}
  %arg155.156 = f32[1024]{0} parameter(155), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.131 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.132, f32[1024]{0} %arg155.156), kind=kLoop, calls=%fused_computation.131, metadata={op_type="Mul" op_name="tower0/group2/block0/conv3/bn/batchnorm/mul"}
  %arg157.158 = f32[1024]{0} parameter(157), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg246.247 = f32[1,1,256,1024]{3,2,1,0} parameter(246), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.119 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg246.247), metadata={op_name="XLA_Args"}
  %custom-call.110 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.133, f32[1,1,256,1024]{1,0,2,3} %copy.119), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block0/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.648 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.110), index=0
  %arg156.157 = f32[1024]{0} parameter(156), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg54.55 = f32[1024]{0} parameter(54), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.128 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.129, f32[1024]{0} %arg55.56, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.649, f32[1,1024,1,1]{3,2,1,0} %fusion.131, f32[1024]{0} %arg157.158, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.648, f32[1024]{0} %arg156.157, f32[1024]{0} %arg54.55), kind=kLoop, calls=%fused_computation.128, metadata={op_type="Relu" op_name="tower0/group2/block0/output"}
  %arg48.49 = f32[256]{0} parameter(48), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.127 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg48.49), kind=kLoop, calls=%fused_computation.127, metadata={op_type="Rsqrt" op_name="tower0/group2/block1/conv1/bn/batchnorm/Rsqrt"}
  %arg49.50 = f32[256]{0} parameter(49), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.126 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.127, f32[256]{0} %arg49.50), kind=kLoop, calls=%fused_computation.126, metadata={op_type="Mul" op_name="tower0/group2/block1/conv1/bn/batchnorm/mul"}
  %arg51.52 = f32[256]{0} parameter(51), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg247.248 = f32[1,1,1024,256]{3,2,1,0} parameter(247), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.121 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg247.248), metadata={op_name="XLA_Args"}
  %custom-call.112 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.128, f32[1,1,1024,256]{1,0,2,3} %copy.121), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block1/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.650 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.112), index=0
  %arg50.51 = f32[256]{0} parameter(50), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.125 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.126, f32[256]{0} %arg51.52, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.650, f32[256]{0} %arg50.51), kind=kLoop, calls=%fused_computation.125, metadata={op_type="Relu" op_name="tower0/group2/block1/conv1/Relu"}
  %arg44.45 = f32[256]{0} parameter(44), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.124 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg44.45), kind=kLoop, calls=%fused_computation.124, metadata={op_type="Rsqrt" op_name="tower0/group2/block1/conv2/bn/batchnorm/Rsqrt"}
  %arg45.46 = f32[256]{0} parameter(45), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.123 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.124, f32[256]{0} %arg45.46), kind=kLoop, calls=%fused_computation.123, metadata={op_type="Mul" op_name="tower0/group2/block1/conv2/bn/batchnorm/mul"}
  %arg47.48 = f32[256]{0} parameter(47), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg248.249 = f32[3,3,256,256]{3,2,1,0} parameter(248), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.122 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg248.249), metadata={op_name="XLA_Args"}
  %custom-call.113 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.125, f32[3,3,256,256]{1,0,2,3} %copy.122), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block1/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.651 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.113), index=0
  %arg46.47 = f32[256]{0} parameter(46), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.122 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.123, f32[256]{0} %arg47.48, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.651, f32[256]{0} %arg46.47), kind=kLoop, calls=%fused_computation.122, metadata={op_type="Relu" op_name="tower0/group2/block1/conv2/Relu"}
  %arg42.43 = f32[1024]{0} parameter(42), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.121 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1024]{0} %arg42.43), kind=kLoop, calls=%fused_computation.121, metadata={op_type="Rsqrt" op_name="tower0/group2/block1/conv3/bn/batchnorm/Rsqrt"}
  %arg43.44 = f32[1024]{0} parameter(43), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.120 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.121, f32[1024]{0} %arg43.44), kind=kLoop, calls=%fused_computation.120, metadata={op_type="Mul" op_name="tower0/group2/block1/conv3/bn/batchnorm/mul"}
  %arg167.168 = f32[1024]{0} parameter(167), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg249.250 = f32[1,1,256,1024]{3,2,1,0} parameter(249), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.123 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg249.250), metadata={op_name="XLA_Args"}
  %custom-call.114 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.122, f32[1,1,256,1024]{1,0,2,3} %copy.123), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block1/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.652 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.114), index=0
  %arg166.167 = f32[1024]{0} parameter(166), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.119 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.120, f32[1024]{0} %arg167.168, f32[1,1024,50,50]{3,2,1,0} %fusion.128, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.652, f32[1024]{0} %arg166.167), kind=kLoop, calls=%fused_computation.119, metadata={op_type="Relu" op_name="tower0/group2/block1/output"}
  %arg38.39 = f32[256]{0} parameter(38), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.118 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg38.39), kind=kLoop, calls=%fused_computation.118, metadata={op_type="Rsqrt" op_name="tower0/group2/block2/conv1/bn/batchnorm/Rsqrt"}
  %arg39.40 = f32[256]{0} parameter(39), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.117 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.118, f32[256]{0} %arg39.40), kind=kLoop, calls=%fused_computation.117, metadata={op_type="Mul" op_name="tower0/group2/block2/conv1/bn/batchnorm/mul"}
  %arg41.42 = f32[256]{0} parameter(41), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg250.251 = f32[1,1,1024,256]{3,2,1,0} parameter(250), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.124 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg250.251), metadata={op_name="XLA_Args"}
  %custom-call.115 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.119, f32[1,1,1024,256]{1,0,2,3} %copy.124), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block2/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.653 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.115), index=0
  %arg40.41 = f32[256]{0} parameter(40), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.116 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.117, f32[256]{0} %arg41.42, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.653, f32[256]{0} %arg40.41), kind=kLoop, calls=%fused_computation.116, metadata={op_type="Relu" op_name="tower0/group2/block2/conv1/Relu"}
  %arg34.35 = f32[256]{0} parameter(34), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.115 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg34.35), kind=kLoop, calls=%fused_computation.115, metadata={op_type="Rsqrt" op_name="tower0/group2/block2/conv2/bn/batchnorm/Rsqrt"}
  %arg35.36 = f32[256]{0} parameter(35), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.114 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.115, f32[256]{0} %arg35.36), kind=kLoop, calls=%fused_computation.114, metadata={op_type="Mul" op_name="tower0/group2/block2/conv2/bn/batchnorm/mul"}
  %arg37.38 = f32[256]{0} parameter(37), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg251.252 = f32[3,3,256,256]{3,2,1,0} parameter(251), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.125 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg251.252), metadata={op_name="XLA_Args"}
  %custom-call.116 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.116, f32[3,3,256,256]{1,0,2,3} %copy.125), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block2/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.654 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.116), index=0
  %arg36.37 = f32[256]{0} parameter(36), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.113 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.114, f32[256]{0} %arg37.38, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.654, f32[256]{0} %arg36.37), kind=kLoop, calls=%fused_computation.113, metadata={op_type="Relu" op_name="tower0/group2/block2/conv2/Relu"}
  %arg32.33 = f32[1024]{0} parameter(32), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.112 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1024]{0} %arg32.33), kind=kLoop, calls=%fused_computation.112, metadata={op_type="Rsqrt" op_name="tower0/group2/block2/conv3/bn/batchnorm/Rsqrt"}
  %arg33.34 = f32[1024]{0} parameter(33), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.111 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.112, f32[1024]{0} %arg33.34), kind=kLoop, calls=%fused_computation.111, metadata={op_type="Mul" op_name="tower0/group2/block2/conv3/bn/batchnorm/mul"}
  %arg169.170 = f32[1024]{0} parameter(169), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg252.253 = f32[1,1,256,1024]{3,2,1,0} parameter(252), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.126 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg252.253), metadata={op_name="XLA_Args"}
  %custom-call.117 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.113, f32[1,1,256,1024]{1,0,2,3} %copy.126), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block2/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.655 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.117), index=0
  %arg168.169 = f32[1024]{0} parameter(168), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.110 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.111, f32[1024]{0} %arg169.170, f32[1,1024,50,50]{3,2,1,0} %fusion.119, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.655, f32[1024]{0} %arg168.169), kind=kLoop, calls=%fused_computation.110, metadata={op_type="Relu" op_name="tower0/group2/block2/output"}
  %arg28.29 = f32[256]{0} parameter(28), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.109 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg28.29), kind=kLoop, calls=%fused_computation.109, metadata={op_type="Rsqrt" op_name="tower0/group2/block3/conv1/bn/batchnorm/Rsqrt"}
  %arg29.30 = f32[256]{0} parameter(29), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.108 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.109, f32[256]{0} %arg29.30), kind=kLoop, calls=%fused_computation.108, metadata={op_type="Mul" op_name="tower0/group2/block3/conv1/bn/batchnorm/mul"}
  %arg31.32 = f32[256]{0} parameter(31), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg253.254 = f32[1,1,1024,256]{3,2,1,0} parameter(253), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.127 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg253.254), metadata={op_name="XLA_Args"}
  %custom-call.118 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.110, f32[1,1,1024,256]{1,0,2,3} %copy.127), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block3/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.656 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.118), index=0
  %arg30.31 = f32[256]{0} parameter(30), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.107 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.108, f32[256]{0} %arg31.32, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.656, f32[256]{0} %arg30.31), kind=kLoop, calls=%fused_computation.107, metadata={op_type="Relu" op_name="tower0/group2/block3/conv1/Relu"}
  %arg24.25 = f32[256]{0} parameter(24), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.106 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg24.25), kind=kLoop, calls=%fused_computation.106, metadata={op_type="Rsqrt" op_name="tower0/group2/block3/conv2/bn/batchnorm/Rsqrt"}
  %arg25.26 = f32[256]{0} parameter(25), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.105 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.106, f32[256]{0} %arg25.26), kind=kLoop, calls=%fused_computation.105, metadata={op_type="Mul" op_name="tower0/group2/block3/conv2/bn/batchnorm/mul"}
  %arg27.28 = f32[256]{0} parameter(27), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg254.255 = f32[3,3,256,256]{3,2,1,0} parameter(254), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.128 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg254.255), metadata={op_name="XLA_Args"}
  %custom-call.119 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.107, f32[3,3,256,256]{1,0,2,3} %copy.128), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block3/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.657 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.119), index=0
  %arg26.27 = f32[256]{0} parameter(26), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.104 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.105, f32[256]{0} %arg27.28, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.657, f32[256]{0} %arg26.27), kind=kLoop, calls=%fused_computation.104, metadata={op_type="Relu" op_name="tower0/group2/block3/conv2/Relu"}
  %arg22.23 = f32[1024]{0} parameter(22), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.103 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1024]{0} %arg22.23), kind=kLoop, calls=%fused_computation.103, metadata={op_type="Rsqrt" op_name="tower0/group2/block3/conv3/bn/batchnorm/Rsqrt"}
  %arg23.24 = f32[1024]{0} parameter(23), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.102 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.103, f32[1024]{0} %arg23.24), kind=kLoop, calls=%fused_computation.102, metadata={op_type="Mul" op_name="tower0/group2/block3/conv3/bn/batchnorm/mul"}
  %arg171.172 = f32[1024]{0} parameter(171), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg255.256 = f32[1,1,256,1024]{3,2,1,0} parameter(255), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.129 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg255.256), metadata={op_name="XLA_Args"}
  %custom-call.120 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.104, f32[1,1,256,1024]{1,0,2,3} %copy.129), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block3/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.658 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.120), index=0
  %arg170.171 = f32[1024]{0} parameter(170), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.101 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.102, f32[1024]{0} %arg171.172, f32[1,1024,50,50]{3,2,1,0} %fusion.110, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.658, f32[1024]{0} %arg170.171), kind=kLoop, calls=%fused_computation.101, metadata={op_type="Relu" op_name="tower0/group2/block3/output"}
  %arg18.19 = f32[256]{0} parameter(18), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.100 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg18.19), kind=kLoop, calls=%fused_computation.100, metadata={op_type="Rsqrt" op_name="tower0/group2/block4/conv1/bn/batchnorm/Rsqrt"}
  %arg19.20 = f32[256]{0} parameter(19), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.99 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.100, f32[256]{0} %arg19.20), kind=kLoop, calls=%fused_computation.99, metadata={op_type="Mul" op_name="tower0/group2/block4/conv1/bn/batchnorm/mul"}
  %arg21.22 = f32[256]{0} parameter(21), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg256.257 = f32[1,1,1024,256]{3,2,1,0} parameter(256), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.130 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg256.257), metadata={op_name="XLA_Args"}
  %custom-call.121 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.101, f32[1,1,1024,256]{1,0,2,3} %copy.130), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block4/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.659 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.121), index=0
  %arg20.21 = f32[256]{0} parameter(20), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.98 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.99, f32[256]{0} %arg21.22, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.659, f32[256]{0} %arg20.21), kind=kLoop, calls=%fused_computation.98, metadata={op_type="Relu" op_name="tower0/group2/block4/conv1/Relu"}
  %arg14.15 = f32[256]{0} parameter(14), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.97 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg14.15), kind=kLoop, calls=%fused_computation.97, metadata={op_type="Rsqrt" op_name="tower0/group2/block4/conv2/bn/batchnorm/Rsqrt"}
  %arg15.16 = f32[256]{0} parameter(15), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.96 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.97, f32[256]{0} %arg15.16), kind=kLoop, calls=%fused_computation.96, metadata={op_type="Mul" op_name="tower0/group2/block4/conv2/bn/batchnorm/mul"}
  %arg17.18 = f32[256]{0} parameter(17), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg257.258 = f32[3,3,256,256]{3,2,1,0} parameter(257), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.131 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg257.258), metadata={op_name="XLA_Args"}
  %custom-call.122 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.98, f32[3,3,256,256]{1,0,2,3} %copy.131), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block4/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.660 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.122), index=0
  %arg16.17 = f32[256]{0} parameter(16), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.95 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.96, f32[256]{0} %arg17.18, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.660, f32[256]{0} %arg16.17), kind=kLoop, calls=%fused_computation.95, metadata={op_type="Relu" op_name="tower0/group2/block4/conv2/Relu"}
  %arg12.13 = f32[1024]{0} parameter(12), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.94 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1024]{0} %arg12.13), kind=kLoop, calls=%fused_computation.94, metadata={op_type="Rsqrt" op_name="tower0/group2/block4/conv3/bn/batchnorm/Rsqrt"}
  %arg13.14 = f32[1024]{0} parameter(13), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.93 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.94, f32[1024]{0} %arg13.14), kind=kLoop, calls=%fused_computation.93, metadata={op_type="Mul" op_name="tower0/group2/block4/conv3/bn/batchnorm/mul"}
  %arg173.174 = f32[1024]{0} parameter(173), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg258.259 = f32[1,1,256,1024]{3,2,1,0} parameter(258), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.132 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg258.259), metadata={op_name="XLA_Args"}
  %custom-call.123 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.95, f32[1,1,256,1024]{1,0,2,3} %copy.132), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block4/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.661 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.123), index=0
  %arg172.173 = f32[1024]{0} parameter(172), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.92 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.93, f32[1024]{0} %arg173.174, f32[1,1024,50,50]{3,2,1,0} %fusion.101, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.661, f32[1024]{0} %arg172.173), kind=kLoop, calls=%fused_computation.92, metadata={op_type="Relu" op_name="tower0/group2/block4/output"}
  %arg8.9 = f32[256]{0} parameter(8), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.91 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg8.9), kind=kLoop, calls=%fused_computation.91, metadata={op_type="Rsqrt" op_name="tower0/group2/block5/conv1/bn/batchnorm/Rsqrt"}
  %arg9.10 = f32[256]{0} parameter(9), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.90 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.91, f32[256]{0} %arg9.10), kind=kLoop, calls=%fused_computation.90, metadata={op_type="Mul" op_name="tower0/group2/block5/conv1/bn/batchnorm/mul"}
  %arg11.12 = f32[256]{0} parameter(11), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg259.260 = f32[1,1,1024,256]{3,2,1,0} parameter(259), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.133 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg259.260), metadata={op_name="XLA_Args"}
  %custom-call.124 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.92, f32[1,1,1024,256]{1,0,2,3} %copy.133), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block5/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.662 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.124), index=0
  %arg10.11 = f32[256]{0} parameter(10), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.89 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.90, f32[256]{0} %arg11.12, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.662, f32[256]{0} %arg10.11), kind=kLoop, calls=%fused_computation.89, metadata={op_type="Relu" op_name="tower0/group2/block5/conv1/Relu"}
  %arg4.5 = f32[256]{0} parameter(4), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.88 = f32[1,256,1,1]{3,2,1,0} fusion(f32[256]{0} %arg4.5), kind=kLoop, calls=%fused_computation.88, metadata={op_type="Rsqrt" op_name="tower0/group2/block5/conv2/bn/batchnorm/Rsqrt"}
  %arg5.6 = f32[256]{0} parameter(5), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.87 = f32[1,256,1,1]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.88, f32[256]{0} %arg5.6), kind=kLoop, calls=%fused_computation.87, metadata={op_type="Mul" op_name="tower0/group2/block5/conv2/bn/batchnorm/mul"}
  %arg7.8 = f32[256]{0} parameter(7), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg260.261 = f32[3,3,256,256]{3,2,1,0} parameter(260), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.134 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg260.261), metadata={op_name="XLA_Args"}
  %custom-call.125 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.89, f32[3,3,256,256]{1,0,2,3} %copy.134), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block5/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.663 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.125), index=0
  %arg6.7 = f32[256]{0} parameter(6), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.86 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,1,1]{3,2,1,0} %fusion.87, f32[256]{0} %arg7.8, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.663, f32[256]{0} %arg6.7), kind=kLoop, calls=%fused_computation.86, metadata={op_type="Relu" op_name="tower0/group2/block5/conv2/Relu"}
  %arg2.3 = f32[1024]{0} parameter(2), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.85 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1024]{0} %arg2.3), kind=kLoop, calls=%fused_computation.85, metadata={op_type="Rsqrt" op_name="tower0/group2/block5/conv3/bn/batchnorm/Rsqrt"}
  %arg3.4 = f32[1024]{0} parameter(3), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.84 = f32[1,1024,1,1]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.85, f32[1024]{0} %arg3.4), kind=kLoop, calls=%fused_computation.84, metadata={op_type="Mul" op_name="tower0/group2/block5/conv3/bn/batchnorm/mul"}
  %arg175.176 = f32[1024]{0} parameter(175), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg261.262 = f32[1,1,256,1024]{3,2,1,0} parameter(261), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.135 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg261.262), metadata={op_name="XLA_Args"}
  %custom-call.126 = (f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.86, f32[1,1,256,1024]{1,0,2,3} %copy.135), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group2/block5/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.664 = f32[1,1024,50,50]{3,2,1,0} get-tuple-element((f32[1,1024,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.126), index=0
  %arg174.175 = f32[1024]{0} parameter(174), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.83 = f32[1,1024,50,50]{3,2,1,0} fusion(f32[1,1024,1,1]{3,2,1,0} %fusion.84, f32[1024]{0} %arg175.176, f32[1,1024,50,50]{3,2,1,0} %fusion.92, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.664, f32[1024]{0} %arg174.175), kind=kLoop, calls=%fused_computation.83, metadata={op_type="Relu" op_name="tower0/group2/block5/output"}
  %arg0.1 = f32[512]{0} parameter(0), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.82 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg0.1), kind=kLoop, calls=%fused_computation.82, metadata={op_type="Rsqrt" op_name="tower0/group3/block0/conv1/bn/batchnorm/Rsqrt"}
  %arg1.2 = f32[512]{0} parameter(1), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.81 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.82, f32[512]{0} %arg1.2), kind=kLoop, calls=%fused_computation.81, metadata={op_type="Mul" op_name="tower0/group3/block0/conv1/bn/batchnorm/mul"}
  %arg177.178 = f32[512]{0} parameter(177), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg262.263 = f32[1,1,1024,512]{3,2,1,0} parameter(262), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.136 = f32[1,1,1024,512]{1,0,2,3} copy(f32[1,1,1024,512]{3,2,1,0} %arg262.263), metadata={op_name="XLA_Args"}
  %custom-call.128 = (f32[1,512,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.83, f32[1,1,1024,512]{1,0,2,3} %copy.136), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block0/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.666 = f32[1,512,50,50]{3,2,1,0} get-tuple-element((f32[1,512,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.128), index=0
  %arg176.177 = f32[512]{0} parameter(176), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.80 = f32[1,512,50,50]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.81, f32[512]{0} %arg177.178, f32[1,512,50,50]{3,2,1,0} %get-tuple-element.666, f32[512]{0} %arg176.177), kind=kLoop, calls=%fused_computation.80, metadata={op_type="Relu" op_name="tower0/group3/block0/conv1/Relu"}
  %pad.11 = f32[1,512,51,51]{3,2,1,0} pad(f32[1,512,50,50]{3,2,1,0} %fusion.80, f32[] %constant_1237), padding=0_0x0_0x1_0x1_0
  %arg178.179 = f32[512]{0} parameter(178), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.79 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg178.179), kind=kLoop, calls=%fused_computation.79, metadata={op_type="Rsqrt" op_name="tower0/group3/block0/conv2/bn/batchnorm/Rsqrt"}
  %arg179.180 = f32[512]{0} parameter(179), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.78 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.79, f32[512]{0} %arg179.180), kind=kLoop, calls=%fused_computation.78, metadata={op_type="Mul" op_name="tower0/group3/block0/conv2/bn/batchnorm/mul"}
  %arg181.182 = f32[512]{0} parameter(181), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg266.267 = f32[3,3,512,512]{3,2,1,0} parameter(266), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.137 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg266.267), metadata={op_name="XLA_Args"}
  %custom-call.160 = (f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,512,51,51]{3,2,1,0} %pad.11, f32[3,3,512,512]{1,0,2,3} %copy.137), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block0/conv2/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.698 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.160), index=0
  %arg180.181 = f32[512]{0} parameter(180), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.77 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.78, f32[512]{0} %arg181.182, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.698, f32[512]{0} %arg180.181), kind=kLoop, calls=%fused_computation.77, metadata={op_type="Relu" op_name="tower0/group3/block0/conv2/Relu"}
  %arg186.187 = f32[2048]{0} parameter(186), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.74 = f32[1,2048,1,1]{3,2,1,0} fusion(f32[2048]{0} %arg186.187), kind=kLoop, calls=%fused_computation.74, metadata={op_type="Rsqrt" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/Rsqrt"}
  %arg187.188 = f32[2048]{0} parameter(187), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.73 = f32[1,2048,1,1]{3,2,1,0} fusion(f32[1,2048,1,1]{3,2,1,0} %fusion.74, f32[2048]{0} %arg187.188), kind=kLoop, calls=%fused_computation.73, metadata={op_type="Mul" op_name="tower0/group3/block0/convshortcut/bn/batchnorm/mul"}
  %arg189.190 = f32[2048]{0} parameter(189), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg263.264 = f32[1,1,1024,2048]{3,2,1,0} parameter(263), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.139 = f32[1,1,1024,2048]{1,0,2,3} copy(f32[1,1,1024,2048]{3,2,1,0} %arg263.264), metadata={op_name="XLA_Args"}
  %custom-call.130 = (f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.83, f32[1,1,1024,2048]{1,0,2,3} %copy.139), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block0/convshortcut/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.668 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) %custom-call.130), index=0
  %arg182.183 = f32[2048]{0} parameter(182), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.76 = f32[1,2048,1,1]{3,2,1,0} fusion(f32[2048]{0} %arg182.183), kind=kLoop, calls=%fused_computation.76, metadata={op_type="Rsqrt" op_name="tower0/group3/block0/conv3/bn/batchnorm/Rsqrt"}
  %arg183.184 = f32[2048]{0} parameter(183), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.75 = f32[1,2048,1,1]{3,2,1,0} fusion(f32[1,2048,1,1]{3,2,1,0} %fusion.76, f32[2048]{0} %arg183.184), kind=kLoop, calls=%fused_computation.75, metadata={op_type="Mul" op_name="tower0/group3/block0/conv3/bn/batchnorm/mul"}
  %arg185.186 = f32[2048]{0} parameter(185), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg267.268 = f32[1,1,512,2048]{3,2,1,0} parameter(267), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.138 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %arg267.268), metadata={op_name="XLA_Args"}
  %custom-call.129 = (f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.77, f32[1,1,512,2048]{1,0,2,3} %copy.138), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block0/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.667 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) %custom-call.129), index=0
  %arg184.185 = f32[2048]{0} parameter(184), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg188.189 = f32[2048]{0} parameter(188), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.72 = f32[1,2048,25,25]{3,2,1,0} fusion(f32[1,2048,1,1]{3,2,1,0} %fusion.73, f32[2048]{0} %arg189.190, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.668, f32[1,2048,1,1]{3,2,1,0} %fusion.75, f32[2048]{0} %arg185.186, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.667, f32[2048]{0} %arg184.185, f32[2048]{0} %arg188.189), kind=kLoop, calls=%fused_computation.72, metadata={op_type="Relu" op_name="tower0/group3/block0/output"}
  %arg190.191 = f32[512]{0} parameter(190), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.71 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg190.191), kind=kLoop, calls=%fused_computation.71, metadata={op_type="Rsqrt" op_name="tower0/group3/block1/conv1/bn/batchnorm/Rsqrt"}
  %arg191.192 = f32[512]{0} parameter(191), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.70 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.71, f32[512]{0} %arg191.192), kind=kLoop, calls=%fused_computation.70, metadata={op_type="Mul" op_name="tower0/group3/block1/conv1/bn/batchnorm/mul"}
  %arg193.194 = f32[512]{0} parameter(193), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg268.269 = f32[1,1,2048,512]{3,2,1,0} parameter(268), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.140 = f32[1,1,2048,512]{1,0,2,3} copy(f32[1,1,2048,512]{3,2,1,0} %arg268.269), metadata={op_name="XLA_Args"}
  %custom-call.131 = (f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %fusion.72, f32[1,1,2048,512]{1,0,2,3} %copy.140), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block1/conv1/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.669 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.131), index=0
  %arg192.193 = f32[512]{0} parameter(192), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.69 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.70, f32[512]{0} %arg193.194, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.669, f32[512]{0} %arg192.193), kind=kLoop, calls=%fused_computation.69, metadata={op_type="Relu" op_name="tower0/group3/block1/conv1/Relu"}
  %arg194.195 = f32[512]{0} parameter(194), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.68 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg194.195), kind=kLoop, calls=%fused_computation.68, metadata={op_type="Rsqrt" op_name="tower0/group3/block1/conv2/bn/batchnorm/Rsqrt"}
  %arg195.196 = f32[512]{0} parameter(195), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.67 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.68, f32[512]{0} %arg195.196), kind=kLoop, calls=%fused_computation.67, metadata={op_type="Mul" op_name="tower0/group3/block1/conv2/bn/batchnorm/mul"}
  %arg197.198 = f32[512]{0} parameter(197), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg269.270 = f32[3,3,512,512]{3,2,1,0} parameter(269), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.141 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg269.270), metadata={op_name="XLA_Args"}
  %custom-call.132 = (f32[1,512,25,25]{3,2,1,0}, u8[26216448]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.69, f32[3,3,512,512]{1,0,2,3} %copy.141), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block1/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.670 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[26216448]{0}) %custom-call.132), index=0
  %arg196.197 = f32[512]{0} parameter(196), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.66 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.67, f32[512]{0} %arg197.198, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.670, f32[512]{0} %arg196.197), kind=kLoop, calls=%fused_computation.66, metadata={op_type="Relu" op_name="tower0/group3/block1/conv2/Relu"}
  %arg198.199 = f32[2048]{0} parameter(198), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.65 = f32[1,2048,1,1]{3,2,1,0} fusion(f32[2048]{0} %arg198.199), kind=kLoop, calls=%fused_computation.65, metadata={op_type="Rsqrt" op_name="tower0/group3/block1/conv3/bn/batchnorm/Rsqrt"}
  %arg199.200 = f32[2048]{0} parameter(199), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.64 = f32[1,2048,1,1]{3,2,1,0} fusion(f32[1,2048,1,1]{3,2,1,0} %fusion.65, f32[2048]{0} %arg199.200), kind=kLoop, calls=%fused_computation.64, metadata={op_type="Mul" op_name="tower0/group3/block1/conv3/bn/batchnorm/mul"}
  %arg201.202 = f32[2048]{0} parameter(201), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg270.271 = f32[1,1,512,2048]{3,2,1,0} parameter(270), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.142 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %arg270.271), metadata={op_name="XLA_Args"}
  %custom-call.133 = (f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.66, f32[1,1,512,2048]{1,0,2,3} %copy.142), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block1/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.671 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) %custom-call.133), index=0
  %arg200.201 = f32[2048]{0} parameter(200), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.63 = f32[1,2048,25,25]{3,2,1,0} fusion(f32[1,2048,1,1]{3,2,1,0} %fusion.64, f32[2048]{0} %arg201.202, f32[1,2048,25,25]{3,2,1,0} %fusion.72, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.671, f32[2048]{0} %arg200.201), kind=kLoop, calls=%fused_computation.63, metadata={op_type="Relu" op_name="tower0/group3/block1/output"}
  %arg202.203 = f32[512]{0} parameter(202), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.62 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg202.203), kind=kLoop, calls=%fused_computation.62, metadata={op_type="Rsqrt" op_name="tower0/group3/block2/conv1/bn/batchnorm/Rsqrt"}
  %arg203.204 = f32[512]{0} parameter(203), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.61 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.62, f32[512]{0} %arg203.204), kind=kLoop, calls=%fused_computation.61, metadata={op_type="Mul" op_name="tower0/group3/block2/conv1/bn/batchnorm/mul"}
  %arg205.206 = f32[512]{0} parameter(205), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg271.272 = f32[1,1,2048,512]{3,2,1,0} parameter(271), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.143 = f32[1,1,2048,512]{1,0,2,3} copy(f32[1,1,2048,512]{3,2,1,0} %arg271.272), metadata={op_name="XLA_Args"}
  %custom-call.134 = (f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %fusion.63, f32[1,1,2048,512]{1,0,2,3} %copy.143), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block2/conv1/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.672 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.134), index=0
  %arg204.205 = f32[512]{0} parameter(204), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.60 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.61, f32[512]{0} %arg205.206, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.672, f32[512]{0} %arg204.205), kind=kLoop, calls=%fused_computation.60, metadata={op_type="Relu" op_name="tower0/group3/block2/conv1/Relu"}
  %arg206.207 = f32[512]{0} parameter(206), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.59 = f32[1,512,1,1]{3,2,1,0} fusion(f32[512]{0} %arg206.207), kind=kLoop, calls=%fused_computation.59, metadata={op_type="Rsqrt" op_name="tower0/group3/block2/conv2/bn/batchnorm/Rsqrt"}
  %arg207.208 = f32[512]{0} parameter(207), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.58 = f32[1,512,1,1]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.59, f32[512]{0} %arg207.208), kind=kLoop, calls=%fused_computation.58, metadata={op_type="Mul" op_name="tower0/group3/block2/conv2/bn/batchnorm/mul"}
  %arg209.210 = f32[512]{0} parameter(209), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg272.273 = f32[3,3,512,512]{3,2,1,0} parameter(272), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.144 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg272.273), metadata={op_name="XLA_Args"}
  %custom-call.135 = (f32[1,512,25,25]{3,2,1,0}, u8[26216448]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.60, f32[3,3,512,512]{1,0,2,3} %copy.144), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block2/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.673 = f32[1,512,25,25]{3,2,1,0} get-tuple-element((f32[1,512,25,25]{3,2,1,0}, u8[26216448]{0}) %custom-call.135), index=0
  %arg208.209 = f32[512]{0} parameter(208), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.57 = f32[1,512,25,25]{3,2,1,0} fusion(f32[1,512,1,1]{3,2,1,0} %fusion.58, f32[512]{0} %arg209.210, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.673, f32[512]{0} %arg208.209), kind=kLoop, calls=%fused_computation.57, metadata={op_type="Relu" op_name="tower0/group3/block2/conv2/Relu"}
  %arg210.211 = f32[2048]{0} parameter(210), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.56 = f32[1,2048,1,1]{3,2,1,0} fusion(f32[2048]{0} %arg210.211), kind=kLoop, calls=%fused_computation.56, metadata={op_type="Rsqrt" op_name="tower0/group3/block2/conv3/bn/batchnorm/Rsqrt"}
  %arg211.212 = f32[2048]{0} parameter(211), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.55 = f32[1,2048,1,1]{3,2,1,0} fusion(f32[1,2048,1,1]{3,2,1,0} %fusion.56, f32[2048]{0} %arg211.212), kind=kLoop, calls=%fused_computation.55, metadata={op_type="Mul" op_name="tower0/group3/block2/conv3/bn/batchnorm/mul"}
  %arg213.214 = f32[2048]{0} parameter(213), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg273.274 = f32[1,1,512,2048]{3,2,1,0} parameter(273), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.145 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %arg273.274), metadata={op_name="XLA_Args"}
  %custom-call.136 = (f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) custom-call(f32[1,512,25,25]{3,2,1,0} %fusion.57, f32[1,1,512,2048]{1,0,2,3} %copy.145), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/group3/block2/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.674 = f32[1,2048,25,25]{3,2,1,0} get-tuple-element((f32[1,2048,25,25]{3,2,1,0}, u8[3756]{0}) %custom-call.136), index=0
  %arg212.213 = f32[2048]{0} parameter(212), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.54 = f32[1,2048,25,25]{3,2,1,0} fusion(f32[1,2048,1,1]{3,2,1,0} %fusion.55, f32[2048]{0} %arg213.214, f32[1,2048,25,25]{3,2,1,0} %fusion.63, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.674, f32[2048]{0} %arg212.213), kind=kLoop, calls=%fused_computation.54, metadata={op_type="Relu" op_name="tower0/group3/block2/output"}
  %arg274.275 = f32[1,1,2048,256]{3,2,1,0} parameter(274), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.146 = f32[1,1,2048,256]{1,0,2,3} copy(f32[1,1,2048,256]{3,2,1,0} %arg274.275), metadata={op_name="XLA_Args"}
  %custom-call.137 = (f32[1,256,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,2048,25,25]{3,2,1,0} %fusion.54, f32[1,1,2048,256]{1,0,2,3} %copy.146), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/fpn/lateral_1x1_c5/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.675 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[1,256,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.137), index=0
  %arg275.276 = f32[256]{0} parameter(275), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.53 = f32[1,256,25,25]{3,2,1,0} fusion(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.675, f32[256]{0} %arg275.276), kind=kLoop, calls=%fused_computation.53, metadata={op_type="BiasAdd" op_name="tower0/fpn/lateral_1x1_c5/BiasAdd"}
  %arg276.277 = f32[3,3,256,256]{3,2,1,0} parameter(276), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.147 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg276.277), metadata={op_name="XLA_Args"}
  %custom-call.138 = (f32[1,256,25,25]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %fusion.53, f32[3,3,256,256]{1,0,2,3} %copy.147), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/fpn/posthoc_3x3_p5/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.676 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[1,256,25,25]{3,2,1,0}, u8[6554624]{0}) %custom-call.138), index=0
  %arg277.278 = f32[256]{0} parameter(277), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.52 = f32[1,256,25,25]{3,2,1,0} fusion(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.676, f32[256]{0} %arg277.278), kind=kLoop, calls=%fused_computation.52, metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p5/BiasAdd"}
  %constant_1242 = f32[] constant(-inf), metadata={op_type="MaxPool" op_name="tower0/pool0/MaxPool"}
  %reduce-window.1827 = f32[1,256,13,13]{3,2,1,0} reduce-window(f32[1,256,25,25]{3,2,1,0} %fusion.52, f32[] %constant_1242), window={size=1x1x1x1 stride=1x1x2x2}, to_apply=%max_F32.1823, metadata={op_type="MaxPool" op_name="tower0/fpn/maxpool_p6/MaxPool"}
  %arg278.279 = f32[3,3,256,256]{3,2,1,0} parameter(278), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.149 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg278.279), metadata={op_name="XLA_Args"}
  %arg279.280 = f32[256]{0} parameter(279), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.153 = (f32[1,256,25,25]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %fusion.52, f32[3,3,256,256]{1,0,2,3} %copy.149, f32[256]{0} %arg279.280), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower0/rpn_3/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.691 = f32[1,256,25,25]{3,2,1,0} get-tuple-element((f32[1,256,25,25]{3,2,1,0}, u8[6554624]{0}) %custom-call.153), index=0
  %custom-call.152 = (f32[1,256,13,13]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,13,13]{3,2,1,0} %reduce-window.1827, f32[3,3,256,256]{1,0,2,3} %copy.149, f32[256]{0} %arg279.280), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower0/rpn_4/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.690 = f32[1,256,13,13]{3,2,1,0} get-tuple-element((f32[1,256,13,13]{3,2,1,0}, u8[6554624]{0}) %custom-call.152), index=0
  %arg282.283 = s32[42,42,3]{2,1,0} parameter(282), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.44 = (pred[25,25,3]{2,1,0}, pred[25,25,3]{2,1,0}) fusion(s32[42,42,3]{2,1,0} %arg282.283), kind=kLoop, calls=%fused_computation.44, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level5/NotEqual"}
  %get-tuple-element.705 = pred[25,25,3]{2,1,0} get-tuple-element((pred[25,25,3]{2,1,0}, pred[25,25,3]{2,1,0}) %fusion.44), index=0
  %get-tuple-element.706 = pred[25,25,3]{2,1,0} get-tuple-element((pred[25,25,3]{2,1,0}, pred[25,25,3]{2,1,0}) %fusion.44), index=1
  %arg283.284 = s32[21,21,3]{2,1,0} parameter(283), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.42 = (pred[13,13,3]{2,1,0}, pred[13,13,3]{2,1,0}) fusion(s32[21,21,3]{2,1,0} %arg283.284), kind=kLoop, calls=%fused_computation.42, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level6/NotEqual"}
  %get-tuple-element.707 = pred[13,13,3]{2,1,0} get-tuple-element((pred[13,13,3]{2,1,0}, pred[13,13,3]{2,1,0}) %fusion.42), index=0
  %get-tuple-element.708 = pred[13,13,3]{2,1,0} get-tuple-element((pred[13,13,3]{2,1,0}, pred[13,13,3]{2,1,0}) %fusion.42), index=1
  %bitcast.254 = pred[1875]{0} bitcast(pred[25,25,3]{2,1,0} %get-tuple-element.705), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level5/boolean_mask_1/Reshape_1"}
  %copy.195 = pred[1875]{0} copy(pred[1875]{0} %bitcast.254)
  %copy.196 = pred[1875]{0} copy(pred[1875]{0} %bitcast.254)
  %bitcast.255 = pred[1875]{0} bitcast(pred[25,25,3]{2,1,0} %get-tuple-element.706), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level5/boolean_mask_3/Reshape_1"}
  %copy.197 = pred[1875]{0} copy(pred[1875]{0} %bitcast.255)
  %bitcast.256 = pred[507]{0} bitcast(pred[13,13,3]{2,1,0} %get-tuple-element.707), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level6/boolean_mask_1/Reshape_1"}
  %copy.198 = pred[507]{0} copy(pred[507]{0} %bitcast.256)
  %copy.199 = pred[507]{0} copy(pred[507]{0} %bitcast.256)
  %bitcast.257 = pred[507]{0} bitcast(pred[13,13,3]{2,1,0} %get-tuple-element.708), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level6/boolean_mask_3/Reshape_1"}
  %copy.200 = pred[507]{0} copy(pred[507]{0} %bitcast.257)
  %arg280.281 = f32[1,1,256,3]{3,2,1,0} parameter(280), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.151 = f32[1,1,256,3]{1,0,2,3} copy(f32[1,1,256,3]{3,2,1,0} %arg280.281), metadata={op_name="XLA_Args"}
  %custom-call.142 = (f32[1,3,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.691, f32[1,1,256,3]{1,0,2,3} %copy.151), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn_3/class/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.680 = f32[1,3,25,25]{3,2,1,0} get-tuple-element((f32[1,3,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.142), index=0
  %arg284.285 = f32[3]{0} parameter(284), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.39 = f32[25,25,3]{2,1,0} fusion(f32[1,3,25,25]{3,2,1,0} %get-tuple-element.680, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.39, metadata={op_name="XLA_Retvals"}
  %custom-call.140 = (f32[1,3,13,13]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,13,13]{3,2,1,0} %get-tuple-element.690, f32[1,1,256,3]{1,0,2,3} %copy.151), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn_4/class/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.678 = f32[1,3,13,13]{3,2,1,0} get-tuple-element((f32[1,3,13,13]{3,2,1,0}, u8[0]{0}) %custom-call.140), index=0
  %fusion.37 = f32[13,13,3]{2,1,0} fusion(f32[1,3,13,13]{3,2,1,0} %get-tuple-element.678, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.37, metadata={op_name="XLA_Retvals"}
  %arg281.282 = f32[1,1,256,12]{3,2,1,0} parameter(281), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.154 = f32[1,1,256,12]{1,0,2,3} copy(f32[1,1,256,12]{3,2,1,0} %arg281.282), metadata={op_name="XLA_Args"}
  %custom-call.141 = (f32[1,12,25,25]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,25,25]{3,2,1,0} %get-tuple-element.691, f32[1,1,256,12]{1,0,2,3} %copy.154), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn_3/box/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.679 = f32[1,12,25,25]{3,2,1,0} get-tuple-element((f32[1,12,25,25]{3,2,1,0}, u8[0]{0}) %custom-call.141), index=0
  %arg285.286 = f32[12]{0} parameter(285), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.36 = f32[1,25,25,12]{3,2,1,0} fusion(f32[1,12,25,25]{3,2,1,0} %get-tuple-element.679, f32[12]{0} %arg285.286), kind=kLoop, calls=%fused_computation.36, metadata={op_name="XLA_Retvals"}
  %arg264.265 = f32[1,1,1024,256]{3,2,1,0} parameter(264), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.153 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg264.265), metadata={op_name="XLA_Args"}
  %custom-call.127 = (f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) custom-call(f32[1,1024,50,50]{3,2,1,0} %fusion.83, f32[1,1,1024,256]{1,0,2,3} %copy.153), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/fpn/lateral_1x1_c4/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.665 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[15008]{0}) %custom-call.127), index=0
  %arg265.266 = f32[256]{0} parameter(265), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.51 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,25,25]{3,2,1,0} %fusion.53, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.665, f32[256]{0} %arg265.266), kind=kLoop, calls=%fused_computation.51, metadata={op_type="Add" op_name="tower0/fpn/add"}
  %custom-call.139 = (f32[1,12,13,13]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,13,13]{3,2,1,0} %get-tuple-element.690, f32[1,1,256,12]{1,0,2,3} %copy.154), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn_4/box/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.677 = f32[1,12,13,13]{3,2,1,0} get-tuple-element((f32[1,12,13,13]{3,2,1,0}, u8[0]{0}) %custom-call.139), index=0
  %fusion.35 = f32[1,13,13,12]{3,2,1,0} fusion(f32[1,12,13,13]{3,2,1,0} %get-tuple-element.677, f32[12]{0} %arg285.286), kind=kLoop, calls=%fused_computation.35, metadata={op_name="XLA_Retvals"}
  %arg286.287 = f32[3,3,256,256]{3,2,1,0} parameter(286), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.155 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg286.287), metadata={op_name="XLA_Args"}
  %custom-call.143 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.51, f32[3,3,256,256]{1,0,2,3} %copy.155), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/fpn/posthoc_3x3_p4/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.681 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.143), index=0
  %arg287.288 = f32[256]{0} parameter(287), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.50 = f32[1,256,50,50]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.681, f32[256]{0} %arg287.288), kind=kLoop, calls=%fused_computation.50, metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p4/BiasAdd"}
  %custom-call.154 = (f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %fusion.50, f32[3,3,256,256]{1,0,2,3} %copy.149, f32[256]{0} %arg279.280), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower0/rpn_2/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.692 = f32[1,256,50,50]{3,2,1,0} get-tuple-element((f32[1,256,50,50]{3,2,1,0}, u8[6554624]{0}) %custom-call.154), index=0
  %arg288.289 = s32[84,84,3]{2,1,0} parameter(288), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.34 = (pred[50,50,3]{2,1,0}, pred[50,50,3]{2,1,0}) fusion(s32[84,84,3]{2,1,0} %arg288.289), kind=kLoop, calls=%fused_computation.34, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level4/NotEqual"}
  %get-tuple-element.703 = pred[50,50,3]{2,1,0} get-tuple-element((pred[50,50,3]{2,1,0}, pred[50,50,3]{2,1,0}) %fusion.34), index=0
  %get-tuple-element.704 = pred[50,50,3]{2,1,0} get-tuple-element((pred[50,50,3]{2,1,0}, pred[50,50,3]{2,1,0}) %fusion.34), index=1
  %bitcast.267 = pred[7500]{0} bitcast(pred[50,50,3]{2,1,0} %get-tuple-element.703), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level4/boolean_mask_1/Reshape_1"}
  %copy.201 = pred[7500]{0} copy(pred[7500]{0} %bitcast.267)
  %copy.202 = pred[7500]{0} copy(pred[7500]{0} %bitcast.267)
  %bitcast.268 = pred[7500]{0} bitcast(pred[50,50,3]{2,1,0} %get-tuple-element.704), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level4/boolean_mask_3/Reshape_1"}
  %copy.203 = pred[7500]{0} copy(pred[7500]{0} %bitcast.268)
  %fusion.31 = f32[1875]{0} fusion(f32[1,3,25,25]{3,2,1,0} %get-tuple-element.680, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.31, metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl5/Reshape_1"}
  %iota.1971 = s32[1875]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %sort.1994 = (f32[1875]{0}, s32[1875]{0}) sort(f32[1875]{0} %fusion.31, s32[1875]{0} %iota.1971), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.1972, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.1997 = s32[1875]{0} get-tuple-element((f32[1875]{0}, s32[1875]{0}) %sort.1994), index=1, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %fusion.32 = f32[1875,1,2]{2,1,0} fusion(f32[1,25,25,12]{3,2,1,0} %fusion.36), kind=kLoop, calls=%fused_computation.32, metadata={op_type="Exp" op_name="tower0/decode_bbox_target_3/Exp"}
  %constant_19 = f32[1875,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower0/decode_bbox_target_3/sub"}
  %constant_18 = f32[1875,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower0/decode_bbox_target_3/add"}
  %fusion.30 = f32[1875,4]{1,0} fusion(s32[1875]{0} %get-tuple-element.1997, f32[1875,1,2]{2,1,0} %fusion.32, f32[1875,1,2]{2,1,0} %constant_19, f32[1875,1,2]{2,1,0} %constant_18, f32[1,25,25,12]{3,2,1,0} %fusion.36), kind=kLoop, calls=%fused_computation.30, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Minimum"}
  %get-tuple-element.1995 = f32[1875]{0} get-tuple-element((f32[1875]{0}, s32[1875]{0}) %sort.1994), index=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %custom-call.145 = (f32[1,3,50,50]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.692, f32[1,1,256,3]{1,0,2,3} %copy.151), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn_2/class/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.683 = f32[1,3,50,50]{3,2,1,0} get-tuple-element((f32[1,3,50,50]{3,2,1,0}, u8[0]{0}) %custom-call.145), index=0
  %fusion.28 = f32[50,50,3]{2,1,0} fusion(f32[1,3,50,50]{3,2,1,0} %get-tuple-element.683, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.28, metadata={op_name="XLA_Retvals"}
  %fusion.26 = f32[507]{0} fusion(f32[1,3,13,13]{3,2,1,0} %get-tuple-element.678, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.26, metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl6/Reshape_1"}
  %iota.1897 = s32[507]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/TopKV2"}
  %sort.1920 = (f32[507]{0}, s32[507]{0}) sort(f32[507]{0} %fusion.26, s32[507]{0} %iota.1897), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.1898, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.1923 = s32[507]{0} get-tuple-element((f32[507]{0}, s32[507]{0}) %sort.1920), index=1, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/TopKV2"}
  %fusion.27 = f32[507,1,2]{2,1,0} fusion(f32[1,13,13,12]{3,2,1,0} %fusion.35), kind=kLoop, calls=%fused_computation.27, metadata={op_type="Exp" op_name="tower0/decode_bbox_target_4/Exp"}
  %constant_27 = f32[507,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower0/decode_bbox_target_4/sub"}
  %constant_26 = f32[507,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower0/decode_bbox_target_4/add"}
  %fusion.25 = f32[507,4]{1,0} fusion(s32[507]{0} %get-tuple-element.1923, f32[507,1,2]{2,1,0} %fusion.27, f32[507,1,2]{2,1,0} %constant_27, f32[507,1,2]{2,1,0} %constant_26, f32[1,13,13,12]{3,2,1,0} %fusion.35), kind=kLoop, calls=%fused_computation.25, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Minimum"}
  %get-tuple-element.1921 = f32[507]{0} get-tuple-element((f32[507]{0}, s32[507]{0}) %sort.1920), index=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/TopKV2"}
  %custom-call.144 = (f32[1,12,50,50]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,50,50]{3,2,1,0} %get-tuple-element.692, f32[1,1,256,12]{1,0,2,3} %copy.154), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn_2/box/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.682 = f32[1,12,50,50]{3,2,1,0} get-tuple-element((f32[1,12,50,50]{3,2,1,0}, u8[0]{0}) %custom-call.144), index=0
  %fusion.24 = f32[1,50,50,12]{3,2,1,0} fusion(f32[1,12,50,50]{3,2,1,0} %get-tuple-element.682, f32[12]{0} %arg285.286), kind=kLoop, calls=%fused_computation.24, metadata={op_name="XLA_Retvals"}
  %arg243.244 = f32[1,1,512,256]{3,2,1,0} parameter(243), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.161 = f32[1,1,512,256]{1,0,2,3} copy(f32[1,1,512,256]{3,2,1,0} %arg243.244), metadata={op_name="XLA_Args"}
  %custom-call.108 = (f32[1,256,100,100]{3,2,1,0}, u8[60008]{0}) custom-call(f32[1,512,100,100]{3,2,1,0} %fusion.139, f32[1,1,512,256]{1,0,2,3} %copy.161), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/fpn/lateral_1x1_c3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.646 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[1,256,100,100]{3,2,1,0}, u8[60008]{0}) %custom-call.108), index=0
  %arg244.245 = f32[256]{0} parameter(244), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.49 = f32[1,256,100,100]{3,2,1,0} fusion(f32[1,256,50,50]{3,2,1,0} %fusion.51, f32[1,256,100,100]{3,2,1,0} %get-tuple-element.646, f32[256]{0} %arg244.245), kind=kLoop, calls=%fused_computation.49, metadata={op_type="Add" op_name="tower0/fpn/add_1"}
  %arg289.290 = f32[3,3,256,256]{3,2,1,0} parameter(289), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.162 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg289.290), metadata={op_name="XLA_Args"}
  %custom-call.146 = (f32[1,256,100,100]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %fusion.49, f32[3,3,256,256]{1,0,2,3} %copy.162), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/fpn/posthoc_3x3_p3/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.684 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[1,256,100,100]{3,2,1,0}, u8[6554624]{0}) %custom-call.146), index=0
  %arg290.291 = f32[256]{0} parameter(290), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.48 = f32[1,256,100,100]{3,2,1,0} fusion(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.684, f32[256]{0} %arg290.291), kind=kLoop, calls=%fused_computation.48, metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p3/BiasAdd"}
  %custom-call.155 = (f32[1,256,100,100]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %fusion.48, f32[3,3,256,256]{1,0,2,3} %copy.149, f32[256]{0} %arg279.280), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower0/rpn_1/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.693 = f32[1,256,100,100]{3,2,1,0} get-tuple-element((f32[1,256,100,100]{3,2,1,0}, u8[6554624]{0}) %custom-call.155), index=0
  %arg291.292 = s32[168,168,3]{2,1,0} parameter(291), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.23 = (pred[100,100,3]{2,1,0}, pred[100,100,3]{2,1,0}) fusion(s32[168,168,3]{2,1,0} %arg291.292), kind=kLoop, calls=%fused_computation.23, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level3/NotEqual"}
  %get-tuple-element.701 = pred[100,100,3]{2,1,0} get-tuple-element((pred[100,100,3]{2,1,0}, pred[100,100,3]{2,1,0}) %fusion.23), index=0
  %get-tuple-element.702 = pred[100,100,3]{2,1,0} get-tuple-element((pred[100,100,3]{2,1,0}, pred[100,100,3]{2,1,0}) %fusion.23), index=1
  %bitcast.279 = pred[30000]{0} bitcast(pred[100,100,3]{2,1,0} %get-tuple-element.701), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level3/boolean_mask_1/Reshape_1"}
  %copy.204 = pred[30000]{0} copy(pred[30000]{0} %bitcast.279)
  %copy.205 = pred[30000]{0} copy(pred[30000]{0} %bitcast.279)
  %bitcast.280 = pred[30000]{0} bitcast(pred[100,100,3]{2,1,0} %get-tuple-element.702), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level3/boolean_mask_3/Reshape_1"}
  %copy.206 = pred[30000]{0} copy(pred[30000]{0} %bitcast.280)
  %fusion.20 = f32[7500]{0} fusion(f32[1,3,50,50]{3,2,1,0} %get-tuple-element.683, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.20, metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl4/Reshape_1"}
  %iota.2137 = s32[7500]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %sort.2160 = (f32[7500]{0}, s32[7500]{0}) sort(f32[7500]{0} %fusion.20, s32[7500]{0} %iota.2137), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.2138, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.2163 = s32[7500]{0} get-tuple-element((f32[7500]{0}, s32[7500]{0}) %sort.2160), index=1, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %fusion.21 = f32[7500,1,2]{2,1,0} fusion(f32[1,50,50,12]{3,2,1,0} %fusion.24), kind=kLoop, calls=%fused_computation.21, metadata={op_type="Exp" op_name="tower0/decode_bbox_target_2/Exp"}
  %constant_33 = f32[7500,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower0/decode_bbox_target_2/sub"}
  %constant_32 = f32[7500,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower0/decode_bbox_target_2/add"}
  %fusion.19 = f32[2000,4]{1,0} fusion(s32[7500]{0} %get-tuple-element.2163, f32[7500,1,2]{2,1,0} %fusion.21, f32[7500,1,2]{2,1,0} %constant_33, f32[7500,1,2]{2,1,0} %constant_32, f32[1,50,50,12]{3,2,1,0} %fusion.24), kind=kLoop, calls=%fused_computation.19, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Minimum"}
  %get-tuple-element.2161 = f32[7500]{0} get-tuple-element((f32[7500]{0}, s32[7500]{0}) %sort.2160), index=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %slice.2162 = f32[2000]{0} slice(f32[7500]{0} %get-tuple-element.2161), slice={[0:2000]}, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %custom-call.148 = (f32[1,3,100,100]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.693, f32[1,1,256,3]{1,0,2,3} %copy.151), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn_1/class/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.686 = f32[1,3,100,100]{3,2,1,0} get-tuple-element((f32[1,3,100,100]{3,2,1,0}, u8[0]{0}) %custom-call.148), index=0
  %fusion.17 = f32[100,100,3]{2,1,0} fusion(f32[1,3,100,100]{3,2,1,0} %get-tuple-element.686, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.17, metadata={op_name="XLA_Retvals"}
  %custom-call.147 = (f32[1,12,100,100]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,100,100]{3,2,1,0} %get-tuple-element.693, f32[1,1,256,12]{1,0,2,3} %copy.154), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn_1/box/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.685 = f32[1,12,100,100]{3,2,1,0} get-tuple-element((f32[1,12,100,100]{3,2,1,0}, u8[0]{0}) %custom-call.147), index=0
  %fusion.16 = f32[1,100,100,12]{3,2,1,0} fusion(f32[1,12,100,100]{3,2,1,0} %get-tuple-element.685, f32[12]{0} %arg285.286), kind=kLoop, calls=%fused_computation.16, metadata={op_name="XLA_Retvals"}
  %arg228.229 = f32[1,1,256,256]{3,2,1,0} parameter(228), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.167 = f32[1,1,256,256]{1,0,2,3} copy(f32[1,1,256,256]{3,2,1,0} %arg228.229), metadata={op_name="XLA_Args"}
  %custom-call.95 = (f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %fusion.177, f32[1,1,256,256]{1,0,2,3} %copy.167), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/fpn/lateral_1x1_c2/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.633 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.95), index=0
  %arg229.230 = f32[256]{0} parameter(229), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.47 = f32[1,256,200,200]{3,2,1,0} fusion(f32[1,256,100,100]{3,2,1,0} %fusion.49, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.633, f32[256]{0} %arg229.230), kind=kLoop, calls=%fused_computation.47, metadata={op_type="Add" op_name="tower0/fpn/add_2"}
  %arg292.293 = f32[3,3,256,256]{3,2,1,0} parameter(292), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.168 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg292.293), metadata={op_name="XLA_Args"}
  %custom-call.149 = (f32[1,256,200,200]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %fusion.47, f32[3,3,256,256]{1,0,2,3} %copy.168), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/fpn/posthoc_3x3_p2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.687 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[6554624]{0}) %custom-call.149), index=0
  %arg293.294 = f32[256]{0} parameter(293), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.46 = f32[1,256,200,200]{3,2,1,0} fusion(f32[1,256,200,200]{3,2,1,0} %get-tuple-element.687, f32[256]{0} %arg293.294), kind=kLoop, calls=%fused_computation.46, metadata={op_type="BiasAdd" op_name="tower0/fpn/posthoc_3x3_p2/BiasAdd"}
  %custom-call.156 = (f32[1,256,200,200]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %fusion.46, f32[3,3,256,256]{1,0,2,3} %copy.149, f32[256]{0} %arg279.280), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower0/rpn/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.694 = f32[1,256,200,200]{3,2,1,0} get-tuple-element((f32[1,256,200,200]{3,2,1,0}, u8[6554624]{0}) %custom-call.156), index=0
  %arg294.295 = s32[336,336,3]{2,1,0} parameter(294), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.15 = (pred[200,200,3]{2,1,0}, pred[200,200,3]{2,1,0}) fusion(s32[336,336,3]{2,1,0} %arg294.295), kind=kLoop, calls=%fused_computation.15, metadata={op_type="NotEqual" op_name="tower0/rpn_losses/level2/NotEqual"}
  %get-tuple-element.699 = pred[200,200,3]{2,1,0} get-tuple-element((pred[200,200,3]{2,1,0}, pred[200,200,3]{2,1,0}) %fusion.15), index=0
  %get-tuple-element.700 = pred[200,200,3]{2,1,0} get-tuple-element((pred[200,200,3]{2,1,0}, pred[200,200,3]{2,1,0}) %fusion.15), index=1
  %bitcast.289 = pred[120000]{0} bitcast(pred[200,200,3]{2,1,0} %get-tuple-element.699), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level2/boolean_mask_1/Reshape_1"}
  %copy.207 = pred[120000]{0} copy(pred[120000]{0} %bitcast.289)
  %copy.208 = pred[120000]{0} copy(pred[120000]{0} %bitcast.289)
  %bitcast.290 = pred[120000]{0} bitcast(pred[200,200,3]{2,1,0} %get-tuple-element.700), metadata={op_type="Reshape" op_name="tower0/rpn_losses/level2/boolean_mask_3/Reshape_1"}
  %copy.209 = pred[120000]{0} copy(pred[120000]{0} %bitcast.290)
  %fusion.12 = f32[30000]{0} fusion(f32[1,3,100,100]{3,2,1,0} %get-tuple-element.686, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.12, metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl3/Reshape_1"}
  %iota.2303 = s32[30000]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %sort.2326 = (f32[30000]{0}, s32[30000]{0}) sort(f32[30000]{0} %fusion.12, s32[30000]{0} %iota.2303), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.2304, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.2329 = s32[30000]{0} get-tuple-element((f32[30000]{0}, s32[30000]{0}) %sort.2326), index=1, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %fusion.13 = f32[30000,1,2]{2,1,0} fusion(f32[1,100,100,12]{3,2,1,0} %fusion.16), kind=kLoop, calls=%fused_computation.13, metadata={op_type="Exp" op_name="tower0/decode_bbox_target_1/Exp"}
  %constant_39 = f32[30000,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower0/decode_bbox_target_1/sub"}
  %constant_38 = f32[30000,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower0/decode_bbox_target_1/add"}
  %fusion.11 = f32[2000,4]{1,0} fusion(s32[30000]{0} %get-tuple-element.2329, f32[30000,1,2]{2,1,0} %fusion.13, f32[30000,1,2]{2,1,0} %constant_39, f32[30000,1,2]{2,1,0} %constant_38, f32[1,100,100,12]{3,2,1,0} %fusion.16), kind=kLoop, calls=%fused_computation.11, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/clip_boxes/Minimum"}
  %get-tuple-element.2327 = f32[30000]{0} get-tuple-element((f32[30000]{0}, s32[30000]{0}) %sort.2326), index=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %slice.2328 = f32[2000]{0} slice(f32[30000]{0} %get-tuple-element.2327), slice={[0:2000]}, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %custom-call.151 = (f32[1,3,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %get-tuple-element.694, f32[1,1,256,3]{1,0,2,3} %copy.151), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn/class/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.689 = f32[1,3,200,200]{3,2,1,0} get-tuple-element((f32[1,3,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.151), index=0
  %fusion.9 = f32[200,200,3]{2,1,0} fusion(f32[1,3,200,200]{3,2,1,0} %get-tuple-element.689, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.9, metadata={op_name="XLA_Retvals"}
  %custom-call.150 = (f32[1,12,200,200]{3,2,1,0}, u8[240008]{0}) custom-call(f32[1,256,200,200]{3,2,1,0} %get-tuple-element.694, f32[1,1,256,12]{1,0,2,3} %copy.154), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower0/rpn/box/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.688 = f32[1,12,200,200]{3,2,1,0} get-tuple-element((f32[1,12,200,200]{3,2,1,0}, u8[240008]{0}) %custom-call.150), index=0
  %fusion.8 = f32[1,200,200,12]{3,2,1,0} fusion(f32[1,12,200,200]{3,2,1,0} %get-tuple-element.688, f32[12]{0} %arg285.286), kind=kLoop, calls=%fused_computation.8, metadata={op_name="XLA_Retvals"}
  %fusion.6 = f32[120000]{0} fusion(f32[1,3,200,200]{3,2,1,0} %get-tuple-element.689, f32[3]{0} %arg284.285), kind=kLoop, calls=%fused_computation.6, metadata={op_type="Reshape" op_name="tower0/generate_fpn_proposals/Lvl2/Reshape_1"}
  %iota.2469 = s32[120000]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %sort.2492 = (f32[120000]{0}, s32[120000]{0}) sort(f32[120000]{0} %fusion.6, s32[120000]{0} %iota.2469), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.2470, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.2495 = s32[120000]{0} get-tuple-element((f32[120000]{0}, s32[120000]{0}) %sort.2492), index=1, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %fusion.7 = f32[120000,1,2]{2,1,0} fusion(f32[1,200,200,12]{3,2,1,0} %fusion.8), kind=kLoop, calls=%fused_computation.7, metadata={op_type="Exp" op_name="tower0/decode_bbox_target/Exp"}
  %constant_44 = f32[120000,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower0/rpn_losses/encode_bbox_target/sub"}
  %constant_45 = f32[120000,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower0/rpn_losses/encode_bbox_target/add"}
  %fusion.5 = f32[2000,4]{1,0} fusion(s32[120000]{0} %get-tuple-element.2495, f32[120000,1,2]{2,1,0} %fusion.7, f32[120000,1,2]{2,1,0} %constant_44, f32[120000,1,2]{2,1,0} %constant_45, f32[1,200,200,12]{3,2,1,0} %fusion.8), kind=kLoop, calls=%fused_computation.5, metadata={op_type="Minimum" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/clip_boxes/Minimum"}
  %get-tuple-element.2493 = f32[120000]{0} get-tuple-element((f32[120000]{0}, s32[120000]{0}) %sort.2492), index=0, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %slice.2494 = f32[2000]{0} slice(f32[120000]{0} %get-tuple-element.2493), slice={[0:2000]}, metadata={op_type="TopKV2" op_name="tower0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %bitcast.252 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %arg212.213), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv3/bn/Reshape_2"}
  %copy.210 = f32[1,2048,1,1]{3,2,1,0} copy(f32[1,2048,1,1]{3,2,1,0} %bitcast.252)
  %bitcast.246 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg208.209), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv2/bn/Reshape_2"}
  %copy.211 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.246)
  %bitcast.240 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg204.205), metadata={op_type="Reshape" op_name="tower0/group3/block2/conv1/bn/Reshape_2"}
  %copy.212 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.240)
  %bitcast.234 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %arg200.201), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv3/bn/Reshape_2"}
  %copy.213 = f32[1,2048,1,1]{3,2,1,0} copy(f32[1,2048,1,1]{3,2,1,0} %bitcast.234)
  %bitcast.228 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg196.197), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv2/bn/Reshape_2"}
  %copy.214 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.228)
  %bitcast.222 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg192.193), metadata={op_type="Reshape" op_name="tower0/group3/block1/conv1/bn/Reshape_2"}
  %copy.215 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.222)
  %bitcast.210 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %arg184.185), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv3/bn/Reshape_2"}
  %copy.216 = f32[1,2048,1,1]{3,2,1,0} copy(f32[1,2048,1,1]{3,2,1,0} %bitcast.210)
  %bitcast.216 = f32[1,2048,1,1]{3,2,1,0} bitcast(f32[2048]{0} %arg188.189), metadata={op_type="Reshape" op_name="tower0/group3/block0/convshortcut/bn/Reshape_2"}
  %copy.217 = f32[1,2048,1,1]{3,2,1,0} copy(f32[1,2048,1,1]{3,2,1,0} %bitcast.216)
  %bitcast.204 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg180.181), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv2/bn/Reshape_2"}
  %copy.218 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.204)
  %bitcast.198 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg176.177), metadata={op_type="Reshape" op_name="tower0/group3/block0/conv1/bn/Reshape_2"}
  %copy.219 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.198)
  %bitcast.192 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %arg174.175), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv3/bn/Reshape_2"}
  %copy.220 = f32[1,1024,1,1]{3,2,1,0} copy(f32[1,1024,1,1]{3,2,1,0} %bitcast.192)
  %bitcast.186 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg6.7), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv2/bn/Reshape_2"}
  %copy.221 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.186)
  %bitcast.180 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg10.11), metadata={op_type="Reshape" op_name="tower0/group2/block5/conv1/bn/Reshape_2"}
  %copy.222 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.180)
  %bitcast.174 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %arg172.173), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv3/bn/Reshape_2"}
  %copy.223 = f32[1,1024,1,1]{3,2,1,0} copy(f32[1,1024,1,1]{3,2,1,0} %bitcast.174)
  %bitcast.168 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg16.17), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv2/bn/Reshape_2"}
  %copy.224 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.168)
  %bitcast.162 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg20.21), metadata={op_type="Reshape" op_name="tower0/group2/block4/conv1/bn/Reshape_2"}
  %copy.225 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.162)
  %bitcast.156 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %arg170.171), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv3/bn/Reshape_2"}
  %copy.226 = f32[1,1024,1,1]{3,2,1,0} copy(f32[1,1024,1,1]{3,2,1,0} %bitcast.156)
  %bitcast.150 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg26.27), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv2/bn/Reshape_2"}
  %copy.227 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.150)
  %bitcast.144 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg30.31), metadata={op_type="Reshape" op_name="tower0/group2/block3/conv1/bn/Reshape_2"}
  %copy.228 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.144)
  %bitcast.138 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %arg168.169), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv3/bn/Reshape_2"}
  %copy.229 = f32[1,1024,1,1]{3,2,1,0} copy(f32[1,1024,1,1]{3,2,1,0} %bitcast.138)
  %bitcast.132 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg36.37), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv2/bn/Reshape_2"}
  %copy.230 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.132)
  %bitcast.126 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg40.41), metadata={op_type="Reshape" op_name="tower0/group2/block2/conv1/bn/Reshape_2"}
  %copy.231 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.126)
  %bitcast.120 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %arg166.167), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv3/bn/Reshape_2"}
  %copy.232 = f32[1,1024,1,1]{3,2,1,0} copy(f32[1,1024,1,1]{3,2,1,0} %bitcast.120)
  %bitcast.114 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg46.47), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv2/bn/Reshape_2"}
  %copy.233 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.114)
  %bitcast.108 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg50.51), metadata={op_type="Reshape" op_name="tower0/group2/block1/conv1/bn/Reshape_2"}
  %copy.234 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.108)
  %bitcast.102 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %arg54.55), metadata={op_type="Reshape" op_name="tower0/group2/block0/convshortcut/bn/Reshape_2"}
  %copy.235 = f32[1,1024,1,1]{3,2,1,0} copy(f32[1,1024,1,1]{3,2,1,0} %bitcast.102)
  %bitcast.96 = f32[1,1024,1,1]{3,2,1,0} bitcast(f32[1024]{0} %arg156.157), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv3/bn/Reshape_2"}
  %copy.236 = f32[1,1024,1,1]{3,2,1,0} copy(f32[1,1024,1,1]{3,2,1,0} %bitcast.96)
  %bitcast.90 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg160.161), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv2/bn/Reshape_2"}
  %copy.237 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.90)
  %bitcast.84 = f32[1,256,1,1]{3,2,1,0} bitcast(f32[256]{0} %arg164.165), metadata={op_type="Reshape" op_name="tower0/group2/block0/conv1/bn/Reshape_2"}
  %copy.238 = f32[1,256,1,1]{3,2,1,0} copy(f32[1,256,1,1]{3,2,1,0} %bitcast.84)
  %bitcast.78 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg152.153), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv3/bn/Reshape_2"}
  %copy.239 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.78)
  %bitcast.72 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %arg60.61), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv2/bn/Reshape_2"}
  %copy.240 = f32[1,128,1,1]{3,2,1,0} copy(f32[1,128,1,1]{3,2,1,0} %bitcast.72)
  %bitcast.66 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %arg64.65), metadata={op_type="Reshape" op_name="tower0/group1/block3/conv1/bn/Reshape_2"}
  %copy.241 = f32[1,128,1,1]{3,2,1,0} copy(f32[1,128,1,1]{3,2,1,0} %bitcast.66)
  %bitcast.60 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg150.151), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv3/bn/Reshape_2"}
  %copy.242 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.60)
  %bitcast.54 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %arg70.71), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv2/bn/Reshape_2"}
  %copy.243 = f32[1,128,1,1]{3,2,1,0} copy(f32[1,128,1,1]{3,2,1,0} %bitcast.54)
  %bitcast.48 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %arg74.75), metadata={op_type="Reshape" op_name="tower0/group1/block2/conv1/bn/Reshape_2"}
  %copy.244 = f32[1,128,1,1]{3,2,1,0} copy(f32[1,128,1,1]{3,2,1,0} %bitcast.48)
  %bitcast.42 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg148.149), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv3/bn/Reshape_2"}
  %copy.245 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.42)
  %bitcast.36 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %arg80.81), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv2/bn/Reshape_2"}
  %copy.246 = f32[1,128,1,1]{3,2,1,0} copy(f32[1,128,1,1]{3,2,1,0} %bitcast.36)
  %bitcast.30 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %arg84.85), metadata={op_type="Reshape" op_name="tower0/group1/block1/conv1/bn/Reshape_2"}
  %copy.247 = f32[1,128,1,1]{3,2,1,0} copy(f32[1,128,1,1]{3,2,1,0} %bitcast.30)
  %bitcast.24 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg88.89), metadata={op_type="Reshape" op_name="tower0/group1/block0/convshortcut/bn/Reshape_2"}
  %copy.248 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.24)
  %bitcast.18 = f32[1,512,1,1]{3,2,1,0} bitcast(f32[512]{0} %arg138.139), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv3/bn/Reshape_2"}
  %copy.249 = f32[1,512,1,1]{3,2,1,0} copy(f32[1,512,1,1]{3,2,1,0} %bitcast.18)
  %bitcast.12 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %arg142.143), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv2/bn/Reshape_2"}
  %copy.250 = f32[1,128,1,1]{3,2,1,0} copy(f32[1,128,1,1]{3,2,1,0} %bitcast.12)
  %bitcast.6 = f32[1,128,1,1]{3,2,1,0} bitcast(f32[128]{0} %arg146.147), metadata={op_type="Reshape" op_name="tower0/group1/block0/conv1/bn/Reshape_2"}
  %copy.251 = f32[1,128,1,1]{3,2,1,0} copy(f32[1,128,1,1]{3,2,1,0} %bitcast.6)
  ROOT %tuple.83 = (f32[], f32[1,128,201,201]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}, f32[1,256,101,101]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,512,51,51]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}, f32[1,256,25,25]{3,2,1,0}, f32[1,256,25,25]{3,2,1,0}, f32[1,256,13,13]{3,2,1,0}, f32[1,256,25,25]{3,2,1,0}, f32[1,256,13,13]{3,2,1,0}, pred[25,25,3]{2,1,0}, pred[25,25,3]{2,1,0}, pred[13,13,3]{2,1,0}, pred[13,13,3]{2,1,0}, pred[1875]{0}, pred[1875]{0}, pred[1875]{0}, pred[507]{0}, pred[507]{0}, pred[507]{0}, f32[25,25,3]{2,1,0}, f32[13,13,3]{2,1,0}, f32[1,25,25,12]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,13,13,12]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, pred[50,50,3]{2,1,0}, pred[50,50,3]{2,1,0}, pred[7500]{0}, pred[7500]{0}, pred[7500]{0}, f32[1875,4]{1,0}, f32[1875]{0}, f32[50,50,3]{2,1,0}, f32[507,4]{1,0}, f32[507]{0}, f32[1,50,50,12]{3,2,1,0}, f32[1,256,100,100]{3,2,1,0}, f32[1,256,100,100]{3,2,1,0}, f32[1,256,100,100]{3,2,1,0}, pred[100,100,3]{2,1,0}, pred[100,100,3]{2,1,0}, pred[30000]{0}, pred[30000]{0}, pred[30000]{0}, f32[2000,4]{1,0}, f32[2000]{0}, f32[100,100,3]{2,1,0}, f32[1,100,100,12]{3,2,1,0}, f32[1,256,200,200]{3,2,1,0}, f32[1,256,200,200]{3,2,1,0}, f32[1,256,200,200]{3,2,1,0}, pred[200,200,3]{2,1,0}, pred[200,200,3]{2,1,0}, pred[120000]{0}, pred[120000]{0}, pred[120000]{0}, f32[2000,4]{1,0}, f32[2000]{0}, f32[200,200,3]{2,1,0}, f32[1,200,200,12]{3,2,1,0}, f32[2000,4]{1,0}, f32[2000]{0}, f32[1,256,200,200]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,2048,25,25]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,2048,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,25,25]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,50,50]{3,2,1,0}, f32[1,512,50,50]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,50,50]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,1024,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,50,50]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,100,100]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,100,100]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,256,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}, f32[1,512,100,100]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,512,1,1]{3,2,1,0}, f32[1,128,100,100]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,200,200]{3,2,1,0}, f32[1,128,200,200]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}, f32[1,128,1,1]{3,2,1,0}) tuple(f32[] %fusion.45, f32[1,128,201,201]{3,2,1,0} %pad.9, f32[1,128,100,100]{3,2,1,0} %fusion.171, f32[1,512,100,100]{3,2,1,0} %fusion.166, f32[1,128,100,100]{3,2,1,0} %fusion.163, f32[1,128,100,100]{3,2,1,0} %fusion.160, f32[1,512,100,100]{3,2,1,0} %fusion.157, f32[1,128,100,100]{3,2,1,0} %fusion.154, f32[1,128,100,100]{3,2,1,0} %fusion.151, f32[1,512,100,100]{3,2,1,0} %fusion.148, f32[1,128,100,100]{3,2,1,0} %fusion.145, f32[1,128,100,100]{3,2,1,0} %fusion.142, f32[1,512,100,100]{3,2,1,0} %fusion.139, f32[1,256,101,101]{3,2,1,0} %pad.10, f32[1,256,50,50]{3,2,1,0} %fusion.133, f32[1,1024,50,50]{3,2,1,0} %fusion.128, f32[1,256,50,50]{3,2,1,0} %fusion.125, f32[1,256,50,50]{3,2,1,0} %fusion.122, f32[1,1024,50,50]{3,2,1,0} %fusion.119, f32[1,256,50,50]{3,2,1,0} %fusion.116, f32[1,256,50,50]{3,2,1,0} %fusion.113, f32[1,1024,50,50]{3,2,1,0} %fusion.110, f32[1,256,50,50]{3,2,1,0} %fusion.107, f32[1,256,50,50]{3,2,1,0} %fusion.104, f32[1,1024,50,50]{3,2,1,0} %fusion.101, f32[1,256,50,50]{3,2,1,0} %fusion.98, f32[1,256,50,50]{3,2,1,0} %fusion.95, f32[1,1024,50,50]{3,2,1,0} %fusion.92, f32[1,256,50,50]{3,2,1,0} %fusion.89, f32[1,256,50,50]{3,2,1,0} %fusion.86, f32[1,1024,50,50]{3,2,1,0} %fusion.83, f32[1,512,51,51]{3,2,1,0} %pad.11, f32[1,512,25,25]{3,2,1,0} %fusion.77, f32[1,2048,25,25]{3,2,1,0} %fusion.72, f32[1,512,25,25]{3,2,1,0} %fusion.69, f32[1,512,25,25]{3,2,1,0} %fusion.66, f32[1,2048,25,25]{3,2,1,0} %fusion.63, f32[1,512,25,25]{3,2,1,0} %fusion.60, f32[1,512,25,25]{3,2,1,0} %fusion.57, f32[1,2048,25,25]{3,2,1,0} %fusion.54, f32[1,256,25,25]{3,2,1,0} %fusion.53, f32[1,256,25,25]{3,2,1,0} %fusion.52, f32[1,256,13,13]{3,2,1,0} %reduce-window.1827, f32[1,256,25,25]{3,2,1,0} %get-tuple-element.691, f32[1,256,13,13]{3,2,1,0} %get-tuple-element.690, pred[25,25,3]{2,1,0} %get-tuple-element.705, pred[25,25,3]{2,1,0} %get-tuple-element.706, pred[13,13,3]{2,1,0} %get-tuple-element.707, pred[13,13,3]{2,1,0} %get-tuple-element.708, pred[1875]{0} %copy.195, pred[1875]{0} %copy.196, pred[1875]{0} %copy.197, pred[507]{0} %copy.198, pred[507]{0} %copy.199, pred[507]{0} %copy.200, f32[25,25,3]{2,1,0} %fusion.39, f32[13,13,3]{2,1,0} %fusion.37, f32[1,25,25,12]{3,2,1,0} %fusion.36, f32[1,256,50,50]{3,2,1,0} %fusion.51, f32[1,13,13,12]{3,2,1,0} %fusion.35, f32[1,256,50,50]{3,2,1,0} %fusion.50, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.692, pred[50,50,3]{2,1,0} %get-tuple-element.703, pred[50,50,3]{2,1,0} %get-tuple-element.704, pred[7500]{0} %copy.201, pred[7500]{0} %copy.202, pred[7500]{0} %copy.203, f32[1875,4]{1,0} %fusion.30, f32[1875]{0} %get-tuple-element.1995, f32[50,50,3]{2,1,0} %fusion.28, f32[507,4]{1,0} %fusion.25, f32[507]{0} %get-tuple-element.1921, f32[1,50,50,12]{3,2,1,0} %fusion.24, f32[1,256,100,100]{3,2,1,0} %fusion.49, f32[1,256,100,100]{3,2,1,0} %fusion.48, f32[1,256,100,100]{3,2,1,0} %get-tuple-element.693, pred[100,100,3]{2,1,0} %get-tuple-element.701, pred[100,100,3]{2,1,0} %get-tuple-element.702, pred[30000]{0} %copy.204, pred[30000]{0} %copy.205, pred[30000]{0} %copy.206, f32[2000,4]{1,0} %fusion.19, f32[2000]{0} %slice.2162, f32[100,100,3]{2,1,0} %fusion.17, f32[1,100,100,12]{3,2,1,0} %fusion.16, f32[1,256,200,200]{3,2,1,0} %fusion.47, f32[1,256,200,200]{3,2,1,0} %fusion.46, f32[1,256,200,200]{3,2,1,0} %get-tuple-element.694, pred[200,200,3]{2,1,0} %get-tuple-element.699, pred[200,200,3]{2,1,0} %get-tuple-element.700, pred[120000]{0} %copy.207, pred[120000]{0} %copy.208, pred[120000]{0} %copy.209, f32[2000,4]{1,0} %fusion.11, f32[2000]{0} %slice.2328, f32[200,200,3]{2,1,0} %fusion.9, f32[1,200,200,12]{3,2,1,0} %fusion.8, f32[2000,4]{1,0} %fusion.5, f32[2000]{0} %slice.2494, f32[1,256,200,200]{3,2,1,0} %fusion.177, f32[1,2048,1,1]{3,2,1,0} %fusion.55, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.674, f32[1,2048,1,1]{3,2,1,0} %copy.210, f32[1,2048,1,1]{3,2,1,0} %fusion.56, f32[1,512,1,1]{3,2,1,0} %fusion.58, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.673, f32[1,512,1,1]{3,2,1,0} %copy.211, f32[1,512,1,1]{3,2,1,0} %fusion.59, f32[1,512,1,1]{3,2,1,0} %fusion.61, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.672, f32[1,512,1,1]{3,2,1,0} %copy.212, f32[1,512,1,1]{3,2,1,0} %fusion.62, f32[1,2048,1,1]{3,2,1,0} %fusion.64, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.671, f32[1,2048,1,1]{3,2,1,0} %copy.213, f32[1,2048,1,1]{3,2,1,0} %fusion.65, f32[1,512,1,1]{3,2,1,0} %fusion.67, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.670, f32[1,512,1,1]{3,2,1,0} %copy.214, f32[1,512,1,1]{3,2,1,0} %fusion.68, f32[1,512,1,1]{3,2,1,0} %fusion.70, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.669, f32[1,512,1,1]{3,2,1,0} %copy.215, f32[1,512,1,1]{3,2,1,0} %fusion.71, f32[1,2048,1,1]{3,2,1,0} %fusion.75, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.667, f32[1,2048,1,1]{3,2,1,0} %fusion.73, f32[1,2048,25,25]{3,2,1,0} %get-tuple-element.668, f32[1,2048,1,1]{3,2,1,0} %copy.216, f32[1,2048,1,1]{3,2,1,0} %copy.217, f32[1,2048,1,1]{3,2,1,0} %fusion.76, f32[1,2048,1,1]{3,2,1,0} %fusion.74, f32[1,512,1,1]{3,2,1,0} %fusion.78, f32[1,512,25,25]{3,2,1,0} %get-tuple-element.698, f32[1,512,1,1]{3,2,1,0} %copy.218, f32[1,512,1,1]{3,2,1,0} %fusion.79, f32[1,512,50,50]{3,2,1,0} %fusion.80, f32[1,512,50,50]{3,2,1,0} %get-tuple-element.666, f32[1,512,1,1]{3,2,1,0} %fusion.81, f32[1,512,1,1]{3,2,1,0} %copy.219, f32[1,512,1,1]{3,2,1,0} %fusion.82, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.664, f32[1,1024,1,1]{3,2,1,0} %fusion.84, f32[1,1024,1,1]{3,2,1,0} %copy.220, f32[1,1024,1,1]{3,2,1,0} %fusion.85, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.663, f32[1,256,1,1]{3,2,1,0} %fusion.87, f32[1,256,1,1]{3,2,1,0} %copy.221, f32[1,256,1,1]{3,2,1,0} %fusion.88, f32[1,256,1,1]{3,2,1,0} %fusion.90, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.662, f32[1,256,1,1]{3,2,1,0} %copy.222, f32[1,256,1,1]{3,2,1,0} %fusion.91, f32[1,1024,1,1]{3,2,1,0} %fusion.93, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.661, f32[1,1024,1,1]{3,2,1,0} %copy.223, f32[1,1024,1,1]{3,2,1,0} %fusion.94, f32[1,256,1,1]{3,2,1,0} %fusion.96, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.660, f32[1,256,1,1]{3,2,1,0} %copy.224, f32[1,256,1,1]{3,2,1,0} %fusion.97, f32[1,256,1,1]{3,2,1,0} %fusion.99, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.659, f32[1,256,1,1]{3,2,1,0} %copy.225, f32[1,256,1,1]{3,2,1,0} %fusion.100, f32[1,1024,1,1]{3,2,1,0} %fusion.102, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.658, f32[1,1024,1,1]{3,2,1,0} %copy.226, f32[1,1024,1,1]{3,2,1,0} %fusion.103, f32[1,256,1,1]{3,2,1,0} %fusion.105, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.657, f32[1,256,1,1]{3,2,1,0} %copy.227, f32[1,256,1,1]{3,2,1,0} %fusion.106, f32[1,256,1,1]{3,2,1,0} %fusion.108, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.656, f32[1,256,1,1]{3,2,1,0} %copy.228, f32[1,256,1,1]{3,2,1,0} %fusion.109, f32[1,1024,1,1]{3,2,1,0} %fusion.111, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.655, f32[1,1024,1,1]{3,2,1,0} %copy.229, f32[1,1024,1,1]{3,2,1,0} %fusion.112, f32[1,256,1,1]{3,2,1,0} %fusion.114, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.654, f32[1,256,1,1]{3,2,1,0} %copy.230, f32[1,256,1,1]{3,2,1,0} %fusion.115, f32[1,256,1,1]{3,2,1,0} %fusion.117, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.653, f32[1,256,1,1]{3,2,1,0} %copy.231, f32[1,256,1,1]{3,2,1,0} %fusion.118, f32[1,1024,1,1]{3,2,1,0} %fusion.120, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.652, f32[1,1024,1,1]{3,2,1,0} %copy.232, f32[1,1024,1,1]{3,2,1,0} %fusion.121, f32[1,256,1,1]{3,2,1,0} %fusion.123, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.651, f32[1,256,1,1]{3,2,1,0} %copy.233, f32[1,256,1,1]{3,2,1,0} %fusion.124, f32[1,256,1,1]{3,2,1,0} %fusion.126, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.650, f32[1,256,1,1]{3,2,1,0} %copy.234, f32[1,256,1,1]{3,2,1,0} %fusion.127, f32[1,1024,1,1]{3,2,1,0} %fusion.129, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.649, f32[1,1024,1,1]{3,2,1,0} %fusion.131, f32[1,1024,50,50]{3,2,1,0} %get-tuple-element.648, f32[1,1024,1,1]{3,2,1,0} %copy.235, f32[1,1024,1,1]{3,2,1,0} %copy.236, f32[1,1024,1,1]{3,2,1,0} %fusion.130, f32[1,1024,1,1]{3,2,1,0} %fusion.132, f32[1,256,1,1]{3,2,1,0} %fusion.134, f32[1,256,50,50]{3,2,1,0} %get-tuple-element.697, f32[1,256,1,1]{3,2,1,0} %copy.237, f32[1,256,1,1]{3,2,1,0} %fusion.135, f32[1,256,100,100]{3,2,1,0} %fusion.136, f32[1,256,1,1]{3,2,1,0} %fusion.137, f32[1,256,100,100]{3,2,1,0} %get-tuple-element.647, f32[1,256,1,1]{3,2,1,0} %copy.238, f32[1,256,1,1]{3,2,1,0} %fusion.138, f32[1,512,1,1]{3,2,1,0} %fusion.140, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.645, f32[1,512,1,1]{3,2,1,0} %copy.239, f32[1,512,1,1]{3,2,1,0} %fusion.141, f32[1,128,1,1]{3,2,1,0} %fusion.143, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.644, f32[1,128,1,1]{3,2,1,0} %copy.240, f32[1,128,1,1]{3,2,1,0} %fusion.144, f32[1,128,1,1]{3,2,1,0} %fusion.146, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.643, f32[1,128,1,1]{3,2,1,0} %copy.241, f32[1,128,1,1]{3,2,1,0} %fusion.147, f32[1,512,1,1]{3,2,1,0} %fusion.149, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.642, f32[1,512,1,1]{3,2,1,0} %copy.242, f32[1,512,1,1]{3,2,1,0} %fusion.150, f32[1,128,1,1]{3,2,1,0} %fusion.152, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.641, f32[1,128,1,1]{3,2,1,0} %copy.243, f32[1,128,1,1]{3,2,1,0} %fusion.153, f32[1,128,1,1]{3,2,1,0} %fusion.155, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.640, f32[1,128,1,1]{3,2,1,0} %copy.244, f32[1,128,1,1]{3,2,1,0} %fusion.156, f32[1,512,1,1]{3,2,1,0} %fusion.158, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.639, f32[1,512,1,1]{3,2,1,0} %copy.245, f32[1,512,1,1]{3,2,1,0} %fusion.159, f32[1,128,1,1]{3,2,1,0} %fusion.161, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.638, f32[1,128,1,1]{3,2,1,0} %copy.246, f32[1,128,1,1]{3,2,1,0} %fusion.162, f32[1,128,1,1]{3,2,1,0} %fusion.164, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.637, f32[1,128,1,1]{3,2,1,0} %copy.247, f32[1,128,1,1]{3,2,1,0} %fusion.165, f32[1,512,1,1]{3,2,1,0} %fusion.167, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.636, f32[1,512,100,100]{3,2,1,0} %get-tuple-element.635, f32[1,512,1,1]{3,2,1,0} %fusion.169, f32[1,512,1,1]{3,2,1,0} %copy.248, f32[1,512,1,1]{3,2,1,0} %copy.249, f32[1,512,1,1]{3,2,1,0} %fusion.168, f32[1,512,1,1]{3,2,1,0} %fusion.170, f32[1,128,100,100]{3,2,1,0} %get-tuple-element.696, f32[1,128,1,1]{3,2,1,0} %fusion.172, f32[1,128,1,1]{3,2,1,0} %copy.250, f32[1,128,1,1]{3,2,1,0} %fusion.173, f32[1,128,200,200]{3,2,1,0} %fusion.174, f32[1,128,200,200]{3,2,1,0} %get-tuple-element.634, f32[1,128,1,1]{3,2,1,0} %fusion.175, f32[1,128,1,1]{3,2,1,0} %copy.251, f32[1,128,1,1]{3,2,1,0} %fusion.176)
}

