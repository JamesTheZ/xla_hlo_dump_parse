HloModule cluster_28__XlaCompiledKernel_true__XlaNumConstantArgs_27__XlaNumResourceArgs_0_.1699

%max_F32.1052 (lhs.1053: f32[], rhs.1054: f32[]) -> f32[] {
  %lhs.1053 = f32[] parameter(0)
  %rhs.1054 = f32[] parameter(1)
  ROOT %maximum.1055 = f32[] maximum(f32[] %lhs.1053, f32[] %rhs.1054)
}

%fused_computation (param_0.2: s32[], param_1.3: s32[], param_2.6: f32[], param_3.10: f32[]) -> pred[] {
  %param_3.10 = f32[] parameter(3)
  %bitcast-convert.35 = s32[] bitcast-convert(f32[] %param_3.10)
  %constant_181 = s32[] constant(0)
  %compare.30 = pred[] compare(s32[] %bitcast-convert.35, s32[] %constant_181), direction=LT
  %constant_180 = u32[] constant(2147483647)
  %bitcast-convert.34 = u32[] bitcast-convert(f32[] %param_3.10)
  %subtract.64 = u32[] subtract(u32[] %constant_180, u32[] %bitcast-convert.34)
  %bitcast-convert.33 = s32[] bitcast-convert(u32[] %subtract.64)
  %select.17 = s32[] select(pred[] %compare.30, s32[] %bitcast-convert.33, s32[] %bitcast-convert.35)
  %param_2.6 = f32[] parameter(2)
  %bitcast-convert.32 = s32[] bitcast-convert(f32[] %param_2.6)
  %compare.29 = pred[] compare(s32[] %bitcast-convert.32, s32[] %constant_181), direction=LT
  %bitcast-convert.31 = u32[] bitcast-convert(f32[] %param_2.6)
  %subtract.63 = u32[] subtract(u32[] %constant_180, u32[] %bitcast-convert.31)
  %bitcast-convert.30 = s32[] bitcast-convert(u32[] %subtract.63)
  %select.16 = s32[] select(pred[] %compare.29, s32[] %bitcast-convert.30, s32[] %bitcast-convert.32)
  %compare.28 = pred[] compare(s32[] %select.17, s32[] %select.16), direction=GT
  %compare.27 = pred[] compare(s32[] %select.16, s32[] %select.17), direction=GT
  %compare.26 = pred[] compare(pred[] %compare.28, pred[] %compare.27), direction=EQ
  %param_0.2 = s32[] parameter(0)
  %param_1.3 = s32[] parameter(1)
  %compare.25 = pred[] compare(s32[] %param_0.2, s32[] %param_1.3), direction=LT
  ROOT %select.15 = pred[] select(pred[] %compare.26, pred[] %compare.25, pred[] %compare.28)
}

%compare-greater-than.1114 (p.0.lhs.1115: f32[], p.0.rhs.1116: f32[], p.1.lhs.1117: s32[], p.1.rhs.1118: s32[]) -> pred[] {
  %p.1.lhs.1117 = s32[] parameter(2)
  %p.1.rhs.1118 = s32[] parameter(3)
  %p.0.rhs.1116 = f32[] parameter(1)
  %p.0.lhs.1115 = f32[] parameter(0)
  ROOT %fusion = pred[] fusion(s32[] %p.1.lhs.1117, s32[] %p.1.rhs.1118, f32[] %p.0.rhs.1116, f32[] %p.0.lhs.1115), kind=kLoop, calls=%fused_computation
}

%fused_computation.1 (param_0.5: s32[], param_1.7: s32[], param_2.13: f32[], param_3.21: f32[]) -> pred[] {
  %param_3.21 = f32[] parameter(3)
  %bitcast-convert.41 = s32[] bitcast-convert(f32[] %param_3.21)
  %constant_183 = s32[] constant(0)
  %compare.36 = pred[] compare(s32[] %bitcast-convert.41, s32[] %constant_183), direction=LT
  %constant_182 = u32[] constant(2147483647)
  %bitcast-convert.40 = u32[] bitcast-convert(f32[] %param_3.21)
  %subtract.66 = u32[] subtract(u32[] %constant_182, u32[] %bitcast-convert.40)
  %bitcast-convert.39 = s32[] bitcast-convert(u32[] %subtract.66)
  %select.20 = s32[] select(pred[] %compare.36, s32[] %bitcast-convert.39, s32[] %bitcast-convert.41)
  %param_2.13 = f32[] parameter(2)
  %bitcast-convert.38 = s32[] bitcast-convert(f32[] %param_2.13)
  %compare.35 = pred[] compare(s32[] %bitcast-convert.38, s32[] %constant_183), direction=LT
  %bitcast-convert.37 = u32[] bitcast-convert(f32[] %param_2.13)
  %subtract.65 = u32[] subtract(u32[] %constant_182, u32[] %bitcast-convert.37)
  %bitcast-convert.36 = s32[] bitcast-convert(u32[] %subtract.65)
  %select.19 = s32[] select(pred[] %compare.35, s32[] %bitcast-convert.36, s32[] %bitcast-convert.38)
  %compare.34 = pred[] compare(s32[] %select.20, s32[] %select.19), direction=GT
  %compare.33 = pred[] compare(s32[] %select.19, s32[] %select.20), direction=GT
  %compare.32 = pred[] compare(pred[] %compare.34, pred[] %compare.33), direction=EQ
  %param_0.5 = s32[] parameter(0)
  %param_1.7 = s32[] parameter(1)
  %compare.31 = pred[] compare(s32[] %param_0.5, s32[] %param_1.7), direction=LT
  ROOT %select.18 = pred[] select(pred[] %compare.32, pred[] %compare.31, pred[] %compare.34)
}

%compare-greater-than.1188 (p.0.lhs.1189: f32[], p.0.rhs.1190: f32[], p.1.lhs.1191: s32[], p.1.rhs.1192: s32[]) -> pred[] {
  %p.1.lhs.1191 = s32[] parameter(2)
  %p.1.rhs.1192 = s32[] parameter(3)
  %p.0.rhs.1190 = f32[] parameter(1)
  %p.0.lhs.1189 = f32[] parameter(0)
  ROOT %fusion.1 = pred[] fusion(s32[] %p.1.lhs.1191, s32[] %p.1.rhs.1192, f32[] %p.0.rhs.1190, f32[] %p.0.lhs.1189), kind=kLoop, calls=%fused_computation.1
}

%fused_computation.2 (param_0.8: s32[], param_1.11: s32[], param_2.20: f32[], param_3.32: f32[]) -> pred[] {
  %param_3.32 = f32[] parameter(3)
  %bitcast-convert.47 = s32[] bitcast-convert(f32[] %param_3.32)
  %constant_185 = s32[] constant(0)
  %compare.42 = pred[] compare(s32[] %bitcast-convert.47, s32[] %constant_185), direction=LT
  %constant_184 = u32[] constant(2147483647)
  %bitcast-convert.46 = u32[] bitcast-convert(f32[] %param_3.32)
  %subtract.68 = u32[] subtract(u32[] %constant_184, u32[] %bitcast-convert.46)
  %bitcast-convert.45 = s32[] bitcast-convert(u32[] %subtract.68)
  %select.23 = s32[] select(pred[] %compare.42, s32[] %bitcast-convert.45, s32[] %bitcast-convert.47)
  %param_2.20 = f32[] parameter(2)
  %bitcast-convert.44 = s32[] bitcast-convert(f32[] %param_2.20)
  %compare.41 = pred[] compare(s32[] %bitcast-convert.44, s32[] %constant_185), direction=LT
  %bitcast-convert.43 = u32[] bitcast-convert(f32[] %param_2.20)
  %subtract.67 = u32[] subtract(u32[] %constant_184, u32[] %bitcast-convert.43)
  %bitcast-convert.42 = s32[] bitcast-convert(u32[] %subtract.67)
  %select.22 = s32[] select(pred[] %compare.41, s32[] %bitcast-convert.42, s32[] %bitcast-convert.44)
  %compare.40 = pred[] compare(s32[] %select.23, s32[] %select.22), direction=GT
  %compare.39 = pred[] compare(s32[] %select.22, s32[] %select.23), direction=GT
  %compare.38 = pred[] compare(pred[] %compare.40, pred[] %compare.39), direction=EQ
  %param_0.8 = s32[] parameter(0)
  %param_1.11 = s32[] parameter(1)
  %compare.37 = pred[] compare(s32[] %param_0.8, s32[] %param_1.11), direction=LT
  ROOT %select.21 = pred[] select(pred[] %compare.38, pred[] %compare.37, pred[] %compare.40)
}

%compare-greater-than.1341 (p.0.lhs.1342: f32[], p.0.rhs.1343: f32[], p.1.lhs.1344: s32[], p.1.rhs.1345: s32[]) -> pred[] {
  %p.1.lhs.1344 = s32[] parameter(2)
  %p.1.rhs.1345 = s32[] parameter(3)
  %p.0.rhs.1343 = f32[] parameter(1)
  %p.0.lhs.1342 = f32[] parameter(0)
  ROOT %fusion.2 = pred[] fusion(s32[] %p.1.lhs.1344, s32[] %p.1.rhs.1345, f32[] %p.0.rhs.1343, f32[] %p.0.lhs.1342), kind=kLoop, calls=%fused_computation.2
}

%fused_computation.3 (param_0.11: s32[], param_1.15: s32[], param_2.27: f32[], param_3.43: f32[]) -> pred[] {
  %param_3.43 = f32[] parameter(3)
  %bitcast-convert.53 = s32[] bitcast-convert(f32[] %param_3.43)
  %constant_187 = s32[] constant(0)
  %compare.48 = pred[] compare(s32[] %bitcast-convert.53, s32[] %constant_187), direction=LT
  %constant_186 = u32[] constant(2147483647)
  %bitcast-convert.52 = u32[] bitcast-convert(f32[] %param_3.43)
  %subtract.70 = u32[] subtract(u32[] %constant_186, u32[] %bitcast-convert.52)
  %bitcast-convert.51 = s32[] bitcast-convert(u32[] %subtract.70)
  %select.26 = s32[] select(pred[] %compare.48, s32[] %bitcast-convert.51, s32[] %bitcast-convert.53)
  %param_2.27 = f32[] parameter(2)
  %bitcast-convert.50 = s32[] bitcast-convert(f32[] %param_2.27)
  %compare.47 = pred[] compare(s32[] %bitcast-convert.50, s32[] %constant_187), direction=LT
  %bitcast-convert.49 = u32[] bitcast-convert(f32[] %param_2.27)
  %subtract.69 = u32[] subtract(u32[] %constant_186, u32[] %bitcast-convert.49)
  %bitcast-convert.48 = s32[] bitcast-convert(u32[] %subtract.69)
  %select.25 = s32[] select(pred[] %compare.47, s32[] %bitcast-convert.48, s32[] %bitcast-convert.50)
  %compare.46 = pred[] compare(s32[] %select.26, s32[] %select.25), direction=GT
  %compare.45 = pred[] compare(s32[] %select.25, s32[] %select.26), direction=GT
  %compare.44 = pred[] compare(pred[] %compare.46, pred[] %compare.45), direction=EQ
  %param_0.11 = s32[] parameter(0)
  %param_1.15 = s32[] parameter(1)
  %compare.43 = pred[] compare(s32[] %param_0.11, s32[] %param_1.15), direction=LT
  ROOT %select.24 = pred[] select(pred[] %compare.44, pred[] %compare.43, pred[] %compare.46)
}

%compare-greater-than.1494 (p.0.lhs.1495: f32[], p.0.rhs.1496: f32[], p.1.lhs.1497: s32[], p.1.rhs.1498: s32[]) -> pred[] {
  %p.1.lhs.1497 = s32[] parameter(2)
  %p.1.rhs.1498 = s32[] parameter(3)
  %p.0.rhs.1496 = f32[] parameter(1)
  %p.0.lhs.1495 = f32[] parameter(0)
  ROOT %fusion.3 = pred[] fusion(s32[] %p.1.lhs.1497, s32[] %p.1.rhs.1498, f32[] %p.0.rhs.1496, f32[] %p.0.lhs.1495), kind=kLoop, calls=%fused_computation.3
}

%fused_computation.4 (param_0.14: s32[], param_1.19: s32[], param_2.34: f32[], param_3.54: f32[]) -> pred[] {
  %param_3.54 = f32[] parameter(3)
  %bitcast-convert.59 = s32[] bitcast-convert(f32[] %param_3.54)
  %constant_189 = s32[] constant(0)
  %compare.54 = pred[] compare(s32[] %bitcast-convert.59, s32[] %constant_189), direction=LT
  %constant_188 = u32[] constant(2147483647)
  %bitcast-convert.58 = u32[] bitcast-convert(f32[] %param_3.54)
  %subtract.72 = u32[] subtract(u32[] %constant_188, u32[] %bitcast-convert.58)
  %bitcast-convert.57 = s32[] bitcast-convert(u32[] %subtract.72)
  %select.29 = s32[] select(pred[] %compare.54, s32[] %bitcast-convert.57, s32[] %bitcast-convert.59)
  %param_2.34 = f32[] parameter(2)
  %bitcast-convert.56 = s32[] bitcast-convert(f32[] %param_2.34)
  %compare.53 = pred[] compare(s32[] %bitcast-convert.56, s32[] %constant_189), direction=LT
  %bitcast-convert.55 = u32[] bitcast-convert(f32[] %param_2.34)
  %subtract.71 = u32[] subtract(u32[] %constant_188, u32[] %bitcast-convert.55)
  %bitcast-convert.54 = s32[] bitcast-convert(u32[] %subtract.71)
  %select.28 = s32[] select(pred[] %compare.53, s32[] %bitcast-convert.54, s32[] %bitcast-convert.56)
  %compare.52 = pred[] compare(s32[] %select.29, s32[] %select.28), direction=GT
  %compare.51 = pred[] compare(s32[] %select.28, s32[] %select.29), direction=GT
  %compare.50 = pred[] compare(pred[] %compare.52, pred[] %compare.51), direction=EQ
  %param_0.14 = s32[] parameter(0)
  %param_1.19 = s32[] parameter(1)
  %compare.49 = pred[] compare(s32[] %param_0.14, s32[] %param_1.19), direction=LT
  ROOT %select.27 = pred[] select(pred[] %compare.50, pred[] %compare.49, pred[] %compare.52)
}

%compare-greater-than.1647 (p.0.lhs.1648: f32[], p.0.rhs.1649: f32[], p.1.lhs.1650: s32[], p.1.rhs.1651: s32[]) -> pred[] {
  %p.1.lhs.1650 = s32[] parameter(2)
  %p.1.rhs.1651 = s32[] parameter(3)
  %p.0.rhs.1649 = f32[] parameter(1)
  %p.0.lhs.1648 = f32[] parameter(0)
  ROOT %fusion.4 = pred[] fusion(s32[] %p.1.lhs.1650, s32[] %p.1.rhs.1651, f32[] %p.0.rhs.1649, f32[] %p.0.lhs.1648), kind=kLoop, calls=%fused_computation.4
}

%fused_computation.6 (param_0.19: f32[1,3,200,304], param_1.24: f32[3]) -> f32[182400] {
  %param_0.19 = f32[1,3,200,304]{3,2,1,0} parameter(0)
  %param_1.24 = f32[3]{0} parameter(1)
  %broadcast.457 = f32[1,3,200,304]{3,2,1,0} broadcast(f32[3]{0} %param_1.24), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn/class/BiasAdd"}
  %add.164 = f32[1,3,200,304]{3,2,1,0} add(f32[1,3,200,304]{3,2,1,0} %param_0.19, f32[1,3,200,304]{3,2,1,0} %broadcast.457), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn/class/BiasAdd"}
  %bitcast.36 = f32[1,200,304,3]{2,1,3,0} bitcast(f32[1,3,200,304]{3,2,1,0} %add.164), metadata={op_type="Transpose" op_name="tower-pred-0/rpn/transpose"}
  ROOT %reshape.24 = f32[182400]{0} reshape(f32[1,200,304,3]{2,1,3,0} %bitcast.36), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/Reshape_1"}
}

%fused_computation.7 (param_0.344: f32[1,12,200,304], param_1.424: f32[12]) -> f32[182400,1,2] {
  %param_0.344 = f32[1,12,200,304]{3,2,1,0} parameter(0)
  %param_1.424 = f32[12]{0} parameter(1)
  %broadcast.880 = f32[1,12,200,304]{3,2,1,0} broadcast(f32[12]{0} %param_1.424), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn/box/BiasAdd"}
  %add.335 = f32[1,12,200,304]{3,2,1,0} add(f32[1,12,200,304]{3,2,1,0} %param_0.344, f32[1,12,200,304]{3,2,1,0} %broadcast.880), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn/box/BiasAdd"}
  %bitcast.113 = f32[1,200,304,12]{2,1,3,0} bitcast(f32[1,12,200,304]{3,2,1,0} %add.335), metadata={op_type="Transpose" op_name="tower-pred-0/rpn/transpose_1"}
  %copy.189 = f32[1,200,304,12]{3,2,1,0} copy(f32[1,200,304,12]{2,1,3,0} %bitcast.113), metadata={op_type="Transpose" op_name="tower-pred-0/rpn/transpose_1"}
  %bitcast.112 = f32[182400,2,2]{2,1,0} bitcast(f32[1,200,304,12]{3,2,1,0} %copy.189), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target/Reshape"}
  %slice.2 = f32[182400,1,2]{2,1,0} slice(f32[182400,2,2]{2,1,0} %bitcast.112), slice={[0:182400], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target/split"}
  %constant_191 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_4/Minimum"}
  %broadcast.458 = f32[182400,1,2]{2,1,0} broadcast(f32[] %constant_191), dimensions={}, metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target/Minimum"}
  %minimum.1 = f32[182400,1,2]{2,1,0} minimum(f32[182400,1,2]{2,1,0} %slice.2, f32[182400,1,2]{2,1,0} %broadcast.458), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target/Minimum"}
  ROOT %exponential.0 = f32[182400,1,2]{2,1,0} exponential(f32[182400,1,2]{2,1,0} %minimum.1), metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target/Exp"}
}

%fused_computation.10 (param_0.32: f32[1,3,100,152], param_1.34: f32[3]) -> f32[45600] {
  %param_0.32 = f32[1,3,100,152]{3,2,1,0} parameter(0)
  %param_1.34 = f32[3]{0} parameter(1)
  %broadcast.461 = f32[1,3,100,152]{3,2,1,0} broadcast(f32[3]{0} %param_1.34), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_1/class/BiasAdd"}
  %add.168 = f32[1,3,100,152]{3,2,1,0} add(f32[1,3,100,152]{3,2,1,0} %param_0.32, f32[1,3,100,152]{3,2,1,0} %broadcast.461), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_1/class/BiasAdd"}
  %bitcast.40 = f32[1,100,152,3]{2,1,3,0} bitcast(f32[1,3,100,152]{3,2,1,0} %add.168), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_1/transpose"}
  ROOT %reshape.25 = f32[45600]{0} reshape(f32[1,100,152,3]{2,1,3,0} %bitcast.40), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/Reshape_1"}
}

%fused_computation.11 (param_0.338: f32[1,12,100,152], param_1.420: f32[12]) -> f32[45600,1,2] {
  %param_0.338 = f32[1,12,100,152]{3,2,1,0} parameter(0)
  %param_1.420 = f32[12]{0} parameter(1)
  %broadcast.876 = f32[1,12,100,152]{3,2,1,0} broadcast(f32[12]{0} %param_1.420), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_1/box/BiasAdd"}
  %add.331 = f32[1,12,100,152]{3,2,1,0} add(f32[1,12,100,152]{3,2,1,0} %param_0.338, f32[1,12,100,152]{3,2,1,0} %broadcast.876), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_1/box/BiasAdd"}
  %bitcast.105 = f32[1,100,152,12]{2,1,3,0} bitcast(f32[1,12,100,152]{3,2,1,0} %add.331), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_1/transpose_1"}
  %copy.185 = f32[1,100,152,12]{3,2,1,0} copy(f32[1,100,152,12]{2,1,3,0} %bitcast.105), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_1/transpose_1"}
  %bitcast.104 = f32[45600,2,2]{2,1,0} bitcast(f32[1,100,152,12]{3,2,1,0} %copy.185), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target_1/Reshape"}
  %slice.5 = f32[45600,1,2]{2,1,0} slice(f32[45600,2,2]{2,1,0} %bitcast.104), slice={[0:45600], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target_1/split"}
  %constant_193 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_4/Minimum"}
  %broadcast.462 = f32[45600,1,2]{2,1,0} broadcast(f32[] %constant_193), dimensions={}, metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_1/Minimum"}
  %minimum.3 = f32[45600,1,2]{2,1,0} minimum(f32[45600,1,2]{2,1,0} %slice.5, f32[45600,1,2]{2,1,0} %broadcast.462), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_1/Minimum"}
  ROOT %exponential.1 = f32[45600,1,2]{2,1,0} exponential(f32[45600,1,2]{2,1,0} %minimum.3), metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target_1/Exp"}
}

%fused_computation.13 (param_0.41: f32[1,256,200,304], param_1.41: f32[256]) -> f32[1,256,200,304] {
  %param_0.41 = f32[1,256,200,304]{3,2,1,0} parameter(0)
  %param_1.41 = f32[256]{0} parameter(1)
  %broadcast.464 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_1.41), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p2/BiasAdd"}
  ROOT %add.170 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %param_0.41, f32[1,256,200,304]{3,2,1,0} %broadcast.464), metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p2/BiasAdd"}
}

%fused_computation.14 (param_0.43: f32[1,256,100,152], param_1.47: f32[1,256,200,304], param_2.42: f32[256]) -> f32[1,256,200,304] {
  %param_1.47 = f32[1,256,200,304]{3,2,1,0} parameter(1)
  %param_2.42 = f32[256]{0} parameter(2)
  %broadcast.466 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_2.42), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/lateral_1x1_c2/BiasAdd"}
  %add.172 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %param_1.47, f32[1,256,200,304]{3,2,1,0} %broadcast.466), metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/lateral_1x1_c2/BiasAdd"}
  %param_0.43 = f32[1,256,100,152]{3,2,1,0} parameter(0)
  %bitcast.44 = f32[3891200]{0} bitcast(f32[1,256,100,152]{3,2,1,0} %param_0.43)
  %broadcast.465 = f32[3891200,4]{1,0} broadcast(f32[3891200]{0} %bitcast.44), dimensions={0}, metadata={op_type="MatMul" op_name="tower-pred-0/fpn/upsample_lat3/Tensordot/MatMul"}
  %reshape.26 = f32[1,256,100,152,2,2]{5,3,4,2,1,0} reshape(f32[3891200,4]{1,0} %broadcast.465), metadata={op_type="Reshape" op_name="tower-pred-0/fpn/upsample_lat3/Tensordot"}
  %bitcast.43 = f32[1,256,200,304]{3,2,1,0} bitcast(f32[1,256,100,152,2,2]{5,3,4,2,1,0} %reshape.26), metadata={op_type="Reshape" op_name="tower-pred-0/fpn/upsample_lat3/Reshape"}
  ROOT %add.171 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %add.172, f32[1,256,200,304]{3,2,1,0} %bitcast.43), metadata={op_type="Add" op_name="tower-pred-0/fpn/add_2"}
}

%fused_computation.16 (param_0.48: f32[1,3,50,76], param_1.52: f32[3]) -> f32[11400] {
  %param_0.48 = f32[1,3,50,76]{3,2,1,0} parameter(0)
  %param_1.52 = f32[3]{0} parameter(1)
  %broadcast.468 = f32[1,3,50,76]{3,2,1,0} broadcast(f32[3]{0} %param_1.52), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_2/class/BiasAdd"}
  %add.175 = f32[1,3,50,76]{3,2,1,0} add(f32[1,3,50,76]{3,2,1,0} %param_0.48, f32[1,3,50,76]{3,2,1,0} %broadcast.468), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_2/class/BiasAdd"}
  %bitcast.46 = f32[1,50,76,3]{2,1,3,0} bitcast(f32[1,3,50,76]{3,2,1,0} %add.175), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_2/transpose"}
  ROOT %reshape.27 = f32[11400]{0} reshape(f32[1,50,76,3]{2,1,3,0} %bitcast.46), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/Reshape_1"}
}

%fused_computation.17 (param_0.332: f32[1,12,50,76], param_1.416: f32[12]) -> f32[11400,1,2] {
  %param_0.332 = f32[1,12,50,76]{3,2,1,0} parameter(0)
  %param_1.416 = f32[12]{0} parameter(1)
  %broadcast.871 = f32[1,12,50,76]{3,2,1,0} broadcast(f32[12]{0} %param_1.416), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_2/box/BiasAdd"}
  %add.327 = f32[1,12,50,76]{3,2,1,0} add(f32[1,12,50,76]{3,2,1,0} %param_0.332, f32[1,12,50,76]{3,2,1,0} %broadcast.871), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_2/box/BiasAdd"}
  %bitcast.97 = f32[1,50,76,12]{2,1,3,0} bitcast(f32[1,12,50,76]{3,2,1,0} %add.327), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_2/transpose_1"}
  %copy.181 = f32[1,50,76,12]{3,2,1,0} copy(f32[1,50,76,12]{2,1,3,0} %bitcast.97), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_2/transpose_1"}
  %bitcast.96 = f32[11400,2,2]{2,1,0} bitcast(f32[1,50,76,12]{3,2,1,0} %copy.181), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target_2/Reshape"}
  %slice.8 = f32[11400,1,2]{2,1,0} slice(f32[11400,2,2]{2,1,0} %bitcast.96), slice={[0:11400], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target_2/split"}
  %constant_195 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_4/Minimum"}
  %broadcast.469 = f32[11400,1,2]{2,1,0} broadcast(f32[] %constant_195), dimensions={}, metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_2/Minimum"}
  %minimum.5 = f32[11400,1,2]{2,1,0} minimum(f32[11400,1,2]{2,1,0} %slice.8, f32[11400,1,2]{2,1,0} %broadcast.469), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_2/Minimum"}
  ROOT %exponential.2 = f32[11400,1,2]{2,1,0} exponential(f32[11400,1,2]{2,1,0} %minimum.5), metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target_2/Exp"}
}

%fused_computation.19 (param_0.57: f32[1,256,100,152], param_1.59: f32[256]) -> f32[1,256,100,152] {
  %param_0.57 = f32[1,256,100,152]{3,2,1,0} parameter(0)
  %param_1.59 = f32[256]{0} parameter(1)
  %broadcast.471 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[256]{0} %param_1.59), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p3/BiasAdd"}
  ROOT %add.177 = f32[1,256,100,152]{3,2,1,0} add(f32[1,256,100,152]{3,2,1,0} %param_0.57, f32[1,256,100,152]{3,2,1,0} %broadcast.471), metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p3/BiasAdd"}
}

%fused_computation.20 (param_0.59: f32[1,256,50,76], param_1.65: f32[1,256,100,152], param_2.47: f32[256]) -> f32[1,256,100,152] {
  %param_1.65 = f32[1,256,100,152]{3,2,1,0} parameter(1)
  %param_2.47 = f32[256]{0} parameter(2)
  %broadcast.473 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[256]{0} %param_2.47), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/lateral_1x1_c3/BiasAdd"}
  %add.179 = f32[1,256,100,152]{3,2,1,0} add(f32[1,256,100,152]{3,2,1,0} %param_1.65, f32[1,256,100,152]{3,2,1,0} %broadcast.473), metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/lateral_1x1_c3/BiasAdd"}
  %param_0.59 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %bitcast.50 = f32[972800]{0} bitcast(f32[1,256,50,76]{3,2,1,0} %param_0.59)
  %broadcast.472 = f32[972800,4]{1,0} broadcast(f32[972800]{0} %bitcast.50), dimensions={0}, metadata={op_type="MatMul" op_name="tower-pred-0/fpn/upsample_lat4/Tensordot/MatMul"}
  %reshape.28 = f32[1,256,50,76,2,2]{5,3,4,2,1,0} reshape(f32[972800,4]{1,0} %broadcast.472), metadata={op_type="Reshape" op_name="tower-pred-0/fpn/upsample_lat4/Tensordot"}
  %bitcast.49 = f32[1,256,100,152]{3,2,1,0} bitcast(f32[1,256,50,76,2,2]{5,3,4,2,1,0} %reshape.28), metadata={op_type="Reshape" op_name="tower-pred-0/fpn/upsample_lat4/Reshape"}
  ROOT %add.178 = f32[1,256,100,152]{3,2,1,0} add(f32[1,256,100,152]{3,2,1,0} %add.179, f32[1,256,100,152]{3,2,1,0} %bitcast.49), metadata={op_type="Add" op_name="tower-pred-0/fpn/add_1"}
}

%fused_computation.21 (param_0.316: s32[741], param_1.408: f32[741,1,2], param_2.305: f32[741,1,2], param_3.353: f32[741,1,2], param_4.311: f32[2], param_5.265: f32[1,12,13,19], param_6.127: f32[12]) -> f32[741,4] {
  %constant_196 = f32[] constant(0.5), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_3/mul"}
  %broadcast.476 = f32[741,1,2]{2,1,0} broadcast(f32[] %constant_196), dimensions={}, metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_4/mul"}
  %param_3.353 = f32[741,1,2]{2,1,0} parameter(3)
  %multiply.124 = f32[741,1,2]{2,1,0} multiply(f32[741,1,2]{2,1,0} %broadcast.476, f32[741,1,2]{2,1,0} %param_3.353), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_4/mul"}
  %param_5.265 = f32[1,12,13,19]{3,2,1,0} parameter(5)
  %param_6.127 = f32[12]{0} parameter(6)
  %broadcast.864 = f32[1,12,13,19]{3,2,1,0} broadcast(f32[12]{0} %param_6.127), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_4/box/BiasAdd"}
  %add.321 = f32[1,12,13,19]{3,2,1,0} add(f32[1,12,13,19]{3,2,1,0} %param_5.265, f32[1,12,13,19]{3,2,1,0} %broadcast.864), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_4/box/BiasAdd"}
  %bitcast.85 = f32[1,13,19,12]{2,1,3,0} bitcast(f32[1,12,13,19]{3,2,1,0} %add.321), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_4/transpose_1"}
  %copy.175 = f32[1,13,19,12]{3,2,1,0} copy(f32[1,13,19,12]{2,1,3,0} %bitcast.85), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_4/transpose_1"}
  %bitcast.84 = f32[741,2,2]{2,1,0} bitcast(f32[1,13,19,12]{3,2,1,0} %copy.175), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target_4/Reshape"}
  %slice.9 = f32[741,1,2]{2,1,0} slice(f32[741,2,2]{2,1,0} %bitcast.84), slice={[0:741], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target_4/split"}
  %param_2.305 = f32[741,1,2]{2,1,0} parameter(2)
  %multiply.123 = f32[741,1,2]{2,1,0} multiply(f32[741,1,2]{2,1,0} %slice.9, f32[741,1,2]{2,1,0} %param_2.305), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_4/mul_2"}
  %add.181 = f32[741,1,2]{2,1,0} add(f32[741,1,2]{2,1,0} %multiply.124, f32[741,1,2]{2,1,0} %multiply.123), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_4/add_1"}
  %param_1.408 = f32[741,1,2]{2,1,0} parameter(1)
  %multiply.122 = f32[741,1,2]{2,1,0} multiply(f32[741,1,2]{2,1,0} %broadcast.476, f32[741,1,2]{2,1,0} %param_2.305), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_4/mul_1"}
  %multiply.121 = f32[741,1,2]{2,1,0} multiply(f32[741,1,2]{2,1,0} %param_1.408, f32[741,1,2]{2,1,0} %multiply.122), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_4/mul_4"}
  %subtract.76 = f32[741,1,2]{2,1,0} subtract(f32[741,1,2]{2,1,0} %add.181, f32[741,1,2]{2,1,0} %multiply.121), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target_4/sub_1"}
  %add.180 = f32[741,1,2]{2,1,0} add(f32[741,1,2]{2,1,0} %add.181, f32[741,1,2]{2,1,0} %multiply.121), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_4/add_2"}
  %concatenate.3 = f32[741,2,2]{2,1,0} concatenate(f32[741,1,2]{2,1,0} %subtract.76, f32[741,1,2]{2,1,0} %add.180), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower-pred-0/decode_bbox_target_4/concat"}
  %bitcast.51 = f32[741,4]{1,0} bitcast(f32[741,2,2]{2,1,0} %concatenate.3), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/Reshape"}
  %param_0.316 = s32[741]{0} parameter(0)
  %gather.3 = f32[741,4]{1,0} gather(f32[741,4]{1,0} %bitcast.51, s32[741]{0} %param_0.316), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/GatherV2"}
  %constant_197 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.475 = f32[741,4]{1,0} broadcast(f32[] %constant_197), dimensions={}, metadata={op_type="Maximum" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Maximum"}
  %maximum.3 = f32[741,4]{1,0} maximum(f32[741,4]{1,0} %gather.3, f32[741,4]{1,0} %broadcast.475), metadata={op_type="Maximum" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Maximum"}
  %param_4.311 = f32[2]{0} parameter(4)
  %broadcast.852 = f32[2,2]{1,0} broadcast(f32[2]{0} %param_4.311), dimensions={1}
  %bitcast.73 = f32[4]{0} bitcast(f32[2,2]{1,0} %broadcast.852), metadata={op_type="Cast" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Cast"}
  %broadcast.474 = f32[741,4]{1,0} broadcast(f32[4]{0} %bitcast.73), dimensions={1}, metadata={op_type="Minimum" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Minimum"}
  ROOT %minimum.6 = f32[741,4]{1,0} minimum(f32[741,4]{1,0} %maximum.3, f32[741,4]{1,0} %broadcast.474), metadata={op_type="Minimum" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Minimum"}
}

%fused_computation.22 (param_0.64: f32[1,3,13,19], param_1.72: f32[3]) -> f32[741] {
  %param_0.64 = f32[1,3,13,19]{3,2,1,0} parameter(0)
  %param_1.72 = f32[3]{0} parameter(1)
  %broadcast.477 = f32[1,3,13,19]{3,2,1,0} broadcast(f32[3]{0} %param_1.72), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_4/class/BiasAdd"}
  %add.182 = f32[1,3,13,19]{3,2,1,0} add(f32[1,3,13,19]{3,2,1,0} %param_0.64, f32[1,3,13,19]{3,2,1,0} %broadcast.477), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_4/class/BiasAdd"}
  %bitcast.52 = f32[1,13,19,3]{2,1,3,0} bitcast(f32[1,3,13,19]{3,2,1,0} %add.182), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_4/transpose"}
  ROOT %reshape.29 = f32[741]{0} reshape(f32[1,13,19,3]{2,1,3,0} %bitcast.52), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/Reshape_1"}
}

%fused_computation.23 (param_0.326: f32[1,12,13,19], param_1.412: f32[12]) -> f32[741,1,2] {
  %param_0.326 = f32[1,12,13,19]{3,2,1,0} parameter(0)
  %param_1.412 = f32[12]{0} parameter(1)
  %broadcast.866 = f32[1,12,13,19]{3,2,1,0} broadcast(f32[12]{0} %param_1.412), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_4/box/BiasAdd"}
  %add.323 = f32[1,12,13,19]{3,2,1,0} add(f32[1,12,13,19]{3,2,1,0} %param_0.326, f32[1,12,13,19]{3,2,1,0} %broadcast.866), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_4/box/BiasAdd"}
  %bitcast.89 = f32[1,13,19,12]{2,1,3,0} bitcast(f32[1,12,13,19]{3,2,1,0} %add.323), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_4/transpose_1"}
  %copy.177 = f32[1,13,19,12]{3,2,1,0} copy(f32[1,13,19,12]{2,1,3,0} %bitcast.89), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_4/transpose_1"}
  %bitcast.88 = f32[741,2,2]{2,1,0} bitcast(f32[1,13,19,12]{3,2,1,0} %copy.177), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target_4/Reshape"}
  %slice.10 = f32[741,1,2]{2,1,0} slice(f32[741,2,2]{2,1,0} %bitcast.88), slice={[0:741], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target_4/split"}
  %constant_198 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_4/Minimum"}
  %broadcast.478 = f32[741,1,2]{2,1,0} broadcast(f32[] %constant_198), dimensions={}, metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_4/Minimum"}
  %minimum.7 = f32[741,1,2]{2,1,0} minimum(f32[741,1,2]{2,1,0} %slice.10, f32[741,1,2]{2,1,0} %broadcast.478), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_4/Minimum"}
  ROOT %exponential.3 = f32[741,1,2]{2,1,0} exponential(f32[741,1,2]{2,1,0} %minimum.7), metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target_4/Exp"}
}

%fused_computation.25 (param_0.263: s32[2850], param_1.305: f32[2850,1,2], param_2.259: f32[2850,1,2], param_3.351: f32[2850,1,2], param_4.310: f32[1,12,25,38], param_5.261: f32[12], param_6.125: f32[2], param_7.68: s32[11400], param_8.30: f32[11400,1,2], param_9.21: f32[11400,1,2], param_10.16: f32[1,12,50,76], param_11.3: f32[11400,1,2], param_12.12: s32[45600], param_13.11: f32[45600,1,2], param_14.5: f32[45600,1,2], param_15.4: f32[1,12,100,152], param_16.3: f32[45600,1,2], param_17.12: s32[182400], param_18.11: f32[182400,1,2], param_19.5: f32[182400,1,2], param_20.4: f32[1,12,200,304], param_21.3: f32[182400,1,2]) -> (f32[1000,4], f32[1000,4], f32[1000,4], f32[1000,4]) {
  %constant_199 = f32[] constant(0.5), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_3/mul"}
  %broadcast.480 = f32[2850,1,2]{2,1,0} broadcast(f32[] %constant_199), dimensions={}, metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_3/mul"}
  %param_3.351 = f32[2850,1,2]{2,1,0} parameter(3)
  %multiply.128 = f32[2850,1,2]{2,1,0} multiply(f32[2850,1,2]{2,1,0} %broadcast.480, f32[2850,1,2]{2,1,0} %param_3.351), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_3/mul"}
  %param_4.310 = f32[1,12,25,38]{3,2,1,0} parameter(4)
  %param_5.261 = f32[12]{0} parameter(5)
  %broadcast.848 = f32[1,12,25,38]{3,2,1,0} broadcast(f32[12]{0} %param_5.261), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_3/box/BiasAdd"}
  %add.317 = f32[1,12,25,38]{3,2,1,0} add(f32[1,12,25,38]{3,2,1,0} %param_4.310, f32[1,12,25,38]{3,2,1,0} %broadcast.848), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_3/box/BiasAdd"}
  %bitcast.67 = f32[1,25,38,12]{2,1,3,0} bitcast(f32[1,12,25,38]{3,2,1,0} %add.317), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_3/transpose_1"}
  %copy.171 = f32[1,25,38,12]{3,2,1,0} copy(f32[1,25,38,12]{2,1,3,0} %bitcast.67), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_3/transpose_1"}
  %bitcast.66 = f32[2850,2,2]{2,1,0} bitcast(f32[1,25,38,12]{3,2,1,0} %copy.171), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target_3/Reshape"}
  %slice.12 = f32[2850,1,2]{2,1,0} slice(f32[2850,2,2]{2,1,0} %bitcast.66), slice={[0:2850], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target_3/split"}
  %param_2.259 = f32[2850,1,2]{2,1,0} parameter(2)
  %multiply.127 = f32[2850,1,2]{2,1,0} multiply(f32[2850,1,2]{2,1,0} %slice.12, f32[2850,1,2]{2,1,0} %param_2.259), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_3/mul_2"}
  %add.185 = f32[2850,1,2]{2,1,0} add(f32[2850,1,2]{2,1,0} %multiply.128, f32[2850,1,2]{2,1,0} %multiply.127), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_3/add_1"}
  %param_1.305 = f32[2850,1,2]{2,1,0} parameter(1)
  %multiply.126 = f32[2850,1,2]{2,1,0} multiply(f32[2850,1,2]{2,1,0} %broadcast.480, f32[2850,1,2]{2,1,0} %param_2.259), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_3/mul_1"}
  %multiply.125 = f32[2850,1,2]{2,1,0} multiply(f32[2850,1,2]{2,1,0} %param_1.305, f32[2850,1,2]{2,1,0} %multiply.126), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_3/mul_4"}
  %subtract.77 = f32[2850,1,2]{2,1,0} subtract(f32[2850,1,2]{2,1,0} %add.185, f32[2850,1,2]{2,1,0} %multiply.125), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target_3/sub_1"}
  %add.184 = f32[2850,1,2]{2,1,0} add(f32[2850,1,2]{2,1,0} %add.185, f32[2850,1,2]{2,1,0} %multiply.125), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_3/add_2"}
  %concatenate.4 = f32[2850,2,2]{2,1,0} concatenate(f32[2850,1,2]{2,1,0} %subtract.77, f32[2850,1,2]{2,1,0} %add.184), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower-pred-0/decode_bbox_target_3/concat"}
  %bitcast.55 = f32[2850,4]{1,0} bitcast(f32[2850,2,2]{2,1,0} %concatenate.4), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/Reshape"}
  %param_0.263 = s32[2850]{0} parameter(0)
  %slice.11 = s32[1000]{0} slice(s32[2850]{0} %param_0.263), slice={[0:1000]}, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %gather.4 = f32[1000,4]{1,0} gather(f32[2850,4]{1,0} %bitcast.55, s32[1000]{0} %slice.11), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/GatherV2"}
  %constant_210 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.732 = f32[1000,4]{1,0} broadcast(f32[] %constant_210), dimensions={}, metadata={op_type="Maximum" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Maximum"}
  %maximum.4 = f32[1000,4]{1,0} maximum(f32[1000,4]{1,0} %gather.4, f32[1000,4]{1,0} %broadcast.732), metadata={op_type="Maximum" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Maximum"}
  %param_6.125 = f32[2]{0} parameter(6)
  %broadcast.862 = f32[2,2]{1,0} broadcast(f32[2]{0} %param_6.125), dimensions={1}
  %bitcast.81 = f32[4]{0} bitcast(f32[2,2]{1,0} %broadcast.862), metadata={op_type="Cast" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Cast"}
  %broadcast.733 = f32[1000,4]{1,0} broadcast(f32[4]{0} %bitcast.81), dimensions={1}, metadata={op_type="Minimum" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Minimum"}
  %minimum.8 = f32[1000,4]{1,0} minimum(f32[1000,4]{1,0} %maximum.4, f32[1000,4]{1,0} %broadcast.733), metadata={op_type="Minimum" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Minimum"}
  %broadcast.467.clone.1 = f32[11400,1,2]{2,1,0} broadcast(f32[] %constant_199), dimensions={}, metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_2/mul"}
  %param_11.3 = f32[11400,1,2]{2,1,0} parameter(11)
  %multiply.120.clone.1 = f32[11400,1,2]{2,1,0} multiply(f32[11400,1,2]{2,1,0} %broadcast.467.clone.1, f32[11400,1,2]{2,1,0} %param_11.3), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_2/mul"}
  %param_10.16 = f32[1,12,50,76]{3,2,1,0} parameter(10)
  %broadcast.869.clone.1 = f32[1,12,50,76]{3,2,1,0} broadcast(f32[12]{0} %param_5.261), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_2/box/BiasAdd"}
  %add.325.clone.1 = f32[1,12,50,76]{3,2,1,0} add(f32[1,12,50,76]{3,2,1,0} %param_10.16, f32[1,12,50,76]{3,2,1,0} %broadcast.869.clone.1), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_2/box/BiasAdd"}
  %bitcast.93.clone.1 = f32[1,50,76,12]{2,1,3,0} bitcast(f32[1,12,50,76]{3,2,1,0} %add.325.clone.1), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_2/transpose_1"}
  %copy.179.clone.1 = f32[1,50,76,12]{3,2,1,0} copy(f32[1,50,76,12]{2,1,3,0} %bitcast.93.clone.1), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_2/transpose_1"}
  %bitcast.92.clone.1 = f32[11400,2,2]{2,1,0} bitcast(f32[1,50,76,12]{3,2,1,0} %copy.179.clone.1), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target_2/Reshape"}
  %slice.7.clone.1 = f32[11400,1,2]{2,1,0} slice(f32[11400,2,2]{2,1,0} %bitcast.92.clone.1), slice={[0:11400], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target_2/split"}
  %param_9.21 = f32[11400,1,2]{2,1,0} parameter(9)
  %multiply.119.clone.1 = f32[11400,1,2]{2,1,0} multiply(f32[11400,1,2]{2,1,0} %slice.7.clone.1, f32[11400,1,2]{2,1,0} %param_9.21), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_2/mul_2"}
  %add.174.clone.1 = f32[11400,1,2]{2,1,0} add(f32[11400,1,2]{2,1,0} %multiply.120.clone.1, f32[11400,1,2]{2,1,0} %multiply.119.clone.1), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_2/add_1"}
  %param_8.30 = f32[11400,1,2]{2,1,0} parameter(8)
  %multiply.118.clone.1 = f32[11400,1,2]{2,1,0} multiply(f32[11400,1,2]{2,1,0} %broadcast.467.clone.1, f32[11400,1,2]{2,1,0} %param_9.21), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_2/mul_1"}
  %multiply.117.clone.1 = f32[11400,1,2]{2,1,0} multiply(f32[11400,1,2]{2,1,0} %param_8.30, f32[11400,1,2]{2,1,0} %multiply.118.clone.1), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_2/mul_4"}
  %subtract.75.clone.1 = f32[11400,1,2]{2,1,0} subtract(f32[11400,1,2]{2,1,0} %add.174.clone.1, f32[11400,1,2]{2,1,0} %multiply.117.clone.1), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target_2/sub_1"}
  %add.173.clone.1 = f32[11400,1,2]{2,1,0} add(f32[11400,1,2]{2,1,0} %add.174.clone.1, f32[11400,1,2]{2,1,0} %multiply.117.clone.1), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_2/add_2"}
  %concatenate.2.clone.1 = f32[11400,2,2]{2,1,0} concatenate(f32[11400,1,2]{2,1,0} %subtract.75.clone.1, f32[11400,1,2]{2,1,0} %add.173.clone.1), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower-pred-0/decode_bbox_target_2/concat"}
  %bitcast.45.clone.1 = f32[11400,4]{1,0} bitcast(f32[11400,2,2]{2,1,0} %concatenate.2.clone.1), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/Reshape"}
  %param_7.68 = s32[11400]{0} parameter(7)
  %slice.6.clone.1 = s32[1000]{0} slice(s32[11400]{0} %param_7.68), slice={[0:1000]}, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %gather.2.clone.1 = f32[1000,4]{1,0} gather(f32[11400,4]{1,0} %bitcast.45.clone.1, s32[1000]{0} %slice.6.clone.1), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/GatherV2"}
  %maximum.2.clone.1 = f32[1000,4]{1,0} maximum(f32[1000,4]{1,0} %gather.2.clone.1, f32[1000,4]{1,0} %broadcast.732), metadata={op_type="Maximum" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Maximum"}
  %minimum.4.clone.1 = f32[1000,4]{1,0} minimum(f32[1000,4]{1,0} %maximum.2.clone.1, f32[1000,4]{1,0} %broadcast.733), metadata={op_type="Minimum" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/clip_boxes/Minimum"}
  %broadcast.460.clone.1 = f32[45600,1,2]{2,1,0} broadcast(f32[] %constant_199), dimensions={}, metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_1/mul"}
  %param_16.3 = f32[45600,1,2]{2,1,0} parameter(16)
  %multiply.116.clone.1 = f32[45600,1,2]{2,1,0} multiply(f32[45600,1,2]{2,1,0} %broadcast.460.clone.1, f32[45600,1,2]{2,1,0} %param_16.3), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_1/mul"}
  %param_15.4 = f32[1,12,100,152]{3,2,1,0} parameter(15)
  %broadcast.873.clone.1 = f32[1,12,100,152]{3,2,1,0} broadcast(f32[12]{0} %param_5.261), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_1/box/BiasAdd"}
  %add.329.clone.1 = f32[1,12,100,152]{3,2,1,0} add(f32[1,12,100,152]{3,2,1,0} %param_15.4, f32[1,12,100,152]{3,2,1,0} %broadcast.873.clone.1), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_1/box/BiasAdd"}
  %bitcast.101.clone.1 = f32[1,100,152,12]{2,1,3,0} bitcast(f32[1,12,100,152]{3,2,1,0} %add.329.clone.1), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_1/transpose_1"}
  %copy.183.clone.1 = f32[1,100,152,12]{3,2,1,0} copy(f32[1,100,152,12]{2,1,3,0} %bitcast.101.clone.1), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_1/transpose_1"}
  %bitcast.100.clone.1 = f32[45600,2,2]{2,1,0} bitcast(f32[1,100,152,12]{3,2,1,0} %copy.183.clone.1), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target_1/Reshape"}
  %slice.4.clone.1 = f32[45600,1,2]{2,1,0} slice(f32[45600,2,2]{2,1,0} %bitcast.100.clone.1), slice={[0:45600], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target_1/split"}
  %param_14.5 = f32[45600,1,2]{2,1,0} parameter(14)
  %multiply.115.clone.1 = f32[45600,1,2]{2,1,0} multiply(f32[45600,1,2]{2,1,0} %slice.4.clone.1, f32[45600,1,2]{2,1,0} %param_14.5), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_1/mul_2"}
  %add.167.clone.1 = f32[45600,1,2]{2,1,0} add(f32[45600,1,2]{2,1,0} %multiply.116.clone.1, f32[45600,1,2]{2,1,0} %multiply.115.clone.1), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_1/add_1"}
  %param_13.11 = f32[45600,1,2]{2,1,0} parameter(13)
  %multiply.114.clone.1 = f32[45600,1,2]{2,1,0} multiply(f32[45600,1,2]{2,1,0} %broadcast.460.clone.1, f32[45600,1,2]{2,1,0} %param_14.5), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_1/mul_1"}
  %multiply.113.clone.1 = f32[45600,1,2]{2,1,0} multiply(f32[45600,1,2]{2,1,0} %param_13.11, f32[45600,1,2]{2,1,0} %multiply.114.clone.1), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target_1/mul_4"}
  %subtract.74.clone.1 = f32[45600,1,2]{2,1,0} subtract(f32[45600,1,2]{2,1,0} %add.167.clone.1, f32[45600,1,2]{2,1,0} %multiply.113.clone.1), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target_1/sub_1"}
  %add.166.clone.1 = f32[45600,1,2]{2,1,0} add(f32[45600,1,2]{2,1,0} %add.167.clone.1, f32[45600,1,2]{2,1,0} %multiply.113.clone.1), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_1/add_2"}
  %concatenate.1.clone.1 = f32[45600,2,2]{2,1,0} concatenate(f32[45600,1,2]{2,1,0} %subtract.74.clone.1, f32[45600,1,2]{2,1,0} %add.166.clone.1), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower-pred-0/decode_bbox_target_1/concat"}
  %bitcast.39.clone.1 = f32[45600,4]{1,0} bitcast(f32[45600,2,2]{2,1,0} %concatenate.1.clone.1), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/Reshape"}
  %param_12.12 = s32[45600]{0} parameter(12)
  %slice.3.clone.1 = s32[1000]{0} slice(s32[45600]{0} %param_12.12), slice={[0:1000]}, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %gather.1.clone.1 = f32[1000,4]{1,0} gather(f32[45600,4]{1,0} %bitcast.39.clone.1, s32[1000]{0} %slice.3.clone.1), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/GatherV2"}
  %maximum.1.clone.1 = f32[1000,4]{1,0} maximum(f32[1000,4]{1,0} %gather.1.clone.1, f32[1000,4]{1,0} %broadcast.732), metadata={op_type="Maximum" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/clip_boxes/Maximum"}
  %minimum.2.clone.1 = f32[1000,4]{1,0} minimum(f32[1000,4]{1,0} %maximum.1.clone.1, f32[1000,4]{1,0} %broadcast.733), metadata={op_type="Minimum" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/clip_boxes/Minimum"}
  %broadcast.456.clone.1 = f32[182400,1,2]{2,1,0} broadcast(f32[] %constant_199), dimensions={}, metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target/mul"}
  %param_21.3 = f32[182400,1,2]{2,1,0} parameter(21)
  %multiply.112.clone.1 = f32[182400,1,2]{2,1,0} multiply(f32[182400,1,2]{2,1,0} %broadcast.456.clone.1, f32[182400,1,2]{2,1,0} %param_21.3), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target/mul"}
  %param_20.4 = f32[1,12,200,304]{3,2,1,0} parameter(20)
  %broadcast.878.clone.1 = f32[1,12,200,304]{3,2,1,0} broadcast(f32[12]{0} %param_5.261), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn/box/BiasAdd"}
  %add.333.clone.1 = f32[1,12,200,304]{3,2,1,0} add(f32[1,12,200,304]{3,2,1,0} %param_20.4, f32[1,12,200,304]{3,2,1,0} %broadcast.878.clone.1), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn/box/BiasAdd"}
  %bitcast.109.clone.1 = f32[1,200,304,12]{2,1,3,0} bitcast(f32[1,12,200,304]{3,2,1,0} %add.333.clone.1), metadata={op_type="Transpose" op_name="tower-pred-0/rpn/transpose_1"}
  %copy.187.clone.1 = f32[1,200,304,12]{3,2,1,0} copy(f32[1,200,304,12]{2,1,3,0} %bitcast.109.clone.1), metadata={op_type="Transpose" op_name="tower-pred-0/rpn/transpose_1"}
  %bitcast.108.clone.1 = f32[182400,2,2]{2,1,0} bitcast(f32[1,200,304,12]{3,2,1,0} %copy.187.clone.1), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target/Reshape"}
  %slice.1.clone.1 = f32[182400,1,2]{2,1,0} slice(f32[182400,2,2]{2,1,0} %bitcast.108.clone.1), slice={[0:182400], [0:1], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target/split"}
  %param_19.5 = f32[182400,1,2]{2,1,0} parameter(19)
  %multiply.111.clone.1 = f32[182400,1,2]{2,1,0} multiply(f32[182400,1,2]{2,1,0} %slice.1.clone.1, f32[182400,1,2]{2,1,0} %param_19.5), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target/mul_2"}
  %add.163.clone.1 = f32[182400,1,2]{2,1,0} add(f32[182400,1,2]{2,1,0} %multiply.112.clone.1, f32[182400,1,2]{2,1,0} %multiply.111.clone.1), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target/add_1"}
  %param_18.11 = f32[182400,1,2]{2,1,0} parameter(18)
  %multiply.110.clone.1 = f32[182400,1,2]{2,1,0} multiply(f32[182400,1,2]{2,1,0} %broadcast.456.clone.1, f32[182400,1,2]{2,1,0} %param_19.5), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target/mul_1"}
  %multiply.109.clone.1 = f32[182400,1,2]{2,1,0} multiply(f32[182400,1,2]{2,1,0} %param_18.11, f32[182400,1,2]{2,1,0} %multiply.110.clone.1), metadata={op_type="Mul" op_name="tower-pred-0/decode_bbox_target/mul_4"}
  %subtract.73.clone.1 = f32[182400,1,2]{2,1,0} subtract(f32[182400,1,2]{2,1,0} %add.163.clone.1, f32[182400,1,2]{2,1,0} %multiply.109.clone.1), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target/sub_1"}
  %add.162.clone.1 = f32[182400,1,2]{2,1,0} add(f32[182400,1,2]{2,1,0} %add.163.clone.1, f32[182400,1,2]{2,1,0} %multiply.109.clone.1), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target/add_2"}
  %concatenate.0.clone.1 = f32[182400,2,2]{2,1,0} concatenate(f32[182400,1,2]{2,1,0} %subtract.73.clone.1, f32[182400,1,2]{2,1,0} %add.162.clone.1), dimensions={1}, metadata={op_type="ConcatV2" op_name="tower-pred-0/decode_bbox_target/concat"}
  %bitcast.35.clone.1 = f32[182400,4]{1,0} bitcast(f32[182400,2,2]{2,1,0} %concatenate.0.clone.1), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/Reshape"}
  %param_17.12 = s32[182400]{0} parameter(17)
  %slice.0.clone.1 = s32[1000]{0} slice(s32[182400]{0} %param_17.12), slice={[0:1000]}, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %gather.0.clone.1 = f32[1000,4]{1,0} gather(f32[182400,4]{1,0} %bitcast.35.clone.1, s32[1000]{0} %slice.0.clone.1), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,4}, metadata={op_type="GatherV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/GatherV2"}
  %maximum.0.clone.1 = f32[1000,4]{1,0} maximum(f32[1000,4]{1,0} %gather.0.clone.1, f32[1000,4]{1,0} %broadcast.732), metadata={op_type="Maximum" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/clip_boxes/Maximum"}
  %minimum.0.clone.1 = f32[1000,4]{1,0} minimum(f32[1000,4]{1,0} %maximum.0.clone.1, f32[1000,4]{1,0} %broadcast.733), metadata={op_type="Minimum" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/clip_boxes/Minimum"}
  ROOT %tuple.78 = (f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}) tuple(f32[1000,4]{1,0} %minimum.8, f32[1000,4]{1,0} %minimum.4.clone.1, f32[1000,4]{1,0} %minimum.2.clone.1, f32[1000,4]{1,0} %minimum.0.clone.1)
}

%fused_computation.27 (param_0.79: f32[1,3,25,38], param_1.82: f32[3]) -> f32[2850] {
  %param_0.79 = f32[1,3,25,38]{3,2,1,0} parameter(0)
  %param_1.82 = f32[3]{0} parameter(1)
  %broadcast.482 = f32[1,3,25,38]{3,2,1,0} broadcast(f32[3]{0} %param_1.82), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_3/class/BiasAdd"}
  %add.186 = f32[1,3,25,38]{3,2,1,0} add(f32[1,3,25,38]{3,2,1,0} %param_0.79, f32[1,3,25,38]{3,2,1,0} %broadcast.482), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_3/class/BiasAdd"}
  %bitcast.57 = f32[1,25,38,3]{2,1,3,0} bitcast(f32[1,3,25,38]{3,2,1,0} %add.186), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_3/transpose"}
  ROOT %reshape.30 = f32[2850]{0} reshape(f32[1,25,38,3]{2,1,3,0} %bitcast.57), metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/Reshape_1"}
}

%fused_computation.28 (param_0.314: f32[1,12,25,38], param_1.407: f32[12]) -> f32[2850,1,2] {
  %param_0.314 = f32[1,12,25,38]{3,2,1,0} parameter(0)
  %param_1.407 = f32[12]{0} parameter(1)
  %broadcast.850 = f32[1,12,25,38]{3,2,1,0} broadcast(f32[12]{0} %param_1.407), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_3/box/BiasAdd"}
  %add.319 = f32[1,12,25,38]{3,2,1,0} add(f32[1,12,25,38]{3,2,1,0} %param_0.314, f32[1,12,25,38]{3,2,1,0} %broadcast.850), metadata={op_type="BiasAdd" op_name="tower-pred-0/rpn_3/box/BiasAdd"}
  %bitcast.71 = f32[1,25,38,12]{2,1,3,0} bitcast(f32[1,12,25,38]{3,2,1,0} %add.319), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_3/transpose_1"}
  %copy.173 = f32[1,25,38,12]{3,2,1,0} copy(f32[1,25,38,12]{2,1,3,0} %bitcast.71), metadata={op_type="Transpose" op_name="tower-pred-0/rpn_3/transpose_1"}
  %bitcast.70 = f32[2850,2,2]{2,1,0} bitcast(f32[1,25,38,12]{3,2,1,0} %copy.173), metadata={op_type="Reshape" op_name="tower-pred-0/decode_bbox_target_3/Reshape"}
  %slice.13 = f32[2850,1,2]{2,1,0} slice(f32[2850,2,2]{2,1,0} %bitcast.70), slice={[0:2850], [1:2], [0:2]}, metadata={op_type="Split" op_name="tower-pred-0/decode_bbox_target_3/split"}
  %constant_200 = f32[] constant(4.43082), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_4/Minimum"}
  %broadcast.483 = f32[2850,1,2]{2,1,0} broadcast(f32[] %constant_200), dimensions={}, metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_3/Minimum"}
  %minimum.9 = f32[2850,1,2]{2,1,0} minimum(f32[2850,1,2]{2,1,0} %slice.13, f32[2850,1,2]{2,1,0} %broadcast.483), metadata={op_type="Minimum" op_name="tower-pred-0/decode_bbox_target_3/Minimum"}
  ROOT %exponential.4 = f32[2850,1,2]{2,1,0} exponential(f32[2850,1,2]{2,1,0} %minimum.9), metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target_3/Exp"}
}

%fused_computation.30 (param_0.87: f32[1,256,50,76], param_1.88: f32[256]) -> f32[1,256,50,76] {
  %param_0.87 = f32[1,256,50,76]{3,2,1,0} parameter(0)
  %param_1.88 = f32[256]{0} parameter(1)
  %broadcast.485 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.88), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p4/BiasAdd"}
  ROOT %add.188 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %param_0.87, f32[1,256,50,76]{3,2,1,0} %broadcast.485), metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p4/BiasAdd"}
}

%fused_computation.31 (param_0.89: f32[1,256,25,38], param_1.94: f32[1,256,50,76], param_2.57: f32[256]) -> f32[1,256,50,76] {
  %param_1.94 = f32[1,256,50,76]{3,2,1,0} parameter(1)
  %param_2.57 = f32[256]{0} parameter(2)
  %broadcast.487 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.57), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/lateral_1x1_c4/BiasAdd"}
  %add.190 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %param_1.94, f32[1,256,50,76]{3,2,1,0} %broadcast.487), metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/lateral_1x1_c4/BiasAdd"}
  %param_0.89 = f32[1,256,25,38]{3,2,1,0} parameter(0)
  %bitcast.61 = f32[243200]{0} bitcast(f32[1,256,25,38]{3,2,1,0} %param_0.89)
  %broadcast.486 = f32[243200,4]{1,0} broadcast(f32[243200]{0} %bitcast.61), dimensions={0}, metadata={op_type="MatMul" op_name="tower-pred-0/fpn/upsample_lat5/Tensordot/MatMul"}
  %reshape.31 = f32[1,256,25,38,2,2]{5,3,4,2,1,0} reshape(f32[243200,4]{1,0} %broadcast.486), metadata={op_type="Reshape" op_name="tower-pred-0/fpn/upsample_lat5/Tensordot"}
  %bitcast.60 = f32[1,256,50,76]{3,2,1,0} bitcast(f32[1,256,25,38,2,2]{5,3,4,2,1,0} %reshape.31), metadata={op_type="Reshape" op_name="tower-pred-0/fpn/upsample_lat5/Reshape"}
  ROOT %add.189 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %add.190, f32[1,256,50,76]{3,2,1,0} %bitcast.60), metadata={op_type="Add" op_name="tower-pred-0/fpn/add"}
}

%fused_computation.32 (param_0.90: f32[1,256,25,38], param_1.96: f32[256]) -> f32[1,256,25,38] {
  %param_0.90 = f32[1,256,25,38]{3,2,1,0} parameter(0)
  %param_1.96 = f32[256]{0} parameter(1)
  %broadcast.488 = f32[1,256,25,38]{3,2,1,0} broadcast(f32[256]{0} %param_1.96), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p5/BiasAdd"}
  ROOT %add.191 = f32[1,256,25,38]{3,2,1,0} add(f32[1,256,25,38]{3,2,1,0} %param_0.90, f32[1,256,25,38]{3,2,1,0} %broadcast.488), metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p5/BiasAdd"}
}

%fused_computation.33 (param_0.91: f32[1,256,25,38], param_1.98: f32[256]) -> f32[1,256,25,38] {
  %param_0.91 = f32[1,256,25,38]{3,2,1,0} parameter(0)
  %param_1.98 = f32[256]{0} parameter(1)
  %broadcast.489 = f32[1,256,25,38]{3,2,1,0} broadcast(f32[256]{0} %param_1.98), dimensions={1}, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/lateral_1x1_c5/BiasAdd"}
  ROOT %add.192 = f32[1,256,25,38]{3,2,1,0} add(f32[1,256,25,38]{3,2,1,0} %param_0.91, f32[1,256,25,38]{3,2,1,0} %broadcast.489), metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/lateral_1x1_c5/BiasAdd"}
}

%fused_computation.34 (param_0.264: f32[1,2048,25,38], param_1.306: f32[2048], param_2.260: f32[2048], param_3.306: f32[2048], param_4.263: f32[1,2048,25,38], param_5.204: f32[2048]) -> f32[1,2048,25,38] {
  %constant_211 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.734 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[] %constant_211), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group3/block2/output"}
  %param_0.264 = f32[1,2048,25,38]{3,2,1,0} parameter(0)
  %param_4.263 = f32[1,2048,25,38]{3,2,1,0} parameter(4)
  %param_5.204 = f32[2048]{0} parameter(5)
  %broadcast.493 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_5.204), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
  %subtract.78 = f32[1,2048,25,38]{3,2,1,0} subtract(f32[1,2048,25,38]{3,2,1,0} %param_4.263, f32[1,2048,25,38]{3,2,1,0} %broadcast.493), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
  %param_3.306 = f32[2048]{0} parameter(3)
  %broadcast.492 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_3.306), dimensions={1}
  %multiply.130 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %subtract.78, f32[1,2048,25,38]{3,2,1,0} %broadcast.492), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
  %param_2.260 = f32[2048]{0} parameter(2)
  %broadcast.491 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_2.260), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
  %multiply.129 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %multiply.130, f32[1,2048,25,38]{3,2,1,0} %broadcast.491), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
  %param_1.306 = f32[2048]{0} parameter(1)
  %broadcast.490 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_1.306), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
  %add.194 = f32[1,2048,25,38]{3,2,1,0} add(f32[1,2048,25,38]{3,2,1,0} %multiply.129, f32[1,2048,25,38]{3,2,1,0} %broadcast.490), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
  %add.193 = f32[1,2048,25,38]{3,2,1,0} add(f32[1,2048,25,38]{3,2,1,0} %param_0.264, f32[1,2048,25,38]{3,2,1,0} %add.194), metadata={op_type="Add" op_name="tower-pred-0/group3/block2/add"}
  ROOT %maximum.5 = f32[1,2048,25,38]{3,2,1,0} maximum(f32[1,2048,25,38]{3,2,1,0} %broadcast.734, f32[1,2048,25,38]{3,2,1,0} %add.193), metadata={op_type="Relu" op_name="tower-pred-0/group3/block2/output"}
}

%fused_computation.35 (param_0.94: f32[2048]) -> f32[2048] {
  %param_0.94 = f32[2048]{0} parameter(0)
  %constant_216 = f32[] constant(1.001e-05)
  %broadcast.739 = f32[2048]{0} broadcast(f32[] %constant_216), dimensions={}
  %add.195 = f32[2048]{0} add(f32[2048]{0} %param_0.94, f32[2048]{0} %broadcast.739), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.106 = f32[2048]{0} rsqrt(f32[2048]{0} %add.195), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
}

%fused_computation.36 (param_0.265: f32[512], param_1.307: f32[512], param_2.261: f32[512], param_3.307: f32[1,512,25,38], param_4.264: f32[512]) -> f32[1,512,25,38] {
  %constant_212 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.735 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_212), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/conv2/Relu"}
  %param_3.307 = f32[1,512,25,38]{3,2,1,0} parameter(3)
  %param_4.264 = f32[512]{0} parameter(4)
  %broadcast.497 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_4.264), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
  %subtract.79 = f32[1,512,25,38]{3,2,1,0} subtract(f32[1,512,25,38]{3,2,1,0} %param_3.307, f32[1,512,25,38]{3,2,1,0} %broadcast.497), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
  %param_2.261 = f32[512]{0} parameter(2)
  %broadcast.496 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_2.261), dimensions={1}
  %multiply.132 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %subtract.79, f32[1,512,25,38]{3,2,1,0} %broadcast.496), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
  %param_1.307 = f32[512]{0} parameter(1)
  %broadcast.495 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_1.307), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
  %multiply.131 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %multiply.132, f32[1,512,25,38]{3,2,1,0} %broadcast.495), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
  %param_0.265 = f32[512]{0} parameter(0)
  %broadcast.494 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_0.265), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
  %add.196 = f32[1,512,25,38]{3,2,1,0} add(f32[1,512,25,38]{3,2,1,0} %multiply.131, f32[1,512,25,38]{3,2,1,0} %broadcast.494), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.6 = f32[1,512,25,38]{3,2,1,0} maximum(f32[1,512,25,38]{3,2,1,0} %broadcast.735, f32[1,512,25,38]{3,2,1,0} %add.196), metadata={op_type="Relu" op_name="tower-pred-0/group3/block2/conv2/Relu"}
}

%fused_computation.37 (param_0.97: f32[512]) -> f32[512] {
  %param_0.97 = f32[512]{0} parameter(0)
  %constant_215 = f32[] constant(1.001e-05)
  %broadcast.738 = f32[512]{0} broadcast(f32[] %constant_215), dimensions={}
  %add.197 = f32[512]{0} add(f32[512]{0} %param_0.97, f32[512]{0} %broadcast.738), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.107 = f32[512]{0} rsqrt(f32[512]{0} %add.197), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
}

%fused_computation.38 (param_0.266: f32[512], param_1.308: f32[512], param_2.262: f32[512], param_3.308: f32[1,512,25,38], param_4.265: f32[512]) -> f32[1,512,25,38] {
  %constant_213 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.736 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_213), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/conv2/Relu"}
  %param_3.308 = f32[1,512,25,38]{3,2,1,0} parameter(3)
  %param_4.265 = f32[512]{0} parameter(4)
  %broadcast.501 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_4.265), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
  %subtract.80 = f32[1,512,25,38]{3,2,1,0} subtract(f32[1,512,25,38]{3,2,1,0} %param_3.308, f32[1,512,25,38]{3,2,1,0} %broadcast.501), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
  %param_2.262 = f32[512]{0} parameter(2)
  %broadcast.500 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_2.262), dimensions={1}
  %multiply.134 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %subtract.80, f32[1,512,25,38]{3,2,1,0} %broadcast.500), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
  %param_1.308 = f32[512]{0} parameter(1)
  %broadcast.499 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_1.308), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
  %multiply.133 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %multiply.134, f32[1,512,25,38]{3,2,1,0} %broadcast.499), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
  %param_0.266 = f32[512]{0} parameter(0)
  %broadcast.498 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_0.266), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
  %add.198 = f32[1,512,25,38]{3,2,1,0} add(f32[1,512,25,38]{3,2,1,0} %multiply.133, f32[1,512,25,38]{3,2,1,0} %broadcast.498), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.7 = f32[1,512,25,38]{3,2,1,0} maximum(f32[1,512,25,38]{3,2,1,0} %broadcast.736, f32[1,512,25,38]{3,2,1,0} %add.198), metadata={op_type="Relu" op_name="tower-pred-0/group3/block2/conv1/Relu"}
}

%fused_computation.39 (param_0.100: f32[512]) -> f32[512] {
  %param_0.100 = f32[512]{0} parameter(0)
  %constant_214 = f32[] constant(1.001e-05)
  %broadcast.737 = f32[512]{0} broadcast(f32[] %constant_214), dimensions={}
  %add.199 = f32[512]{0} add(f32[512]{0} %param_0.100, f32[512]{0} %broadcast.737), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.108 = f32[512]{0} rsqrt(f32[512]{0} %add.199), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
}

%fused_computation.40 (param_0.267: f32[1,2048,25,38], param_1.312: f32[2048], param_2.263: f32[2048], param_3.309: f32[2048], param_4.266: f32[1,2048,25,38], param_5.207: f32[2048]) -> f32[1,2048,25,38] {
  %constant_217 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.740 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[] %constant_217), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group3/block2/output"}
  %param_0.267 = f32[1,2048,25,38]{3,2,1,0} parameter(0)
  %param_4.266 = f32[1,2048,25,38]{3,2,1,0} parameter(4)
  %param_5.207 = f32[2048]{0} parameter(5)
  %broadcast.505 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_5.207), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
  %subtract.81 = f32[1,2048,25,38]{3,2,1,0} subtract(f32[1,2048,25,38]{3,2,1,0} %param_4.266, f32[1,2048,25,38]{3,2,1,0} %broadcast.505), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
  %param_3.309 = f32[2048]{0} parameter(3)
  %broadcast.504 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_3.309), dimensions={1}
  %multiply.136 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %subtract.81, f32[1,2048,25,38]{3,2,1,0} %broadcast.504), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
  %param_2.263 = f32[2048]{0} parameter(2)
  %broadcast.503 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_2.263), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
  %multiply.135 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %multiply.136, f32[1,2048,25,38]{3,2,1,0} %broadcast.503), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
  %param_1.312 = f32[2048]{0} parameter(1)
  %broadcast.502 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_1.312), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
  %add.201 = f32[1,2048,25,38]{3,2,1,0} add(f32[1,2048,25,38]{3,2,1,0} %multiply.135, f32[1,2048,25,38]{3,2,1,0} %broadcast.502), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
  %add.200 = f32[1,2048,25,38]{3,2,1,0} add(f32[1,2048,25,38]{3,2,1,0} %param_0.267, f32[1,2048,25,38]{3,2,1,0} %add.201), metadata={op_type="Add" op_name="tower-pred-0/group3/block1/add"}
  ROOT %maximum.8 = f32[1,2048,25,38]{3,2,1,0} maximum(f32[1,2048,25,38]{3,2,1,0} %broadcast.740, f32[1,2048,25,38]{3,2,1,0} %add.200), metadata={op_type="Relu" op_name="tower-pred-0/group3/block1/output"}
}

%fused_computation.41 (param_0.103: f32[2048]) -> f32[2048] {
  %param_0.103 = f32[2048]{0} parameter(0)
  %constant_222 = f32[] constant(1.001e-05)
  %broadcast.746 = f32[2048]{0} broadcast(f32[] %constant_222), dimensions={}
  %add.202 = f32[2048]{0} add(f32[2048]{0} %param_0.103, f32[2048]{0} %broadcast.746), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.109 = f32[2048]{0} rsqrt(f32[2048]{0} %add.202), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
}

%fused_computation.42 (param_0.268: f32[512], param_1.313: f32[512], param_2.264: f32[512], param_3.310: f32[1,512,25,38], param_4.267: f32[512]) -> f32[1,512,25,38] {
  %constant_218 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.741 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_218), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/conv2/Relu"}
  %param_3.310 = f32[1,512,25,38]{3,2,1,0} parameter(3)
  %param_4.267 = f32[512]{0} parameter(4)
  %broadcast.509 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_4.267), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
  %subtract.82 = f32[1,512,25,38]{3,2,1,0} subtract(f32[1,512,25,38]{3,2,1,0} %param_3.310, f32[1,512,25,38]{3,2,1,0} %broadcast.509), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
  %param_2.264 = f32[512]{0} parameter(2)
  %broadcast.508 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_2.264), dimensions={1}
  %multiply.138 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %subtract.82, f32[1,512,25,38]{3,2,1,0} %broadcast.508), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
  %param_1.313 = f32[512]{0} parameter(1)
  %broadcast.507 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_1.313), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
  %multiply.137 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %multiply.138, f32[1,512,25,38]{3,2,1,0} %broadcast.507), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
  %param_0.268 = f32[512]{0} parameter(0)
  %broadcast.506 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_0.268), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
  %add.203 = f32[1,512,25,38]{3,2,1,0} add(f32[1,512,25,38]{3,2,1,0} %multiply.137, f32[1,512,25,38]{3,2,1,0} %broadcast.506), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.9 = f32[1,512,25,38]{3,2,1,0} maximum(f32[1,512,25,38]{3,2,1,0} %broadcast.741, f32[1,512,25,38]{3,2,1,0} %add.203), metadata={op_type="Relu" op_name="tower-pred-0/group3/block1/conv2/Relu"}
}

%fused_computation.43 (param_0.106: f32[512]) -> f32[512] {
  %param_0.106 = f32[512]{0} parameter(0)
  %constant_221 = f32[] constant(1.001e-05)
  %broadcast.745 = f32[512]{0} broadcast(f32[] %constant_221), dimensions={}
  %add.204 = f32[512]{0} add(f32[512]{0} %param_0.106, f32[512]{0} %broadcast.745), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.110 = f32[512]{0} rsqrt(f32[512]{0} %add.204), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
}

%fused_computation.44 (param_0.269: f32[512], param_1.314: f32[512], param_2.265: f32[512], param_3.311: f32[1,512,25,38], param_4.268: f32[512]) -> f32[1,512,25,38] {
  %constant_219 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.743 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_219), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/conv2/Relu"}
  %param_3.311 = f32[1,512,25,38]{3,2,1,0} parameter(3)
  %param_4.268 = f32[512]{0} parameter(4)
  %broadcast.513 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_4.268), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
  %subtract.83 = f32[1,512,25,38]{3,2,1,0} subtract(f32[1,512,25,38]{3,2,1,0} %param_3.311, f32[1,512,25,38]{3,2,1,0} %broadcast.513), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
  %param_2.265 = f32[512]{0} parameter(2)
  %broadcast.512 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_2.265), dimensions={1}
  %multiply.140 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %subtract.83, f32[1,512,25,38]{3,2,1,0} %broadcast.512), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
  %param_1.314 = f32[512]{0} parameter(1)
  %broadcast.511 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_1.314), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
  %multiply.139 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %multiply.140, f32[1,512,25,38]{3,2,1,0} %broadcast.511), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
  %param_0.269 = f32[512]{0} parameter(0)
  %broadcast.510 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_0.269), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
  %add.205 = f32[1,512,25,38]{3,2,1,0} add(f32[1,512,25,38]{3,2,1,0} %multiply.139, f32[1,512,25,38]{3,2,1,0} %broadcast.510), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.10 = f32[1,512,25,38]{3,2,1,0} maximum(f32[1,512,25,38]{3,2,1,0} %broadcast.743, f32[1,512,25,38]{3,2,1,0} %add.205), metadata={op_type="Relu" op_name="tower-pred-0/group3/block1/conv1/Relu"}
}

%fused_computation.45 (param_0.109: f32[512]) -> f32[512] {
  %param_0.109 = f32[512]{0} parameter(0)
  %constant_220 = f32[] constant(1.001e-05)
  %broadcast.744 = f32[512]{0} broadcast(f32[] %constant_220), dimensions={}
  %add.206 = f32[512]{0} add(f32[512]{0} %param_0.109, f32[512]{0} %broadcast.744), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.111 = f32[512]{0} rsqrt(f32[512]{0} %add.206), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
}

%fused_computation.46 (param_0.270: f32[2048], param_1.318: f32[2048], param_2.266: f32[2048], param_3.312: f32[1,2048,25,38], param_4.269: f32[2048], param_5.210: f32[2048], param_6.108: f32[2048], param_7.61: f32[2048], param_8.16: f32[1,2048,25,38], param_9.12: f32[2048]) -> f32[1,2048,25,38] {
  %constant_223 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.747 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[] %constant_223), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group3/block2/output"}
  %param_8.16 = f32[1,2048,25,38]{3,2,1,0} parameter(8)
  %param_9.12 = f32[2048]{0} parameter(9)
  %broadcast.521 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_9.12), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
  %subtract.85 = f32[1,2048,25,38]{3,2,1,0} subtract(f32[1,2048,25,38]{3,2,1,0} %param_8.16, f32[1,2048,25,38]{3,2,1,0} %broadcast.521), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
  %param_7.61 = f32[2048]{0} parameter(7)
  %broadcast.520 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_7.61), dimensions={1}
  %multiply.144 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %subtract.85, f32[1,2048,25,38]{3,2,1,0} %broadcast.520), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
  %param_6.108 = f32[2048]{0} parameter(6)
  %broadcast.519 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_6.108), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
  %multiply.143 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %multiply.144, f32[1,2048,25,38]{3,2,1,0} %broadcast.519), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
  %param_5.210 = f32[2048]{0} parameter(5)
  %broadcast.518 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_5.210), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
  %add.209 = f32[1,2048,25,38]{3,2,1,0} add(f32[1,2048,25,38]{3,2,1,0} %multiply.143, f32[1,2048,25,38]{3,2,1,0} %broadcast.518), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
  %param_3.312 = f32[1,2048,25,38]{3,2,1,0} parameter(3)
  %param_4.269 = f32[2048]{0} parameter(4)
  %broadcast.517 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_4.269), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
  %subtract.84 = f32[1,2048,25,38]{3,2,1,0} subtract(f32[1,2048,25,38]{3,2,1,0} %param_3.312, f32[1,2048,25,38]{3,2,1,0} %broadcast.517), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
  %param_2.266 = f32[2048]{0} parameter(2)
  %broadcast.516 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_2.266), dimensions={1}
  %multiply.142 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %subtract.84, f32[1,2048,25,38]{3,2,1,0} %broadcast.516), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
  %param_1.318 = f32[2048]{0} parameter(1)
  %broadcast.515 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_1.318), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
  %multiply.141 = f32[1,2048,25,38]{3,2,1,0} multiply(f32[1,2048,25,38]{3,2,1,0} %multiply.142, f32[1,2048,25,38]{3,2,1,0} %broadcast.515), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
  %param_0.270 = f32[2048]{0} parameter(0)
  %broadcast.514 = f32[1,2048,25,38]{3,2,1,0} broadcast(f32[2048]{0} %param_0.270), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
  %add.208 = f32[1,2048,25,38]{3,2,1,0} add(f32[1,2048,25,38]{3,2,1,0} %multiply.141, f32[1,2048,25,38]{3,2,1,0} %broadcast.514), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
  %add.207 = f32[1,2048,25,38]{3,2,1,0} add(f32[1,2048,25,38]{3,2,1,0} %add.209, f32[1,2048,25,38]{3,2,1,0} %add.208), metadata={op_type="Add" op_name="tower-pred-0/group3/block0/add"}
  ROOT %maximum.11 = f32[1,2048,25,38]{3,2,1,0} maximum(f32[1,2048,25,38]{3,2,1,0} %broadcast.747, f32[1,2048,25,38]{3,2,1,0} %add.207), metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/output"}
}

%fused_computation.47 (param_0.112: f32[2048]) -> f32[2048] {
  %param_0.112 = f32[2048]{0} parameter(0)
  %constant_308 = f32[] constant(1.001e-05)
  %broadcast.845 = f32[2048]{0} broadcast(f32[] %constant_308), dimensions={}
  %add.210 = f32[2048]{0} add(f32[2048]{0} %param_0.112, f32[2048]{0} %broadcast.845), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
  ROOT %rsqrt.112 = f32[2048]{0} rsqrt(f32[2048]{0} %add.210), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
}

%fused_computation.48 (param_0.114: f32[2048]) -> f32[2048] {
  %param_0.114 = f32[2048]{0} parameter(0)
  %constant_227 = f32[] constant(1.001e-05)
  %broadcast.752 = f32[2048]{0} broadcast(f32[] %constant_227), dimensions={}
  %add.211 = f32[2048]{0} add(f32[2048]{0} %param_0.114, f32[2048]{0} %broadcast.752), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.113 = f32[2048]{0} rsqrt(f32[2048]{0} %add.211), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
}

%fused_computation.49 (param_0.271: f32[512], param_1.319: f32[512], param_2.267: f32[512], param_3.313: f32[1,512,25,38], param_4.270: f32[512]) -> f32[1,512,25,38] {
  %constant_224 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.748 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[] %constant_224), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/conv2/Relu"}
  %param_3.313 = f32[1,512,25,38]{3,2,1,0} parameter(3)
  %param_4.270 = f32[512]{0} parameter(4)
  %broadcast.525 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_4.270), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
  %subtract.86 = f32[1,512,25,38]{3,2,1,0} subtract(f32[1,512,25,38]{3,2,1,0} %param_3.313, f32[1,512,25,38]{3,2,1,0} %broadcast.525), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
  %param_2.267 = f32[512]{0} parameter(2)
  %broadcast.524 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_2.267), dimensions={1}
  %multiply.146 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %subtract.86, f32[1,512,25,38]{3,2,1,0} %broadcast.524), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
  %param_1.319 = f32[512]{0} parameter(1)
  %broadcast.523 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_1.319), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
  %multiply.145 = f32[1,512,25,38]{3,2,1,0} multiply(f32[1,512,25,38]{3,2,1,0} %multiply.146, f32[1,512,25,38]{3,2,1,0} %broadcast.523), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
  %param_0.271 = f32[512]{0} parameter(0)
  %broadcast.522 = f32[1,512,25,38]{3,2,1,0} broadcast(f32[512]{0} %param_0.271), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
  %add.212 = f32[1,512,25,38]{3,2,1,0} add(f32[1,512,25,38]{3,2,1,0} %multiply.145, f32[1,512,25,38]{3,2,1,0} %broadcast.522), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.12 = f32[1,512,25,38]{3,2,1,0} maximum(f32[1,512,25,38]{3,2,1,0} %broadcast.748, f32[1,512,25,38]{3,2,1,0} %add.212), metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/conv2/Relu"}
}

%fused_computation.50 (param_0.117: f32[512]) -> f32[512] {
  %param_0.117 = f32[512]{0} parameter(0)
  %constant_226 = f32[] constant(1.001e-05)
  %broadcast.751 = f32[512]{0} broadcast(f32[] %constant_226), dimensions={}
  %add.213 = f32[512]{0} add(f32[512]{0} %param_0.117, f32[512]{0} %broadcast.751), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.114 = f32[512]{0} rsqrt(f32[512]{0} %add.213), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
}

%fused_computation.51 (param_0.120: f32[512], param_1.133: f32[512], param_2.92: f32[512], param_3.123: f32[1,512,50,76], param_4.109: f32[512]) -> f32[1,512,51,77] {
  %constant_201 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.530 = f32[1,512,50,76]{3,2,1,0} broadcast(f32[] %constant_201), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/conv1/Relu"}
  %param_3.123 = f32[1,512,50,76]{3,2,1,0} parameter(3)
  %param_4.109 = f32[512]{0} parameter(4)
  %broadcast.529 = f32[1,512,50,76]{3,2,1,0} broadcast(f32[512]{0} %param_4.109), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
  %subtract.87 = f32[1,512,50,76]{3,2,1,0} subtract(f32[1,512,50,76]{3,2,1,0} %param_3.123, f32[1,512,50,76]{3,2,1,0} %broadcast.529), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
  %param_2.92 = f32[512]{0} parameter(2)
  %broadcast.528 = f32[1,512,50,76]{3,2,1,0} broadcast(f32[512]{0} %param_2.92), dimensions={1}
  %multiply.148 = f32[1,512,50,76]{3,2,1,0} multiply(f32[1,512,50,76]{3,2,1,0} %subtract.87, f32[1,512,50,76]{3,2,1,0} %broadcast.528), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
  %param_1.133 = f32[512]{0} parameter(1)
  %broadcast.527 = f32[1,512,50,76]{3,2,1,0} broadcast(f32[512]{0} %param_1.133), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
  %multiply.147 = f32[1,512,50,76]{3,2,1,0} multiply(f32[1,512,50,76]{3,2,1,0} %multiply.148, f32[1,512,50,76]{3,2,1,0} %broadcast.527), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
  %param_0.120 = f32[512]{0} parameter(0)
  %broadcast.526 = f32[1,512,50,76]{3,2,1,0} broadcast(f32[512]{0} %param_0.120), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
  %add.214 = f32[1,512,50,76]{3,2,1,0} add(f32[1,512,50,76]{3,2,1,0} %multiply.147, f32[1,512,50,76]{3,2,1,0} %broadcast.526), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
  %maximum.13 = f32[1,512,50,76]{3,2,1,0} maximum(f32[1,512,50,76]{3,2,1,0} %broadcast.530, f32[1,512,50,76]{3,2,1,0} %add.214), metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/conv1/Relu"}
  ROOT %pad.12 = f32[1,512,51,77]{3,2,1,0} pad(f32[1,512,50,76]{3,2,1,0} %maximum.13, f32[] %constant_201), padding=0_0x0_0x1_0x1_0
}

%fused_computation.52 (param_0.122: f32[512]) -> f32[512] {
  %param_0.122 = f32[512]{0} parameter(0)
  %constant_225 = f32[] constant(1.001e-05)
  %broadcast.750 = f32[512]{0} broadcast(f32[] %constant_225), dimensions={}
  %add.215 = f32[512]{0} add(f32[512]{0} %param_0.122, f32[512]{0} %broadcast.750), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.115 = f32[512]{0} rsqrt(f32[512]{0} %add.215), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
}

%fused_computation.53 (param_0.272: f32[1,1024,50,76], param_1.323: f32[1024], param_2.268: f32[1024], param_3.314: f32[1024], param_4.271: f32[1,1024,50,76], param_5.212: f32[1024]) -> f32[1,1024,50,76] {
  %constant_228 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.753 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_228), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/output"}
  %param_0.272 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_4.271 = f32[1,1024,50,76]{3,2,1,0} parameter(4)
  %param_5.212 = f32[1024]{0} parameter(5)
  %broadcast.534 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_5.212), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
  %subtract.88 = f32[1,1024,50,76]{3,2,1,0} subtract(f32[1,1024,50,76]{3,2,1,0} %param_4.271, f32[1,1024,50,76]{3,2,1,0} %broadcast.534), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
  %param_3.314 = f32[1024]{0} parameter(3)
  %broadcast.533 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_3.314), dimensions={1}
  %multiply.150 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %subtract.88, f32[1,1024,50,76]{3,2,1,0} %broadcast.533), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
  %param_2.268 = f32[1024]{0} parameter(2)
  %broadcast.532 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_2.268), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
  %multiply.149 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %multiply.150, f32[1,1024,50,76]{3,2,1,0} %broadcast.532), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
  %param_1.323 = f32[1024]{0} parameter(1)
  %broadcast.531 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_1.323), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
  %add.217 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %multiply.149, f32[1,1024,50,76]{3,2,1,0} %broadcast.531), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
  %add.216 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_0.272, f32[1,1024,50,76]{3,2,1,0} %add.217), metadata={op_type="Add" op_name="tower-pred-0/group2/block5/add"}
  ROOT %maximum.14 = f32[1,1024,50,76]{3,2,1,0} maximum(f32[1,1024,50,76]{3,2,1,0} %broadcast.753, f32[1,1024,50,76]{3,2,1,0} %add.216), metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/output"}
}

%fused_computation.54 (param_0.125: f32[1024]) -> f32[1024] {
  %param_0.125 = f32[1024]{0} parameter(0)
  %constant_233 = f32[] constant(1.001e-05)
  %broadcast.759 = f32[1024]{0} broadcast(f32[] %constant_233), dimensions={}
  %add.218 = f32[1024]{0} add(f32[1024]{0} %param_0.125, f32[1024]{0} %broadcast.759), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.116 = f32[1024]{0} rsqrt(f32[1024]{0} %add.218), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
}

%fused_computation.55 (param_0.273: f32[256], param_1.324: f32[256], param_2.269: f32[256], param_3.315: f32[1,256,50,76], param_4.272: f32[256]) -> f32[1,256,50,76] {
  %constant_229 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.754 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_229), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.315 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.272 = f32[256]{0} parameter(4)
  %broadcast.538 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.272), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
  %subtract.89 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.315, f32[1,256,50,76]{3,2,1,0} %broadcast.538), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
  %param_2.269 = f32[256]{0} parameter(2)
  %broadcast.537 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.269), dimensions={1}
  %multiply.152 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.89, f32[1,256,50,76]{3,2,1,0} %broadcast.537), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
  %param_1.324 = f32[256]{0} parameter(1)
  %broadcast.536 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.324), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
  %multiply.151 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.152, f32[1,256,50,76]{3,2,1,0} %broadcast.536), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
  %param_0.273 = f32[256]{0} parameter(0)
  %broadcast.535 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.273), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
  %add.219 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.151, f32[1,256,50,76]{3,2,1,0} %broadcast.535), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.15 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.754, f32[1,256,50,76]{3,2,1,0} %add.219), metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/conv2/Relu"}
}

%fused_computation.56 (param_0.128: f32[256]) -> f32[256] {
  %param_0.128 = f32[256]{0} parameter(0)
  %constant_232 = f32[] constant(1.001e-05)
  %broadcast.758 = f32[256]{0} broadcast(f32[] %constant_232), dimensions={}
  %add.220 = f32[256]{0} add(f32[256]{0} %param_0.128, f32[256]{0} %broadcast.758), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.117 = f32[256]{0} rsqrt(f32[256]{0} %add.220), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
}

%fused_computation.57 (param_0.274: f32[256], param_1.325: f32[256], param_2.270: f32[256], param_3.316: f32[1,256,50,76], param_4.273: f32[256]) -> f32[1,256,50,76] {
  %constant_230 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.755 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_230), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.316 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.273 = f32[256]{0} parameter(4)
  %broadcast.542 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.273), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
  %subtract.90 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.316, f32[1,256,50,76]{3,2,1,0} %broadcast.542), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
  %param_2.270 = f32[256]{0} parameter(2)
  %broadcast.541 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.270), dimensions={1}
  %multiply.154 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.90, f32[1,256,50,76]{3,2,1,0} %broadcast.541), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
  %param_1.325 = f32[256]{0} parameter(1)
  %broadcast.540 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.325), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
  %multiply.153 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.154, f32[1,256,50,76]{3,2,1,0} %broadcast.540), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
  %param_0.274 = f32[256]{0} parameter(0)
  %broadcast.539 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.274), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
  %add.221 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.153, f32[1,256,50,76]{3,2,1,0} %broadcast.539), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.16 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.755, f32[1,256,50,76]{3,2,1,0} %add.221), metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/conv1/Relu"}
}

%fused_computation.58 (param_0.131: f32[256]) -> f32[256] {
  %param_0.131 = f32[256]{0} parameter(0)
  %constant_231 = f32[] constant(1.001e-05)
  %broadcast.757 = f32[256]{0} broadcast(f32[] %constant_231), dimensions={}
  %add.222 = f32[256]{0} add(f32[256]{0} %param_0.131, f32[256]{0} %broadcast.757), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.118 = f32[256]{0} rsqrt(f32[256]{0} %add.222), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
}

%fused_computation.59 (param_0.275: f32[1,1024,50,76], param_1.329: f32[1024], param_2.271: f32[1024], param_3.317: f32[1024], param_4.274: f32[1,1024,50,76], param_5.215: f32[1024]) -> f32[1,1024,50,76] {
  %constant_234 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.760 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_234), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/output"}
  %param_0.275 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_4.274 = f32[1,1024,50,76]{3,2,1,0} parameter(4)
  %param_5.215 = f32[1024]{0} parameter(5)
  %broadcast.546 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_5.215), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
  %subtract.91 = f32[1,1024,50,76]{3,2,1,0} subtract(f32[1,1024,50,76]{3,2,1,0} %param_4.274, f32[1,1024,50,76]{3,2,1,0} %broadcast.546), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
  %param_3.317 = f32[1024]{0} parameter(3)
  %broadcast.545 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_3.317), dimensions={1}
  %multiply.156 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %subtract.91, f32[1,1024,50,76]{3,2,1,0} %broadcast.545), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
  %param_2.271 = f32[1024]{0} parameter(2)
  %broadcast.544 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_2.271), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
  %multiply.155 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %multiply.156, f32[1,1024,50,76]{3,2,1,0} %broadcast.544), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
  %param_1.329 = f32[1024]{0} parameter(1)
  %broadcast.543 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_1.329), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
  %add.224 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %multiply.155, f32[1,1024,50,76]{3,2,1,0} %broadcast.543), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
  %add.223 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_0.275, f32[1,1024,50,76]{3,2,1,0} %add.224), metadata={op_type="Add" op_name="tower-pred-0/group2/block4/add"}
  ROOT %maximum.17 = f32[1,1024,50,76]{3,2,1,0} maximum(f32[1,1024,50,76]{3,2,1,0} %broadcast.760, f32[1,1024,50,76]{3,2,1,0} %add.223), metadata={op_type="Relu" op_name="tower-pred-0/group2/block4/output"}
}

%fused_computation.60 (param_0.134: f32[1024]) -> f32[1024] {
  %param_0.134 = f32[1024]{0} parameter(0)
  %constant_239 = f32[] constant(1.001e-05)
  %broadcast.766 = f32[1024]{0} broadcast(f32[] %constant_239), dimensions={}
  %add.225 = f32[1024]{0} add(f32[1024]{0} %param_0.134, f32[1024]{0} %broadcast.766), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.119 = f32[1024]{0} rsqrt(f32[1024]{0} %add.225), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
}

%fused_computation.61 (param_0.276: f32[256], param_1.330: f32[256], param_2.272: f32[256], param_3.318: f32[1,256,50,76], param_4.275: f32[256]) -> f32[1,256,50,76] {
  %constant_235 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.761 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_235), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.318 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.275 = f32[256]{0} parameter(4)
  %broadcast.550 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.275), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
  %subtract.92 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.318, f32[1,256,50,76]{3,2,1,0} %broadcast.550), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
  %param_2.272 = f32[256]{0} parameter(2)
  %broadcast.549 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.272), dimensions={1}
  %multiply.158 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.92, f32[1,256,50,76]{3,2,1,0} %broadcast.549), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
  %param_1.330 = f32[256]{0} parameter(1)
  %broadcast.548 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.330), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
  %multiply.157 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.158, f32[1,256,50,76]{3,2,1,0} %broadcast.548), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
  %param_0.276 = f32[256]{0} parameter(0)
  %broadcast.547 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.276), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
  %add.226 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.157, f32[1,256,50,76]{3,2,1,0} %broadcast.547), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.18 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.761, f32[1,256,50,76]{3,2,1,0} %add.226), metadata={op_type="Relu" op_name="tower-pred-0/group2/block4/conv2/Relu"}
}

%fused_computation.62 (param_0.137: f32[256]) -> f32[256] {
  %param_0.137 = f32[256]{0} parameter(0)
  %constant_238 = f32[] constant(1.001e-05)
  %broadcast.765 = f32[256]{0} broadcast(f32[] %constant_238), dimensions={}
  %add.227 = f32[256]{0} add(f32[256]{0} %param_0.137, f32[256]{0} %broadcast.765), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.120 = f32[256]{0} rsqrt(f32[256]{0} %add.227), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
}

%fused_computation.63 (param_0.277: f32[256], param_1.331: f32[256], param_2.273: f32[256], param_3.319: f32[1,256,50,76], param_4.276: f32[256]) -> f32[1,256,50,76] {
  %constant_236 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.762 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_236), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.319 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.276 = f32[256]{0} parameter(4)
  %broadcast.554 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.276), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
  %subtract.93 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.319, f32[1,256,50,76]{3,2,1,0} %broadcast.554), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
  %param_2.273 = f32[256]{0} parameter(2)
  %broadcast.553 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.273), dimensions={1}
  %multiply.160 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.93, f32[1,256,50,76]{3,2,1,0} %broadcast.553), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
  %param_1.331 = f32[256]{0} parameter(1)
  %broadcast.552 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.331), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
  %multiply.159 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.160, f32[1,256,50,76]{3,2,1,0} %broadcast.552), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
  %param_0.277 = f32[256]{0} parameter(0)
  %broadcast.551 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.277), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
  %add.228 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.159, f32[1,256,50,76]{3,2,1,0} %broadcast.551), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.19 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.762, f32[1,256,50,76]{3,2,1,0} %add.228), metadata={op_type="Relu" op_name="tower-pred-0/group2/block4/conv1/Relu"}
}

%fused_computation.64 (param_0.140: f32[256]) -> f32[256] {
  %param_0.140 = f32[256]{0} parameter(0)
  %constant_237 = f32[] constant(1.001e-05)
  %broadcast.763 = f32[256]{0} broadcast(f32[] %constant_237), dimensions={}
  %add.229 = f32[256]{0} add(f32[256]{0} %param_0.140, f32[256]{0} %broadcast.763), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.121 = f32[256]{0} rsqrt(f32[256]{0} %add.229), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
}

%fused_computation.65 (param_0.278: f32[1,1024,50,76], param_1.335: f32[1024], param_2.274: f32[1024], param_3.320: f32[1024], param_4.277: f32[1,1024,50,76], param_5.218: f32[1024]) -> f32[1,1024,50,76] {
  %constant_240 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.767 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_240), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/output"}
  %param_0.278 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_4.277 = f32[1,1024,50,76]{3,2,1,0} parameter(4)
  %param_5.218 = f32[1024]{0} parameter(5)
  %broadcast.558 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_5.218), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
  %subtract.94 = f32[1,1024,50,76]{3,2,1,0} subtract(f32[1,1024,50,76]{3,2,1,0} %param_4.277, f32[1,1024,50,76]{3,2,1,0} %broadcast.558), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
  %param_3.320 = f32[1024]{0} parameter(3)
  %broadcast.557 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_3.320), dimensions={1}
  %multiply.162 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %subtract.94, f32[1,1024,50,76]{3,2,1,0} %broadcast.557), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
  %param_2.274 = f32[1024]{0} parameter(2)
  %broadcast.556 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_2.274), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
  %multiply.161 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %multiply.162, f32[1,1024,50,76]{3,2,1,0} %broadcast.556), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
  %param_1.335 = f32[1024]{0} parameter(1)
  %broadcast.555 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_1.335), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
  %add.231 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %multiply.161, f32[1,1024,50,76]{3,2,1,0} %broadcast.555), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
  %add.230 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_0.278, f32[1,1024,50,76]{3,2,1,0} %add.231), metadata={op_type="Add" op_name="tower-pred-0/group2/block3/add"}
  ROOT %maximum.20 = f32[1,1024,50,76]{3,2,1,0} maximum(f32[1,1024,50,76]{3,2,1,0} %broadcast.767, f32[1,1024,50,76]{3,2,1,0} %add.230), metadata={op_type="Relu" op_name="tower-pred-0/group2/block3/output"}
}

%fused_computation.66 (param_0.143: f32[1024]) -> f32[1024] {
  %param_0.143 = f32[1024]{0} parameter(0)
  %constant_245 = f32[] constant(1.001e-05)
  %broadcast.773 = f32[1024]{0} broadcast(f32[] %constant_245), dimensions={}
  %add.232 = f32[1024]{0} add(f32[1024]{0} %param_0.143, f32[1024]{0} %broadcast.773), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.122 = f32[1024]{0} rsqrt(f32[1024]{0} %add.232), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
}

%fused_computation.67 (param_0.279: f32[256], param_1.336: f32[256], param_2.275: f32[256], param_3.321: f32[1,256,50,76], param_4.278: f32[256]) -> f32[1,256,50,76] {
  %constant_241 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.768 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_241), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.321 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.278 = f32[256]{0} parameter(4)
  %broadcast.562 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.278), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
  %subtract.95 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.321, f32[1,256,50,76]{3,2,1,0} %broadcast.562), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
  %param_2.275 = f32[256]{0} parameter(2)
  %broadcast.561 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.275), dimensions={1}
  %multiply.164 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.95, f32[1,256,50,76]{3,2,1,0} %broadcast.561), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
  %param_1.336 = f32[256]{0} parameter(1)
  %broadcast.560 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.336), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
  %multiply.163 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.164, f32[1,256,50,76]{3,2,1,0} %broadcast.560), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
  %param_0.279 = f32[256]{0} parameter(0)
  %broadcast.559 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.279), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
  %add.233 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.163, f32[1,256,50,76]{3,2,1,0} %broadcast.559), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.21 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.768, f32[1,256,50,76]{3,2,1,0} %add.233), metadata={op_type="Relu" op_name="tower-pred-0/group2/block3/conv2/Relu"}
}

%fused_computation.68 (param_0.146: f32[256]) -> f32[256] {
  %param_0.146 = f32[256]{0} parameter(0)
  %constant_244 = f32[] constant(1.001e-05)
  %broadcast.772 = f32[256]{0} broadcast(f32[] %constant_244), dimensions={}
  %add.234 = f32[256]{0} add(f32[256]{0} %param_0.146, f32[256]{0} %broadcast.772), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.123 = f32[256]{0} rsqrt(f32[256]{0} %add.234), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
}

%fused_computation.69 (param_0.280: f32[256], param_1.337: f32[256], param_2.276: f32[256], param_3.322: f32[1,256,50,76], param_4.279: f32[256]) -> f32[1,256,50,76] {
  %constant_242 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.769 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_242), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.322 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.279 = f32[256]{0} parameter(4)
  %broadcast.566 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.279), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
  %subtract.96 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.322, f32[1,256,50,76]{3,2,1,0} %broadcast.566), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
  %param_2.276 = f32[256]{0} parameter(2)
  %broadcast.565 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.276), dimensions={1}
  %multiply.166 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.96, f32[1,256,50,76]{3,2,1,0} %broadcast.565), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
  %param_1.337 = f32[256]{0} parameter(1)
  %broadcast.564 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.337), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
  %multiply.165 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.166, f32[1,256,50,76]{3,2,1,0} %broadcast.564), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
  %param_0.280 = f32[256]{0} parameter(0)
  %broadcast.563 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.280), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
  %add.235 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.165, f32[1,256,50,76]{3,2,1,0} %broadcast.563), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.22 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.769, f32[1,256,50,76]{3,2,1,0} %add.235), metadata={op_type="Relu" op_name="tower-pred-0/group2/block3/conv1/Relu"}
}

%fused_computation.70 (param_0.149: f32[256]) -> f32[256] {
  %param_0.149 = f32[256]{0} parameter(0)
  %constant_243 = f32[] constant(1.001e-05)
  %broadcast.770 = f32[256]{0} broadcast(f32[] %constant_243), dimensions={}
  %add.236 = f32[256]{0} add(f32[256]{0} %param_0.149, f32[256]{0} %broadcast.770), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.124 = f32[256]{0} rsqrt(f32[256]{0} %add.236), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
}

%fused_computation.71 (param_0.281: f32[1,1024,50,76], param_1.341: f32[1024], param_2.277: f32[1024], param_3.323: f32[1024], param_4.280: f32[1,1024,50,76], param_5.221: f32[1024]) -> f32[1,1024,50,76] {
  %constant_246 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.774 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_246), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/output"}
  %param_0.281 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_4.280 = f32[1,1024,50,76]{3,2,1,0} parameter(4)
  %param_5.221 = f32[1024]{0} parameter(5)
  %broadcast.570 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_5.221), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
  %subtract.97 = f32[1,1024,50,76]{3,2,1,0} subtract(f32[1,1024,50,76]{3,2,1,0} %param_4.280, f32[1,1024,50,76]{3,2,1,0} %broadcast.570), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
  %param_3.323 = f32[1024]{0} parameter(3)
  %broadcast.569 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_3.323), dimensions={1}
  %multiply.168 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %subtract.97, f32[1,1024,50,76]{3,2,1,0} %broadcast.569), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
  %param_2.277 = f32[1024]{0} parameter(2)
  %broadcast.568 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_2.277), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
  %multiply.167 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %multiply.168, f32[1,1024,50,76]{3,2,1,0} %broadcast.568), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
  %param_1.341 = f32[1024]{0} parameter(1)
  %broadcast.567 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_1.341), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
  %add.238 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %multiply.167, f32[1,1024,50,76]{3,2,1,0} %broadcast.567), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
  %add.237 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_0.281, f32[1,1024,50,76]{3,2,1,0} %add.238), metadata={op_type="Add" op_name="tower-pred-0/group2/block2/add"}
  ROOT %maximum.23 = f32[1,1024,50,76]{3,2,1,0} maximum(f32[1,1024,50,76]{3,2,1,0} %broadcast.774, f32[1,1024,50,76]{3,2,1,0} %add.237), metadata={op_type="Relu" op_name="tower-pred-0/group2/block2/output"}
}

%fused_computation.72 (param_0.152: f32[1024]) -> f32[1024] {
  %param_0.152 = f32[1024]{0} parameter(0)
  %constant_251 = f32[] constant(1.001e-05)
  %broadcast.780 = f32[1024]{0} broadcast(f32[] %constant_251), dimensions={}
  %add.239 = f32[1024]{0} add(f32[1024]{0} %param_0.152, f32[1024]{0} %broadcast.780), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.125 = f32[1024]{0} rsqrt(f32[1024]{0} %add.239), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
}

%fused_computation.73 (param_0.282: f32[256], param_1.342: f32[256], param_2.278: f32[256], param_3.324: f32[1,256,50,76], param_4.281: f32[256]) -> f32[1,256,50,76] {
  %constant_247 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.775 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_247), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.324 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.281 = f32[256]{0} parameter(4)
  %broadcast.574 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.281), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
  %subtract.98 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.324, f32[1,256,50,76]{3,2,1,0} %broadcast.574), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
  %param_2.278 = f32[256]{0} parameter(2)
  %broadcast.573 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.278), dimensions={1}
  %multiply.170 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.98, f32[1,256,50,76]{3,2,1,0} %broadcast.573), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
  %param_1.342 = f32[256]{0} parameter(1)
  %broadcast.572 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.342), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
  %multiply.169 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.170, f32[1,256,50,76]{3,2,1,0} %broadcast.572), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
  %param_0.282 = f32[256]{0} parameter(0)
  %broadcast.571 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.282), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
  %add.240 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.169, f32[1,256,50,76]{3,2,1,0} %broadcast.571), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.24 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.775, f32[1,256,50,76]{3,2,1,0} %add.240), metadata={op_type="Relu" op_name="tower-pred-0/group2/block2/conv2/Relu"}
}

%fused_computation.74 (param_0.155: f32[256]) -> f32[256] {
  %param_0.155 = f32[256]{0} parameter(0)
  %constant_250 = f32[] constant(1.001e-05)
  %broadcast.779 = f32[256]{0} broadcast(f32[] %constant_250), dimensions={}
  %add.241 = f32[256]{0} add(f32[256]{0} %param_0.155, f32[256]{0} %broadcast.779), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.126 = f32[256]{0} rsqrt(f32[256]{0} %add.241), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
}

%fused_computation.75 (param_0.283: f32[256], param_1.343: f32[256], param_2.279: f32[256], param_3.325: f32[1,256,50,76], param_4.282: f32[256]) -> f32[1,256,50,76] {
  %constant_248 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.776 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_248), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.325 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.282 = f32[256]{0} parameter(4)
  %broadcast.578 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.282), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
  %subtract.99 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.325, f32[1,256,50,76]{3,2,1,0} %broadcast.578), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
  %param_2.279 = f32[256]{0} parameter(2)
  %broadcast.577 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.279), dimensions={1}
  %multiply.172 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.99, f32[1,256,50,76]{3,2,1,0} %broadcast.577), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
  %param_1.343 = f32[256]{0} parameter(1)
  %broadcast.576 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.343), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
  %multiply.171 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.172, f32[1,256,50,76]{3,2,1,0} %broadcast.576), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
  %param_0.283 = f32[256]{0} parameter(0)
  %broadcast.575 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.283), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
  %add.242 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.171, f32[1,256,50,76]{3,2,1,0} %broadcast.575), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.25 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.776, f32[1,256,50,76]{3,2,1,0} %add.242), metadata={op_type="Relu" op_name="tower-pred-0/group2/block2/conv1/Relu"}
}

%fused_computation.76 (param_0.158: f32[256]) -> f32[256] {
  %param_0.158 = f32[256]{0} parameter(0)
  %constant_249 = f32[] constant(1.001e-05)
  %broadcast.777 = f32[256]{0} broadcast(f32[] %constant_249), dimensions={}
  %add.243 = f32[256]{0} add(f32[256]{0} %param_0.158, f32[256]{0} %broadcast.777), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.127 = f32[256]{0} rsqrt(f32[256]{0} %add.243), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
}

%fused_computation.77 (param_0.284: f32[1,1024,50,76], param_1.347: f32[1024], param_2.280: f32[1024], param_3.326: f32[1024], param_4.283: f32[1,1024,50,76], param_5.224: f32[1024]) -> f32[1,1024,50,76] {
  %constant_252 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.781 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_252), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/output"}
  %param_0.284 = f32[1,1024,50,76]{3,2,1,0} parameter(0)
  %param_4.283 = f32[1,1024,50,76]{3,2,1,0} parameter(4)
  %param_5.224 = f32[1024]{0} parameter(5)
  %broadcast.584 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_5.224), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
  %subtract.100 = f32[1,1024,50,76]{3,2,1,0} subtract(f32[1,1024,50,76]{3,2,1,0} %param_4.283, f32[1,1024,50,76]{3,2,1,0} %broadcast.584), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
  %param_3.326 = f32[1024]{0} parameter(3)
  %broadcast.583 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_3.326), dimensions={1}
  %multiply.174 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %subtract.100, f32[1,1024,50,76]{3,2,1,0} %broadcast.583), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
  %param_2.280 = f32[1024]{0} parameter(2)
  %broadcast.581 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_2.280), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
  %multiply.173 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %multiply.174, f32[1,1024,50,76]{3,2,1,0} %broadcast.581), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
  %param_1.347 = f32[1024]{0} parameter(1)
  %broadcast.580 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_1.347), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
  %add.245 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %multiply.173, f32[1,1024,50,76]{3,2,1,0} %broadcast.580), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
  %add.244 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %param_0.284, f32[1,1024,50,76]{3,2,1,0} %add.245), metadata={op_type="Add" op_name="tower-pred-0/group2/block1/add"}
  ROOT %maximum.26 = f32[1,1024,50,76]{3,2,1,0} maximum(f32[1,1024,50,76]{3,2,1,0} %broadcast.781, f32[1,1024,50,76]{3,2,1,0} %add.244), metadata={op_type="Relu" op_name="tower-pred-0/group2/block1/output"}
}

%fused_computation.78 (param_0.161: f32[1024]) -> f32[1024] {
  %param_0.161 = f32[1024]{0} parameter(0)
  %constant_257 = f32[] constant(1.001e-05)
  %broadcast.787 = f32[1024]{0} broadcast(f32[] %constant_257), dimensions={}
  %add.246 = f32[1024]{0} add(f32[1024]{0} %param_0.161, f32[1024]{0} %broadcast.787), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.128 = f32[1024]{0} rsqrt(f32[1024]{0} %add.246), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
}

%fused_computation.79 (param_0.285: f32[256], param_1.348: f32[256], param_2.281: f32[256], param_3.327: f32[1,256,50,76], param_4.284: f32[256]) -> f32[1,256,50,76] {
  %constant_253 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.782 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_253), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.327 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.284 = f32[256]{0} parameter(4)
  %broadcast.588 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.284), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
  %subtract.101 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.327, f32[1,256,50,76]{3,2,1,0} %broadcast.588), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
  %param_2.281 = f32[256]{0} parameter(2)
  %broadcast.587 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.281), dimensions={1}
  %multiply.176 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.101, f32[1,256,50,76]{3,2,1,0} %broadcast.587), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
  %param_1.348 = f32[256]{0} parameter(1)
  %broadcast.586 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.348), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
  %multiply.175 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.176, f32[1,256,50,76]{3,2,1,0} %broadcast.586), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
  %param_0.285 = f32[256]{0} parameter(0)
  %broadcast.585 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.285), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
  %add.247 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.175, f32[1,256,50,76]{3,2,1,0} %broadcast.585), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.27 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.782, f32[1,256,50,76]{3,2,1,0} %add.247), metadata={op_type="Relu" op_name="tower-pred-0/group2/block1/conv2/Relu"}
}

%fused_computation.80 (param_0.164: f32[256]) -> f32[256] {
  %param_0.164 = f32[256]{0} parameter(0)
  %constant_256 = f32[] constant(1.001e-05)
  %broadcast.785 = f32[256]{0} broadcast(f32[] %constant_256), dimensions={}
  %add.248 = f32[256]{0} add(f32[256]{0} %param_0.164, f32[256]{0} %broadcast.785), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.129 = f32[256]{0} rsqrt(f32[256]{0} %add.248), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
}

%fused_computation.81 (param_0.286: f32[256], param_1.349: f32[256], param_2.282: f32[256], param_3.328: f32[1,256,50,76], param_4.285: f32[256]) -> f32[1,256,50,76] {
  %constant_254 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.783 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_254), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.328 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.285 = f32[256]{0} parameter(4)
  %broadcast.593 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.285), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
  %subtract.102 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.328, f32[1,256,50,76]{3,2,1,0} %broadcast.593), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
  %param_2.282 = f32[256]{0} parameter(2)
  %broadcast.592 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.282), dimensions={1}
  %multiply.178 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.102, f32[1,256,50,76]{3,2,1,0} %broadcast.592), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
  %param_1.349 = f32[256]{0} parameter(1)
  %broadcast.591 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.349), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
  %multiply.177 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.178, f32[1,256,50,76]{3,2,1,0} %broadcast.591), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
  %param_0.286 = f32[256]{0} parameter(0)
  %broadcast.589 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.286), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
  %add.249 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.177, f32[1,256,50,76]{3,2,1,0} %broadcast.589), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.28 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.783, f32[1,256,50,76]{3,2,1,0} %add.249), metadata={op_type="Relu" op_name="tower-pred-0/group2/block1/conv1/Relu"}
}

%fused_computation.82 (param_0.167: f32[256]) -> f32[256] {
  %param_0.167 = f32[256]{0} parameter(0)
  %constant_255 = f32[] constant(1.001e-05)
  %broadcast.784 = f32[256]{0} broadcast(f32[] %constant_255), dimensions={}
  %add.250 = f32[256]{0} add(f32[256]{0} %param_0.167, f32[256]{0} %broadcast.784), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.130 = f32[256]{0} rsqrt(f32[256]{0} %add.250), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
}

%fused_computation.83 (param_0.287: f32[1024], param_1.353: f32[1024], param_2.283: f32[1024], param_3.329: f32[1,1024,50,76], param_4.286: f32[1024], param_5.227: f32[1024], param_6.114: f32[1024], param_7.62: f32[1024], param_8.17: f32[1,1024,50,76], param_9.13: f32[1024]) -> f32[1,1024,50,76] {
  %constant_258 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.788 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[] %constant_258), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/output"}
  %param_8.17 = f32[1,1024,50,76]{3,2,1,0} parameter(8)
  %param_9.13 = f32[1024]{0} parameter(9)
  %broadcast.603 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_9.13), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
  %subtract.104 = f32[1,1024,50,76]{3,2,1,0} subtract(f32[1,1024,50,76]{3,2,1,0} %param_8.17, f32[1,1024,50,76]{3,2,1,0} %broadcast.603), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
  %param_7.62 = f32[1024]{0} parameter(7)
  %broadcast.602 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_7.62), dimensions={1}
  %multiply.182 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %subtract.104, f32[1,1024,50,76]{3,2,1,0} %broadcast.602), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
  %param_6.114 = f32[1024]{0} parameter(6)
  %broadcast.601 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_6.114), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
  %multiply.181 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %multiply.182, f32[1,1024,50,76]{3,2,1,0} %broadcast.601), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
  %param_5.227 = f32[1024]{0} parameter(5)
  %broadcast.600 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_5.227), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
  %add.253 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %multiply.181, f32[1,1024,50,76]{3,2,1,0} %broadcast.600), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
  %param_3.329 = f32[1,1024,50,76]{3,2,1,0} parameter(3)
  %param_4.286 = f32[1024]{0} parameter(4)
  %broadcast.599 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_4.286), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
  %subtract.103 = f32[1,1024,50,76]{3,2,1,0} subtract(f32[1,1024,50,76]{3,2,1,0} %param_3.329, f32[1,1024,50,76]{3,2,1,0} %broadcast.599), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
  %param_2.283 = f32[1024]{0} parameter(2)
  %broadcast.597 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_2.283), dimensions={1}
  %multiply.180 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %subtract.103, f32[1,1024,50,76]{3,2,1,0} %broadcast.597), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
  %param_1.353 = f32[1024]{0} parameter(1)
  %broadcast.596 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_1.353), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
  %multiply.179 = f32[1,1024,50,76]{3,2,1,0} multiply(f32[1,1024,50,76]{3,2,1,0} %multiply.180, f32[1,1024,50,76]{3,2,1,0} %broadcast.596), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
  %param_0.287 = f32[1024]{0} parameter(0)
  %broadcast.595 = f32[1,1024,50,76]{3,2,1,0} broadcast(f32[1024]{0} %param_0.287), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
  %add.252 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %multiply.179, f32[1,1024,50,76]{3,2,1,0} %broadcast.595), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
  %add.251 = f32[1,1024,50,76]{3,2,1,0} add(f32[1,1024,50,76]{3,2,1,0} %add.253, f32[1,1024,50,76]{3,2,1,0} %add.252), metadata={op_type="Add" op_name="tower-pred-0/group2/block0/add"}
  ROOT %maximum.29 = f32[1,1024,50,76]{3,2,1,0} maximum(f32[1,1024,50,76]{3,2,1,0} %broadcast.788, f32[1,1024,50,76]{3,2,1,0} %add.251), metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/output"}
}

%fused_computation.84 (param_0.170: f32[1024]) -> f32[1024] {
  %param_0.170 = f32[1024]{0} parameter(0)
  %constant_307 = f32[] constant(1.001e-05)
  %broadcast.844 = f32[1024]{0} broadcast(f32[] %constant_307), dimensions={}
  %add.254 = f32[1024]{0} add(f32[1024]{0} %param_0.170, f32[1024]{0} %broadcast.844), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
  ROOT %rsqrt.131 = f32[1024]{0} rsqrt(f32[1024]{0} %add.254), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
}

%fused_computation.85 (param_0.172: f32[1024]) -> f32[1024] {
  %param_0.172 = f32[1024]{0} parameter(0)
  %constant_262 = f32[] constant(1.001e-05)
  %broadcast.792 = f32[1024]{0} broadcast(f32[] %constant_262), dimensions={}
  %add.255 = f32[1024]{0} add(f32[1024]{0} %param_0.172, f32[1024]{0} %broadcast.792), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.132 = f32[1024]{0} rsqrt(f32[1024]{0} %add.255), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
}

%fused_computation.86 (param_0.288: f32[256], param_1.354: f32[256], param_2.284: f32[256], param_3.330: f32[1,256,50,76], param_4.287: f32[256]) -> f32[1,256,50,76] {
  %constant_259 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.789 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[] %constant_259), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %param_3.330 = f32[1,256,50,76]{3,2,1,0} parameter(3)
  %param_4.287 = f32[256]{0} parameter(4)
  %broadcast.607 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_4.287), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
  %subtract.105 = f32[1,256,50,76]{3,2,1,0} subtract(f32[1,256,50,76]{3,2,1,0} %param_3.330, f32[1,256,50,76]{3,2,1,0} %broadcast.607), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
  %param_2.284 = f32[256]{0} parameter(2)
  %broadcast.606 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_2.284), dimensions={1}
  %multiply.184 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %subtract.105, f32[1,256,50,76]{3,2,1,0} %broadcast.606), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
  %param_1.354 = f32[256]{0} parameter(1)
  %broadcast.605 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_1.354), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
  %multiply.183 = f32[1,256,50,76]{3,2,1,0} multiply(f32[1,256,50,76]{3,2,1,0} %multiply.184, f32[1,256,50,76]{3,2,1,0} %broadcast.605), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
  %param_0.288 = f32[256]{0} parameter(0)
  %broadcast.604 = f32[1,256,50,76]{3,2,1,0} broadcast(f32[256]{0} %param_0.288), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
  %add.256 = f32[1,256,50,76]{3,2,1,0} add(f32[1,256,50,76]{3,2,1,0} %multiply.183, f32[1,256,50,76]{3,2,1,0} %broadcast.604), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.30 = f32[1,256,50,76]{3,2,1,0} maximum(f32[1,256,50,76]{3,2,1,0} %broadcast.789, f32[1,256,50,76]{3,2,1,0} %add.256), metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
}

%fused_computation.87 (param_0.175: f32[256]) -> f32[256] {
  %param_0.175 = f32[256]{0} parameter(0)
  %constant_261 = f32[] constant(1.001e-05)
  %broadcast.791 = f32[256]{0} broadcast(f32[] %constant_261), dimensions={}
  %add.257 = f32[256]{0} add(f32[256]{0} %param_0.175, f32[256]{0} %broadcast.791), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.133 = f32[256]{0} rsqrt(f32[256]{0} %add.257), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
}

%fused_computation.88 (param_0.178: f32[256], param_1.202: f32[256], param_2.160: f32[256], param_3.198: f32[1,256,100,152], param_4.173: f32[256]) -> f32[1,256,101,153] {
  %constant_202 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.612 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[] %constant_202), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv1/Relu"}
  %param_3.198 = f32[1,256,100,152]{3,2,1,0} parameter(3)
  %param_4.173 = f32[256]{0} parameter(4)
  %broadcast.611 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[256]{0} %param_4.173), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
  %subtract.106 = f32[1,256,100,152]{3,2,1,0} subtract(f32[1,256,100,152]{3,2,1,0} %param_3.198, f32[1,256,100,152]{3,2,1,0} %broadcast.611), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
  %param_2.160 = f32[256]{0} parameter(2)
  %broadcast.610 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[256]{0} %param_2.160), dimensions={1}
  %multiply.186 = f32[1,256,100,152]{3,2,1,0} multiply(f32[1,256,100,152]{3,2,1,0} %subtract.106, f32[1,256,100,152]{3,2,1,0} %broadcast.610), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
  %param_1.202 = f32[256]{0} parameter(1)
  %broadcast.609 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[256]{0} %param_1.202), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
  %multiply.185 = f32[1,256,100,152]{3,2,1,0} multiply(f32[1,256,100,152]{3,2,1,0} %multiply.186, f32[1,256,100,152]{3,2,1,0} %broadcast.609), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
  %param_0.178 = f32[256]{0} parameter(0)
  %broadcast.608 = f32[1,256,100,152]{3,2,1,0} broadcast(f32[256]{0} %param_0.178), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
  %add.258 = f32[1,256,100,152]{3,2,1,0} add(f32[1,256,100,152]{3,2,1,0} %multiply.185, f32[1,256,100,152]{3,2,1,0} %broadcast.608), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
  %maximum.31 = f32[1,256,100,152]{3,2,1,0} maximum(f32[1,256,100,152]{3,2,1,0} %broadcast.612, f32[1,256,100,152]{3,2,1,0} %add.258), metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv1/Relu"}
  ROOT %pad.13 = f32[1,256,101,153]{3,2,1,0} pad(f32[1,256,100,152]{3,2,1,0} %maximum.31, f32[] %constant_202), padding=0_0x0_0x1_0x1_0
}

%fused_computation.89 (param_0.180: f32[256]) -> f32[256] {
  %param_0.180 = f32[256]{0} parameter(0)
  %constant_260 = f32[] constant(1.001e-05)
  %broadcast.790 = f32[256]{0} broadcast(f32[] %constant_260), dimensions={}
  %add.259 = f32[256]{0} add(f32[256]{0} %param_0.180, f32[256]{0} %broadcast.790), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.134 = f32[256]{0} rsqrt(f32[256]{0} %add.259), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
}

%fused_computation.90 (param_0.289: f32[1,512,100,152], param_1.358: f32[512], param_2.285: f32[512], param_3.331: f32[512], param_4.288: f32[1,512,100,152], param_5.229: f32[512]) -> f32[1,512,100,152] {
  %constant_263 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.794 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[] %constant_263), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/output"}
  %param_0.289 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %param_4.288 = f32[1,512,100,152]{3,2,1,0} parameter(4)
  %param_5.229 = f32[512]{0} parameter(5)
  %broadcast.616 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_5.229), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
  %subtract.107 = f32[1,512,100,152]{3,2,1,0} subtract(f32[1,512,100,152]{3,2,1,0} %param_4.288, f32[1,512,100,152]{3,2,1,0} %broadcast.616), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
  %param_3.331 = f32[512]{0} parameter(3)
  %broadcast.615 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_3.331), dimensions={1}
  %multiply.188 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %subtract.107, f32[1,512,100,152]{3,2,1,0} %broadcast.615), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
  %param_2.285 = f32[512]{0} parameter(2)
  %broadcast.614 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_2.285), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
  %multiply.187 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %multiply.188, f32[1,512,100,152]{3,2,1,0} %broadcast.614), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
  %param_1.358 = f32[512]{0} parameter(1)
  %broadcast.613 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_1.358), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
  %add.261 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %multiply.187, f32[1,512,100,152]{3,2,1,0} %broadcast.613), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
  %add.260 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %param_0.289, f32[1,512,100,152]{3,2,1,0} %add.261), metadata={op_type="Add" op_name="tower-pred-0/group1/block3/add"}
  ROOT %maximum.32 = f32[1,512,100,152]{3,2,1,0} maximum(f32[1,512,100,152]{3,2,1,0} %broadcast.794, f32[1,512,100,152]{3,2,1,0} %add.260), metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/output"}
}

%fused_computation.91 (param_0.183: f32[512]) -> f32[512] {
  %param_0.183 = f32[512]{0} parameter(0)
  %constant_268 = f32[] constant(1.001e-05)
  %broadcast.799 = f32[512]{0} broadcast(f32[] %constant_268), dimensions={}
  %add.262 = f32[512]{0} add(f32[512]{0} %param_0.183, f32[512]{0} %broadcast.799), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.135 = f32[512]{0} rsqrt(f32[512]{0} %add.262), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
}

%fused_computation.92 (param_0.290: f32[128], param_1.359: f32[128], param_2.286: f32[128], param_3.332: f32[1,128,100,152], param_4.289: f32[128]) -> f32[1,128,100,152] {
  %constant_264 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.795 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_264), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv2/Relu"}
  %param_3.332 = f32[1,128,100,152]{3,2,1,0} parameter(3)
  %param_4.289 = f32[128]{0} parameter(4)
  %broadcast.620 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_4.289), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
  %subtract.108 = f32[1,128,100,152]{3,2,1,0} subtract(f32[1,128,100,152]{3,2,1,0} %param_3.332, f32[1,128,100,152]{3,2,1,0} %broadcast.620), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
  %param_2.286 = f32[128]{0} parameter(2)
  %broadcast.619 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_2.286), dimensions={1}
  %multiply.190 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %subtract.108, f32[1,128,100,152]{3,2,1,0} %broadcast.619), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
  %param_1.359 = f32[128]{0} parameter(1)
  %broadcast.618 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_1.359), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
  %multiply.189 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %multiply.190, f32[1,128,100,152]{3,2,1,0} %broadcast.618), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
  %param_0.290 = f32[128]{0} parameter(0)
  %broadcast.617 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_0.290), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
  %add.263 = f32[1,128,100,152]{3,2,1,0} add(f32[1,128,100,152]{3,2,1,0} %multiply.189, f32[1,128,100,152]{3,2,1,0} %broadcast.617), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.33 = f32[1,128,100,152]{3,2,1,0} maximum(f32[1,128,100,152]{3,2,1,0} %broadcast.795, f32[1,128,100,152]{3,2,1,0} %add.263), metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/conv2/Relu"}
}

%fused_computation.93 (param_0.186: f32[128]) -> f32[128] {
  %param_0.186 = f32[128]{0} parameter(0)
  %constant_267 = f32[] constant(1.001e-05)
  %broadcast.798 = f32[128]{0} broadcast(f32[] %constant_267), dimensions={}
  %add.264 = f32[128]{0} add(f32[128]{0} %param_0.186, f32[128]{0} %broadcast.798), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.136 = f32[128]{0} rsqrt(f32[128]{0} %add.264), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
}

%fused_computation.94 (param_0.291: f32[128], param_1.360: f32[128], param_2.287: f32[128], param_3.333: f32[1,128,100,152], param_4.290: f32[128]) -> f32[1,128,100,152] {
  %constant_265 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.796 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_265), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv2/Relu"}
  %param_3.333 = f32[1,128,100,152]{3,2,1,0} parameter(3)
  %param_4.290 = f32[128]{0} parameter(4)
  %broadcast.624 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_4.290), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
  %subtract.109 = f32[1,128,100,152]{3,2,1,0} subtract(f32[1,128,100,152]{3,2,1,0} %param_3.333, f32[1,128,100,152]{3,2,1,0} %broadcast.624), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
  %param_2.287 = f32[128]{0} parameter(2)
  %broadcast.623 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_2.287), dimensions={1}
  %multiply.192 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %subtract.109, f32[1,128,100,152]{3,2,1,0} %broadcast.623), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
  %param_1.360 = f32[128]{0} parameter(1)
  %broadcast.622 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_1.360), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
  %multiply.191 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %multiply.192, f32[1,128,100,152]{3,2,1,0} %broadcast.622), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
  %param_0.291 = f32[128]{0} parameter(0)
  %broadcast.621 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_0.291), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
  %add.265 = f32[1,128,100,152]{3,2,1,0} add(f32[1,128,100,152]{3,2,1,0} %multiply.191, f32[1,128,100,152]{3,2,1,0} %broadcast.621), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.34 = f32[1,128,100,152]{3,2,1,0} maximum(f32[1,128,100,152]{3,2,1,0} %broadcast.796, f32[1,128,100,152]{3,2,1,0} %add.265), metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/conv1/Relu"}
}

%fused_computation.95 (param_0.189: f32[128]) -> f32[128] {
  %param_0.189 = f32[128]{0} parameter(0)
  %constant_266 = f32[] constant(1.001e-05)
  %broadcast.797 = f32[128]{0} broadcast(f32[] %constant_266), dimensions={}
  %add.266 = f32[128]{0} add(f32[128]{0} %param_0.189, f32[128]{0} %broadcast.797), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.137 = f32[128]{0} rsqrt(f32[128]{0} %add.266), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
}

%fused_computation.96 (param_0.292: f32[1,512,100,152], param_1.364: f32[512], param_2.288: f32[512], param_3.334: f32[512], param_4.291: f32[1,512,100,152], param_5.232: f32[512]) -> f32[1,512,100,152] {
  %constant_269 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.801 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[] %constant_269), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/output"}
  %param_0.292 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %param_4.291 = f32[1,512,100,152]{3,2,1,0} parameter(4)
  %param_5.232 = f32[512]{0} parameter(5)
  %broadcast.628 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_5.232), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
  %subtract.110 = f32[1,512,100,152]{3,2,1,0} subtract(f32[1,512,100,152]{3,2,1,0} %param_4.291, f32[1,512,100,152]{3,2,1,0} %broadcast.628), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
  %param_3.334 = f32[512]{0} parameter(3)
  %broadcast.627 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_3.334), dimensions={1}
  %multiply.194 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %subtract.110, f32[1,512,100,152]{3,2,1,0} %broadcast.627), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
  %param_2.288 = f32[512]{0} parameter(2)
  %broadcast.626 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_2.288), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
  %multiply.193 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %multiply.194, f32[1,512,100,152]{3,2,1,0} %broadcast.626), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
  %param_1.364 = f32[512]{0} parameter(1)
  %broadcast.625 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_1.364), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
  %add.268 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %multiply.193, f32[1,512,100,152]{3,2,1,0} %broadcast.625), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
  %add.267 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %param_0.292, f32[1,512,100,152]{3,2,1,0} %add.268), metadata={op_type="Add" op_name="tower-pred-0/group1/block2/add"}
  ROOT %maximum.35 = f32[1,512,100,152]{3,2,1,0} maximum(f32[1,512,100,152]{3,2,1,0} %broadcast.801, f32[1,512,100,152]{3,2,1,0} %add.267), metadata={op_type="Relu" op_name="tower-pred-0/group1/block2/output"}
}

%fused_computation.97 (param_0.192: f32[512]) -> f32[512] {
  %param_0.192 = f32[512]{0} parameter(0)
  %constant_274 = f32[] constant(1.001e-05)
  %broadcast.806 = f32[512]{0} broadcast(f32[] %constant_274), dimensions={}
  %add.269 = f32[512]{0} add(f32[512]{0} %param_0.192, f32[512]{0} %broadcast.806), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.138 = f32[512]{0} rsqrt(f32[512]{0} %add.269), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
}

%fused_computation.98 (param_0.293: f32[128], param_1.365: f32[128], param_2.289: f32[128], param_3.335: f32[1,128,100,152], param_4.292: f32[128]) -> f32[1,128,100,152] {
  %constant_270 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.802 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_270), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv2/Relu"}
  %param_3.335 = f32[1,128,100,152]{3,2,1,0} parameter(3)
  %param_4.292 = f32[128]{0} parameter(4)
  %broadcast.632 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_4.292), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
  %subtract.111 = f32[1,128,100,152]{3,2,1,0} subtract(f32[1,128,100,152]{3,2,1,0} %param_3.335, f32[1,128,100,152]{3,2,1,0} %broadcast.632), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
  %param_2.289 = f32[128]{0} parameter(2)
  %broadcast.631 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_2.289), dimensions={1}
  %multiply.196 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %subtract.111, f32[1,128,100,152]{3,2,1,0} %broadcast.631), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
  %param_1.365 = f32[128]{0} parameter(1)
  %broadcast.630 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_1.365), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
  %multiply.195 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %multiply.196, f32[1,128,100,152]{3,2,1,0} %broadcast.630), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
  %param_0.293 = f32[128]{0} parameter(0)
  %broadcast.629 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_0.293), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
  %add.270 = f32[1,128,100,152]{3,2,1,0} add(f32[1,128,100,152]{3,2,1,0} %multiply.195, f32[1,128,100,152]{3,2,1,0} %broadcast.629), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.36 = f32[1,128,100,152]{3,2,1,0} maximum(f32[1,128,100,152]{3,2,1,0} %broadcast.802, f32[1,128,100,152]{3,2,1,0} %add.270), metadata={op_type="Relu" op_name="tower-pred-0/group1/block2/conv2/Relu"}
}

%fused_computation.99 (param_0.195: f32[128]) -> f32[128] {
  %param_0.195 = f32[128]{0} parameter(0)
  %constant_273 = f32[] constant(1.001e-05)
  %broadcast.805 = f32[128]{0} broadcast(f32[] %constant_273), dimensions={}
  %add.271 = f32[128]{0} add(f32[128]{0} %param_0.195, f32[128]{0} %broadcast.805), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.139 = f32[128]{0} rsqrt(f32[128]{0} %add.271), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
}

%fused_computation.100 (param_0.294: f32[128], param_1.366: f32[128], param_2.290: f32[128], param_3.336: f32[1,128,100,152], param_4.293: f32[128]) -> f32[1,128,100,152] {
  %constant_271 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.803 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_271), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv2/Relu"}
  %param_3.336 = f32[1,128,100,152]{3,2,1,0} parameter(3)
  %param_4.293 = f32[128]{0} parameter(4)
  %broadcast.637 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_4.293), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
  %subtract.112 = f32[1,128,100,152]{3,2,1,0} subtract(f32[1,128,100,152]{3,2,1,0} %param_3.336, f32[1,128,100,152]{3,2,1,0} %broadcast.637), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
  %param_2.290 = f32[128]{0} parameter(2)
  %broadcast.636 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_2.290), dimensions={1}
  %multiply.198 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %subtract.112, f32[1,128,100,152]{3,2,1,0} %broadcast.636), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
  %param_1.366 = f32[128]{0} parameter(1)
  %broadcast.635 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_1.366), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
  %multiply.197 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %multiply.198, f32[1,128,100,152]{3,2,1,0} %broadcast.635), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
  %param_0.294 = f32[128]{0} parameter(0)
  %broadcast.634 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_0.294), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
  %add.272 = f32[1,128,100,152]{3,2,1,0} add(f32[1,128,100,152]{3,2,1,0} %multiply.197, f32[1,128,100,152]{3,2,1,0} %broadcast.634), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.37 = f32[1,128,100,152]{3,2,1,0} maximum(f32[1,128,100,152]{3,2,1,0} %broadcast.803, f32[1,128,100,152]{3,2,1,0} %add.272), metadata={op_type="Relu" op_name="tower-pred-0/group1/block2/conv1/Relu"}
}

%fused_computation.101 (param_0.198: f32[128]) -> f32[128] {
  %param_0.198 = f32[128]{0} parameter(0)
  %constant_272 = f32[] constant(1.001e-05)
  %broadcast.804 = f32[128]{0} broadcast(f32[] %constant_272), dimensions={}
  %add.273 = f32[128]{0} add(f32[128]{0} %param_0.198, f32[128]{0} %broadcast.804), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.140 = f32[128]{0} rsqrt(f32[128]{0} %add.273), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
}

%fused_computation.102 (param_0.295: f32[1,512,100,152], param_1.370: f32[512], param_2.291: f32[512], param_3.337: f32[512], param_4.294: f32[1,512,100,152], param_5.235: f32[512]) -> f32[1,512,100,152] {
  %constant_275 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.807 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[] %constant_275), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/output"}
  %param_0.295 = f32[1,512,100,152]{3,2,1,0} parameter(0)
  %param_4.294 = f32[1,512,100,152]{3,2,1,0} parameter(4)
  %param_5.235 = f32[512]{0} parameter(5)
  %broadcast.641 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_5.235), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
  %subtract.113 = f32[1,512,100,152]{3,2,1,0} subtract(f32[1,512,100,152]{3,2,1,0} %param_4.294, f32[1,512,100,152]{3,2,1,0} %broadcast.641), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
  %param_3.337 = f32[512]{0} parameter(3)
  %broadcast.640 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_3.337), dimensions={1}
  %multiply.200 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %subtract.113, f32[1,512,100,152]{3,2,1,0} %broadcast.640), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
  %param_2.291 = f32[512]{0} parameter(2)
  %broadcast.639 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_2.291), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
  %multiply.199 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %multiply.200, f32[1,512,100,152]{3,2,1,0} %broadcast.639), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
  %param_1.370 = f32[512]{0} parameter(1)
  %broadcast.638 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_1.370), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
  %add.275 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %multiply.199, f32[1,512,100,152]{3,2,1,0} %broadcast.638), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
  %add.274 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %param_0.295, f32[1,512,100,152]{3,2,1,0} %add.275), metadata={op_type="Add" op_name="tower-pred-0/group1/block1/add"}
  ROOT %maximum.38 = f32[1,512,100,152]{3,2,1,0} maximum(f32[1,512,100,152]{3,2,1,0} %broadcast.807, f32[1,512,100,152]{3,2,1,0} %add.274), metadata={op_type="Relu" op_name="tower-pred-0/group1/block1/output"}
}

%fused_computation.103 (param_0.201: f32[512]) -> f32[512] {
  %param_0.201 = f32[512]{0} parameter(0)
  %constant_280 = f32[] constant(1.001e-05)
  %broadcast.814 = f32[512]{0} broadcast(f32[] %constant_280), dimensions={}
  %add.276 = f32[512]{0} add(f32[512]{0} %param_0.201, f32[512]{0} %broadcast.814), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.141 = f32[512]{0} rsqrt(f32[512]{0} %add.276), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
}

%fused_computation.104 (param_0.296: f32[128], param_1.371: f32[128], param_2.292: f32[128], param_3.338: f32[1,128,100,152], param_4.295: f32[128]) -> f32[1,128,100,152] {
  %constant_276 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.809 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_276), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv2/Relu"}
  %param_3.338 = f32[1,128,100,152]{3,2,1,0} parameter(3)
  %param_4.295 = f32[128]{0} parameter(4)
  %broadcast.645 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_4.295), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
  %subtract.114 = f32[1,128,100,152]{3,2,1,0} subtract(f32[1,128,100,152]{3,2,1,0} %param_3.338, f32[1,128,100,152]{3,2,1,0} %broadcast.645), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
  %param_2.292 = f32[128]{0} parameter(2)
  %broadcast.644 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_2.292), dimensions={1}
  %multiply.202 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %subtract.114, f32[1,128,100,152]{3,2,1,0} %broadcast.644), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
  %param_1.371 = f32[128]{0} parameter(1)
  %broadcast.643 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_1.371), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
  %multiply.201 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %multiply.202, f32[1,128,100,152]{3,2,1,0} %broadcast.643), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
  %param_0.296 = f32[128]{0} parameter(0)
  %broadcast.642 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_0.296), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
  %add.277 = f32[1,128,100,152]{3,2,1,0} add(f32[1,128,100,152]{3,2,1,0} %multiply.201, f32[1,128,100,152]{3,2,1,0} %broadcast.642), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.39 = f32[1,128,100,152]{3,2,1,0} maximum(f32[1,128,100,152]{3,2,1,0} %broadcast.809, f32[1,128,100,152]{3,2,1,0} %add.277), metadata={op_type="Relu" op_name="tower-pred-0/group1/block1/conv2/Relu"}
}

%fused_computation.105 (param_0.204: f32[128]) -> f32[128] {
  %param_0.204 = f32[128]{0} parameter(0)
  %constant_279 = f32[] constant(1.001e-05)
  %broadcast.813 = f32[128]{0} broadcast(f32[] %constant_279), dimensions={}
  %add.278 = f32[128]{0} add(f32[128]{0} %param_0.204, f32[128]{0} %broadcast.813), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.142 = f32[128]{0} rsqrt(f32[128]{0} %add.278), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
}

%fused_computation.106 (param_0.297: f32[128], param_1.372: f32[128], param_2.293: f32[128], param_3.339: f32[1,128,100,152], param_4.296: f32[128]) -> f32[1,128,100,152] {
  %constant_277 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.810 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_277), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv2/Relu"}
  %param_3.339 = f32[1,128,100,152]{3,2,1,0} parameter(3)
  %param_4.296 = f32[128]{0} parameter(4)
  %broadcast.650 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_4.296), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
  %subtract.115 = f32[1,128,100,152]{3,2,1,0} subtract(f32[1,128,100,152]{3,2,1,0} %param_3.339, f32[1,128,100,152]{3,2,1,0} %broadcast.650), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
  %param_2.293 = f32[128]{0} parameter(2)
  %broadcast.649 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_2.293), dimensions={1}
  %multiply.204 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %subtract.115, f32[1,128,100,152]{3,2,1,0} %broadcast.649), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
  %param_1.372 = f32[128]{0} parameter(1)
  %broadcast.647 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_1.372), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
  %multiply.203 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %multiply.204, f32[1,128,100,152]{3,2,1,0} %broadcast.647), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
  %param_0.297 = f32[128]{0} parameter(0)
  %broadcast.646 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_0.297), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
  %add.279 = f32[1,128,100,152]{3,2,1,0} add(f32[1,128,100,152]{3,2,1,0} %multiply.203, f32[1,128,100,152]{3,2,1,0} %broadcast.646), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.40 = f32[1,128,100,152]{3,2,1,0} maximum(f32[1,128,100,152]{3,2,1,0} %broadcast.810, f32[1,128,100,152]{3,2,1,0} %add.279), metadata={op_type="Relu" op_name="tower-pred-0/group1/block1/conv1/Relu"}
}

%fused_computation.107 (param_0.207: f32[128]) -> f32[128] {
  %param_0.207 = f32[128]{0} parameter(0)
  %constant_278 = f32[] constant(1.001e-05)
  %broadcast.812 = f32[128]{0} broadcast(f32[] %constant_278), dimensions={}
  %add.280 = f32[128]{0} add(f32[128]{0} %param_0.207, f32[128]{0} %broadcast.812), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.143 = f32[128]{0} rsqrt(f32[128]{0} %add.280), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
}

%fused_computation.108 (param_0.298: f32[512], param_1.376: f32[512], param_2.294: f32[512], param_3.340: f32[1,512,100,152], param_4.297: f32[512], param_5.238: f32[512], param_6.118: f32[512], param_7.63: f32[512], param_8.18: f32[1,512,100,152], param_9.14: f32[512]) -> f32[1,512,100,152] {
  %constant_281 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.815 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[] %constant_281), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/output"}
  %param_8.18 = f32[1,512,100,152]{3,2,1,0} parameter(8)
  %param_9.14 = f32[512]{0} parameter(9)
  %broadcast.659 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_9.14), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
  %subtract.117 = f32[1,512,100,152]{3,2,1,0} subtract(f32[1,512,100,152]{3,2,1,0} %param_8.18, f32[1,512,100,152]{3,2,1,0} %broadcast.659), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
  %param_7.63 = f32[512]{0} parameter(7)
  %broadcast.658 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_7.63), dimensions={1}
  %multiply.208 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %subtract.117, f32[1,512,100,152]{3,2,1,0} %broadcast.658), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
  %param_6.118 = f32[512]{0} parameter(6)
  %broadcast.657 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_6.118), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
  %multiply.207 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %multiply.208, f32[1,512,100,152]{3,2,1,0} %broadcast.657), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
  %param_5.238 = f32[512]{0} parameter(5)
  %broadcast.656 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_5.238), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
  %add.283 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %multiply.207, f32[1,512,100,152]{3,2,1,0} %broadcast.656), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
  %param_3.340 = f32[1,512,100,152]{3,2,1,0} parameter(3)
  %param_4.297 = f32[512]{0} parameter(4)
  %broadcast.654 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_4.297), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
  %subtract.116 = f32[1,512,100,152]{3,2,1,0} subtract(f32[1,512,100,152]{3,2,1,0} %param_3.340, f32[1,512,100,152]{3,2,1,0} %broadcast.654), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
  %param_2.294 = f32[512]{0} parameter(2)
  %broadcast.653 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_2.294), dimensions={1}
  %multiply.206 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %subtract.116, f32[1,512,100,152]{3,2,1,0} %broadcast.653), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
  %param_1.376 = f32[512]{0} parameter(1)
  %broadcast.652 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_1.376), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
  %multiply.205 = f32[1,512,100,152]{3,2,1,0} multiply(f32[1,512,100,152]{3,2,1,0} %multiply.206, f32[1,512,100,152]{3,2,1,0} %broadcast.652), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
  %param_0.298 = f32[512]{0} parameter(0)
  %broadcast.651 = f32[1,512,100,152]{3,2,1,0} broadcast(f32[512]{0} %param_0.298), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
  %add.282 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %multiply.205, f32[1,512,100,152]{3,2,1,0} %broadcast.651), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
  %add.281 = f32[1,512,100,152]{3,2,1,0} add(f32[1,512,100,152]{3,2,1,0} %add.283, f32[1,512,100,152]{3,2,1,0} %add.282), metadata={op_type="Add" op_name="tower-pred-0/group1/block0/add"}
  ROOT %maximum.41 = f32[1,512,100,152]{3,2,1,0} maximum(f32[1,512,100,152]{3,2,1,0} %broadcast.815, f32[1,512,100,152]{3,2,1,0} %add.281), metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/output"}
}

%fused_computation.109 (param_0.210: f32[512]) -> f32[512] {
  %param_0.210 = f32[512]{0} parameter(0)
  %constant_306 = f32[] constant(1.001e-05)
  %broadcast.843 = f32[512]{0} broadcast(f32[] %constant_306), dimensions={}
  %add.284 = f32[512]{0} add(f32[512]{0} %param_0.210, f32[512]{0} %broadcast.843), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
  ROOT %rsqrt.144 = f32[512]{0} rsqrt(f32[512]{0} %add.284), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
}

%fused_computation.110 (param_0.212: f32[512]) -> f32[512] {
  %param_0.212 = f32[512]{0} parameter(0)
  %constant_285 = f32[] constant(1.001e-05)
  %broadcast.820 = f32[512]{0} broadcast(f32[] %constant_285), dimensions={}
  %add.285 = f32[512]{0} add(f32[512]{0} %param_0.212, f32[512]{0} %broadcast.820), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.145 = f32[512]{0} rsqrt(f32[512]{0} %add.285), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
}

%fused_computation.111 (param_0.299: f32[128], param_1.377: f32[128], param_2.295: f32[128], param_3.341: f32[1,128,100,152], param_4.298: f32[128]) -> f32[1,128,100,152] {
  %constant_282 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.816 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[] %constant_282), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv2/Relu"}
  %param_3.341 = f32[1,128,100,152]{3,2,1,0} parameter(3)
  %param_4.298 = f32[128]{0} parameter(4)
  %broadcast.663 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_4.298), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
  %subtract.118 = f32[1,128,100,152]{3,2,1,0} subtract(f32[1,128,100,152]{3,2,1,0} %param_3.341, f32[1,128,100,152]{3,2,1,0} %broadcast.663), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
  %param_2.295 = f32[128]{0} parameter(2)
  %broadcast.662 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_2.295), dimensions={1}
  %multiply.210 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %subtract.118, f32[1,128,100,152]{3,2,1,0} %broadcast.662), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
  %param_1.377 = f32[128]{0} parameter(1)
  %broadcast.661 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_1.377), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
  %multiply.209 = f32[1,128,100,152]{3,2,1,0} multiply(f32[1,128,100,152]{3,2,1,0} %multiply.210, f32[1,128,100,152]{3,2,1,0} %broadcast.661), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
  %param_0.299 = f32[128]{0} parameter(0)
  %broadcast.660 = f32[1,128,100,152]{3,2,1,0} broadcast(f32[128]{0} %param_0.299), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
  %add.286 = f32[1,128,100,152]{3,2,1,0} add(f32[1,128,100,152]{3,2,1,0} %multiply.209, f32[1,128,100,152]{3,2,1,0} %broadcast.660), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.42 = f32[1,128,100,152]{3,2,1,0} maximum(f32[1,128,100,152]{3,2,1,0} %broadcast.816, f32[1,128,100,152]{3,2,1,0} %add.286), metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv2/Relu"}
}

%fused_computation.112 (param_0.215: f32[128]) -> f32[128] {
  %param_0.215 = f32[128]{0} parameter(0)
  %constant_284 = f32[] constant(1.001e-05)
  %broadcast.819 = f32[128]{0} broadcast(f32[] %constant_284), dimensions={}
  %add.287 = f32[128]{0} add(f32[128]{0} %param_0.215, f32[128]{0} %broadcast.819), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.146 = f32[128]{0} rsqrt(f32[128]{0} %add.287), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
}

%fused_computation.113 (param_0.218: f32[128], param_1.249: f32[128], param_2.206: f32[128], param_3.249: f32[1,128,200,304], param_4.217: f32[128]) -> f32[1,128,201,305] {
  %constant_203 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.669 = f32[1,128,200,304]{3,2,1,0} broadcast(f32[] %constant_203), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv1/Relu"}
  %param_3.249 = f32[1,128,200,304]{3,2,1,0} parameter(3)
  %param_4.217 = f32[128]{0} parameter(4)
  %broadcast.668 = f32[1,128,200,304]{3,2,1,0} broadcast(f32[128]{0} %param_4.217), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
  %subtract.119 = f32[1,128,200,304]{3,2,1,0} subtract(f32[1,128,200,304]{3,2,1,0} %param_3.249, f32[1,128,200,304]{3,2,1,0} %broadcast.668), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
  %param_2.206 = f32[128]{0} parameter(2)
  %broadcast.666 = f32[1,128,200,304]{3,2,1,0} broadcast(f32[128]{0} %param_2.206), dimensions={1}
  %multiply.212 = f32[1,128,200,304]{3,2,1,0} multiply(f32[1,128,200,304]{3,2,1,0} %subtract.119, f32[1,128,200,304]{3,2,1,0} %broadcast.666), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
  %param_1.249 = f32[128]{0} parameter(1)
  %broadcast.665 = f32[1,128,200,304]{3,2,1,0} broadcast(f32[128]{0} %param_1.249), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
  %multiply.211 = f32[1,128,200,304]{3,2,1,0} multiply(f32[1,128,200,304]{3,2,1,0} %multiply.212, f32[1,128,200,304]{3,2,1,0} %broadcast.665), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
  %param_0.218 = f32[128]{0} parameter(0)
  %broadcast.664 = f32[1,128,200,304]{3,2,1,0} broadcast(f32[128]{0} %param_0.218), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
  %add.288 = f32[1,128,200,304]{3,2,1,0} add(f32[1,128,200,304]{3,2,1,0} %multiply.211, f32[1,128,200,304]{3,2,1,0} %broadcast.664), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
  %maximum.43 = f32[1,128,200,304]{3,2,1,0} maximum(f32[1,128,200,304]{3,2,1,0} %broadcast.669, f32[1,128,200,304]{3,2,1,0} %add.288), metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv1/Relu"}
  ROOT %pad.14 = f32[1,128,201,305]{3,2,1,0} pad(f32[1,128,200,304]{3,2,1,0} %maximum.43, f32[] %constant_203), padding=0_0x0_0x1_0x1_0
}

%fused_computation.114 (param_0.220: f32[128]) -> f32[128] {
  %param_0.220 = f32[128]{0} parameter(0)
  %constant_283 = f32[] constant(1.001e-05)
  %broadcast.817 = f32[128]{0} broadcast(f32[] %constant_283), dimensions={}
  %add.289 = f32[128]{0} add(f32[128]{0} %param_0.220, f32[128]{0} %broadcast.817), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.147 = f32[128]{0} rsqrt(f32[128]{0} %add.289), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
}

%fused_computation.115 (param_0.300: f32[1,256,200,304], param_1.381: f32[256], param_2.296: f32[256], param_3.342: f32[256], param_4.299: f32[1,256,200,304], param_5.240: f32[256]) -> f32[1,256,200,304] {
  %constant_286 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.821 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[] %constant_286), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group0/block2/output"}
  %param_0.300 = f32[1,256,200,304]{3,2,1,0} parameter(0)
  %param_4.299 = f32[1,256,200,304]{3,2,1,0} parameter(4)
  %param_5.240 = f32[256]{0} parameter(5)
  %broadcast.673 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_5.240), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
  %subtract.120 = f32[1,256,200,304]{3,2,1,0} subtract(f32[1,256,200,304]{3,2,1,0} %param_4.299, f32[1,256,200,304]{3,2,1,0} %broadcast.673), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
  %param_3.342 = f32[256]{0} parameter(3)
  %broadcast.672 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_3.342), dimensions={1}
  %multiply.214 = f32[1,256,200,304]{3,2,1,0} multiply(f32[1,256,200,304]{3,2,1,0} %subtract.120, f32[1,256,200,304]{3,2,1,0} %broadcast.672), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
  %param_2.296 = f32[256]{0} parameter(2)
  %broadcast.671 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_2.296), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
  %multiply.213 = f32[1,256,200,304]{3,2,1,0} multiply(f32[1,256,200,304]{3,2,1,0} %multiply.214, f32[1,256,200,304]{3,2,1,0} %broadcast.671), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
  %param_1.381 = f32[256]{0} parameter(1)
  %broadcast.670 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_1.381), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
  %add.291 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %multiply.213, f32[1,256,200,304]{3,2,1,0} %broadcast.670), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
  %add.290 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %param_0.300, f32[1,256,200,304]{3,2,1,0} %add.291), metadata={op_type="Add" op_name="tower-pred-0/group0/block2/add"}
  ROOT %maximum.44 = f32[1,256,200,304]{3,2,1,0} maximum(f32[1,256,200,304]{3,2,1,0} %broadcast.821, f32[1,256,200,304]{3,2,1,0} %add.290), metadata={op_type="Relu" op_name="tower-pred-0/group0/block2/output"}
}

%fused_computation.116 (param_0.223: f32[256]) -> f32[256] {
  %param_0.223 = f32[256]{0} parameter(0)
  %constant_291 = f32[] constant(1.001e-05)
  %broadcast.826 = f32[256]{0} broadcast(f32[] %constant_291), dimensions={}
  %add.292 = f32[256]{0} add(f32[256]{0} %param_0.223, f32[256]{0} %broadcast.826), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.148 = f32[256]{0} rsqrt(f32[256]{0} %add.292), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
}

%fused_computation.117 (param_0.301: f32[64], param_1.382: f32[64], param_2.297: f32[64], param_3.343: f32[1,64,200,304], param_4.300: f32[64]) -> f32[1,64,200,304] {
  %constant_287 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.822 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[] %constant_287), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv2/Relu"}
  %param_3.343 = f32[1,64,200,304]{3,2,1,0} parameter(3)
  %param_4.300 = f32[64]{0} parameter(4)
  %broadcast.678 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_4.300), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
  %subtract.121 = f32[1,64,200,304]{3,2,1,0} subtract(f32[1,64,200,304]{3,2,1,0} %param_3.343, f32[1,64,200,304]{3,2,1,0} %broadcast.678), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
  %param_2.297 = f32[64]{0} parameter(2)
  %broadcast.677 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_2.297), dimensions={1}
  %multiply.216 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %subtract.121, f32[1,64,200,304]{3,2,1,0} %broadcast.677), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
  %param_1.382 = f32[64]{0} parameter(1)
  %broadcast.676 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_1.382), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
  %multiply.215 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %multiply.216, f32[1,64,200,304]{3,2,1,0} %broadcast.676), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
  %param_0.301 = f32[64]{0} parameter(0)
  %broadcast.675 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_0.301), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
  %add.293 = f32[1,64,200,304]{3,2,1,0} add(f32[1,64,200,304]{3,2,1,0} %multiply.215, f32[1,64,200,304]{3,2,1,0} %broadcast.675), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.45 = f32[1,64,200,304]{3,2,1,0} maximum(f32[1,64,200,304]{3,2,1,0} %broadcast.822, f32[1,64,200,304]{3,2,1,0} %add.293), metadata={op_type="Relu" op_name="tower-pred-0/group0/block2/conv2/Relu"}
}

%fused_computation.118 (param_0.226: f32[64]) -> f32[64] {
  %param_0.226 = f32[64]{0} parameter(0)
  %constant_290 = f32[] constant(1.001e-05)
  %broadcast.825 = f32[64]{0} broadcast(f32[] %constant_290), dimensions={}
  %add.294 = f32[64]{0} add(f32[64]{0} %param_0.226, f32[64]{0} %broadcast.825), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.149 = f32[64]{0} rsqrt(f32[64]{0} %add.294), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
}

%fused_computation.119 (param_0.302: f32[64], param_1.383: f32[64], param_2.298: f32[64], param_3.344: f32[1,64,200,304], param_4.301: f32[64]) -> f32[1,64,200,304] {
  %constant_288 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.823 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[] %constant_288), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv2/Relu"}
  %param_3.344 = f32[1,64,200,304]{3,2,1,0} parameter(3)
  %param_4.301 = f32[64]{0} parameter(4)
  %broadcast.683 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_4.301), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
  %subtract.122 = f32[1,64,200,304]{3,2,1,0} subtract(f32[1,64,200,304]{3,2,1,0} %param_3.344, f32[1,64,200,304]{3,2,1,0} %broadcast.683), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
  %param_2.298 = f32[64]{0} parameter(2)
  %broadcast.682 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_2.298), dimensions={1}
  %multiply.218 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %subtract.122, f32[1,64,200,304]{3,2,1,0} %broadcast.682), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
  %param_1.383 = f32[64]{0} parameter(1)
  %broadcast.680 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_1.383), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
  %multiply.217 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %multiply.218, f32[1,64,200,304]{3,2,1,0} %broadcast.680), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
  %param_0.302 = f32[64]{0} parameter(0)
  %broadcast.679 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_0.302), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
  %add.295 = f32[1,64,200,304]{3,2,1,0} add(f32[1,64,200,304]{3,2,1,0} %multiply.217, f32[1,64,200,304]{3,2,1,0} %broadcast.679), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.46 = f32[1,64,200,304]{3,2,1,0} maximum(f32[1,64,200,304]{3,2,1,0} %broadcast.823, f32[1,64,200,304]{3,2,1,0} %add.295), metadata={op_type="Relu" op_name="tower-pred-0/group0/block2/conv1/Relu"}
}

%fused_computation.120 (param_0.229: f32[64]) -> f32[64] {
  %param_0.229 = f32[64]{0} parameter(0)
  %constant_289 = f32[] constant(1.001e-05)
  %broadcast.824 = f32[64]{0} broadcast(f32[] %constant_289), dimensions={}
  %add.296 = f32[64]{0} add(f32[64]{0} %param_0.229, f32[64]{0} %broadcast.824), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.150 = f32[64]{0} rsqrt(f32[64]{0} %add.296), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
}

%fused_computation.121 (param_0.303: f32[1,256,200,304], param_1.387: f32[256], param_2.299: f32[256], param_3.345: f32[256], param_4.302: f32[1,256,200,304], param_5.243: f32[256]) -> f32[1,256,200,304] {
  %constant_292 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.828 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[] %constant_292), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group0/block2/output"}
  %param_0.303 = f32[1,256,200,304]{3,2,1,0} parameter(0)
  %param_4.302 = f32[1,256,200,304]{3,2,1,0} parameter(4)
  %param_5.243 = f32[256]{0} parameter(5)
  %broadcast.687 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_5.243), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
  %subtract.123 = f32[1,256,200,304]{3,2,1,0} subtract(f32[1,256,200,304]{3,2,1,0} %param_4.302, f32[1,256,200,304]{3,2,1,0} %broadcast.687), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
  %param_3.345 = f32[256]{0} parameter(3)
  %broadcast.686 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_3.345), dimensions={1}
  %multiply.220 = f32[1,256,200,304]{3,2,1,0} multiply(f32[1,256,200,304]{3,2,1,0} %subtract.123, f32[1,256,200,304]{3,2,1,0} %broadcast.686), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
  %param_2.299 = f32[256]{0} parameter(2)
  %broadcast.685 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_2.299), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
  %multiply.219 = f32[1,256,200,304]{3,2,1,0} multiply(f32[1,256,200,304]{3,2,1,0} %multiply.220, f32[1,256,200,304]{3,2,1,0} %broadcast.685), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
  %param_1.387 = f32[256]{0} parameter(1)
  %broadcast.684 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_1.387), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
  %add.298 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %multiply.219, f32[1,256,200,304]{3,2,1,0} %broadcast.684), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
  %add.297 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %param_0.303, f32[1,256,200,304]{3,2,1,0} %add.298), metadata={op_type="Add" op_name="tower-pred-0/group0/block1/add"}
  ROOT %maximum.47 = f32[1,256,200,304]{3,2,1,0} maximum(f32[1,256,200,304]{3,2,1,0} %broadcast.828, f32[1,256,200,304]{3,2,1,0} %add.297), metadata={op_type="Relu" op_name="tower-pred-0/group0/block1/output"}
}

%fused_computation.122 (param_0.232: f32[256]) -> f32[256] {
  %param_0.232 = f32[256]{0} parameter(0)
  %constant_297 = f32[] constant(1.001e-05)
  %broadcast.833 = f32[256]{0} broadcast(f32[] %constant_297), dimensions={}
  %add.299 = f32[256]{0} add(f32[256]{0} %param_0.232, f32[256]{0} %broadcast.833), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.151 = f32[256]{0} rsqrt(f32[256]{0} %add.299), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
}

%fused_computation.123 (param_0.304: f32[64], param_1.388: f32[64], param_2.300: f32[64], param_3.346: f32[1,64,200,304], param_4.303: f32[64]) -> f32[1,64,200,304] {
  %constant_293 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.829 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[] %constant_293), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv2/Relu"}
  %param_3.346 = f32[1,64,200,304]{3,2,1,0} parameter(3)
  %param_4.303 = f32[64]{0} parameter(4)
  %broadcast.692 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_4.303), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
  %subtract.124 = f32[1,64,200,304]{3,2,1,0} subtract(f32[1,64,200,304]{3,2,1,0} %param_3.346, f32[1,64,200,304]{3,2,1,0} %broadcast.692), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
  %param_2.300 = f32[64]{0} parameter(2)
  %broadcast.691 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_2.300), dimensions={1}
  %multiply.222 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %subtract.124, f32[1,64,200,304]{3,2,1,0} %broadcast.691), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
  %param_1.388 = f32[64]{0} parameter(1)
  %broadcast.690 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_1.388), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
  %multiply.221 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %multiply.222, f32[1,64,200,304]{3,2,1,0} %broadcast.690), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
  %param_0.304 = f32[64]{0} parameter(0)
  %broadcast.688 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_0.304), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
  %add.300 = f32[1,64,200,304]{3,2,1,0} add(f32[1,64,200,304]{3,2,1,0} %multiply.221, f32[1,64,200,304]{3,2,1,0} %broadcast.688), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.48 = f32[1,64,200,304]{3,2,1,0} maximum(f32[1,64,200,304]{3,2,1,0} %broadcast.829, f32[1,64,200,304]{3,2,1,0} %add.300), metadata={op_type="Relu" op_name="tower-pred-0/group0/block1/conv2/Relu"}
}

%fused_computation.124 (param_0.235: f32[64]) -> f32[64] {
  %param_0.235 = f32[64]{0} parameter(0)
  %constant_296 = f32[] constant(1.001e-05)
  %broadcast.832 = f32[64]{0} broadcast(f32[] %constant_296), dimensions={}
  %add.301 = f32[64]{0} add(f32[64]{0} %param_0.235, f32[64]{0} %broadcast.832), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.152 = f32[64]{0} rsqrt(f32[64]{0} %add.301), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
}

%fused_computation.125 (param_0.305: f32[64], param_1.389: f32[64], param_2.301: f32[64], param_3.347: f32[1,64,200,304], param_4.304: f32[64]) -> f32[1,64,200,304] {
  %constant_294 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.830 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[] %constant_294), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv2/Relu"}
  %param_3.347 = f32[1,64,200,304]{3,2,1,0} parameter(3)
  %param_4.304 = f32[64]{0} parameter(4)
  %broadcast.697 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_4.304), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
  %subtract.125 = f32[1,64,200,304]{3,2,1,0} subtract(f32[1,64,200,304]{3,2,1,0} %param_3.347, f32[1,64,200,304]{3,2,1,0} %broadcast.697), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
  %param_2.301 = f32[64]{0} parameter(2)
  %broadcast.695 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_2.301), dimensions={1}
  %multiply.224 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %subtract.125, f32[1,64,200,304]{3,2,1,0} %broadcast.695), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
  %param_1.389 = f32[64]{0} parameter(1)
  %broadcast.694 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_1.389), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
  %multiply.223 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %multiply.224, f32[1,64,200,304]{3,2,1,0} %broadcast.694), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
  %param_0.305 = f32[64]{0} parameter(0)
  %broadcast.693 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_0.305), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
  %add.302 = f32[1,64,200,304]{3,2,1,0} add(f32[1,64,200,304]{3,2,1,0} %multiply.223, f32[1,64,200,304]{3,2,1,0} %broadcast.693), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.49 = f32[1,64,200,304]{3,2,1,0} maximum(f32[1,64,200,304]{3,2,1,0} %broadcast.830, f32[1,64,200,304]{3,2,1,0} %add.302), metadata={op_type="Relu" op_name="tower-pred-0/group0/block1/conv1/Relu"}
}

%fused_computation.126 (param_0.238: f32[64]) -> f32[64] {
  %param_0.238 = f32[64]{0} parameter(0)
  %constant_295 = f32[] constant(1.001e-05)
  %broadcast.831 = f32[64]{0} broadcast(f32[] %constant_295), dimensions={}
  %add.303 = f32[64]{0} add(f32[64]{0} %param_0.238, f32[64]{0} %broadcast.831), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.153 = f32[64]{0} rsqrt(f32[64]{0} %add.303), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
}

%fused_computation.127 (param_0.306: f32[256], param_1.393: f32[256], param_2.302: f32[256], param_3.348: f32[1,256,200,304], param_4.305: f32[256], param_5.246: f32[256], param_6.121: f32[256], param_7.64: f32[256], param_8.19: f32[1,256,200,304], param_9.15: f32[256]) -> f32[1,256,200,304] {
  %constant_298 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.834 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[] %constant_298), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group0/block2/output"}
  %param_8.19 = f32[1,256,200,304]{3,2,1,0} parameter(8)
  %param_9.15 = f32[256]{0} parameter(9)
  %broadcast.706 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_9.15), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
  %subtract.127 = f32[1,256,200,304]{3,2,1,0} subtract(f32[1,256,200,304]{3,2,1,0} %param_8.19, f32[1,256,200,304]{3,2,1,0} %broadcast.706), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
  %param_7.64 = f32[256]{0} parameter(7)
  %broadcast.705 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_7.64), dimensions={1}
  %multiply.228 = f32[1,256,200,304]{3,2,1,0} multiply(f32[1,256,200,304]{3,2,1,0} %subtract.127, f32[1,256,200,304]{3,2,1,0} %broadcast.705), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
  %param_6.121 = f32[256]{0} parameter(6)
  %broadcast.704 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_6.121), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
  %multiply.227 = f32[1,256,200,304]{3,2,1,0} multiply(f32[1,256,200,304]{3,2,1,0} %multiply.228, f32[1,256,200,304]{3,2,1,0} %broadcast.704), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
  %param_5.246 = f32[256]{0} parameter(5)
  %broadcast.702 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_5.246), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
  %add.306 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %multiply.227, f32[1,256,200,304]{3,2,1,0} %broadcast.702), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
  %param_3.348 = f32[1,256,200,304]{3,2,1,0} parameter(3)
  %param_4.305 = f32[256]{0} parameter(4)
  %broadcast.701 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_4.305), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
  %subtract.126 = f32[1,256,200,304]{3,2,1,0} subtract(f32[1,256,200,304]{3,2,1,0} %param_3.348, f32[1,256,200,304]{3,2,1,0} %broadcast.701), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
  %param_2.302 = f32[256]{0} parameter(2)
  %broadcast.700 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_2.302), dimensions={1}
  %multiply.226 = f32[1,256,200,304]{3,2,1,0} multiply(f32[1,256,200,304]{3,2,1,0} %subtract.126, f32[1,256,200,304]{3,2,1,0} %broadcast.700), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
  %param_1.393 = f32[256]{0} parameter(1)
  %broadcast.699 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_1.393), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
  %multiply.225 = f32[1,256,200,304]{3,2,1,0} multiply(f32[1,256,200,304]{3,2,1,0} %multiply.226, f32[1,256,200,304]{3,2,1,0} %broadcast.699), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
  %param_0.306 = f32[256]{0} parameter(0)
  %broadcast.698 = f32[1,256,200,304]{3,2,1,0} broadcast(f32[256]{0} %param_0.306), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
  %add.305 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %multiply.225, f32[1,256,200,304]{3,2,1,0} %broadcast.698), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
  %add.304 = f32[1,256,200,304]{3,2,1,0} add(f32[1,256,200,304]{3,2,1,0} %add.306, f32[1,256,200,304]{3,2,1,0} %add.305), metadata={op_type="Add" op_name="tower-pred-0/group0/block0/add"}
  ROOT %maximum.50 = f32[1,256,200,304]{3,2,1,0} maximum(f32[1,256,200,304]{3,2,1,0} %broadcast.834, f32[1,256,200,304]{3,2,1,0} %add.304), metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/output"}
}

%fused_computation.128 (param_0.241: f32[256]) -> f32[256] {
  %param_0.241 = f32[256]{0} parameter(0)
  %constant_305 = f32[] constant(1.001e-05)
  %broadcast.842 = f32[256]{0} broadcast(f32[] %constant_305), dimensions={}
  %add.307 = f32[256]{0} add(f32[256]{0} %param_0.241, f32[256]{0} %broadcast.842), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
  ROOT %rsqrt.154 = f32[256]{0} rsqrt(f32[256]{0} %add.307), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
}

%fused_computation.129 (param_0.243: f32[256]) -> f32[256] {
  %param_0.243 = f32[256]{0} parameter(0)
  %constant_303 = f32[] constant(1.001e-05)
  %broadcast.840 = f32[256]{0} broadcast(f32[] %constant_303), dimensions={}
  %add.308 = f32[256]{0} add(f32[256]{0} %param_0.243, f32[256]{0} %broadcast.840), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
  ROOT %rsqrt.155 = f32[256]{0} rsqrt(f32[256]{0} %add.308), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
}

%fused_computation.130 (param_0.307: f32[64], param_1.394: f32[64], param_2.303: f32[64], param_3.349: f32[1,64,200,304], param_4.306: f32[64]) -> f32[1,64,200,304] {
  %constant_299 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.835 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[] %constant_299), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv2/Relu"}
  %param_3.349 = f32[1,64,200,304]{3,2,1,0} parameter(3)
  %param_4.306 = f32[64]{0} parameter(4)
  %broadcast.710 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_4.306), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
  %subtract.128 = f32[1,64,200,304]{3,2,1,0} subtract(f32[1,64,200,304]{3,2,1,0} %param_3.349, f32[1,64,200,304]{3,2,1,0} %broadcast.710), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
  %param_2.303 = f32[64]{0} parameter(2)
  %broadcast.709 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_2.303), dimensions={1}
  %multiply.230 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %subtract.128, f32[1,64,200,304]{3,2,1,0} %broadcast.709), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
  %param_1.394 = f32[64]{0} parameter(1)
  %broadcast.708 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_1.394), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
  %multiply.229 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %multiply.230, f32[1,64,200,304]{3,2,1,0} %broadcast.708), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
  %param_0.307 = f32[64]{0} parameter(0)
  %broadcast.707 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_0.307), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
  %add.309 = f32[1,64,200,304]{3,2,1,0} add(f32[1,64,200,304]{3,2,1,0} %multiply.229, f32[1,64,200,304]{3,2,1,0} %broadcast.707), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
  ROOT %maximum.51 = f32[1,64,200,304]{3,2,1,0} maximum(f32[1,64,200,304]{3,2,1,0} %broadcast.835, f32[1,64,200,304]{3,2,1,0} %add.309), metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv2/Relu"}
}

%fused_computation.131 (param_0.246: f32[64]) -> f32[64] {
  %param_0.246 = f32[64]{0} parameter(0)
  %constant_302 = f32[] constant(1.001e-05)
  %broadcast.838 = f32[64]{0} broadcast(f32[] %constant_302), dimensions={}
  %add.310 = f32[64]{0} add(f32[64]{0} %param_0.246, f32[64]{0} %broadcast.838), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
  ROOT %rsqrt.156 = f32[64]{0} rsqrt(f32[64]{0} %add.310), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
}

%fused_computation.132 (param_0.308: f32[64], param_1.395: f32[64], param_2.304: f32[64], param_3.350: f32[1,64,200,304], param_4.307: f32[64]) -> f32[1,64,200,304] {
  %constant_300 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.836 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[] %constant_300), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv2/Relu"}
  %param_3.350 = f32[1,64,200,304]{3,2,1,0} parameter(3)
  %param_4.307 = f32[64]{0} parameter(4)
  %broadcast.716 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_4.307), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
  %subtract.129 = f32[1,64,200,304]{3,2,1,0} subtract(f32[1,64,200,304]{3,2,1,0} %param_3.350, f32[1,64,200,304]{3,2,1,0} %broadcast.716), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
  %param_2.304 = f32[64]{0} parameter(2)
  %broadcast.715 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_2.304), dimensions={1}
  %multiply.232 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %subtract.129, f32[1,64,200,304]{3,2,1,0} %broadcast.715), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
  %param_1.395 = f32[64]{0} parameter(1)
  %broadcast.713 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_1.395), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
  %multiply.231 = f32[1,64,200,304]{3,2,1,0} multiply(f32[1,64,200,304]{3,2,1,0} %multiply.232, f32[1,64,200,304]{3,2,1,0} %broadcast.713), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
  %param_0.308 = f32[64]{0} parameter(0)
  %broadcast.712 = f32[1,64,200,304]{3,2,1,0} broadcast(f32[64]{0} %param_0.308), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
  %add.311 = f32[1,64,200,304]{3,2,1,0} add(f32[1,64,200,304]{3,2,1,0} %multiply.231, f32[1,64,200,304]{3,2,1,0} %broadcast.712), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
  ROOT %maximum.52 = f32[1,64,200,304]{3,2,1,0} maximum(f32[1,64,200,304]{3,2,1,0} %broadcast.836, f32[1,64,200,304]{3,2,1,0} %add.311), metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv1/Relu"}
}

%fused_computation.133 (param_0.249: f32[64]) -> f32[64] {
  %param_0.249 = f32[64]{0} parameter(0)
  %constant_301 = f32[] constant(1.001e-05)
  %broadcast.837 = f32[64]{0} broadcast(f32[] %constant_301), dimensions={}
  %add.312 = f32[64]{0} add(f32[64]{0} %param_0.249, f32[64]{0} %broadcast.837), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
  ROOT %rsqrt.157 = f32[64]{0} rsqrt(f32[64]{0} %add.312), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
}

%max_F32.638 (lhs.639: f32[], rhs.640: f32[]) -> f32[] {
  %lhs.639 = f32[] parameter(0)
  %rhs.640 = f32[] parameter(1)
  ROOT %maximum.641 = f32[] maximum(f32[] %lhs.639, f32[] %rhs.640)
}

%fused_computation.134 (param_0.254: f32[64], param_1.291: f32[64], param_2.246: f32[64], param_3.292: f32[1,64,400,608], param_4.254: f32[64]) -> f32[1,64,200,304] {
  %constant_205 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %broadcast.722 = f32[1,64,400,608]{3,2,1,0} broadcast(f32[] %constant_205), dimensions={}, metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %param_3.292 = f32[1,64,400,608]{3,2,1,0} parameter(3)
  %param_4.254 = f32[64]{0} parameter(4)
  %broadcast.720 = f32[1,64,400,608]{3,2,1,0} broadcast(f32[64]{0} %param_4.254), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
  %subtract.130 = f32[1,64,400,608]{3,2,1,0} subtract(f32[1,64,400,608]{3,2,1,0} %param_3.292, f32[1,64,400,608]{3,2,1,0} %broadcast.720), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
  %param_2.246 = f32[64]{0} parameter(2)
  %broadcast.719 = f32[1,64,400,608]{3,2,1,0} broadcast(f32[64]{0} %param_2.246), dimensions={1}
  %multiply.234 = f32[1,64,400,608]{3,2,1,0} multiply(f32[1,64,400,608]{3,2,1,0} %subtract.130, f32[1,64,400,608]{3,2,1,0} %broadcast.719), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
  %param_1.291 = f32[64]{0} parameter(1)
  %broadcast.718 = f32[1,64,400,608]{3,2,1,0} broadcast(f32[64]{0} %param_1.291), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
  %multiply.233 = f32[1,64,400,608]{3,2,1,0} multiply(f32[1,64,400,608]{3,2,1,0} %multiply.234, f32[1,64,400,608]{3,2,1,0} %broadcast.718), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
  %param_0.254 = f32[64]{0} parameter(0)
  %broadcast.717 = f32[1,64,400,608]{3,2,1,0} broadcast(f32[64]{0} %param_0.254), dimensions={1}, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
  %add.313 = f32[1,64,400,608]{3,2,1,0} add(f32[1,64,400,608]{3,2,1,0} %multiply.233, f32[1,64,400,608]{3,2,1,0} %broadcast.717), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
  %maximum.53 = f32[1,64,400,608]{3,2,1,0} maximum(f32[1,64,400,608]{3,2,1,0} %broadcast.722, f32[1,64,400,608]{3,2,1,0} %add.313), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  %pad.15 = f32[1,64,401,609]{3,2,1,0} pad(f32[1,64,400,608]{3,2,1,0} %maximum.53, f32[] %constant_205), padding=0_0x0_0x1_0x1_0, metadata={op_type="Pad" op_name="tower-pred-0/Pad_1"}
  %constant_204 = f32[] constant(-inf), metadata={op_type="MaxPool" op_name="tower-pred-0/pool0/MaxPool"}
  ROOT %reduce-window.0 = f32[1,64,200,304]{3,2,1,0} reduce-window(f32[1,64,401,609]{3,2,1,0} %pad.15, f32[] %constant_204), window={size=1x1x3x3 stride=1x1x2x2}, to_apply=%max_F32.638, metadata={op_type="MaxPool" op_name="tower-pred-0/pool0/MaxPool"}
}

%fused_computation.135 (param_0.256: f32[64]) -> f32[64] {
  %param_0.256 = f32[64]{0} parameter(0)
  %constant_304 = f32[] constant(1.001e-05)
  %broadcast.841 = f32[64]{0} broadcast(f32[] %constant_304), dimensions={}
  %add.314 = f32[64]{0} add(f32[64]{0} %param_0.256, f32[64]{0} %broadcast.841), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
  ROOT %rsqrt.158 = f32[64]{0} rsqrt(f32[64]{0} %add.314), metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
}

%fused_computation.136 (param_0.259: f32[3], param_1.297: f32[800,1199,3], param_2.251: f32[3]) -> f32[1,3,805,1221] {
  %param_2.251 = f32[3]{0} parameter(2)
  %broadcast.724 = f32[1,800,1199,3]{2,1,3,0} broadcast(f32[3]{0} %param_2.251), dimensions={3}, metadata={op_type="Mul" op_name="tower-pred-0/image_preprocess/mul"}
  %param_1.297 = f32[800,1199,3]{2,1,0} parameter(1)
  %copy.169 = f32[800,1199,3]{1,0,2} copy(f32[800,1199,3]{2,1,0} %param_1.297), metadata={op_name="XLA_Args"}
  %bitcast.63 = f32[1,800,1199,3]{2,1,3,0} bitcast(f32[800,1199,3]{1,0,2} %copy.169), metadata={op_type="ExpandDims" op_name="tower-pred-0/ExpandDims_1"}
  %param_0.259 = f32[3]{0} parameter(0)
  %broadcast.723 = f32[1,800,1199,3]{2,1,3,0} broadcast(f32[3]{0} %param_0.259), dimensions={3}
  %add.315 = f32[1,800,1199,3]{2,1,3,0} add(f32[1,800,1199,3]{2,1,3,0} %bitcast.63, f32[1,800,1199,3]{2,1,3,0} %broadcast.723), metadata={op_type="Sub" op_name="tower-pred-0/image_preprocess/sub"}
  %multiply.235 = f32[1,800,1199,3]{2,1,3,0} multiply(f32[1,800,1199,3]{2,1,3,0} %broadcast.724, f32[1,800,1199,3]{2,1,3,0} %add.315), metadata={op_type="Mul" op_name="tower-pred-0/image_preprocess/mul"}
  %bitcast.62 = f32[1,3,800,1199]{3,2,1,0} bitcast(f32[1,800,1199,3]{2,1,3,0} %multiply.235), metadata={op_type="Transpose" op_name="tower-pred-0/transpose"}
  %constant_206 = f32[] constant(0), metadata={op_type="Relu" op_name="tower-pred-0/conv0/Relu"}
  ROOT %pad.16 = f32[1,3,805,1221]{3,2,1,0} pad(f32[1,3,800,1199]{3,2,1,0} %bitcast.62, f32[] %constant_206), padding=0_0x0_0x3_2x3_19
}

ENTRY %cluster_28__XlaCompiledKernel_true__XlaNumConstantArgs_27__XlaNumResourceArgs_0_.1699 (arg0.1: f32[7,7,3,64], arg1.2: f32[64], arg2.3: f32[64], arg3.4: f32[64], arg4.5: f32[64], arg5.6: f32[1,1,64,256], arg6.7: f32[1,1,64,64], arg7.8: f32[256], arg8.9: f32[256], arg9.10: f32[256], arg10.11: f32[256], arg11.12: f32[64], arg12.13: f32[64], arg13.14: f32[64], arg14.15: f32[64], arg15.16: f32[3,3,64,64], arg16.17: f32[64], arg17.18: f32[64], arg18.19: f32[64], arg19.20: f32[64], arg20.21: f32[1,1,64,256], arg21.22: f32[256], arg22.23: f32[256], arg23.24: f32[256], arg24.25: f32[256], arg25.26: f32[1,1,256,64], arg26.27: f32[64], arg27.28: f32[64], arg28.29: f32[64], arg29.30: f32[64], arg30.31: f32[3,3,64,64], arg31.32: f32[64], arg32.33: f32[64], arg33.34: f32[64], arg34.35: f32[64], arg35.36: f32[1,1,64,256], arg36.37: f32[256], arg37.38: f32[256], arg38.39: f32[256], arg39.40: f32[256], arg40.41: f32[1,1,256,64], arg41.42: f32[64], arg42.43: f32[64], arg43.44: f32[64], arg44.45: f32[64], arg45.46: f32[3,3,64,64], arg46.47: f32[64], arg47.48: f32[64], arg48.49: f32[64], arg49.50: f32[64], arg50.51: f32[1,1,64,256], arg51.52: f32[256], arg52.53: f32[256], arg53.54: f32[256], arg54.55: f32[256], arg55.56: f32[1,1,256,512], arg56.57: f32[1,1,256,128], arg57.58: f32[1,1,256,256], arg58.59: f32[512], arg59.60: f32[512], arg60.61: f32[512], arg61.62: f32[512], arg62.63: f32[128], arg63.64: f32[128], arg64.65: f32[128], arg65.66: f32[128], arg66.67: f32[256], arg67.68: f32[3,3,128,128], arg68.69: f32[128], arg69.70: f32[128], arg70.71: f32[128], arg71.72: f32[128], arg72.73: f32[1,1,128,512], arg73.74: f32[512], arg74.75: f32[512], arg75.76: f32[512], arg76.77: f32[512], arg77.78: f32[1,1,512,128], arg78.79: f32[128], arg79.80: f32[128], arg80.81: f32[128], arg81.82: f32[128], arg82.83: f32[3,3,128,128], arg83.84: f32[128], arg84.85: f32[128], arg85.86: f32[128], arg86.87: f32[128], arg87.88: f32[1,1,128,512], arg88.89: f32[512], arg89.90: f32[512], arg90.91: f32[512], arg91.92: f32[512], arg92.93: f32[1,1,512,128], arg93.94: f32[128], arg94.95: f32[128], arg95.96: f32[128], arg96.97: f32[128], arg97.98: f32[3,3,128,128], arg98.99: f32[128], arg99.100: f32[128], arg100.101: f32[128], arg101.102: f32[128], arg102.103: f32[1,1,128,512], arg103.104: f32[512], arg104.105: f32[512], arg105.106: f32[512], arg106.107: f32[512], arg107.108: f32[1,1,512,128], arg108.109: f32[128], arg109.110: f32[128], arg110.111: f32[128], arg111.112: f32[128], arg112.113: f32[3,3,128,128], arg113.114: f32[128], arg114.115: f32[128], arg115.116: f32[128], arg116.117: f32[128], arg117.118: f32[1,1,128,512], arg118.119: f32[512], arg119.120: f32[512], arg120.121: f32[512], arg121.122: f32[512], arg122.123: f32[1,1,512,1024], arg123.124: f32[1,1,512,256], arg124.125: f32[1,1,512,256], arg125.126: f32[1024], arg126.127: f32[1024], arg127.128: f32[1024], arg128.129: f32[1024], arg129.130: f32[256], arg130.131: f32[256], arg131.132: f32[256], arg132.133: f32[256], arg133.134: f32[256], arg134.135: f32[3,3,256,256], arg135.136: f32[256], arg136.137: f32[256], arg137.138: f32[256], arg138.139: f32[256], arg139.140: f32[1,1,256,1024], arg140.141: f32[1024], arg141.142: f32[1024], arg142.143: f32[1024], arg143.144: f32[1024], arg144.145: f32[1,1,1024,256], arg145.146: f32[256], arg146.147: f32[256], arg147.148: f32[256], arg148.149: f32[256], arg149.150: f32[3,3,256,256], arg150.151: f32[256], arg151.152: f32[256], arg152.153: f32[256], arg153.154: f32[256], arg154.155: f32[1,1,256,1024], arg155.156: f32[1024], arg156.157: f32[1024], arg157.158: f32[1024], arg158.159: f32[1024], arg159.160: f32[1,1,1024,256], arg160.161: f32[256], arg161.162: f32[256], arg162.163: f32[256], arg163.164: f32[256], arg164.165: f32[3,3,256,256], arg165.166: f32[256], arg166.167: f32[256], arg167.168: f32[256], arg168.169: f32[256], arg169.170: f32[1,1,256,1024], arg170.171: f32[1024], arg171.172: f32[1024], arg172.173: f32[1024], arg173.174: f32[1024], arg174.175: f32[1,1,1024,256], arg175.176: f32[256], arg176.177: f32[256], arg177.178: f32[256], arg178.179: f32[256], arg179.180: f32[3,3,256,256], arg180.181: f32[256], arg181.182: f32[256], arg182.183: f32[256], arg183.184: f32[256], arg184.185: f32[1,1,256,1024], arg185.186: f32[1024], arg186.187: f32[1024], arg187.188: f32[1024], arg188.189: f32[1024], arg189.190: f32[1,1,1024,256], arg190.191: f32[256], arg191.192: f32[256], arg192.193: f32[256], arg193.194: f32[256], arg194.195: f32[3,3,256,256], arg195.196: f32[256], arg196.197: f32[256], arg197.198: f32[256], arg198.199: f32[256], arg199.200: f32[1,1,256,1024], arg200.201: f32[1024], arg201.202: f32[1024], arg202.203: f32[1024], arg203.204: f32[1024], arg204.205: f32[1,1,1024,256], arg205.206: f32[256], arg206.207: f32[256], arg207.208: f32[256], arg208.209: f32[256], arg209.210: f32[3,3,256,256], arg210.211: f32[256], arg211.212: f32[256], arg212.213: f32[256], arg213.214: f32[256], arg214.215: f32[1,1,256,1024], arg215.216: f32[1024], arg216.217: f32[1024], arg217.218: f32[1024], arg218.219: f32[1024], arg219.220: f32[1,1,1024,2048], arg220.221: f32[1,1,1024,512], arg221.222: f32[1,1,1024,256], arg222.223: f32[2048], arg223.224: f32[2048], arg224.225: f32[2048], arg225.226: f32[2048], arg226.227: f32[512], arg227.228: f32[512], arg228.229: f32[512], arg229.230: f32[512], arg230.231: f32[256], arg231.232: f32[3,3,512,512], arg232.233: f32[512], arg233.234: f32[512], arg234.235: f32[512], arg235.236: f32[512], arg236.237: f32[1,1,512,2048], arg237.238: f32[2048], arg238.239: f32[2048], arg239.240: f32[2048], arg240.241: f32[2048], arg241.242: f32[1,1,2048,512], arg242.243: f32[512], arg243.244: f32[512], arg244.245: f32[512], arg245.246: f32[512], arg246.247: f32[3,3,512,512], arg247.248: f32[512], arg248.249: f32[512], arg249.250: f32[512], arg250.251: f32[512], arg251.252: f32[1,1,512,2048], arg252.253: f32[2048], arg253.254: f32[2048], arg254.255: f32[2048], arg255.256: f32[2048], arg256.257: f32[1,1,2048,512], arg257.258: f32[512], arg258.259: f32[512], arg259.260: f32[512], arg260.261: f32[512], arg261.262: f32[3,3,512,512], arg262.263: f32[512], arg263.264: f32[512], arg264.265: f32[512], arg265.266: f32[512], arg266.267: f32[1,1,512,2048], arg267.268: f32[2048], arg268.269: f32[2048], arg269.270: f32[2048], arg270.271: f32[2048], arg271.272: f32[1,1,2048,256], arg272.273: f32[256], arg273.274: f32[3,3,256,256], arg274.275: f32[256], arg275.276: f32[3,3,256,256], arg276.277: f32[256], arg277.278: f32[1,1,256,3], arg278.279: f32[1,1,256,12], arg279.280: f32[3], arg280.281: f32[12], arg281.282: f32[3,3,256,256], arg282.283: f32[256], arg283.284: f32[3,3,256,256], arg284.285: f32[256], arg285.286: f32[3,3,256,256], arg286.287: f32[256], arg287.288: f32[800,1199,3]) -> (f32[1,256,25,38], f32[1,256,50,76], f32[1000,4], f32[1000], f32[741,4], f32[741], f32[1,256,100,152], f32[1000,4], f32[1000], f32[1,256,200,304], f32[1000,4], f32[1000], f32[1000,4], f32[1000]) {
  %arg223.224 = f32[2048]{0} parameter(223), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg222.223 = f32[2048]{0} parameter(222), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg225.226 = f32[2048]{0} parameter(225), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.47 = f32[2048]{0} fusion(f32[2048]{0} %arg225.226), kind=kLoop, calls=%fused_computation.47, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/convshortcut/bn/FusedBatchNorm"}
  %arg126.127 = f32[1024]{0} parameter(126), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg125.126 = f32[1024]{0} parameter(125), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg128.129 = f32[1024]{0} parameter(128), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.84 = f32[1024]{0} fusion(f32[1024]{0} %arg128.129), kind=kLoop, calls=%fused_computation.84, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/convshortcut/bn/FusedBatchNorm"}
  %arg59.60 = f32[512]{0} parameter(59), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg58.59 = f32[512]{0} parameter(58), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg61.62 = f32[512]{0} parameter(61), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.109 = f32[512]{0} fusion(f32[512]{0} %arg61.62), kind=kLoop, calls=%fused_computation.109, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/convshortcut/bn/FusedBatchNorm"}
  %arg8.9 = f32[256]{0} parameter(8), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg7.8 = f32[256]{0} parameter(7), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg10.11 = f32[256]{0} parameter(10), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.128 = f32[256]{0} fusion(f32[256]{0} %arg10.11), kind=kLoop, calls=%fused_computation.128, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/convshortcut/bn/FusedBatchNorm"}
  %arg2.3 = f32[64]{0} parameter(2), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg1.2 = f32[64]{0} parameter(1), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg4.5 = f32[64]{0} parameter(4), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.135 = f32[64]{0} fusion(f32[64]{0} %arg4.5), kind=kLoop, calls=%fused_computation.135, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/conv0/bn/FusedBatchNorm"}
  %constant_66 = f32[3]{0} constant({-103.53, -116.28, -123.675})
  %arg287.288 = f32[800,1199,3]{2,1,0} parameter(287), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %constant_581 = f32[3]{0} constant({0.0174292, 0.017507, 0.0171248}), metadata={op_type="Mul" op_name="tower-pred-0/image_preprocess/mul"}
  %fusion.136 = f32[1,3,805,1221]{3,2,1,0} fusion(f32[3]{0} %constant_66, f32[800,1199,3]{2,1,0} %arg287.288, f32[3]{0} %constant_581), kind=kLoop, calls=%fused_computation.136
  %arg0.1 = f32[7,7,3,64]{3,2,1,0} parameter(0), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.83 = f32[7,7,3,64]{1,0,2,3} copy(f32[7,7,3,64]{3,2,1,0} %arg0.1), metadata={op_name="XLA_Args"}
  %custom-call.157 = (f32[1,64,400,608]{3,2,1,0}, u8[1459208]{0}) custom-call(f32[1,3,805,1221]{3,2,1,0} %fusion.136, f32[7,7,3,64]{1,0,2,3} %copy.83), window={size=7x7 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/conv0/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.153 = f32[1,64,400,608]{3,2,1,0} get-tuple-element((f32[1,64,400,608]{3,2,1,0}, u8[1459208]{0}) %custom-call.157), index=0
  %arg3.4 = f32[64]{0} parameter(3), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.134 = f32[1,64,200,304]{3,2,1,0} fusion(f32[64]{0} %arg2.3, f32[64]{0} %arg1.2, f32[64]{0} %fusion.135, f32[1,64,400,608]{3,2,1,0} %get-tuple-element.153, f32[64]{0} %arg3.4), kind=kLoop, calls=%fused_computation.134, metadata={op_type="MaxPool" op_name="tower-pred-0/pool0/MaxPool"}
  %arg5.6 = f32[1,1,64,256]{3,2,1,0} parameter(5), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.87 = f32[1,1,64,256]{1,0,2,3} copy(f32[1,1,64,256]{3,2,1,0} %arg5.6), metadata={op_name="XLA_Args"}
  %custom-call.88 = (f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,64,200,304]{3,2,1,0} %fusion.134, f32[1,1,64,256]{1,0,2,3} %copy.87), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block0/convshortcut/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.84 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.88), index=0
  %arg9.10 = f32[256]{0} parameter(9), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg22.23 = f32[256]{0} parameter(22), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg21.22 = f32[256]{0} parameter(21), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg24.25 = f32[256]{0} parameter(24), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.129 = f32[256]{0} fusion(f32[256]{0} %arg24.25), kind=kLoop, calls=%fused_computation.129, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv3/bn/FusedBatchNorm"}
  %arg17.18 = f32[64]{0} parameter(17), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg16.17 = f32[64]{0} parameter(16), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg19.20 = f32[64]{0} parameter(19), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.131 = f32[64]{0} fusion(f32[64]{0} %arg19.20), kind=kLoop, calls=%fused_computation.131, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv2/bn/FusedBatchNorm"}
  %arg12.13 = f32[64]{0} parameter(12), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg11.12 = f32[64]{0} parameter(11), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg14.15 = f32[64]{0} parameter(14), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.133 = f32[64]{0} fusion(f32[64]{0} %arg14.15), kind=kLoop, calls=%fused_computation.133, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block0/conv1/bn/FusedBatchNorm"}
  %arg6.7 = f32[1,1,64,64]{3,2,1,0} parameter(6), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.84 = f32[1,1,64,64]{1,0,2,3} copy(f32[1,1,64,64]{3,2,1,0} %arg6.7), metadata={op_name="XLA_Args"}
  %custom-call.85 = (f32[1,64,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,64,200,304]{3,2,1,0} %fusion.134, f32[1,1,64,64]{1,0,2,3} %copy.84), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block0/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.81 = f32[1,64,200,304]{3,2,1,0} get-tuple-element((f32[1,64,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.85), index=0
  %arg13.14 = f32[64]{0} parameter(13), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.132 = f32[1,64,200,304]{3,2,1,0} fusion(f32[64]{0} %arg12.13, f32[64]{0} %arg11.12, f32[64]{0} %fusion.133, f32[1,64,200,304]{3,2,1,0} %get-tuple-element.81, f32[64]{0} %arg13.14), kind=kLoop, calls=%fused_computation.132, metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv1/Relu"}
  %arg15.16 = f32[3,3,64,64]{3,2,1,0} parameter(15), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.85 = f32[3,3,64,64]{1,0,2,3} copy(f32[3,3,64,64]{3,2,1,0} %arg15.16), metadata={op_name="XLA_Args"}
  %custom-call.86 = (f32[1,64,200,304]{3,2,1,0}, u8[409856]{0}) custom-call(f32[1,64,200,304]{3,2,1,0} %fusion.132, f32[3,3,64,64]{1,0,2,3} %copy.85), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block0/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.82 = f32[1,64,200,304]{3,2,1,0} get-tuple-element((f32[1,64,200,304]{3,2,1,0}, u8[409856]{0}) %custom-call.86), index=0
  %arg18.19 = f32[64]{0} parameter(18), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.130 = f32[1,64,200,304]{3,2,1,0} fusion(f32[64]{0} %arg17.18, f32[64]{0} %arg16.17, f32[64]{0} %fusion.131, f32[1,64,200,304]{3,2,1,0} %get-tuple-element.82, f32[64]{0} %arg18.19), kind=kLoop, calls=%fused_computation.130, metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/conv2/Relu"}
  %arg20.21 = f32[1,1,64,256]{3,2,1,0} parameter(20), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.86 = f32[1,1,64,256]{1,0,2,3} copy(f32[1,1,64,256]{3,2,1,0} %arg20.21), metadata={op_name="XLA_Args"}
  %custom-call.87 = (f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,64,200,304]{3,2,1,0} %fusion.130, f32[1,1,64,256]{1,0,2,3} %copy.86), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block0/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.83 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.87), index=0
  %arg23.24 = f32[256]{0} parameter(23), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.127 = f32[1,256,200,304]{3,2,1,0} fusion(f32[256]{0} %arg8.9, f32[256]{0} %arg7.8, f32[256]{0} %fusion.128, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.84, f32[256]{0} %arg9.10, f32[256]{0} %arg22.23, f32[256]{0} %arg21.22, f32[256]{0} %fusion.129, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.83, f32[256]{0} %arg23.24), kind=kLoop, calls=%fused_computation.127, metadata={op_type="Relu" op_name="tower-pred-0/group0/block0/output"}
  %arg37.38 = f32[256]{0} parameter(37), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg36.37 = f32[256]{0} parameter(36), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg39.40 = f32[256]{0} parameter(39), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.122 = f32[256]{0} fusion(f32[256]{0} %arg39.40), kind=kLoop, calls=%fused_computation.122, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv3/bn/FusedBatchNorm"}
  %arg32.33 = f32[64]{0} parameter(32), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg31.32 = f32[64]{0} parameter(31), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg34.35 = f32[64]{0} parameter(34), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.124 = f32[64]{0} fusion(f32[64]{0} %arg34.35), kind=kLoop, calls=%fused_computation.124, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv2/bn/FusedBatchNorm"}
  %arg27.28 = f32[64]{0} parameter(27), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg26.27 = f32[64]{0} parameter(26), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg29.30 = f32[64]{0} parameter(29), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.126 = f32[64]{0} fusion(f32[64]{0} %arg29.30), kind=kLoop, calls=%fused_computation.126, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block1/conv1/bn/FusedBatchNorm"}
  %arg25.26 = f32[1,1,256,64]{3,2,1,0} parameter(25), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.88 = f32[1,1,256,64]{1,0,2,3} copy(f32[1,1,256,64]{3,2,1,0} %arg25.26), metadata={op_name="XLA_Args"}
  %custom-call.89 = (f32[1,64,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %fusion.127, f32[1,1,256,64]{1,0,2,3} %copy.88), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block1/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.85 = f32[1,64,200,304]{3,2,1,0} get-tuple-element((f32[1,64,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.89), index=0
  %arg28.29 = f32[64]{0} parameter(28), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.125 = f32[1,64,200,304]{3,2,1,0} fusion(f32[64]{0} %arg27.28, f32[64]{0} %arg26.27, f32[64]{0} %fusion.126, f32[1,64,200,304]{3,2,1,0} %get-tuple-element.85, f32[64]{0} %arg28.29), kind=kLoop, calls=%fused_computation.125, metadata={op_type="Relu" op_name="tower-pred-0/group0/block1/conv1/Relu"}
  %arg30.31 = f32[3,3,64,64]{3,2,1,0} parameter(30), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.89 = f32[3,3,64,64]{1,0,2,3} copy(f32[3,3,64,64]{3,2,1,0} %arg30.31), metadata={op_name="XLA_Args"}
  %custom-call.90 = (f32[1,64,200,304]{3,2,1,0}, u8[409856]{0}) custom-call(f32[1,64,200,304]{3,2,1,0} %fusion.125, f32[3,3,64,64]{1,0,2,3} %copy.89), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block1/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.86 = f32[1,64,200,304]{3,2,1,0} get-tuple-element((f32[1,64,200,304]{3,2,1,0}, u8[409856]{0}) %custom-call.90), index=0
  %arg33.34 = f32[64]{0} parameter(33), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.123 = f32[1,64,200,304]{3,2,1,0} fusion(f32[64]{0} %arg32.33, f32[64]{0} %arg31.32, f32[64]{0} %fusion.124, f32[1,64,200,304]{3,2,1,0} %get-tuple-element.86, f32[64]{0} %arg33.34), kind=kLoop, calls=%fused_computation.123, metadata={op_type="Relu" op_name="tower-pred-0/group0/block1/conv2/Relu"}
  %arg35.36 = f32[1,1,64,256]{3,2,1,0} parameter(35), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.90 = f32[1,1,64,256]{1,0,2,3} copy(f32[1,1,64,256]{3,2,1,0} %arg35.36), metadata={op_name="XLA_Args"}
  %custom-call.91 = (f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,64,200,304]{3,2,1,0} %fusion.123, f32[1,1,64,256]{1,0,2,3} %copy.90), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block1/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.87 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.91), index=0
  %arg38.39 = f32[256]{0} parameter(38), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.121 = f32[1,256,200,304]{3,2,1,0} fusion(f32[1,256,200,304]{3,2,1,0} %fusion.127, f32[256]{0} %arg37.38, f32[256]{0} %arg36.37, f32[256]{0} %fusion.122, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.87, f32[256]{0} %arg38.39), kind=kLoop, calls=%fused_computation.121, metadata={op_type="Relu" op_name="tower-pred-0/group0/block1/output"}
  %arg52.53 = f32[256]{0} parameter(52), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg51.52 = f32[256]{0} parameter(51), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg54.55 = f32[256]{0} parameter(54), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.116 = f32[256]{0} fusion(f32[256]{0} %arg54.55), kind=kLoop, calls=%fused_computation.116, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv3/bn/FusedBatchNorm"}
  %arg47.48 = f32[64]{0} parameter(47), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg46.47 = f32[64]{0} parameter(46), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg49.50 = f32[64]{0} parameter(49), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.118 = f32[64]{0} fusion(f32[64]{0} %arg49.50), kind=kLoop, calls=%fused_computation.118, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv2/bn/FusedBatchNorm"}
  %arg42.43 = f32[64]{0} parameter(42), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg41.42 = f32[64]{0} parameter(41), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg44.45 = f32[64]{0} parameter(44), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.120 = f32[64]{0} fusion(f32[64]{0} %arg44.45), kind=kLoop, calls=%fused_computation.120, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group0/block2/conv1/bn/FusedBatchNorm"}
  %arg40.41 = f32[1,1,256,64]{3,2,1,0} parameter(40), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.91 = f32[1,1,256,64]{1,0,2,3} copy(f32[1,1,256,64]{3,2,1,0} %arg40.41), metadata={op_name="XLA_Args"}
  %custom-call.92 = (f32[1,64,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %fusion.121, f32[1,1,256,64]{1,0,2,3} %copy.91), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block2/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.88 = f32[1,64,200,304]{3,2,1,0} get-tuple-element((f32[1,64,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.92), index=0
  %arg43.44 = f32[64]{0} parameter(43), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.119 = f32[1,64,200,304]{3,2,1,0} fusion(f32[64]{0} %arg42.43, f32[64]{0} %arg41.42, f32[64]{0} %fusion.120, f32[1,64,200,304]{3,2,1,0} %get-tuple-element.88, f32[64]{0} %arg43.44), kind=kLoop, calls=%fused_computation.119, metadata={op_type="Relu" op_name="tower-pred-0/group0/block2/conv1/Relu"}
  %arg45.46 = f32[3,3,64,64]{3,2,1,0} parameter(45), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.92 = f32[3,3,64,64]{1,0,2,3} copy(f32[3,3,64,64]{3,2,1,0} %arg45.46), metadata={op_name="XLA_Args"}
  %custom-call.93 = (f32[1,64,200,304]{3,2,1,0}, u8[409856]{0}) custom-call(f32[1,64,200,304]{3,2,1,0} %fusion.119, f32[3,3,64,64]{1,0,2,3} %copy.92), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block2/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.89 = f32[1,64,200,304]{3,2,1,0} get-tuple-element((f32[1,64,200,304]{3,2,1,0}, u8[409856]{0}) %custom-call.93), index=0
  %arg48.49 = f32[64]{0} parameter(48), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.117 = f32[1,64,200,304]{3,2,1,0} fusion(f32[64]{0} %arg47.48, f32[64]{0} %arg46.47, f32[64]{0} %fusion.118, f32[1,64,200,304]{3,2,1,0} %get-tuple-element.89, f32[64]{0} %arg48.49), kind=kLoop, calls=%fused_computation.117, metadata={op_type="Relu" op_name="tower-pred-0/group0/block2/conv2/Relu"}
  %arg50.51 = f32[1,1,64,256]{3,2,1,0} parameter(50), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.93 = f32[1,1,64,256]{1,0,2,3} copy(f32[1,1,64,256]{3,2,1,0} %arg50.51), metadata={op_name="XLA_Args"}
  %custom-call.94 = (f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,64,200,304]{3,2,1,0} %fusion.117, f32[1,1,64,256]{1,0,2,3} %copy.93), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group0/block2/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.90 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.94), index=0
  %arg53.54 = f32[256]{0} parameter(53), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.115 = f32[1,256,200,304]{3,2,1,0} fusion(f32[1,256,200,304]{3,2,1,0} %fusion.121, f32[256]{0} %arg52.53, f32[256]{0} %arg51.52, f32[256]{0} %fusion.116, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.90, f32[256]{0} %arg53.54), kind=kLoop, calls=%fused_computation.115, metadata={op_type="Relu" op_name="tower-pred-0/group0/block2/output"}
  %arg55.56 = f32[1,1,256,512]{3,2,1,0} parameter(55), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.97 = f32[1,1,256,512]{1,0,2,3} copy(f32[1,1,256,512]{3,2,1,0} %arg55.56), metadata={op_name="XLA_Args"}
  %custom-call.98 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %fusion.115, f32[1,1,256,512]{1,0,2,3} %copy.97), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block0/convshortcut/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.94 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.98), index=0
  %arg60.61 = f32[512]{0} parameter(60), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg74.75 = f32[512]{0} parameter(74), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg73.74 = f32[512]{0} parameter(73), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg76.77 = f32[512]{0} parameter(76), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.110 = f32[512]{0} fusion(f32[512]{0} %arg76.77), kind=kLoop, calls=%fused_computation.110, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv3/bn/FusedBatchNorm"}
  %arg69.70 = f32[128]{0} parameter(69), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg68.69 = f32[128]{0} parameter(68), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg71.72 = f32[128]{0} parameter(71), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.112 = f32[128]{0} fusion(f32[128]{0} %arg71.72), kind=kLoop, calls=%fused_computation.112, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv2/bn/FusedBatchNorm"}
  %arg63.64 = f32[128]{0} parameter(63), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg62.63 = f32[128]{0} parameter(62), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg65.66 = f32[128]{0} parameter(65), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.114 = f32[128]{0} fusion(f32[128]{0} %arg65.66), kind=kLoop, calls=%fused_computation.114, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block0/conv1/bn/FusedBatchNorm"}
  %arg56.57 = f32[1,1,256,128]{3,2,1,0} parameter(56), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.94 = f32[1,1,256,128]{1,0,2,3} copy(f32[1,1,256,128]{3,2,1,0} %arg56.57), metadata={op_name="XLA_Args"}
  %custom-call.96 = (f32[1,128,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %fusion.115, f32[1,1,256,128]{1,0,2,3} %copy.94), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block0/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.92 = f32[1,128,200,304]{3,2,1,0} get-tuple-element((f32[1,128,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.96), index=0
  %arg64.65 = f32[128]{0} parameter(64), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.113 = f32[1,128,201,305]{3,2,1,0} fusion(f32[128]{0} %arg63.64, f32[128]{0} %arg62.63, f32[128]{0} %fusion.114, f32[1,128,200,304]{3,2,1,0} %get-tuple-element.92, f32[128]{0} %arg64.65), kind=kLoop, calls=%fused_computation.113
  %arg67.68 = f32[3,3,128,128]{3,2,1,0} parameter(67), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.95 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg67.68), metadata={op_name="XLA_Args"}
  %custom-call.158 = (f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,128,201,305]{3,2,1,0} %fusion.113, f32[3,3,128,128]{1,0,2,3} %copy.95), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block0/conv2/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.154 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.158), index=0
  %arg70.71 = f32[128]{0} parameter(70), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.111 = f32[1,128,100,152]{3,2,1,0} fusion(f32[128]{0} %arg69.70, f32[128]{0} %arg68.69, f32[128]{0} %fusion.112, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.154, f32[128]{0} %arg70.71), kind=kLoop, calls=%fused_computation.111, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/conv2/Relu"}
  %arg72.73 = f32[1,1,128,512]{3,2,1,0} parameter(72), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.96 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %arg72.73), metadata={op_name="XLA_Args"}
  %custom-call.97 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.111, f32[1,1,128,512]{1,0,2,3} %copy.96), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block0/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.93 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.97), index=0
  %arg75.76 = f32[512]{0} parameter(75), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.108 = f32[1,512,100,152]{3,2,1,0} fusion(f32[512]{0} %arg59.60, f32[512]{0} %arg58.59, f32[512]{0} %fusion.109, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.94, f32[512]{0} %arg60.61, f32[512]{0} %arg74.75, f32[512]{0} %arg73.74, f32[512]{0} %fusion.110, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.93, f32[512]{0} %arg75.76), kind=kLoop, calls=%fused_computation.108, metadata={op_type="Relu" op_name="tower-pred-0/group1/block0/output"}
  %arg89.90 = f32[512]{0} parameter(89), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg88.89 = f32[512]{0} parameter(88), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg91.92 = f32[512]{0} parameter(91), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.103 = f32[512]{0} fusion(f32[512]{0} %arg91.92), kind=kLoop, calls=%fused_computation.103, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv3/bn/FusedBatchNorm"}
  %arg84.85 = f32[128]{0} parameter(84), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg83.84 = f32[128]{0} parameter(83), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg86.87 = f32[128]{0} parameter(86), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.105 = f32[128]{0} fusion(f32[128]{0} %arg86.87), kind=kLoop, calls=%fused_computation.105, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv2/bn/FusedBatchNorm"}
  %arg79.80 = f32[128]{0} parameter(79), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg78.79 = f32[128]{0} parameter(78), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg81.82 = f32[128]{0} parameter(81), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.107 = f32[128]{0} fusion(f32[128]{0} %arg81.82), kind=kLoop, calls=%fused_computation.107, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block1/conv1/bn/FusedBatchNorm"}
  %arg77.78 = f32[1,1,512,128]{3,2,1,0} parameter(77), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.98 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %arg77.78), metadata={op_name="XLA_Args"}
  %custom-call.99 = (f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %fusion.108, f32[1,1,512,128]{1,0,2,3} %copy.98), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block1/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.95 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.99), index=0
  %arg80.81 = f32[128]{0} parameter(80), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.106 = f32[1,128,100,152]{3,2,1,0} fusion(f32[128]{0} %arg79.80, f32[128]{0} %arg78.79, f32[128]{0} %fusion.107, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.95, f32[128]{0} %arg80.81), kind=kLoop, calls=%fused_computation.106, metadata={op_type="Relu" op_name="tower-pred-0/group1/block1/conv1/Relu"}
  %arg82.83 = f32[3,3,128,128]{3,2,1,0} parameter(82), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.99 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg82.83), metadata={op_name="XLA_Args"}
  %custom-call.100 = (f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.106, f32[3,3,128,128]{1,0,2,3} %copy.99), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block1/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.96 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) %custom-call.100), index=0
  %arg85.86 = f32[128]{0} parameter(85), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.104 = f32[1,128,100,152]{3,2,1,0} fusion(f32[128]{0} %arg84.85, f32[128]{0} %arg83.84, f32[128]{0} %fusion.105, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.96, f32[128]{0} %arg85.86), kind=kLoop, calls=%fused_computation.104, metadata={op_type="Relu" op_name="tower-pred-0/group1/block1/conv2/Relu"}
  %arg87.88 = f32[1,1,128,512]{3,2,1,0} parameter(87), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.100 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %arg87.88), metadata={op_name="XLA_Args"}
  %custom-call.101 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.104, f32[1,1,128,512]{1,0,2,3} %copy.100), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block1/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.97 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.101), index=0
  %arg90.91 = f32[512]{0} parameter(90), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.102 = f32[1,512,100,152]{3,2,1,0} fusion(f32[1,512,100,152]{3,2,1,0} %fusion.108, f32[512]{0} %arg89.90, f32[512]{0} %arg88.89, f32[512]{0} %fusion.103, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.97, f32[512]{0} %arg90.91), kind=kLoop, calls=%fused_computation.102, metadata={op_type="Relu" op_name="tower-pred-0/group1/block1/output"}
  %arg104.105 = f32[512]{0} parameter(104), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg103.104 = f32[512]{0} parameter(103), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg106.107 = f32[512]{0} parameter(106), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.97 = f32[512]{0} fusion(f32[512]{0} %arg106.107), kind=kLoop, calls=%fused_computation.97, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv3/bn/FusedBatchNorm"}
  %arg99.100 = f32[128]{0} parameter(99), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg98.99 = f32[128]{0} parameter(98), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg101.102 = f32[128]{0} parameter(101), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.99 = f32[128]{0} fusion(f32[128]{0} %arg101.102), kind=kLoop, calls=%fused_computation.99, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv2/bn/FusedBatchNorm"}
  %arg94.95 = f32[128]{0} parameter(94), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg93.94 = f32[128]{0} parameter(93), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg96.97 = f32[128]{0} parameter(96), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.101 = f32[128]{0} fusion(f32[128]{0} %arg96.97), kind=kLoop, calls=%fused_computation.101, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block2/conv1/bn/FusedBatchNorm"}
  %arg92.93 = f32[1,1,512,128]{3,2,1,0} parameter(92), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.101 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %arg92.93), metadata={op_name="XLA_Args"}
  %custom-call.102 = (f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %fusion.102, f32[1,1,512,128]{1,0,2,3} %copy.101), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block2/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.98 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.102), index=0
  %arg95.96 = f32[128]{0} parameter(95), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.100 = f32[1,128,100,152]{3,2,1,0} fusion(f32[128]{0} %arg94.95, f32[128]{0} %arg93.94, f32[128]{0} %fusion.101, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.98, f32[128]{0} %arg95.96), kind=kLoop, calls=%fused_computation.100, metadata={op_type="Relu" op_name="tower-pred-0/group1/block2/conv1/Relu"}
  %arg97.98 = f32[3,3,128,128]{3,2,1,0} parameter(97), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.102 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg97.98), metadata={op_name="XLA_Args"}
  %custom-call.103 = (f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.100, f32[3,3,128,128]{1,0,2,3} %copy.102), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block2/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.99 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) %custom-call.103), index=0
  %arg100.101 = f32[128]{0} parameter(100), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.98 = f32[1,128,100,152]{3,2,1,0} fusion(f32[128]{0} %arg99.100, f32[128]{0} %arg98.99, f32[128]{0} %fusion.99, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.99, f32[128]{0} %arg100.101), kind=kLoop, calls=%fused_computation.98, metadata={op_type="Relu" op_name="tower-pred-0/group1/block2/conv2/Relu"}
  %arg102.103 = f32[1,1,128,512]{3,2,1,0} parameter(102), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.103 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %arg102.103), metadata={op_name="XLA_Args"}
  %custom-call.104 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.98, f32[1,1,128,512]{1,0,2,3} %copy.103), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block2/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.100 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.104), index=0
  %arg105.106 = f32[512]{0} parameter(105), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.96 = f32[1,512,100,152]{3,2,1,0} fusion(f32[1,512,100,152]{3,2,1,0} %fusion.102, f32[512]{0} %arg104.105, f32[512]{0} %arg103.104, f32[512]{0} %fusion.97, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.100, f32[512]{0} %arg105.106), kind=kLoop, calls=%fused_computation.96, metadata={op_type="Relu" op_name="tower-pred-0/group1/block2/output"}
  %arg119.120 = f32[512]{0} parameter(119), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg118.119 = f32[512]{0} parameter(118), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg121.122 = f32[512]{0} parameter(121), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.91 = f32[512]{0} fusion(f32[512]{0} %arg121.122), kind=kLoop, calls=%fused_computation.91, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv3/bn/FusedBatchNorm"}
  %arg114.115 = f32[128]{0} parameter(114), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg113.114 = f32[128]{0} parameter(113), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg116.117 = f32[128]{0} parameter(116), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.93 = f32[128]{0} fusion(f32[128]{0} %arg116.117), kind=kLoop, calls=%fused_computation.93, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv2/bn/FusedBatchNorm"}
  %arg109.110 = f32[128]{0} parameter(109), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg108.109 = f32[128]{0} parameter(108), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg111.112 = f32[128]{0} parameter(111), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.95 = f32[128]{0} fusion(f32[128]{0} %arg111.112), kind=kLoop, calls=%fused_computation.95, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group1/block3/conv1/bn/FusedBatchNorm"}
  %arg107.108 = f32[1,1,512,128]{3,2,1,0} parameter(107), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.104 = f32[1,1,512,128]{1,0,2,3} copy(f32[1,1,512,128]{3,2,1,0} %arg107.108), metadata={op_name="XLA_Args"}
  %custom-call.105 = (f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %fusion.96, f32[1,1,512,128]{1,0,2,3} %copy.104), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block3/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.101 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.105), index=0
  %arg110.111 = f32[128]{0} parameter(110), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.94 = f32[1,128,100,152]{3,2,1,0} fusion(f32[128]{0} %arg109.110, f32[128]{0} %arg108.109, f32[128]{0} %fusion.95, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.101, f32[128]{0} %arg110.111), kind=kLoop, calls=%fused_computation.94, metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/conv1/Relu"}
  %arg112.113 = f32[3,3,128,128]{3,2,1,0} parameter(112), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.105 = f32[3,3,128,128]{1,0,2,3} copy(f32[3,3,128,128]{3,2,1,0} %arg112.113), metadata={op_name="XLA_Args"}
  %custom-call.106 = (f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.94, f32[3,3,128,128]{1,0,2,3} %copy.105), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block3/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.102 = f32[1,128,100,152]{3,2,1,0} get-tuple-element((f32[1,128,100,152]{3,2,1,0}, u8[1638912]{0}) %custom-call.106), index=0
  %arg115.116 = f32[128]{0} parameter(115), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.92 = f32[1,128,100,152]{3,2,1,0} fusion(f32[128]{0} %arg114.115, f32[128]{0} %arg113.114, f32[128]{0} %fusion.93, f32[1,128,100,152]{3,2,1,0} %get-tuple-element.102, f32[128]{0} %arg115.116), kind=kLoop, calls=%fused_computation.92, metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/conv2/Relu"}
  %arg117.118 = f32[1,1,128,512]{3,2,1,0} parameter(117), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.106 = f32[1,1,128,512]{1,0,2,3} copy(f32[1,1,128,512]{3,2,1,0} %arg117.118), metadata={op_name="XLA_Args"}
  %custom-call.107 = (f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,128,100,152]{3,2,1,0} %fusion.92, f32[1,1,128,512]{1,0,2,3} %copy.106), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group1/block3/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.103 = f32[1,512,100,152]{3,2,1,0} get-tuple-element((f32[1,512,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.107), index=0
  %arg120.121 = f32[512]{0} parameter(120), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.90 = f32[1,512,100,152]{3,2,1,0} fusion(f32[1,512,100,152]{3,2,1,0} %fusion.96, f32[512]{0} %arg119.120, f32[512]{0} %arg118.119, f32[512]{0} %fusion.91, f32[1,512,100,152]{3,2,1,0} %get-tuple-element.103, f32[512]{0} %arg120.121), kind=kLoop, calls=%fused_computation.90, metadata={op_type="Relu" op_name="tower-pred-0/group1/block3/output"}
  %arg122.123 = f32[1,1,512,1024]{3,2,1,0} parameter(122), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.110 = f32[1,1,512,1024]{1,0,2,3} copy(f32[1,1,512,1024]{3,2,1,0} %arg122.123), metadata={op_name="XLA_Args"}
  %custom-call.111 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %fusion.90, f32[1,1,512,1024]{1,0,2,3} %copy.110), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block0/convshortcut/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.107 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.111), index=0
  %arg127.128 = f32[1024]{0} parameter(127), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg141.142 = f32[1024]{0} parameter(141), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg140.141 = f32[1024]{0} parameter(140), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg143.144 = f32[1024]{0} parameter(143), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.85 = f32[1024]{0} fusion(f32[1024]{0} %arg143.144), kind=kLoop, calls=%fused_computation.85, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv3/bn/FusedBatchNorm"}
  %arg136.137 = f32[256]{0} parameter(136), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg135.136 = f32[256]{0} parameter(135), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg138.139 = f32[256]{0} parameter(138), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.87 = f32[256]{0} fusion(f32[256]{0} %arg138.139), kind=kLoop, calls=%fused_computation.87, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv2/bn/FusedBatchNorm"}
  %arg130.131 = f32[256]{0} parameter(130), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg129.130 = f32[256]{0} parameter(129), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg132.133 = f32[256]{0} parameter(132), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.89 = f32[256]{0} fusion(f32[256]{0} %arg132.133), kind=kLoop, calls=%fused_computation.89, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block0/conv1/bn/FusedBatchNorm"}
  %arg123.124 = f32[1,1,512,256]{3,2,1,0} parameter(123), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.107 = f32[1,1,512,256]{1,0,2,3} copy(f32[1,1,512,256]{3,2,1,0} %arg123.124), metadata={op_name="XLA_Args"}
  %custom-call.109 = (f32[1,256,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %fusion.90, f32[1,1,512,256]{1,0,2,3} %copy.107), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block0/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.105 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[1,256,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.109), index=0
  %arg131.132 = f32[256]{0} parameter(131), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.88 = f32[1,256,101,153]{3,2,1,0} fusion(f32[256]{0} %arg130.131, f32[256]{0} %arg129.130, f32[256]{0} %fusion.89, f32[1,256,100,152]{3,2,1,0} %get-tuple-element.105, f32[256]{0} %arg131.132), kind=kLoop, calls=%fused_computation.88
  %arg134.135 = f32[3,3,256,256]{3,2,1,0} parameter(134), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.108 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg134.135), metadata={op_name="XLA_Args"}
  %custom-call.159 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,101,153]{3,2,1,0} %fusion.88, f32[3,3,256,256]{1,0,2,3} %copy.108), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block0/conv2/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.155 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.159), index=0
  %arg137.138 = f32[256]{0} parameter(137), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.86 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg136.137, f32[256]{0} %arg135.136, f32[256]{0} %fusion.87, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.155, f32[256]{0} %arg137.138), kind=kLoop, calls=%fused_computation.86, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/conv2/Relu"}
  %arg139.140 = f32[1,1,256,1024]{3,2,1,0} parameter(139), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.109 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg139.140), metadata={op_name="XLA_Args"}
  %custom-call.110 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.86, f32[1,1,256,1024]{1,0,2,3} %copy.109), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block0/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.106 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.110), index=0
  %arg142.143 = f32[1024]{0} parameter(142), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.83 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1024]{0} %arg126.127, f32[1024]{0} %arg125.126, f32[1024]{0} %fusion.84, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.107, f32[1024]{0} %arg127.128, f32[1024]{0} %arg141.142, f32[1024]{0} %arg140.141, f32[1024]{0} %fusion.85, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.106, f32[1024]{0} %arg142.143), kind=kLoop, calls=%fused_computation.83, metadata={op_type="Relu" op_name="tower-pred-0/group2/block0/output"}
  %arg156.157 = f32[1024]{0} parameter(156), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg155.156 = f32[1024]{0} parameter(155), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg158.159 = f32[1024]{0} parameter(158), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.78 = f32[1024]{0} fusion(f32[1024]{0} %arg158.159), kind=kLoop, calls=%fused_computation.78, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv3/bn/FusedBatchNorm"}
  %arg151.152 = f32[256]{0} parameter(151), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg150.151 = f32[256]{0} parameter(150), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg153.154 = f32[256]{0} parameter(153), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.80 = f32[256]{0} fusion(f32[256]{0} %arg153.154), kind=kLoop, calls=%fused_computation.80, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv2/bn/FusedBatchNorm"}
  %arg146.147 = f32[256]{0} parameter(146), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg145.146 = f32[256]{0} parameter(145), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg148.149 = f32[256]{0} parameter(148), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.82 = f32[256]{0} fusion(f32[256]{0} %arg148.149), kind=kLoop, calls=%fused_computation.82, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block1/conv1/bn/FusedBatchNorm"}
  %arg144.145 = f32[1,1,1024,256]{3,2,1,0} parameter(144), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.111 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg144.145), metadata={op_name="XLA_Args"}
  %custom-call.112 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.83, f32[1,1,1024,256]{1,0,2,3} %copy.111), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block1/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.108 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.112), index=0
  %arg147.148 = f32[256]{0} parameter(147), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.81 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg146.147, f32[256]{0} %arg145.146, f32[256]{0} %fusion.82, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.108, f32[256]{0} %arg147.148), kind=kLoop, calls=%fused_computation.81, metadata={op_type="Relu" op_name="tower-pred-0/group2/block1/conv1/Relu"}
  %arg149.150 = f32[3,3,256,256]{3,2,1,0} parameter(149), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.112 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg149.150), metadata={op_name="XLA_Args"}
  %custom-call.113 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.81, f32[3,3,256,256]{1,0,2,3} %copy.112), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block1/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.109 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.113), index=0
  %arg152.153 = f32[256]{0} parameter(152), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.79 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg151.152, f32[256]{0} %arg150.151, f32[256]{0} %fusion.80, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.109, f32[256]{0} %arg152.153), kind=kLoop, calls=%fused_computation.79, metadata={op_type="Relu" op_name="tower-pred-0/group2/block1/conv2/Relu"}
  %arg154.155 = f32[1,1,256,1024]{3,2,1,0} parameter(154), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.113 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg154.155), metadata={op_name="XLA_Args"}
  %custom-call.114 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.79, f32[1,1,256,1024]{1,0,2,3} %copy.113), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block1/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.110 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.114), index=0
  %arg157.158 = f32[1024]{0} parameter(157), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.77 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %fusion.83, f32[1024]{0} %arg156.157, f32[1024]{0} %arg155.156, f32[1024]{0} %fusion.78, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.110, f32[1024]{0} %arg157.158), kind=kLoop, calls=%fused_computation.77, metadata={op_type="Relu" op_name="tower-pred-0/group2/block1/output"}
  %arg171.172 = f32[1024]{0} parameter(171), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg170.171 = f32[1024]{0} parameter(170), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg173.174 = f32[1024]{0} parameter(173), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.72 = f32[1024]{0} fusion(f32[1024]{0} %arg173.174), kind=kLoop, calls=%fused_computation.72, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv3/bn/FusedBatchNorm"}
  %arg166.167 = f32[256]{0} parameter(166), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg165.166 = f32[256]{0} parameter(165), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg168.169 = f32[256]{0} parameter(168), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.74 = f32[256]{0} fusion(f32[256]{0} %arg168.169), kind=kLoop, calls=%fused_computation.74, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv2/bn/FusedBatchNorm"}
  %arg161.162 = f32[256]{0} parameter(161), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg160.161 = f32[256]{0} parameter(160), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg163.164 = f32[256]{0} parameter(163), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.76 = f32[256]{0} fusion(f32[256]{0} %arg163.164), kind=kLoop, calls=%fused_computation.76, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block2/conv1/bn/FusedBatchNorm"}
  %arg159.160 = f32[1,1,1024,256]{3,2,1,0} parameter(159), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.114 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg159.160), metadata={op_name="XLA_Args"}
  %custom-call.115 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.77, f32[1,1,1024,256]{1,0,2,3} %copy.114), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block2/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.111 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.115), index=0
  %arg162.163 = f32[256]{0} parameter(162), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.75 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg161.162, f32[256]{0} %arg160.161, f32[256]{0} %fusion.76, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.111, f32[256]{0} %arg162.163), kind=kLoop, calls=%fused_computation.75, metadata={op_type="Relu" op_name="tower-pred-0/group2/block2/conv1/Relu"}
  %arg164.165 = f32[3,3,256,256]{3,2,1,0} parameter(164), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.115 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg164.165), metadata={op_name="XLA_Args"}
  %custom-call.116 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.75, f32[3,3,256,256]{1,0,2,3} %copy.115), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block2/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.112 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.116), index=0
  %arg167.168 = f32[256]{0} parameter(167), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.73 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg166.167, f32[256]{0} %arg165.166, f32[256]{0} %fusion.74, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.112, f32[256]{0} %arg167.168), kind=kLoop, calls=%fused_computation.73, metadata={op_type="Relu" op_name="tower-pred-0/group2/block2/conv2/Relu"}
  %arg169.170 = f32[1,1,256,1024]{3,2,1,0} parameter(169), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.116 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg169.170), metadata={op_name="XLA_Args"}
  %custom-call.117 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.73, f32[1,1,256,1024]{1,0,2,3} %copy.116), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block2/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.113 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.117), index=0
  %arg172.173 = f32[1024]{0} parameter(172), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.71 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %fusion.77, f32[1024]{0} %arg171.172, f32[1024]{0} %arg170.171, f32[1024]{0} %fusion.72, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.113, f32[1024]{0} %arg172.173), kind=kLoop, calls=%fused_computation.71, metadata={op_type="Relu" op_name="tower-pred-0/group2/block2/output"}
  %arg186.187 = f32[1024]{0} parameter(186), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg185.186 = f32[1024]{0} parameter(185), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg188.189 = f32[1024]{0} parameter(188), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.66 = f32[1024]{0} fusion(f32[1024]{0} %arg188.189), kind=kLoop, calls=%fused_computation.66, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv3/bn/FusedBatchNorm"}
  %arg181.182 = f32[256]{0} parameter(181), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg180.181 = f32[256]{0} parameter(180), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg183.184 = f32[256]{0} parameter(183), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.68 = f32[256]{0} fusion(f32[256]{0} %arg183.184), kind=kLoop, calls=%fused_computation.68, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv2/bn/FusedBatchNorm"}
  %arg176.177 = f32[256]{0} parameter(176), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg175.176 = f32[256]{0} parameter(175), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg178.179 = f32[256]{0} parameter(178), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.70 = f32[256]{0} fusion(f32[256]{0} %arg178.179), kind=kLoop, calls=%fused_computation.70, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block3/conv1/bn/FusedBatchNorm"}
  %arg174.175 = f32[1,1,1024,256]{3,2,1,0} parameter(174), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.117 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg174.175), metadata={op_name="XLA_Args"}
  %custom-call.118 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.71, f32[1,1,1024,256]{1,0,2,3} %copy.117), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block3/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.114 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.118), index=0
  %arg177.178 = f32[256]{0} parameter(177), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.69 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg176.177, f32[256]{0} %arg175.176, f32[256]{0} %fusion.70, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.114, f32[256]{0} %arg177.178), kind=kLoop, calls=%fused_computation.69, metadata={op_type="Relu" op_name="tower-pred-0/group2/block3/conv1/Relu"}
  %arg179.180 = f32[3,3,256,256]{3,2,1,0} parameter(179), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.118 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg179.180), metadata={op_name="XLA_Args"}
  %custom-call.119 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.69, f32[3,3,256,256]{1,0,2,3} %copy.118), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block3/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.115 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.119), index=0
  %arg182.183 = f32[256]{0} parameter(182), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.67 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg181.182, f32[256]{0} %arg180.181, f32[256]{0} %fusion.68, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.115, f32[256]{0} %arg182.183), kind=kLoop, calls=%fused_computation.67, metadata={op_type="Relu" op_name="tower-pred-0/group2/block3/conv2/Relu"}
  %arg184.185 = f32[1,1,256,1024]{3,2,1,0} parameter(184), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.119 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg184.185), metadata={op_name="XLA_Args"}
  %custom-call.120 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.67, f32[1,1,256,1024]{1,0,2,3} %copy.119), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block3/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.116 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.120), index=0
  %arg187.188 = f32[1024]{0} parameter(187), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.65 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %fusion.71, f32[1024]{0} %arg186.187, f32[1024]{0} %arg185.186, f32[1024]{0} %fusion.66, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.116, f32[1024]{0} %arg187.188), kind=kLoop, calls=%fused_computation.65, metadata={op_type="Relu" op_name="tower-pred-0/group2/block3/output"}
  %arg201.202 = f32[1024]{0} parameter(201), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg200.201 = f32[1024]{0} parameter(200), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg203.204 = f32[1024]{0} parameter(203), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.60 = f32[1024]{0} fusion(f32[1024]{0} %arg203.204), kind=kLoop, calls=%fused_computation.60, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv3/bn/FusedBatchNorm"}
  %arg196.197 = f32[256]{0} parameter(196), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg195.196 = f32[256]{0} parameter(195), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg198.199 = f32[256]{0} parameter(198), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.62 = f32[256]{0} fusion(f32[256]{0} %arg198.199), kind=kLoop, calls=%fused_computation.62, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv2/bn/FusedBatchNorm"}
  %arg191.192 = f32[256]{0} parameter(191), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg190.191 = f32[256]{0} parameter(190), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg193.194 = f32[256]{0} parameter(193), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.64 = f32[256]{0} fusion(f32[256]{0} %arg193.194), kind=kLoop, calls=%fused_computation.64, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block4/conv1/bn/FusedBatchNorm"}
  %arg189.190 = f32[1,1,1024,256]{3,2,1,0} parameter(189), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.120 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg189.190), metadata={op_name="XLA_Args"}
  %custom-call.121 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.65, f32[1,1,1024,256]{1,0,2,3} %copy.120), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block4/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.117 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.121), index=0
  %arg192.193 = f32[256]{0} parameter(192), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.63 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg191.192, f32[256]{0} %arg190.191, f32[256]{0} %fusion.64, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.117, f32[256]{0} %arg192.193), kind=kLoop, calls=%fused_computation.63, metadata={op_type="Relu" op_name="tower-pred-0/group2/block4/conv1/Relu"}
  %arg194.195 = f32[3,3,256,256]{3,2,1,0} parameter(194), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.121 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg194.195), metadata={op_name="XLA_Args"}
  %custom-call.122 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.63, f32[3,3,256,256]{1,0,2,3} %copy.121), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block4/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.118 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.122), index=0
  %arg197.198 = f32[256]{0} parameter(197), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.61 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg196.197, f32[256]{0} %arg195.196, f32[256]{0} %fusion.62, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.118, f32[256]{0} %arg197.198), kind=kLoop, calls=%fused_computation.61, metadata={op_type="Relu" op_name="tower-pred-0/group2/block4/conv2/Relu"}
  %arg199.200 = f32[1,1,256,1024]{3,2,1,0} parameter(199), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.122 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg199.200), metadata={op_name="XLA_Args"}
  %custom-call.123 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.61, f32[1,1,256,1024]{1,0,2,3} %copy.122), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block4/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.119 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.123), index=0
  %arg202.203 = f32[1024]{0} parameter(202), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.59 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %fusion.65, f32[1024]{0} %arg201.202, f32[1024]{0} %arg200.201, f32[1024]{0} %fusion.60, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.119, f32[1024]{0} %arg202.203), kind=kLoop, calls=%fused_computation.59, metadata={op_type="Relu" op_name="tower-pred-0/group2/block4/output"}
  %arg216.217 = f32[1024]{0} parameter(216), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg215.216 = f32[1024]{0} parameter(215), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg218.219 = f32[1024]{0} parameter(218), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.54 = f32[1024]{0} fusion(f32[1024]{0} %arg218.219), kind=kLoop, calls=%fused_computation.54, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv3/bn/FusedBatchNorm"}
  %arg211.212 = f32[256]{0} parameter(211), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg210.211 = f32[256]{0} parameter(210), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg213.214 = f32[256]{0} parameter(213), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.56 = f32[256]{0} fusion(f32[256]{0} %arg213.214), kind=kLoop, calls=%fused_computation.56, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv2/bn/FusedBatchNorm"}
  %arg206.207 = f32[256]{0} parameter(206), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg205.206 = f32[256]{0} parameter(205), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg208.209 = f32[256]{0} parameter(208), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.58 = f32[256]{0} fusion(f32[256]{0} %arg208.209), kind=kLoop, calls=%fused_computation.58, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group2/block5/conv1/bn/FusedBatchNorm"}
  %arg204.205 = f32[1,1,1024,256]{3,2,1,0} parameter(204), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.123 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg204.205), metadata={op_name="XLA_Args"}
  %custom-call.124 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.59, f32[1,1,1024,256]{1,0,2,3} %copy.123), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block5/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.120 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.124), index=0
  %arg207.208 = f32[256]{0} parameter(207), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.57 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg206.207, f32[256]{0} %arg205.206, f32[256]{0} %fusion.58, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.120, f32[256]{0} %arg207.208), kind=kLoop, calls=%fused_computation.57, metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/conv1/Relu"}
  %arg209.210 = f32[3,3,256,256]{3,2,1,0} parameter(209), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.124 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg209.210), metadata={op_name="XLA_Args"}
  %custom-call.125 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.57, f32[3,3,256,256]{1,0,2,3} %copy.124), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block5/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.121 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.125), index=0
  %arg212.213 = f32[256]{0} parameter(212), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.55 = f32[1,256,50,76]{3,2,1,0} fusion(f32[256]{0} %arg211.212, f32[256]{0} %arg210.211, f32[256]{0} %fusion.56, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.121, f32[256]{0} %arg212.213), kind=kLoop, calls=%fused_computation.55, metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/conv2/Relu"}
  %arg214.215 = f32[1,1,256,1024]{3,2,1,0} parameter(214), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.125 = f32[1,1,256,1024]{1,0,2,3} copy(f32[1,1,256,1024]{3,2,1,0} %arg214.215), metadata={op_name="XLA_Args"}
  %custom-call.126 = (f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.55, f32[1,1,256,1024]{1,0,2,3} %copy.125), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group2/block5/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.122 = f32[1,1024,50,76]{3,2,1,0} get-tuple-element((f32[1,1024,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.126), index=0
  %arg217.218 = f32[1024]{0} parameter(217), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.53 = f32[1,1024,50,76]{3,2,1,0} fusion(f32[1,1024,50,76]{3,2,1,0} %fusion.59, f32[1024]{0} %arg216.217, f32[1024]{0} %arg215.216, f32[1024]{0} %fusion.54, f32[1,1024,50,76]{3,2,1,0} %get-tuple-element.122, f32[1024]{0} %arg217.218), kind=kLoop, calls=%fused_computation.53, metadata={op_type="Relu" op_name="tower-pred-0/group2/block5/output"}
  %arg219.220 = f32[1,1,1024,2048]{3,2,1,0} parameter(219), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.129 = f32[1,1,1024,2048]{1,0,2,3} copy(f32[1,1,1024,2048]{3,2,1,0} %arg219.220), metadata={op_name="XLA_Args"}
  %custom-call.130 = (f32[1,2048,25,38]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.53, f32[1,1,1024,2048]{1,0,2,3} %copy.129), window={size=1x1 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block0/convshortcut/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.126 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[1,2048,25,38]{3,2,1,0}, u8[0]{0}) %custom-call.130), index=0
  %arg224.225 = f32[2048]{0} parameter(224), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg238.239 = f32[2048]{0} parameter(238), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg237.238 = f32[2048]{0} parameter(237), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg240.241 = f32[2048]{0} parameter(240), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.48 = f32[2048]{0} fusion(f32[2048]{0} %arg240.241), kind=kLoop, calls=%fused_computation.48, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv3/bn/FusedBatchNorm"}
  %arg233.234 = f32[512]{0} parameter(233), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg232.233 = f32[512]{0} parameter(232), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg235.236 = f32[512]{0} parameter(235), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.50 = f32[512]{0} fusion(f32[512]{0} %arg235.236), kind=kLoop, calls=%fused_computation.50, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv2/bn/FusedBatchNorm"}
  %arg227.228 = f32[512]{0} parameter(227), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg226.227 = f32[512]{0} parameter(226), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg229.230 = f32[512]{0} parameter(229), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.52 = f32[512]{0} fusion(f32[512]{0} %arg229.230), kind=kLoop, calls=%fused_computation.52, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block0/conv1/bn/FusedBatchNorm"}
  %arg220.221 = f32[1,1,1024,512]{3,2,1,0} parameter(220), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.126 = f32[1,1,1024,512]{1,0,2,3} copy(f32[1,1,1024,512]{3,2,1,0} %arg220.221), metadata={op_name="XLA_Args"}
  %custom-call.128 = (f32[1,512,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.53, f32[1,1,1024,512]{1,0,2,3} %copy.126), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block0/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.124 = f32[1,512,50,76]{3,2,1,0} get-tuple-element((f32[1,512,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.128), index=0
  %arg228.229 = f32[512]{0} parameter(228), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.51 = f32[1,512,51,77]{3,2,1,0} fusion(f32[512]{0} %arg227.228, f32[512]{0} %arg226.227, f32[512]{0} %fusion.52, f32[1,512,50,76]{3,2,1,0} %get-tuple-element.124, f32[512]{0} %arg228.229), kind=kLoop, calls=%fused_computation.51
  %arg231.232 = f32[3,3,512,512]{3,2,1,0} parameter(231), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.127 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg231.232), metadata={op_name="XLA_Args"}
  %custom-call.160 = (f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,512,51,77]{3,2,1,0} %fusion.51, f32[3,3,512,512]{1,0,2,3} %copy.127), window={size=3x3 stride=2x2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block0/conv2/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.156 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.160), index=0
  %arg234.235 = f32[512]{0} parameter(234), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.49 = f32[1,512,25,38]{3,2,1,0} fusion(f32[512]{0} %arg233.234, f32[512]{0} %arg232.233, f32[512]{0} %fusion.50, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.156, f32[512]{0} %arg234.235), kind=kLoop, calls=%fused_computation.49, metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/conv2/Relu"}
  %arg236.237 = f32[1,1,512,2048]{3,2,1,0} parameter(236), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.128 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %arg236.237), metadata={op_name="XLA_Args"}
  %custom-call.129 = (f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.49, f32[1,1,512,2048]{1,0,2,3} %copy.128), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block0/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.125 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.129), index=0
  %arg239.240 = f32[2048]{0} parameter(239), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.46 = f32[1,2048,25,38]{3,2,1,0} fusion(f32[2048]{0} %arg223.224, f32[2048]{0} %arg222.223, f32[2048]{0} %fusion.47, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.126, f32[2048]{0} %arg224.225, f32[2048]{0} %arg238.239, f32[2048]{0} %arg237.238, f32[2048]{0} %fusion.48, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.125, f32[2048]{0} %arg239.240), kind=kLoop, calls=%fused_computation.46, metadata={op_type="Relu" op_name="tower-pred-0/group3/block0/output"}
  %arg253.254 = f32[2048]{0} parameter(253), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg252.253 = f32[2048]{0} parameter(252), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg255.256 = f32[2048]{0} parameter(255), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.41 = f32[2048]{0} fusion(f32[2048]{0} %arg255.256), kind=kLoop, calls=%fused_computation.41, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv3/bn/FusedBatchNorm"}
  %arg248.249 = f32[512]{0} parameter(248), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg247.248 = f32[512]{0} parameter(247), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg250.251 = f32[512]{0} parameter(250), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.43 = f32[512]{0} fusion(f32[512]{0} %arg250.251), kind=kLoop, calls=%fused_computation.43, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv2/bn/FusedBatchNorm"}
  %arg243.244 = f32[512]{0} parameter(243), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg242.243 = f32[512]{0} parameter(242), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg245.246 = f32[512]{0} parameter(245), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.45 = f32[512]{0} fusion(f32[512]{0} %arg245.246), kind=kLoop, calls=%fused_computation.45, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block1/conv1/bn/FusedBatchNorm"}
  %arg241.242 = f32[1,1,2048,512]{3,2,1,0} parameter(241), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.130 = f32[1,1,2048,512]{1,0,2,3} copy(f32[1,1,2048,512]{3,2,1,0} %arg241.242), metadata={op_name="XLA_Args"}
  %custom-call.131 = (f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %fusion.46, f32[1,1,2048,512]{1,0,2,3} %copy.130), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block1/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.127 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.131), index=0
  %arg244.245 = f32[512]{0} parameter(244), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.44 = f32[1,512,25,38]{3,2,1,0} fusion(f32[512]{0} %arg243.244, f32[512]{0} %arg242.243, f32[512]{0} %fusion.45, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.127, f32[512]{0} %arg244.245), kind=kLoop, calls=%fused_computation.44, metadata={op_type="Relu" op_name="tower-pred-0/group3/block1/conv1/Relu"}
  %arg246.247 = f32[3,3,512,512]{3,2,1,0} parameter(246), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.131 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg246.247), metadata={op_name="XLA_Args"}
  %custom-call.132 = (f32[1,512,25,38]{3,2,1,0}, u8[26216448]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.44, f32[3,3,512,512]{1,0,2,3} %copy.131), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block1/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.128 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[26216448]{0}) %custom-call.132), index=0
  %arg249.250 = f32[512]{0} parameter(249), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.42 = f32[1,512,25,38]{3,2,1,0} fusion(f32[512]{0} %arg248.249, f32[512]{0} %arg247.248, f32[512]{0} %fusion.43, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.128, f32[512]{0} %arg249.250), kind=kLoop, calls=%fused_computation.42, metadata={op_type="Relu" op_name="tower-pred-0/group3/block1/conv2/Relu"}
  %arg251.252 = f32[1,1,512,2048]{3,2,1,0} parameter(251), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.132 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %arg251.252), metadata={op_name="XLA_Args"}
  %custom-call.133 = (f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.42, f32[1,1,512,2048]{1,0,2,3} %copy.132), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block1/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.129 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.133), index=0
  %arg254.255 = f32[2048]{0} parameter(254), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.40 = f32[1,2048,25,38]{3,2,1,0} fusion(f32[1,2048,25,38]{3,2,1,0} %fusion.46, f32[2048]{0} %arg253.254, f32[2048]{0} %arg252.253, f32[2048]{0} %fusion.41, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.129, f32[2048]{0} %arg254.255), kind=kLoop, calls=%fused_computation.40, metadata={op_type="Relu" op_name="tower-pred-0/group3/block1/output"}
  %arg268.269 = f32[2048]{0} parameter(268), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg267.268 = f32[2048]{0} parameter(267), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg270.271 = f32[2048]{0} parameter(270), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.35 = f32[2048]{0} fusion(f32[2048]{0} %arg270.271), kind=kLoop, calls=%fused_computation.35, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv3/bn/FusedBatchNorm"}
  %arg263.264 = f32[512]{0} parameter(263), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg262.263 = f32[512]{0} parameter(262), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg265.266 = f32[512]{0} parameter(265), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.37 = f32[512]{0} fusion(f32[512]{0} %arg265.266), kind=kLoop, calls=%fused_computation.37, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv2/bn/FusedBatchNorm"}
  %arg258.259 = f32[512]{0} parameter(258), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg257.258 = f32[512]{0} parameter(257), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %arg260.261 = f32[512]{0} parameter(260), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.39 = f32[512]{0} fusion(f32[512]{0} %arg260.261), kind=kLoop, calls=%fused_computation.39, metadata={op_type="FusedBatchNorm" op_name="tower-pred-0/group3/block2/conv1/bn/FusedBatchNorm"}
  %arg256.257 = f32[1,1,2048,512]{3,2,1,0} parameter(256), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.133 = f32[1,1,2048,512]{1,0,2,3} copy(f32[1,1,2048,512]{3,2,1,0} %arg256.257), metadata={op_name="XLA_Args"}
  %custom-call.134 = (f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %fusion.40, f32[1,1,2048,512]{1,0,2,3} %copy.133), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block2/conv1/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.130 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.134), index=0
  %arg259.260 = f32[512]{0} parameter(259), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.38 = f32[1,512,25,38]{3,2,1,0} fusion(f32[512]{0} %arg258.259, f32[512]{0} %arg257.258, f32[512]{0} %fusion.39, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.130, f32[512]{0} %arg259.260), kind=kLoop, calls=%fused_computation.38, metadata={op_type="Relu" op_name="tower-pred-0/group3/block2/conv1/Relu"}
  %arg261.262 = f32[3,3,512,512]{3,2,1,0} parameter(261), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.134 = f32[3,3,512,512]{1,0,2,3} copy(f32[3,3,512,512]{3,2,1,0} %arg261.262), metadata={op_name="XLA_Args"}
  %custom-call.135 = (f32[1,512,25,38]{3,2,1,0}, u8[26216448]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.38, f32[3,3,512,512]{1,0,2,3} %copy.134), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block2/conv2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.131 = f32[1,512,25,38]{3,2,1,0} get-tuple-element((f32[1,512,25,38]{3,2,1,0}, u8[26216448]{0}) %custom-call.135), index=0
  %arg264.265 = f32[512]{0} parameter(264), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.36 = f32[1,512,25,38]{3,2,1,0} fusion(f32[512]{0} %arg263.264, f32[512]{0} %arg262.263, f32[512]{0} %fusion.37, f32[1,512,25,38]{3,2,1,0} %get-tuple-element.131, f32[512]{0} %arg264.265), kind=kLoop, calls=%fused_computation.36, metadata={op_type="Relu" op_name="tower-pred-0/group3/block2/conv2/Relu"}
  %arg266.267 = f32[1,1,512,2048]{3,2,1,0} parameter(266), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.135 = f32[1,1,512,2048]{1,0,2,3} copy(f32[1,1,512,2048]{3,2,1,0} %arg266.267), metadata={op_name="XLA_Args"}
  %custom-call.136 = (f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) custom-call(f32[1,512,25,38]{3,2,1,0} %fusion.36, f32[1,1,512,2048]{1,0,2,3} %copy.135), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/group3/block2/conv3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.132 = f32[1,2048,25,38]{3,2,1,0} get-tuple-element((f32[1,2048,25,38]{3,2,1,0}, u8[5708]{0}) %custom-call.136), index=0
  %arg269.270 = f32[2048]{0} parameter(269), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.34 = f32[1,2048,25,38]{3,2,1,0} fusion(f32[1,2048,25,38]{3,2,1,0} %fusion.40, f32[2048]{0} %arg268.269, f32[2048]{0} %arg267.268, f32[2048]{0} %fusion.35, f32[1,2048,25,38]{3,2,1,0} %get-tuple-element.132, f32[2048]{0} %arg269.270), kind=kLoop, calls=%fused_computation.34, metadata={op_type="Relu" op_name="tower-pred-0/group3/block2/output"}
  %arg271.272 = f32[1,1,2048,256]{3,2,1,0} parameter(271), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.136 = f32[1,1,2048,256]{1,0,2,3} copy(f32[1,1,2048,256]{3,2,1,0} %arg271.272), metadata={op_name="XLA_Args"}
  %custom-call.137 = (f32[1,256,25,38]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,2048,25,38]{3,2,1,0} %fusion.34, f32[1,1,2048,256]{1,0,2,3} %copy.136), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/fpn/lateral_1x1_c5/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.133 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[1,256,25,38]{3,2,1,0}, u8[0]{0}) %custom-call.137), index=0
  %arg272.273 = f32[256]{0} parameter(272), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.33 = f32[1,256,25,38]{3,2,1,0} fusion(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.133, f32[256]{0} %arg272.273), kind=kLoop, calls=%fused_computation.33, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/lateral_1x1_c5/BiasAdd"}
  %arg273.274 = f32[3,3,256,256]{3,2,1,0} parameter(273), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.137 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg273.274), metadata={op_name="XLA_Args"}
  %custom-call.138 = (f32[1,256,25,38]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %fusion.33, f32[3,3,256,256]{1,0,2,3} %copy.137), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/fpn/posthoc_3x3_p5/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.134 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[1,256,25,38]{3,2,1,0}, u8[6554624]{0}) %custom-call.138), index=0
  %arg274.275 = f32[256]{0} parameter(274), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.32 = f32[1,256,25,38]{3,2,1,0} fusion(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.134, f32[256]{0} %arg274.275), kind=kLoop, calls=%fused_computation.32, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p5/BiasAdd"}
  %arg221.222 = f32[1,1,1024,256]{3,2,1,0} parameter(221), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.138 = f32[1,1,1024,256]{1,0,2,3} copy(f32[1,1,1024,256]{3,2,1,0} %arg221.222), metadata={op_name="XLA_Args"}
  %custom-call.127 = (f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) custom-call(f32[1,1024,50,76]{3,2,1,0} %fusion.53, f32[1,1,1024,256]{1,0,2,3} %copy.138), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/fpn/lateral_1x1_c4/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.123 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[22808]{0}) %custom-call.127), index=0
  %arg230.231 = f32[256]{0} parameter(230), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.31 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,25,38]{3,2,1,0} %fusion.33, f32[1,256,50,76]{3,2,1,0} %get-tuple-element.123, f32[256]{0} %arg230.231), kind=kLoop, calls=%fused_computation.31, metadata={op_type="Add" op_name="tower-pred-0/fpn/add"}
  %arg281.282 = f32[3,3,256,256]{3,2,1,0} parameter(281), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.139 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg281.282), metadata={op_name="XLA_Args"}
  %custom-call.143 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.31, f32[3,3,256,256]{1,0,2,3} %copy.139), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/fpn/posthoc_3x3_p4/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.139 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.143), index=0
  %arg282.283 = f32[256]{0} parameter(282), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.30 = f32[1,256,50,76]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.139, f32[256]{0} %arg282.283), kind=kLoop, calls=%fused_computation.30, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p4/BiasAdd"}
  %arg275.276 = f32[3,3,256,256]{3,2,1,0} parameter(275), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.140 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg275.276), metadata={op_name="XLA_Args"}
  %arg276.277 = f32[256]{0} parameter(276), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %custom-call.153 = (f32[1,256,25,38]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %fusion.32, f32[3,3,256,256]{1,0,2,3} %copy.140, f32[256]{0} %arg276.277), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_3/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.149 = f32[1,256,25,38]{3,2,1,0} get-tuple-element((f32[1,256,25,38]{3,2,1,0}, u8[6554624]{0}) %custom-call.153), index=0
  %arg277.278 = f32[1,1,256,3]{3,2,1,0} parameter(277), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.143 = f32[1,1,256,3]{1,0,2,3} copy(f32[1,1,256,3]{3,2,1,0} %arg277.278), metadata={op_name="XLA_Args"}
  %custom-call.142 = (f32[1,3,25,38]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.149, f32[1,1,256,3]{1,0,2,3} %copy.143), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_3/class/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.138 = f32[1,3,25,38]{3,2,1,0} get-tuple-element((f32[1,3,25,38]{3,2,1,0}, u8[0]{0}) %custom-call.142), index=0
  %arg279.280 = f32[3]{0} parameter(279), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.27 = f32[2850]{0} fusion(f32[1,3,25,38]{3,2,1,0} %get-tuple-element.138, f32[3]{0} %arg279.280), kind=kLoop, calls=%fused_computation.27, metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/Reshape_1"}
  %iota.1187 = s32[2850]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %sort.1210 = (f32[2850]{0}, s32[2850]{0}) sort(f32[2850]{0} %fusion.27, s32[2850]{0} %iota.1187), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.1188, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.1213 = s32[2850]{0} get-tuple-element((f32[2850]{0}, s32[2850]{0}) %sort.1210), index=1, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %arg278.279 = f32[1,1,256,12]{3,2,1,0} parameter(278), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.141 = f32[1,1,256,12]{1,0,2,3} copy(f32[1,1,256,12]{3,2,1,0} %arg278.279), metadata={op_name="XLA_Args"}
  %custom-call.141 = (f32[1,12,25,38]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,25,38]{3,2,1,0} %get-tuple-element.149, f32[1,1,256,12]{1,0,2,3} %copy.141), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_3/box/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.137 = f32[1,12,25,38]{3,2,1,0} get-tuple-element((f32[1,12,25,38]{3,2,1,0}, u8[0]{0}) %custom-call.141), index=0
  %arg280.281 = f32[12]{0} parameter(280), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.28 = f32[2850,1,2]{2,1,0} fusion(f32[1,12,25,38]{3,2,1,0} %get-tuple-element.137, f32[12]{0} %arg280.281), kind=kLoop, calls=%fused_computation.28, metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target_3/Exp"}
  %constant_72 = f32[2850,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target_3/sub"}
  %constant_71 = f32[2850,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_3/add"}
  %constant_99 = f32[2]{0} constant({1199, 800}), metadata={op_type="Cast" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Cast"}
  %custom-call.154 = (f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %fusion.30, f32[3,3,256,256]{1,0,2,3} %copy.140, f32[256]{0} %arg276.277), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_2/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.150 = f32[1,256,50,76]{3,2,1,0} get-tuple-element((f32[1,256,50,76]{3,2,1,0}, u8[6554624]{0}) %custom-call.154), index=0
  %custom-call.145 = (f32[1,3,50,76]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.150, f32[1,1,256,3]{1,0,2,3} %copy.143), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_2/class/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.141 = f32[1,3,50,76]{3,2,1,0} get-tuple-element((f32[1,3,50,76]{3,2,1,0}, u8[0]{0}) %custom-call.145), index=0
  %fusion.16 = f32[11400]{0} fusion(f32[1,3,50,76]{3,2,1,0} %get-tuple-element.141, f32[3]{0} %arg279.280), kind=kLoop, calls=%fused_computation.16, metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/Reshape_1"}
  %iota.1340 = s32[11400]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %sort.1363 = (f32[11400]{0}, s32[11400]{0}) sort(f32[11400]{0} %fusion.16, s32[11400]{0} %iota.1340), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.1341, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.1366 = s32[11400]{0} get-tuple-element((f32[11400]{0}, s32[11400]{0}) %sort.1363), index=1, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %custom-call.144 = (f32[1,12,50,76]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,50,76]{3,2,1,0} %get-tuple-element.150, f32[1,1,256,12]{1,0,2,3} %copy.141), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_2/box/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.140 = f32[1,12,50,76]{3,2,1,0} get-tuple-element((f32[1,12,50,76]{3,2,1,0}, u8[0]{0}) %custom-call.144), index=0
  %fusion.17 = f32[11400,1,2]{2,1,0} fusion(f32[1,12,50,76]{3,2,1,0} %get-tuple-element.140, f32[12]{0} %arg280.281), kind=kLoop, calls=%fused_computation.17, metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target_2/Exp"}
  %constant_86 = f32[11400,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target_2/sub"}
  %constant_85 = f32[11400,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_2/add"}
  %arg124.125 = f32[1,1,512,256]{3,2,1,0} parameter(124), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.148 = f32[1,1,512,256]{1,0,2,3} copy(f32[1,1,512,256]{3,2,1,0} %arg124.125), metadata={op_name="XLA_Args"}
  %custom-call.108 = (f32[1,256,100,152]{3,2,1,0}, u8[91208]{0}) custom-call(f32[1,512,100,152]{3,2,1,0} %fusion.90, f32[1,1,512,256]{1,0,2,3} %copy.148), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/fpn/lateral_1x1_c3/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.104 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[1,256,100,152]{3,2,1,0}, u8[91208]{0}) %custom-call.108), index=0
  %arg133.134 = f32[256]{0} parameter(133), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.20 = f32[1,256,100,152]{3,2,1,0} fusion(f32[1,256,50,76]{3,2,1,0} %fusion.31, f32[1,256,100,152]{3,2,1,0} %get-tuple-element.104, f32[256]{0} %arg133.134), kind=kLoop, calls=%fused_computation.20, metadata={op_type="Add" op_name="tower-pred-0/fpn/add_1"}
  %arg283.284 = f32[3,3,256,256]{3,2,1,0} parameter(283), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.149 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg283.284), metadata={op_name="XLA_Args"}
  %custom-call.146 = (f32[1,256,100,152]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %fusion.20, f32[3,3,256,256]{1,0,2,3} %copy.149), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/fpn/posthoc_3x3_p3/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.142 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[1,256,100,152]{3,2,1,0}, u8[6554624]{0}) %custom-call.146), index=0
  %arg284.285 = f32[256]{0} parameter(284), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.19 = f32[1,256,100,152]{3,2,1,0} fusion(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.142, f32[256]{0} %arg284.285), kind=kLoop, calls=%fused_computation.19, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p3/BiasAdd"}
  %custom-call.155 = (f32[1,256,100,152]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %fusion.19, f32[3,3,256,256]{1,0,2,3} %copy.140, f32[256]{0} %arg276.277), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_1/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.151 = f32[1,256,100,152]{3,2,1,0} get-tuple-element((f32[1,256,100,152]{3,2,1,0}, u8[6554624]{0}) %custom-call.155), index=0
  %custom-call.148 = (f32[1,3,100,152]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.151, f32[1,1,256,3]{1,0,2,3} %copy.143), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_1/class/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.144 = f32[1,3,100,152]{3,2,1,0} get-tuple-element((f32[1,3,100,152]{3,2,1,0}, u8[0]{0}) %custom-call.148), index=0
  %fusion.10 = f32[45600]{0} fusion(f32[1,3,100,152]{3,2,1,0} %get-tuple-element.144, f32[3]{0} %arg279.280), kind=kLoop, calls=%fused_computation.10, metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/Reshape_1"}
  %iota.1493 = s32[45600]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %sort.1516 = (f32[45600]{0}, s32[45600]{0}) sort(f32[45600]{0} %fusion.10, s32[45600]{0} %iota.1493), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.1494, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.1519 = s32[45600]{0} get-tuple-element((f32[45600]{0}, s32[45600]{0}) %sort.1516), index=1, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %custom-call.147 = (f32[1,12,100,152]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,100,152]{3,2,1,0} %get-tuple-element.151, f32[1,1,256,12]{1,0,2,3} %copy.141), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_1/box/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.143 = f32[1,12,100,152]{3,2,1,0} get-tuple-element((f32[1,12,100,152]{3,2,1,0}, u8[0]{0}) %custom-call.147), index=0
  %fusion.11 = f32[45600,1,2]{2,1,0} fusion(f32[1,12,100,152]{3,2,1,0} %get-tuple-element.143, f32[12]{0} %arg280.281), kind=kLoop, calls=%fused_computation.11, metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target_1/Exp"}
  %constant_92 = f32[45600,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target_1/sub"}
  %constant_91 = f32[45600,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_1/add"}
  %arg57.58 = f32[1,1,256,256]{3,2,1,0} parameter(57), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.154 = f32[1,1,256,256]{1,0,2,3} copy(f32[1,1,256,256]{3,2,1,0} %arg57.58), metadata={op_name="XLA_Args"}
  %custom-call.95 = (f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %fusion.115, f32[1,1,256,256]{1,0,2,3} %copy.154), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/fpn/lateral_1x1_c2/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.91 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.95), index=0
  %arg66.67 = f32[256]{0} parameter(66), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.14 = f32[1,256,200,304]{3,2,1,0} fusion(f32[1,256,100,152]{3,2,1,0} %fusion.20, f32[1,256,200,304]{3,2,1,0} %get-tuple-element.91, f32[256]{0} %arg66.67), kind=kLoop, calls=%fused_computation.14, metadata={op_type="Add" op_name="tower-pred-0/fpn/add_2"}
  %arg285.286 = f32[3,3,256,256]{3,2,1,0} parameter(285), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.155 = f32[3,3,256,256]{1,0,2,3} copy(f32[3,3,256,256]{3,2,1,0} %arg285.286), metadata={op_name="XLA_Args"}
  %custom-call.149 = (f32[1,256,200,304]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %fusion.14, f32[3,3,256,256]{1,0,2,3} %copy.155), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/fpn/posthoc_3x3_p2/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"convResultScale\":1}"
  %get-tuple-element.145 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[6554624]{0}) %custom-call.149), index=0
  %arg286.287 = f32[256]{0} parameter(286), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.13 = f32[1,256,200,304]{3,2,1,0} fusion(f32[1,256,200,304]{3,2,1,0} %get-tuple-element.145, f32[256]{0} %arg286.287), kind=kLoop, calls=%fused_computation.13, metadata={op_type="BiasAdd" op_name="tower-pred-0/fpn/posthoc_3x3_p2/BiasAdd"}
  %custom-call.156 = (f32[1,256,200,304]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %fusion.13, f32[3,3,256,256]{1,0,2,3} %copy.140, f32[256]{0} %arg276.277), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.152 = f32[1,256,200,304]{3,2,1,0} get-tuple-element((f32[1,256,200,304]{3,2,1,0}, u8[6554624]{0}) %custom-call.156), index=0
  %custom-call.151 = (f32[1,3,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %get-tuple-element.152, f32[1,1,256,3]{1,0,2,3} %copy.143), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn/class/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"tensorOpsEnabled\":true,\"convResultScale\":1}"
  %get-tuple-element.147 = f32[1,3,200,304]{3,2,1,0} get-tuple-element((f32[1,3,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.151), index=0
  %fusion.6 = f32[182400]{0} fusion(f32[1,3,200,304]{3,2,1,0} %get-tuple-element.147, f32[3]{0} %arg279.280), kind=kLoop, calls=%fused_computation.6, metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/Reshape_1"}
  %iota.1646 = s32[182400]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %sort.1669 = (f32[182400]{0}, s32[182400]{0}) sort(f32[182400]{0} %fusion.6, s32[182400]{0} %iota.1646), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.1647, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.1672 = s32[182400]{0} get-tuple-element((f32[182400]{0}, s32[182400]{0}) %sort.1669), index=1, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %custom-call.150 = (f32[1,12,200,304]{3,2,1,0}, u8[364808]{0}) custom-call(f32[1,256,200,304]{3,2,1,0} %get-tuple-element.152, f32[1,1,256,12]{1,0,2,3} %copy.141), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn/box/Conv2D"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.146 = f32[1,12,200,304]{3,2,1,0} get-tuple-element((f32[1,12,200,304]{3,2,1,0}, u8[364808]{0}) %custom-call.150), index=0
  %fusion.7 = f32[182400,1,2]{2,1,0} fusion(f32[1,12,200,304]{3,2,1,0} %get-tuple-element.146, f32[12]{0} %arg280.281), kind=kLoop, calls=%fused_computation.7, metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target/Exp"}
  %constant_98 = f32[182400,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target/sub"}
  %constant_97 = f32[182400,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target/add"}
  %fusion.25 = (f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}) fusion(s32[2850]{0} %get-tuple-element.1213, f32[2850,1,2]{2,1,0} %fusion.28, f32[2850,1,2]{2,1,0} %constant_72, f32[2850,1,2]{2,1,0} %constant_71, f32[1,12,25,38]{3,2,1,0} %get-tuple-element.137, f32[12]{0} %arg280.281, f32[2]{0} %constant_99, s32[11400]{0} %get-tuple-element.1366, f32[11400,1,2]{2,1,0} %fusion.17, f32[11400,1,2]{2,1,0} %constant_86, f32[1,12,50,76]{3,2,1,0} %get-tuple-element.140, f32[11400,1,2]{2,1,0} %constant_85, s32[45600]{0} %get-tuple-element.1519, f32[45600,1,2]{2,1,0} %fusion.11, f32[45600,1,2]{2,1,0} %constant_92, f32[1,12,100,152]{3,2,1,0} %get-tuple-element.143, f32[45600,1,2]{2,1,0} %constant_91, s32[182400]{0} %get-tuple-element.1672, f32[182400,1,2]{2,1,0} %fusion.7, f32[182400,1,2]{2,1,0} %constant_98, f32[1,12,200,304]{3,2,1,0} %get-tuple-element.146, f32[182400,1,2]{2,1,0} %constant_97), kind=kLoop, calls=%fused_computation.25, metadata={op_type="Minimum" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/clip_boxes/Minimum"}
  %get-tuple-element.157 = f32[1000,4]{1,0} get-tuple-element((f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}) %fusion.25), index=0
  %get-tuple-element.1211 = f32[2850]{0} get-tuple-element((f32[2850]{0}, s32[2850]{0}) %sort.1210), index=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %slice.1212 = f32[1000]{0} slice(f32[2850]{0} %get-tuple-element.1211), slice={[0:1000]}, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl5/generate_rpn_proposals/TopKV2"}
  %constant_637 = f32[] constant(-inf), metadata={op_type="MaxPool" op_name="tower-pred-0/pool0/MaxPool"}
  %reduce-window.1056 = f32[1,256,13,19]{3,2,1,0} reduce-window(f32[1,256,25,38]{3,2,1,0} %fusion.32, f32[] %constant_637), window={size=1x1x1x1 stride=1x1x2x2}, to_apply=%max_F32.1052, metadata={op_type="MaxPool" op_name="tower-pred-0/fpn/maxpool_p6/MaxPool"}
  %custom-call.152 = (f32[1,256,13,19]{3,2,1,0}, u8[6554624]{0}) custom-call(f32[1,256,13,19]{3,2,1,0} %reduce-window.1056, f32[3,3,256,256]{1,0,2,3} %copy.140, f32[256]{0} %arg276.277), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_4/conv0/Conv2D"}, backend_config="{\"algorithm\":\"6\",\"activationMode\":\"2\",\"convResultScale\":1}"
  %get-tuple-element.148 = f32[1,256,13,19]{3,2,1,0} get-tuple-element((f32[1,256,13,19]{3,2,1,0}, u8[6554624]{0}) %custom-call.152), index=0
  %custom-call.140 = (f32[1,3,13,19]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,13,19]{3,2,1,0} %get-tuple-element.148, f32[1,1,256,3]{1,0,2,3} %copy.143), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_4/class/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.136 = f32[1,3,13,19]{3,2,1,0} get-tuple-element((f32[1,3,13,19]{3,2,1,0}, u8[0]{0}) %custom-call.140), index=0
  %fusion.22 = f32[741]{0} fusion(f32[1,3,13,19]{3,2,1,0} %get-tuple-element.136, f32[3]{0} %arg279.280), kind=kLoop, calls=%fused_computation.22, metadata={op_type="Reshape" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/Reshape_1"}
  %iota.1113 = s32[741]{0} iota(), iota_dimension=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/TopKV2"}
  %sort.1136 = (f32[741]{0}, s32[741]{0}) sort(f32[741]{0} %fusion.22, s32[741]{0} %iota.1113), dimensions={0}, is_stable=true, to_apply=%compare-greater-than.1114, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.1139 = s32[741]{0} get-tuple-element((f32[741]{0}, s32[741]{0}) %sort.1136), index=1, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/TopKV2"}
  %custom-call.139 = (f32[1,12,13,19]{3,2,1,0}, u8[0]{0}) custom-call(f32[1,256,13,19]{3,2,1,0} %get-tuple-element.148, f32[1,1,256,12]{1,0,2,3} %copy.141), window={size=1x1}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="tower-pred-0/rpn_4/box/Conv2D"}, backend_config="{\"convResultScale\":1}"
  %get-tuple-element.135 = f32[1,12,13,19]{3,2,1,0} get-tuple-element((f32[1,12,13,19]{3,2,1,0}, u8[0]{0}) %custom-call.139), index=0
  %fusion.23 = f32[741,1,2]{2,1,0} fusion(f32[1,12,13,19]{3,2,1,0} %get-tuple-element.135, f32[12]{0} %arg280.281), kind=kLoop, calls=%fused_computation.23, metadata={op_type="Exp" op_name="tower-pred-0/decode_bbox_target_4/Exp"}
  %constant_80 = f32[741,1,2]{2,1,0} constant({...}), metadata={op_type="Sub" op_name="tower-pred-0/decode_bbox_target_4/sub"}
  %constant_79 = f32[741,1,2]{2,1,0} constant({...}), metadata={op_type="Add" op_name="tower-pred-0/decode_bbox_target_4/add"}
  %fusion.21 = f32[741,4]{1,0} fusion(s32[741]{0} %get-tuple-element.1139, f32[741,1,2]{2,1,0} %fusion.23, f32[741,1,2]{2,1,0} %constant_80, f32[741,1,2]{2,1,0} %constant_79, f32[2]{0} %constant_99, f32[1,12,13,19]{3,2,1,0} %get-tuple-element.135, f32[12]{0} %arg280.281), kind=kLoop, calls=%fused_computation.21, metadata={op_type="Minimum" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/clip_boxes/Minimum"}
  %get-tuple-element.1137 = f32[741]{0} get-tuple-element((f32[741]{0}, s32[741]{0}) %sort.1136), index=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl6/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.158 = f32[1000,4]{1,0} get-tuple-element((f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}) %fusion.25), index=1
  %get-tuple-element.1364 = f32[11400]{0} get-tuple-element((f32[11400]{0}, s32[11400]{0}) %sort.1363), index=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %slice.1365 = f32[1000]{0} slice(f32[11400]{0} %get-tuple-element.1364), slice={[0:1000]}, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl4/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.159 = f32[1000,4]{1,0} get-tuple-element((f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}) %fusion.25), index=2
  %get-tuple-element.1517 = f32[45600]{0} get-tuple-element((f32[45600]{0}, s32[45600]{0}) %sort.1516), index=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %slice.1518 = f32[1000]{0} slice(f32[45600]{0} %get-tuple-element.1517), slice={[0:1000]}, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl3/generate_rpn_proposals/TopKV2"}
  %get-tuple-element.160 = f32[1000,4]{1,0} get-tuple-element((f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}, f32[1000,4]{1,0}) %fusion.25), index=3
  %get-tuple-element.1670 = f32[182400]{0} get-tuple-element((f32[182400]{0}, s32[182400]{0}) %sort.1669), index=0, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  %slice.1671 = f32[1000]{0} slice(f32[182400]{0} %get-tuple-element.1670), slice={[0:1000]}, metadata={op_type="TopKV2" op_name="tower-pred-0/generate_fpn_proposals/Lvl2/generate_rpn_proposals/TopKV2"}
  ROOT %tuple.1698 = (f32[1,256,25,38]{3,2,1,0}, f32[1,256,50,76]{3,2,1,0}, f32[1000,4]{1,0}, f32[1000]{0}, f32[741,4]{1,0}, f32[741]{0}, f32[1,256,100,152]{3,2,1,0}, f32[1000,4]{1,0}, f32[1000]{0}, f32[1,256,200,304]{3,2,1,0}, f32[1000,4]{1,0}, f32[1000]{0}, f32[1000,4]{1,0}, f32[1000]{0}) tuple(f32[1,256,25,38]{3,2,1,0} %fusion.32, f32[1,256,50,76]{3,2,1,0} %fusion.30, f32[1000,4]{1,0} %get-tuple-element.157, f32[1000]{0} %slice.1212, f32[741,4]{1,0} %fusion.21, f32[741]{0} %get-tuple-element.1137, f32[1,256,100,152]{3,2,1,0} %fusion.19, f32[1000,4]{1,0} %get-tuple-element.158, f32[1000]{0} %slice.1365, f32[1,256,200,304]{3,2,1,0} %fusion.13, f32[1000,4]{1,0} %get-tuple-element.159, f32[1000]{0} %slice.1518, f32[1000,4]{1,0} %get-tuple-element.160, f32[1000]{0} %slice.1671), metadata={op_name="XLA_Retvals"}
}

